<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">


<meta name="referrer" content="no-referrer">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32-next.png?v=">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=">


  <link rel="mask-icon" href="/images/favicon-32x32-next.png?v=" color="#222">





  <meta name="keywords" content="nlp,自然语言处理,">





  <link rel="alternate" href="/atom.xml" title="牧师先生" type="application/atom+xml">






<meta name="description" content="视觉问答机器人（VQA) 原理与实现本章概述2.1 视觉问答机器人问题介绍 2.2 基于图像信息和文本信息抽取匹配的VQA实现方案 2.3 基于注意力（attention）的深度学习VQA实现方案 2.4 【实战】使用keras完成CNN+RNN基础VQA模型 2.5 【实战】基于attention 的深度学习VQA模型实现 2.1 视觉问答机器人问题介绍 视觉问答任务的定义是对于一张图片和一个跟">
<meta name="keywords" content="nlp,自然语言处理">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP系列">
<meta property="og:url" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual Q&A(VQA)/Visual Q&A(VQA)/index.html">
<meta property="og:site_name" content="牧师先生">
<meta property="og:description" content="视觉问答机器人（VQA) 原理与实现本章概述2.1 视觉问答机器人问题介绍 2.2 基于图像信息和文本信息抽取匹配的VQA实现方案 2.3 基于注意力（attention）的深度学习VQA实现方案 2.4 【实战】使用keras完成CNN+RNN基础VQA模型 2.5 【实战】基于attention 的深度学习VQA模型实现 2.1 视觉问答机器人问题介绍 视觉问答任务的定义是对于一张图片和一个跟">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/vqa-example.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/dataset-coco.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/dataset-daquar.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/dataset-coco-qa.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/dataset-vqa.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/question-type.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/visual-question-answering-approach.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/baseline.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/baseline-results.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/approach-attention.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/approach-attention-bounding-box.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/san-cnn.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/san-q-rnn.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/san-q-cnn.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/san.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/vqa-attend.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/vqa-model.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/test.jpg">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/model_vqa.png">
<meta property="og:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/train_log.png">
<meta property="og:updated_time" content="2019-10-10T09:47:31.751Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP系列">
<meta name="twitter:description" content="视觉问答机器人（VQA) 原理与实现本章概述2.1 视觉问答机器人问题介绍 2.2 基于图像信息和文本信息抽取匹配的VQA实现方案 2.3 基于注意力（attention）的深度学习VQA实现方案 2.4 【实战】使用keras完成CNN+RNN基础VQA模型 2.5 【实战】基于attention 的深度学习VQA模型实现 2.1 视觉问答机器人问题介绍 视觉问答任务的定义是对于一张图片和一个跟">
<meta name="twitter:image" content="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual%20Q&A(VQA)/Visual%20Q&A(VQA)/img/vqa-example.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9246611541606574",
    enable_page_level_ads: true
  });
</script>


  <link rel="canonical" href="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual Q&A(VQA)/Visual Q&A(VQA)/">





  <title>NLP系列 | 牧师先生</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">牧师先生</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">成长是一场漫长的自我战争</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tools">
          <a href="/tools/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tools"></i> <br>
            
            小工具
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual Q&A(VQA)/Visual Q&A(VQA)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="pastor">
      <meta itemprop="description" content>
      <meta itemprop="image" content="http://irudder.me/resume/img/me.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="牧师先生">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NLP系列</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-21T19:19:18+08:00">
                2019-03-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="视觉问答机器人（VQA-原理与实现"><a href="#视觉问答机器人（VQA-原理与实现" class="headerlink" title="视觉问答机器人（VQA) 原理与实现"></a>视觉问答机器人（VQA) 原理与实现</h1><h2 id="本章概述"><a href="#本章概述" class="headerlink" title="本章概述"></a>本章概述</h2><pre><code>2.1 视觉问答机器人问题介绍
2.2 基于图像信息和文本信息抽取匹配的VQA实现方案
2.3 基于注意力（attention）的深度学习VQA实现方案
2.4 【实战】使用keras完成CNN+RNN基础VQA模型
2.5 【实战】基于attention 的深度学习VQA模型实现
</code></pre><h2 id="2-1-视觉问答机器人问题介绍"><a href="#2-1-视觉问答机器人问题介绍" class="headerlink" title="2.1 视觉问答机器人问题介绍"></a>2.1 视觉问答机器人问题介绍</h2><ul>
<li>视觉问答任务的定义是对于一张图片和一个跟这幅图片相关的问题，机器需要根据图片信息对问题进行回答。</li>
<li>输入：一张图片和一个关于图片信息的问题，常见的问题形式有选择题，判断题</li>
<li>输出：挑选出正确答案<br><img src="./img/vqa-example.jpg" alt></li>
<li>问题: how many players are in the image?</li>
<li>答案: eleven </li>
<li>人可以清楚地指出图片中的运动员，而且不会把观众也计算在内，我们希望AI机器人也能够对图片信息进行理解，根据问题进行筛选，之后返回正确的答案。</li>
</ul>
<h2 id="2-1-视觉问答机器人问题介绍-1"><a href="#2-1-视觉问答机器人问题介绍-1" class="headerlink" title="2.1 视觉问答机器人问题介绍"></a>2.1 视觉问答机器人问题介绍</h2><ul>
<li>视觉问答任务本质上是一个多模态的研究问题。这个任务需要我们结合自然语言处理（NLP）和计算机视觉（CV)的技术来进行回答。</li>
<li>自然语言处理（NLP）<ul>
<li>先理解问题</li>
<li>再产生答案</li>
<li>举一个在NLP领域常见的基于文本的Q&amp;A问题：how many bridges are there in Paris?</li>
<li>一个NLP Q&amp;A 系统需要首先识别出这是一个什么类型的问题，比如这里是一个“how many” 关于计数的问题，所以答案应该是一个数字。之后系统需要提取出哪个物体（object）需要机器去计数，比如这里是 “bridges“。最后需要我们提取出问题中的背景（context），比如这个问题计数的限定范围是在巴黎这个城市。</li>
<li>当一个Q&amp;A系统分析完问题，系统需要根据知识库（knowledge base）去得到答案。</li>
</ul>
</li>
<li>机器视觉（CV)<ul>
<li>VQA区别于传统的text QA在于搜索答案和推理部分都是基于图片的内容。所以系统需要进行目标检测（object detection），再进行分类（classification），之后系统需要对图片中物体之间的关系进行推理。</li>
</ul>
</li>
<li>总结来说，一个好的VQA系统需要具备能够解决传统的NLP及CV的基础任务，所以这是一个交叉学科，多模态的研究问题。</li>
</ul>
<h2 id="2-1-视觉问答机器人问题介绍¶"><a href="#2-1-视觉问答机器人问题介绍¶" class="headerlink" title="2.1 视觉问答机器人问题介绍¶"></a>2.1 视觉问答机器人问题介绍¶</h2><ul>
<li>图片数据集<ul>
<li>Microsoft Common Objects in Context (MSCOCO) 包含了328000张图片，91类物体，2500000个标注数据，这些物体能够被一个4岁小孩轻易地识别出来。<br><img src="./img/dataset-coco.jpg" alt></li>
</ul>
</li>
<li><p>常见的VQA数据集：一个好的数据集需要尽量避免数据采集过程中的偏差（bias），比如说一个数据集中，90%的判断题的答案都是yes，那么一个只输出yes的系统的准确率有90%。</p>
<ul>
<li><p>DAtaset for QUestion Answering on Real-world images (DAQUAR)，第一个重要的VQA数据集，包含了6794个训练样本，5674个测试样本，图片都来自NYU-Depth V2数据集，平均一张图片包含了9个问题答案对（QA pair），这个数据集的缺点是数据太小，不足以训练一个复杂的VQA系统。<br><img src="./img/dataset-daquar.jpg" alt></p>
</li>
<li><p>COCO-QA数据集使用了MSCOCO中123287张图片，其中78736个QA对作为训练，38948个QA对作为测试。这个数据集是通过对MSCOCO中的图片标题（caption）使用NLP工具自动生成出问题和答案对（QA pair），比如一个标题“two chairs in a room”，可以生成一个问题”how many chairs are there？“，所有的答案都是一个单词。虽然这个数据集足够大，但是这种产生QA pair的方法会使得语法错误，或者信息不完整地错误。而且这个数据集只包含了4类问题，且这四类问题的数量不均等，object（69.84%），color（16.59%），counting（7.47%）， location（6.10%）<br><img src="./img/dataset-coco-qa.jpg" alt></p>
</li>
<li>the VQA dataset 相对来说更大一些，出来204721张来自MSCOCO的图片，还包含了50000张抽象的卡通图片。一张图片平均有3个问题，一个问题平均有10个答案，总共有超过760000个问题和10000000个答案。全部问题和答案对都是Amazon Mechanical Turk上让人标注的。同时问题包括开放性问题和多选项问题。对于开放性问题，至少3个人提供了一模一样的答案才能作为正确的答案。对于多选题，他们创建了18个候选答案，其中正确（correct）答案是10个人都认为正确的一个答案，有可能（plausible）答案是由三个人没有看过图片只根据问题提供的三个答案，常见（popular）答案是由10个最常见的回答组成（yes，no，1，2，3，4，white，red，blue，green），随机（random）答案是从其他问题的正确答案中随机挑选出来的一个答案。这个数据集的缺点是有些问题太主观了。另一个缺点是有些问题根本不需要图片信息，比如“how many legs does the dog have?” 或者 “what color are the trees?”<br><img src="./img/dataset-vqa.jpg" alt><br><img src="./img/question-type.png" alt></li>
</ul>
</li>
</ul>
<h2 id="2-2-基于图像信息和文本信息抽取匹配的VQA实现方案"><a href="#2-2-基于图像信息和文本信息抽取匹配的VQA实现方案" class="headerlink" title="2.2 基于图像信息和文本信息抽取匹配的VQA实现方案"></a>2.2 基于图像信息和文本信息抽取匹配的VQA实现方案</h2><ul>
<li>通常，一个VQA系统包含了以下三个步骤：<ol>
<li>抽取问题特征</li>
<li>抽取图片特征</li>
<li>结合图片和问题特征去生成答案<br><img src="./img/visual-question-answering-approach.jpg" alt></li>
</ol>
</li>
</ul>
<h2 id="2-2-基于图像信息和文本信息抽取匹配的VQA实现方案-1"><a href="#2-2-基于图像信息和文本信息抽取匹配的VQA实现方案-1" class="headerlink" title="2.2 基于图像信息和文本信息抽取匹配的VQA实现方案"></a>2.2 基于图像信息和文本信息抽取匹配的VQA实现方案</h2><ul>
<li>抽取问题特征<ul>
<li>我们通常可以用Bag-of-Words (BOW) 或者LSTM去编码一个问题信息</li>
</ul>
</li>
<li>抽取图片信息<ul>
<li>我们通常使用在ImageNet上预训练好的CNN模型</li>
</ul>
</li>
<li>生成答案经常被简化为一个分类问题</li>
<li>各种方法之间比较不一样的是如何把文字特征与图片特征结合。比如我们可以通过把两个特征拼接（concatenation）在一起之后接上一个线性分类器。或者通过Bayesian的方法去预测问题，图片及答案三者之间的特征分布的关系。</li>
</ul>
<h2 id="2-2-基于图像信息和文本信息抽取匹配的VQA实现方案-2"><a href="#2-2-基于图像信息和文本信息抽取匹配的VQA实现方案-2" class="headerlink" title="2.2 基于图像信息和文本信息抽取匹配的VQA实现方案"></a>2.2 基于图像信息和文本信息抽取匹配的VQA实现方案</h2><ul>
<li>基本方法（baselines), Antol et al. (2016) “VQA: Visual Question Answering”，该文章提出通过简单的特征拼接（concatenation）或者element-wise sum/product的方式去融合文本和图片的特征。其中图片特征使用了VGGNet最后一层的1024维特征，文本特征有以下两种方法<ol>
<li>使用BOW的方法去编码一个问题的文本特征，之后再用一个多层的感知器（multi-layer perceptron，MLP）去预测答案。其中MLP包含了两个隐含层，1000个隐含元，使用了tanh 非线性函数，0.5的dropout。</li>
<li>一个LSTM模型，通过softmax 去预测答案<br><img src="./img/baseline.png" alt></li>
</ol>
</li>
<li>这些基本方法的结果很有意思，如果一个模型只使用了文本特征，其正确率为48.09%，如果一个模型只使用了图片特征，其正确率为28.13%，而他们最好的模型是使用了LSTM去编码文本特征的，能达到53.74%的正确率。而且多选题的结果会显著好于开放式问题的效果。所有的模型预测的结果都远不如人类的表现。<br><img src="./img/baseline-results.png" alt>  </li>
</ul>
<h2 id="2-3-基于注意力（attention）的深度学习VQA实现方案"><a href="#2-3-基于注意力（attention）的深度学习VQA实现方案" class="headerlink" title="2.3 基于注意力（attention）的深度学习VQA实现方案"></a>2.3 基于注意力（attention）的深度学习VQA实现方案</h2><ul>
<li>基于注意力的深度学习VQA方法是通过关注图片中相关的部位来获得答案，比如一个问题“what color is the ball?”，则图片中包含了球ball这个object的小区域是比其他区域更具有信息量，比其他区域更相关。相似的，”color“ 和”ball“也比其他单词更加相关。</li>
<li>另一个常见的VQA方案是使用位置注意力（spatial attention）去生成关于区域（region）的位置特征，并训练一个CNN网络。一般有两种方法去获得一张图片关于方位的区域。<ol>
<li>通过将一张图片划分成网格状（grid），并根据问题与图片特征去预测每一个网格的attention weight，将图片的CNN的feature通过加权求和的方式得到attention weighted feature，再通过attention weighted feature发现相对比较重要的区域<br><img src="./img/approach-attention.jpg" alt></li>
<li>通过目标识别的方式生成很多bounding box<br><img src="./img/approach-attention-bounding-box.jpg" alt></li>
</ol>
</li>
<li>根据生成的区域（region），使用问题去找到最相关的区域，并利用这些区域去生成答案。</li>
</ul>
<h2 id="2-3-基于注意力（attention）的深度学习VQA实现方案-1"><a href="#2-3-基于注意力（attention）的深度学习VQA实现方案-1" class="headerlink" title="2.3 基于注意力（attention）的深度学习VQA实现方案"></a>2.3 基于注意力（attention）的深度学习VQA实现方案</h2><ul>
<li>Yang et al. 2016  Stacked Attention Networks for Image Question Answering，提出了一个基于堆叠注意力的VQA系统</li>
<li>图片使用CNN 编码<br>  $$f_I = CNN_{vgg}(I)$$<br><img src="./img/san-cnn.png" alt>    </li>
<li>问题使用LSTM编码<br>  $$h_t= LSTM(q), ~~ h_t=CNN(q)$$<br><img src="./img/san-q-rnn.png" alt><br><img src="./img/san-q-cnn.png" alt></li>
<li>Stacked Attention，多次重复question-image attention<br><img src="./img/san.png" alt></li>
</ul>
<h2 id="2-3-基于注意力（attention）的深度学习VQA实现方案-2"><a href="#2-3-基于注意力（attention）的深度学习VQA实现方案-2" class="headerlink" title="2.3 基于注意力（attention）的深度学习VQA实现方案"></a>2.3 基于注意力（attention）的深度学习VQA实现方案</h2><ul>
<li>Kazemi (2017 et al.) Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering，提出了一个基于注意力的VQA系统<br><img src="./img/vqa-attend.png" alt></li>
<li>图片使用CNN 编码<br>  $$\phi = CNN(I)$$</li>
<li>问题使用LSTM编码<br>  $$s= LSTM(E_q)$$</li>
<li>Stacked Attention<br>  $$\alpha_{c,l} \propto \exp F_c(s, \phi_l) ,~~  \sum_{l=1}^L \alpha_{c,l}=1, ~~ x_c = \sum_l \alpha_{c,l}\phi_l$$</li>
<li>classifier, 其中G=[G_1, G_2, …, G_M]是两层的全连接层<br>  $$P(a_i|I,q) \propto \exp G_i(x,s),~~ x=[x_1, x2,…,x_C]$$<br><img src="./img/vqa-model.png" alt></li>
</ul>
<h2 id="2-4-【实战】使用keras完成CNN-RNN基础VQA模型"><a href="#2-4-【实战】使用keras完成CNN-RNN基础VQA模型" class="headerlink" title="2.4 【实战】使用keras完成CNN+RNN基础VQA模型"></a>2.4 【实战】使用keras完成CNN+RNN基础VQA模型</h2><ul>
<li><p>Keras VQA Demo <a href="https://github.com/iamaaditya/VQA_Demo" target="_blank" rel="noopener">https://github.com/iamaaditya/VQA_Demo</a></p>
<ol>
<li>Keras version 2.0+</li>
<li>Tensorflow 1.2+ </li>
<li>scikit-learn</li>
<li><p>Spacy version 2.0+，用于下载Glove Word embeddings</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m spacy download en_vectors_web_lg</span><br></pre></td></tr></table></figure>
</li>
<li><p>OpenCV，用于resize图片成224x224大小</p>
</li>
<li>VGG 16，预训练好的权重</li>
</ol>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python demo.py -image_file_name test.jpg -question <span class="string">"Is there a man in the picture?"</span></span><br></pre></td></tr></table></figure>
<p><img src="./test.jpg" alt></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line">! git <span class="built_in">clone</span> https://github.com/iamaaditya/VQA_Demo</span><br><span class="line">! <span class="built_in">cd</span> VQA_Demo</span><br></pre></td></tr></table></figure>
<pre><code>Cloning into &apos;VQA_Demo&apos;...
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VQA_MODEL</span><span class="params">()</span>:</span></span><br><span class="line">    image_feature_size          = <span class="number">4096</span></span><br><span class="line">    word_feature_size           = <span class="number">300</span></span><br><span class="line">    number_of_LSTM              = <span class="number">3</span></span><br><span class="line">    number_of_hidden_units_LSTM = <span class="number">512</span></span><br><span class="line">    max_length_questions        = <span class="number">30</span></span><br><span class="line">    number_of_dense_layers      = <span class="number">3</span></span><br><span class="line">    number_of_hidden_units      = <span class="number">1024</span></span><br><span class="line">    activation_function         = <span class="string">'tanh'</span></span><br><span class="line">    dropout_pct                 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Image model</span></span><br><span class="line">    model_image = Sequential()</span><br><span class="line">    model_image.add(Reshape((image_feature_size,), input_shape=(image_feature_size,)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Language Model</span></span><br><span class="line">    model_language = Sequential()</span><br><span class="line">    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=<span class="keyword">True</span>, input_shape=(max_length_questions, word_feature_size)))</span><br><span class="line">    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=<span class="keyword">True</span>))</span><br><span class="line">    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=<span class="keyword">False</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># combined model</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Merge([model_language, model_image], mode=<span class="string">'concat'</span>, concat_axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> xrange(number_of_dense_layers):</span><br><span class="line">        model.add(Dense(number_of_hidden_units, kernel_initializer=<span class="string">'uniform'</span>))</span><br><span class="line">        model.add(Activation(activation_function))</span><br><span class="line">        model.add(Dropout(dropout_pct))</span><br><span class="line"></span><br><span class="line">    model.add(Dense(<span class="number">1000</span>))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p><img src="./img/model_vqa.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入库</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"><span class="keyword">import</span> os, argparse</span><br><span class="line"><span class="keyword">import</span> cv2, spacy, numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> model_from_json</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> plot_model</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_first'</span>)</span><br><span class="line"><span class="comment">#K.set_image_dim_ordering('th')</span></span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入模型的权重</span></span><br><span class="line"><span class="comment"># 需要下载 VGG weights</span></span><br><span class="line">VQA_model_file_name      = <span class="string">'models/VQA/VQA_MODEL.json'</span></span><br><span class="line">VQA_weights_file_name   = <span class="string">'models/VQA/VQA_MODEL_WEIGHTS.hdf5'</span></span><br><span class="line">label_encoder_file_name  = <span class="string">'models/VQA/FULL_labelencoder_trainval.pkl'</span></span><br><span class="line">CNN_weights_file_name   = <span class="string">'models/CNN/vgg16_weights.h5'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译图像模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image_model</span><span class="params">(CNN_weights_file_name)</span>:</span></span><br><span class="line">    <span class="string">''' Takes the CNN weights file, and returns the VGG model update </span></span><br><span class="line"><span class="string">    with the weights. Requires the file VGG.py inside models/CNN '''</span></span><br><span class="line">    <span class="keyword">from</span> models.CNN.VGG <span class="keyword">import</span> VGG_16</span><br><span class="line">    image_model = VGG_16(CNN_weights_file_name)</span><br><span class="line">    image_model.layers.pop()</span><br><span class="line">    image_model.layers.pop()</span><br><span class="line">    <span class="comment"># this is standard VGG 16 without the last two layers</span></span><br><span class="line">    sgd = SGD(lr=<span class="number">0.1</span>, decay=<span class="number">1e-6</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># one may experiment with "adam" optimizer, but the loss function for</span></span><br><span class="line">    <span class="comment"># this kind of task is pretty standard</span></span><br><span class="line">    image_model.compile(optimizer=sgd, loss=<span class="string">'categorical_crossentropy'</span>)</span><br><span class="line">    <span class="keyword">return</span> image_model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得图像特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image_features</span><span class="params">(image_file_name)</span>:</span></span><br><span class="line">    <span class="string">''' Runs the given image_file to VGG 16 model and returns the </span></span><br><span class="line"><span class="string">    weights (filters) as a 1, 4096 dimension vector '''</span></span><br><span class="line">    image_features = np.zeros((<span class="number">1</span>, <span class="number">4096</span>))</span><br><span class="line">    <span class="comment"># Magic_Number = 4096  &gt; Comes from last layer of VGG Model</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since VGG was trained as a image of 224x224, every new image</span></span><br><span class="line">    <span class="comment"># is required to go through the same transformation</span></span><br><span class="line">    im = cv2.resize(cv2.imread(image_file_name), (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    im = im.transpose((<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)) <span class="comment"># convert the image to RGBA</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># this axis dimension is required because VGG was trained on a dimension</span></span><br><span class="line">    <span class="comment"># of 1, 3, 224, 224 (first axis is for the batch size</span></span><br><span class="line">    <span class="comment"># even though we are using only one image, we have to keep the dimensions consistent</span></span><br><span class="line">    im = np.expand_dims(im, axis=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">    image_features[<span class="number">0</span>,:] = image_model.predict(im)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> image_features</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得问题特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_question_features</span><span class="params">(question)</span>:</span></span><br><span class="line">    <span class="string">''' For a given question, a unicode string, returns the time series vector</span></span><br><span class="line"><span class="string">    with each word (token) transformed into a 300 dimension representation</span></span><br><span class="line"><span class="string">    calculated using Glove Vector '''</span></span><br><span class="line">    word_embeddings = spacy.load(<span class="string">'en_vectors_web_lg'</span>)</span><br><span class="line">    tokens = word_embeddings(question)</span><br><span class="line">    question_tensor = np.zeros((<span class="number">1</span>, <span class="number">30</span>, <span class="number">300</span>))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(len(tokens)):</span><br><span class="line">        question_tensor[<span class="number">0</span>,j,:] = tokens[j].vector</span><br><span class="line">    <span class="keyword">return</span> question_tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建VQA系统</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_VQA_model</span><span class="params">(VQA_model_file_name, VQA_weights_file_name)</span>:</span></span><br><span class="line">    <span class="string">''' Given the VQA model and its weights, compiles and returns the model '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># thanks the keras function for loading a model from JSON, this becomes</span></span><br><span class="line">    <span class="comment"># very easy to understand and work. Alternative would be to load model</span></span><br><span class="line">    <span class="comment"># from binary like cPickle but then model would be obfuscated to users</span></span><br><span class="line">    vqa_model = model_from_json(open(VQA_model_file_name).read())</span><br><span class="line">    vqa_model.load_weights(VQA_weights_file_name)</span><br><span class="line">    vqa_model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line">    <span class="keyword">return</span> vqa_model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image_model = get_image_model(CNN_weights_file_name)</span><br><span class="line">plot_model(image_model, to_file=<span class="string">'model_vgg.png'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试一张图片和问题</span></span><br><span class="line">image_file_name = <span class="string">'test.jpg'</span></span><br><span class="line">question = <span class="string">u"What vehicle is in the picture?"</span></span><br><span class="line"><span class="comment"># 获取图片特征</span></span><br><span class="line">image_features = get_image_features(image_file_name)</span><br><span class="line"><span class="comment"># 获取问题特征</span></span><br><span class="line">question_features = get_question_features(question)</span><br><span class="line"></span><br><span class="line">y_output = model_vqa.predict([question_features, image_features])</span><br><span class="line"></span><br><span class="line"><span class="comment"># This task here is represented as a classification into a 1000 top answers</span></span><br><span class="line"><span class="comment"># this means some of the answers were not part of training and thus would </span></span><br><span class="line"><span class="comment"># not show up in the result.</span></span><br><span class="line"><span class="comment"># These 1000 answers are stored in the sklearn Encoder class</span></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>, category=DeprecationWarning)</span><br><span class="line">labelencoder = joblib.load(label_encoder_file_name)</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> reversed(np.argsort(y_output)[<span class="number">0</span>,<span class="number">-5</span>:]):</span><br><span class="line">    print(str(round(y_output[<span class="number">0</span>,label]*<span class="number">100</span>,<span class="number">2</span>)).zfill(<span class="number">5</span>), <span class="string">"% "</span>, labelencoder.inverse_transform(label))</span><br></pre></td></tr></table></figure>
<h2 id="【2-5-实战】基于attention-的深度学习VQA模型实现"><a href="#【2-5-实战】基于attention-的深度学习VQA模型实现" class="headerlink" title="【2.5 实战】基于attention 的深度学习VQA模型实现"></a>【2.5 实战】基于attention 的深度学习VQA模型实现</h2><ul>
<li>pytorch Attention VQA <a href="https://github.com/Cyanogenoid/pytorch-vqa" target="_blank" rel="noopener">https://github.com/Cyanogenoid/pytorch-vqa</a><ul>
<li>python 3.6</li>
<li>torch</li>
<li>torchvision</li>
<li>h5py</li>
<li>tqdm</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line"><span class="comment"># 下载github repo</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Cyanogenoid/pytorch-vqa --recursive</span><br></pre></td></tr></table></figure>
<pre><code>Submodule path &apos;resnet&apos;: checked out &apos;9332392b01317d57e92f81e00933c48f423ff503&apos;


Cloning into &apos;pytorch-vqa&apos;...
Submodule &apos;resnet&apos; (https://github.com/Cyanogenoid/pytorch-resnet) registered for path &apos;resnet&apos;
Cloning into &apos;/Users/jjhu/MT/slides/MT-course/vqa/pytorch-vqa/resnet&apos;...
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line"><span class="comment"># 预处理图片与vocab</span></span><br><span class="line">python preprocess-images.py</span><br><span class="line">python preprocess-vocab.py</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line"><span class="comment"># 开始训练模型</span></span><br><span class="line">python train.py</span><br></pre></td></tr></table></figure>
<p><img src="./img/train_log.png" alt></p>
<h3 id="训练代码"><a href="#训练代码" class="headerlink" title="训练代码"></a>训练代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练的main 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) &gt; <span class="number">1</span>:</span><br><span class="line">        name = <span class="string">' '</span>.join(sys.argv[<span class="number">1</span>:])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">        name = datetime.now().strftime(<span class="string">"%Y-%m-%d_%H:%M:%S"</span>)</span><br><span class="line">    target_name = os.path.join(<span class="string">'logs'</span>, <span class="string">'&#123;&#125;.pth'</span>.format(name))</span><br><span class="line">    print(<span class="string">'will save to &#123;&#125;'</span>.format(target_name))</span><br><span class="line"></span><br><span class="line">    cudnn.benchmark = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载训练数据及validation数据</span></span><br><span class="line">    train_loader = data.get_loader(train=<span class="keyword">True</span>)</span><br><span class="line">    val_loader = data.get_loader(val=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载vqa模型及优化器</span></span><br><span class="line">    net = nn.DataParallel(model.Net(train_loader.dataset.num_tokens)).cuda()</span><br><span class="line">    optimizer = optim.Adam([p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad])</span><br><span class="line"></span><br><span class="line">    tracker = utils.Tracker()</span><br><span class="line">    config_as_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> vars(config).items() <span class="keyword">if</span> <span class="keyword">not</span> k.startswith(<span class="string">'__'</span>)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(config.epochs):</span><br><span class="line">        _ = run(net, train_loader, optimizer, tracker, train=<span class="keyword">True</span>, prefix=<span class="string">'train'</span>, epoch=i)</span><br><span class="line">        r = run(net, val_loader, optimizer, tracker, train=<span class="keyword">False</span>, prefix=<span class="string">'val'</span>, epoch=i)</span><br><span class="line"></span><br><span class="line">        results = &#123;</span><br><span class="line">            <span class="string">'name'</span>: name,</span><br><span class="line">            <span class="string">'tracker'</span>: tracker.to_dict(),</span><br><span class="line">            <span class="string">'config'</span>: config_as_dict,</span><br><span class="line">            <span class="string">'weights'</span>: net.state_dict(),</span><br><span class="line">            <span class="string">'eval'</span>: &#123;</span><br><span class="line">                <span class="string">'answers'</span>: r[<span class="number">0</span>],</span><br><span class="line">                <span class="string">'accuracies'</span>: r[<span class="number">1</span>],</span><br><span class="line">                <span class="string">'idx'</span>: r[<span class="number">2</span>],</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">'vocab'</span>: train_loader.dataset.vocab,</span><br><span class="line">        &#125;</span><br><span class="line">        torch.save(results, target_name)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(net, loader, optimizer, tracker, train=False, prefix=<span class="string">''</span>, epoch=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Run an epoch over the given loader """</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        net.train()</span><br><span class="line">        tracker_class, tracker_params = tracker.MovingMeanMonitor, &#123;<span class="string">'momentum'</span>: <span class="number">0.99</span>&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        net.eval()</span><br><span class="line">        tracker_class, tracker_params = tracker.MeanMonitor, &#123;&#125;</span><br><span class="line">        answ = []</span><br><span class="line">        idxs = []</span><br><span class="line">        accs = []</span><br><span class="line"></span><br><span class="line">    tq = tqdm(loader, desc=<span class="string">'&#123;&#125; E&#123;:03d&#125;'</span>.format(prefix, epoch), ncols=<span class="number">0</span>)</span><br><span class="line">    loss_tracker = tracker.track(<span class="string">'&#123;&#125;_loss'</span>.format(prefix), tracker_class(**tracker_params))</span><br><span class="line">    acc_tracker = tracker.track(<span class="string">'&#123;&#125;_acc'</span>.format(prefix), tracker_class(**tracker_params))</span><br><span class="line"></span><br><span class="line">    log_softmax = nn.LogSoftmax().cuda()</span><br><span class="line">    <span class="keyword">for</span> v, q, a, idx, q_len <span class="keyword">in</span> tq:</span><br><span class="line">        var_params = &#123;</span><br><span class="line">            <span class="string">'volatile'</span>: <span class="keyword">not</span> train,</span><br><span class="line">            <span class="string">'requires_grad'</span>: <span class="keyword">False</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        v = Variable(v.cuda(<span class="keyword">async</span>=<span class="keyword">True</span>), **var_params)</span><br><span class="line">        q = Variable(q.cuda(<span class="keyword">async</span>=<span class="keyword">True</span>), **var_params)</span><br><span class="line">        a = Variable(a.cuda(<span class="keyword">async</span>=<span class="keyword">True</span>), **var_params)</span><br><span class="line">        q_len = Variable(q_len.cuda(<span class="keyword">async</span>=<span class="keyword">True</span>), **var_params)</span><br><span class="line"></span><br><span class="line">        out = net(v, q, q_len)</span><br><span class="line">        nll = -log_softmax(out)</span><br><span class="line">        loss = (nll * a / <span class="number">10</span>).sum(dim=<span class="number">1</span>).mean()</span><br><span class="line">        acc = utils.batch_accuracy(out.data, a.data).cpu()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            <span class="keyword">global</span> total_iterations</span><br><span class="line">            update_learning_rate(optimizer, total_iterations)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            total_iterations += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># store information about evaluation of this minibatch</span></span><br><span class="line">            _, answer = out.data.cpu().max(dim=<span class="number">1</span>)</span><br><span class="line">            answ.append(answer.view(<span class="number">-1</span>))</span><br><span class="line">            accs.append(acc.view(<span class="number">-1</span>))</span><br><span class="line">            idxs.append(idx.view(<span class="number">-1</span>).clone())</span><br><span class="line"></span><br><span class="line">        loss_tracker.append(loss.data[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># acc_tracker.append(acc.mean())</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> acc:</span><br><span class="line">            acc_tracker.append(a.item())</span><br><span class="line">        fmt = <span class="string">'&#123;:.4f&#125;'</span>.format</span><br><span class="line">        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> train:</span><br><span class="line">        answ = list(torch.cat(answ, dim=<span class="number">0</span>))</span><br><span class="line">        accs = list(torch.cat(accs, dim=<span class="number">0</span>))</span><br><span class="line">        idxs = list(torch.cat(idxs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> answ, accs, idxs</span><br></pre></td></tr></table></figure>
<h3 id="attention-VQA-模型代码讲解"><a href="#attention-VQA-模型代码讲解" class="headerlink" title="attention VQA 模型代码讲解"></a>attention VQA 模型代码讲解</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Re-implementation of ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' [0]</span></span><br><span class="line"><span class="string">    [0]: https://arxiv.org/abs/1704.03162</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_tokens)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        question_features = <span class="number">1024</span></span><br><span class="line">        vision_features = config.output_features</span><br><span class="line">        glimpses = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.text = TextProcessor(</span><br><span class="line">            embedding_tokens=embedding_tokens,</span><br><span class="line">            embedding_features=<span class="number">300</span>,</span><br><span class="line">            lstm_features=question_features,</span><br><span class="line">            drop=<span class="number">0.5</span>,</span><br><span class="line">        )</span><br><span class="line">        self.attention = Attention(</span><br><span class="line">            v_features=vision_features,</span><br><span class="line">            q_features=question_features,</span><br><span class="line">            mid_features=<span class="number">512</span>,</span><br><span class="line">            glimpses=<span class="number">2</span>,</span><br><span class="line">            drop=<span class="number">0.5</span>,</span><br><span class="line">        )</span><br><span class="line">        self.classifier = Classifier(</span><br><span class="line">            in_features=glimpses * vision_features + question_features,</span><br><span class="line">            mid_features=<span class="number">1024</span>,</span><br><span class="line">            out_features=config.max_answers,</span><br><span class="line">            drop=<span class="number">0.5</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Linear) <span class="keyword">or</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                init.xavier_uniform(m.weight)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, v, q, q_len)</span>:</span></span><br><span class="line">        q = self.text(q, list(q_len.data))</span><br><span class="line"></span><br><span class="line">        v = v / (v.norm(p=<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>).expand_as(v) + <span class="number">1e-8</span>)</span><br><span class="line">        a = self.attention(v, q)</span><br><span class="line">        v = apply_attention(v, a)</span><br><span class="line"></span><br><span class="line">        combined = torch.cat([v, q], dim=<span class="number">1</span>)</span><br><span class="line">        answer = self.classifier(combined)</span><br><span class="line">        <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure>
<h3 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Classifier</span><span class="params">(nn.Sequential)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, mid_features, out_features, drop=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(Classifier, self).__init__()</span><br><span class="line">        self.add_module(<span class="string">'drop1'</span>, nn.Dropout(drop))</span><br><span class="line">        self.add_module(<span class="string">'lin1'</span>, nn.Linear(in_features, mid_features))</span><br><span class="line">        self.add_module(<span class="string">'relu'</span>, nn.ReLU())</span><br><span class="line">        self.add_module(<span class="string">'drop2'</span>, nn.Dropout(drop))</span><br><span class="line">        self.add_module(<span class="string">'lin2'</span>, nn.Linear(mid_features, out_features))</span><br></pre></td></tr></table></figure>
<h3 id="attention-层"><a href="#attention-层" class="headerlink" title="attention 层"></a>attention 层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, v_features, q_features, mid_features, glimpses, drop=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line">        self.v_conv = nn.Conv2d(v_features, mid_features, <span class="number">1</span>, bias=<span class="keyword">False</span>)  <span class="comment"># let self.lin take care of bias</span></span><br><span class="line">        self.q_lin = nn.Linear(q_features, mid_features)</span><br><span class="line">        self.x_conv = nn.Conv2d(mid_features, glimpses, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, v, q)</span>:</span></span><br><span class="line">        v = self.v_conv(self.drop(v))</span><br><span class="line">        q = self.q_lin(self.drop(q))</span><br><span class="line">        q = tile_2d_over_nd(q, v)</span><br><span class="line">        x = self.relu(v + q)</span><br><span class="line">        x = self.x_conv(self.drop(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_attention</span><span class="params">(input, attention)</span>:</span></span><br><span class="line">    <span class="string">""" Apply any number of attention maps over the input.</span></span><br><span class="line"><span class="string">        The attention map has to have the same size in all dimensions except dim=1.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n, c = input.size()[:<span class="number">2</span>]</span><br><span class="line">    glimpses = attention.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># flatten the spatial dims into the third dim, since we don't need to care about how they are arranged</span></span><br><span class="line">    input = input.view(n, c, <span class="number">-1</span>)</span><br><span class="line">    attention = attention.view(n, glimpses, <span class="number">-1</span>)</span><br><span class="line">    s = input.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply a softmax to each attention map separately</span></span><br><span class="line">    <span class="comment"># since softmax only takes 2d inputs, we have to collapse the first two dimensions together</span></span><br><span class="line">    <span class="comment"># so that each glimpse is normalized separately</span></span><br><span class="line">    attention = attention.view(n * glimpses, <span class="number">-1</span>)</span><br><span class="line">    attention = F.softmax(attention)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply the weighting by creating a new dim to tile both tensors over</span></span><br><span class="line">    target_size = [n, glimpses, c, s]</span><br><span class="line">    input = input.view(n, <span class="number">1</span>, c, s).expand(*target_size)</span><br><span class="line">    attention = attention.view(n, glimpses, <span class="number">1</span>, s).expand(*target_size)</span><br><span class="line">    weighted = input * attention</span><br><span class="line">    <span class="comment"># sum over only the spatial dimension</span></span><br><span class="line">    weighted_mean = weighted.sum(dim=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># the shape at this point is (n, glimpses, c, 1)</span></span><br><span class="line">    <span class="keyword">return</span> weighted_mean.view(n, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><pre><code>2.1 视觉问答机器人问题介绍
2.2 基于图像信息和文本信息抽取匹配的VQA实现方案
2.3 基于注意力（attention）的深度学习VQA实现方案
2.4 【实战】使用keras完成CNN+RNN基础VQA模型
2.5 【实战】基于attention 的深度学习VQA模型实现
</code></pre>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢你对我的支持 让我继续努力分享有用的技术和知识点.</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="pastor 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="pastor 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    pastor
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual Q&A(VQA)/Visual Q&A(VQA)/" title="NLP系列">https://blog.irudder.me/2019-03-21/nlp/10Visual_Text_Task--Image_Caption_and_VQA/Chapter2_Visual Q&A(VQA)/Visual Q&A(VQA)/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    
    
    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/nlp/" rel="tag"># nlp</a>
          
            <a href="/tags/自然语言处理/" rel="tag"># 自然语言处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter2_Text_semantic_matching_based_on_deep_learning/02DSSM-based_question_semantic_similarity_matching/" rel="next" title="NLP系列">
                <i class="fa fa-chevron-left"></i> NLP系列
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019-03-21/nlp/2NLP_Basics2/lesson2/1.English_text_analysis_and_processing-nltk/" rel="prev" title="NLP系列">
                NLP系列 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="http://irudder.me/resume/img/me.jpg" alt="pastor">
            
              <p class="site-author-name" itemprop="name">pastor</p>
              <p class="site-description motion-element" itemprop="description">最富内涵的博文,最有哲理的谈论</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">152</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">51</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/irudder" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons">
              </a>
            </div>
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#视觉问答机器人（VQA-原理与实现"><span class="nav-number">1.</span> <span class="nav-text">视觉问答机器人（VQA) 原理与实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#本章概述"><span class="nav-number">1.1.</span> <span class="nav-text">本章概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-视觉问答机器人问题介绍"><span class="nav-number">1.2.</span> <span class="nav-text">2.1 视觉问答机器人问题介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-视觉问答机器人问题介绍-1"><span class="nav-number">1.3.</span> <span class="nav-text">2.1 视觉问答机器人问题介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-视觉问答机器人问题介绍¶"><span class="nav-number">1.4.</span> <span class="nav-text">2.1 视觉问答机器人问题介绍¶</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-基于图像信息和文本信息抽取匹配的VQA实现方案"><span class="nav-number">1.5.</span> <span class="nav-text">2.2 基于图像信息和文本信息抽取匹配的VQA实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-基于图像信息和文本信息抽取匹配的VQA实现方案-1"><span class="nav-number">1.6.</span> <span class="nav-text">2.2 基于图像信息和文本信息抽取匹配的VQA实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-基于图像信息和文本信息抽取匹配的VQA实现方案-2"><span class="nav-number">1.7.</span> <span class="nav-text">2.2 基于图像信息和文本信息抽取匹配的VQA实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-基于注意力（attention）的深度学习VQA实现方案"><span class="nav-number">1.8.</span> <span class="nav-text">2.3 基于注意力（attention）的深度学习VQA实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-基于注意力（attention）的深度学习VQA实现方案-1"><span class="nav-number">1.9.</span> <span class="nav-text">2.3 基于注意力（attention）的深度学习VQA实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-基于注意力（attention）的深度学习VQA实现方案-2"><span class="nav-number">1.10.</span> <span class="nav-text">2.3 基于注意力（attention）的深度学习VQA实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-【实战】使用keras完成CNN-RNN基础VQA模型"><span class="nav-number">1.11.</span> <span class="nav-text">2.4 【实战】使用keras完成CNN+RNN基础VQA模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【2-5-实战】基于attention-的深度学习VQA模型实现"><span class="nav-number">1.12.</span> <span class="nav-text">【2.5 实战】基于attention 的深度学习VQA模型实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#训练代码"><span class="nav-number">1.12.1.</span> <span class="nav-text">训练代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-VQA-模型代码讲解"><span class="nav-number">1.12.2.</span> <span class="nav-text">attention VQA 模型代码讲解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类器"><span class="nav-number">1.12.3.</span> <span class="nav-text">分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-层"><span class="nav-number">1.12.4.</span> <span class="nav-text">attention 层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#本章小结"><span class="nav-number">1.13.</span> <span class="nav-text">本章小结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; long long ago &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">pastor</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  








  
  





  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_sphere.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v="></script>

  <script type="text/javascript" src="/js/src/motion.js?v="></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v="></script>
<script type="text/javascript" src="/js/src/post-details.js?v="></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v="></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
