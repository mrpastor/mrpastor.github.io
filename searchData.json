[{"title":"Thinking modelBiological thinking Biological thinking: looking at the business world from an evolutionary perspective","url":"/2020-01-16/thinking/Thinking modelBiological thinking Biological thinking looking at the business world from an evolutionary perspective/","content":"\n  I find it useful to think of a free economy market economy—or a partially free market economy—as an ecosystem. Animals can reproduce in the right places. Similarly, as long as people find their own place in society, they can also be very successful.\n\n***---- Charlie Munger***\n\nThe multiple thinking model is a very important tool for analyzing and evaluating business by Charlie Munger , and **[biological thinking]** is a very important part of the Charlie Munger's multiple thinking model.\n\nI reviewed my biological thinking last weekend. There are new thoughts on how to use biological thinking to look at business and business opportunities.\n\n**Ecosystem thinking model**\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-7e9cef72c30f384c.jpg)\n\n**1.1 Model**\n\n*   **Living resources** . The conditions on which an ecosystem depends for establishment and continued prosperity are the sum of the survival factors of a species, including but not limited to food resources, space resources, relationships with other species, and all other living conditions.\n\n*   **Differentiate** . The key element of the existence of an ecosystem is its characteristics of diversity. From the perspective of competition, the most intense competition often occurs between biological populations occupying the same niche, and corresponding to the business perspective, different companies that provide the same or similar products or services also compete for incentives. Therefore, for a company to survive in the ecosystem for a long time, it is to be separated from the niche of other companies. Establish sufficient differentiation from product, technology, marketing, brand, business model, etc.\n\n*   **Natural selection** . This is the core of evolution. Natural selection in the biological world eliminates individuals or species that are not adaptable to the environment, and those individuals or species that are most adaptable to the environment can survive. In the commercial field, in the incentive competition, companies must evolve their core capabilities to adapt to the market environment to ensure that they are not eliminated. At the same time, we also need to be able to iterate continuously and constantly strengthen our core capabilities to make ourselves more suitable for the environment.\n\n*   **Transform the environment** . The natural selection and evolution of ecosystems develop to a certain degree, which will in turn affect ecosystems and even cause major changes to the entire ecosystem. From a business perspective, companies adopt new technologies to interact positively with the ecosystem and form symbiotic relationships. Promote healthy growth of the entire ecosystem.\n\n**1.2 Features**\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-a2966ec0ad334089.jpg)\n\nThe ability of the ecosystem to regulate itself, or its own stability, is a concept that we need to focus on when we look at the business world using ecosystem thinking. He can be divided into two levels of stability:\n\n*   **Resistance stability** . The ability of the ecosystem to resist external disturbances is the stability of resistance, and the stability of resistance is positively related to the ability of ecological self-regulation. The ecosystem with strong stability and stability has strong self-regulation ability, and the ecological balance cannot be easily broken. The characteristic of resistance stability is that the **more complicated, the more stable** .\n\n*   **Resilience stability** . Resilience stability refers to the ability of the ecosystem to return to its original state after it has been destroyed. The relationship between the stability of resilience and the self-regulation ability of the ecosystem is subtle. The stability of the resilience of overly complex ecosystems (such as tropical rain forests) is not high because the complex structure requires a long time to rebuild Ecosystems with low self-regulation capabilities (such as tundra and deserts) have little resilience stability and low resistance stability; only ecosystems with moderate regulation capabilities have higher resilience stability, for example The stability of the grassland's resilience is relatively high.\n\n**1.3 The first impetus for the business ecosystem of science and technology**\n\nOnce an ecosystem, especially a complex one, is formed, because it has strong **resistance and stability** , if there is no significant external influence, the ecosystem will often solidify like this. The ecosystem of the business world has a large number of participants, and the circulation of funds, information, and logistics is very responsible. Therefore, it is itself a super complex ecosystem with **strong resistance and stability** .\n\nBut from the general perception of everyone, the ecosystem of the business world is not so stable. In contrast, the ecosystem of the business world iterates very quickly. 10 years Hedong, 10 years Hexi. Even if it takes less than 10 years, just a few years, the ecosystem of the entire business society will undergo subversive changes. Why is this? There is only one answer. The development and application of new technologies, which are the first driving force of the business ecosystem, are fast and constantly accelerating.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-f2dab024a34ea2e8.jpg)\n\nThe new technology replaces the original product or service by providing better functions, higher efficiency and lower cost. The ecosystem of the entire business world is in a fast and accelerating iterative process. Therefore, in modern business society, whether to be able to master the latest technology and apply it to products or services has become the key to business success.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-905bb3acd05d0705.jpg)\n\n**2\\. [Efficiency / cost ratio] model based on ecological thinking model**\n\nBased on the analysis and discussion in the first part, I found that whether the product or service can be successful in the ecosystem of the business world is dominated by the following two factors:\n\n*   Function (function, efficiency)\n*   cost\n\n**We can use an equation:**\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-f99acc44e9b613de.jpg)\n\nFrom this equation we can see that the success of a product or service depends on whether its product or service's cost-effectiveness ratio is better than its competitive product.\nI further found that the meaning expressed by this power / cost equation can be usedInterpretation of the **second curve principle** .\n\nEverything in the world has its own rules to follow. The \"second curve\" theory of Charles Handy, a well-known European management master, tells us that any growth curve will slip through the apex of the parabola (the limit of growth). The secret of continuous growth is in the Start a new S-curve before a curve disappears. At this time, time, resources, and motivation are enough to make the new curve go through its initial struggling process of exploration.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-5c266dcdec5aa817.jpg)\n\n**The X in the above figure represents** : the efficacy of products and services\n**Y represents** : cost (in order to be more simple and clear, it is expressed as 1 / Y), that is, the higher the cost of products and services, the lower the cost.\n**Dotted upward oblique line** : This curve represents the critical point of the power / cost ratio of the products and services purchased by the user.\n**Here is my interpretation of the second curve model of the [power cost model]:**\n\n*   The S curve on the left in the figure is the current mainstream product or service. Its cost efficiency ratio is already in the upper half of the S curve.\n\n*   The S curve on the right in the figure is a new product or service. Compared with the old product or service, the new product or service has higher efficiency, but the cost is relatively high.\n\n*   With the continuous iteration of new products or services, its efficiency is further improved, and its cost is further reduced. When the S curve crosses the efficiency / cost ratio critical point curve from the bottom up, high-end users have begun to use the product or service.\n\n*   When the new product or service continues to iterate, its cost is further reduced, its efficiency is further improved, and its cost is close to or lower than the old product or service, the new product cargo service will replace the old product or service on a large scale.\n\nTo make it easier for everyone to understand, here is a teared-out case:\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-a16e2958bd8d03f9.jpg)\n\n*   The icon's first S curve (left) represents a traditional packaging carton\n\n*   The second S curve (right) represents a torn product\n\n*   Obviously, after tearing out, its efficacy is better than traditional carton, but its cost is relatively high. When the current cost is still at the level of 3-4 yuan, To B users who are extremely sensitive to cost will not adopt a tear-off solution.\n\n*   As soon as it was torn down through its internal innovation, its cost was reduced to 1 yuan or less from the three dimensions of materialization costs, accessories and manufacturing costs, more and more users began to use its products\n\n*   Obviously, when a tear is lifted further along the S curve, a teared product will replace the traditional carton on a larger scale.\n\n**3\\. Summary: Looking at the business world from an evolutionary perspective**\n\n*   New technologies and their applications are the driving force of the business ecosystem\n*   Through new technologies and their applications, people will continue to upgrade the entire ecosystem, and the boundaries of the ecosystem will continue to expand-the\n    new technology will make the ecological resources we have almost unlimited expansion\n\n**3.2 A few words for entrepreneurs**\n\n*   Focusing on new technologies and applying them properly is a force that can disrupt existing business ecosystems.\n*   The winner will be the one who pushes the efficiency / cost ratio to the extreme\n\n*   Brisk walking\n*   Fast iteration\n*   Chain innovation\n\n**—— End ——**\n","tags":["Biological","Thinking"],"categories":["Biological Thinking"]},{"title":"Modern Darwin Integrated Model Biological Thinking Mode Opening God is Perspective","url":"/2020-01-13/thinking/Modern Darwin Integrated Model  Biological Thinking Mode Opening God Perspective/","content":"\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-80fa6e3dd4aa855e.jpg) \n\nModern Darwinism, also known as modern evolutionary synthesis, combines the work of experimental geneticists, natural scientists, and paleontologists, and combines two important findings: evolutionary selection genes and evolutionary mechanisms. Its basic point include:\n\n**1. ****All evolution can be explained in a way that a genetic mechanism and observed phenomena are unified.**\n\nFor example, we want to explain the phenomenon that the teeth of modern people are degenerating relative to the teeth of primitive people. Because modern people inherit the teeth (genetic mechanism), but we eat cooked and soft foods (observed phenomena), the teeth do not need to be so hard to provide sufficient nutrition for the body.\n\n**2 ****. ****Natural selection determine the direction of evolution; biological adaptation to the environment is the result of long-term natural selection.**\n\nThe accumulation of small favorable mutations over a long period of time can also lead to significant changes in traits; natural selection affects the performance of species in terms of their individual morphology and function in the environment in which they live— **a technical term for biology is called phenotype** . The result of the interaction is different from traits. Phenotypes are non-heritable. For example, the traits of ancestors are not inherited to the next generation.\n\n3.  **The role of genetic drift cannot be ignored. ****Evolution is affected by two factors, \"natural selection\" and \"genetic drift\".**\n\n**The drive of** natural selection to chase the **fittest** tirelessly pushes life towards the most adaptive environment, and the result is adaptive evolution. **However, evolution is not always adaptive, because natural selection is not the only driving force for evolution** . In fact, stochastic factors can also dominate the evolution process, allowing evolution to wander around and even move in an unsuitable direction. This kind of random power that can control evolution but has nothing to do with adaptation is called **genetic drift** .\n\n![Groups are the basic unit of evolution, not individuals](https://upload-images.jianshu.io/upload_images/18390058-b5f9ce243f24369e.jpg)\n\n<figcaption id=\"caption-attachment-2728\" class=\"wp-caption-text\" style=\"box-sizing: border-box; display: block; font-family: Verdana, Geneva, sans-serif; text-align: left; margin: 6px 0px 26px; font-size: 11px; font-style: italic; font-weight: normal; line-height: 17px; color: rgb(68, 68, 68);\">Groups are the basic unit of evolution, not individuals</figcaption>\n\n4.  **Groups are the basic units of evolution, not individuals. **Evolution is due to major changes in the frequency of genes in populations.\n\nFor example, the species pig is very successful in terms of hereditary evolution, because after so many years of human breeding, the total number of pigs has increased many times, and genes have continued and spread.\n\nBut individually, they are in a very bad situation, and most pigs are slaughtered early. Far less comfortable and comfortable than their ancestors.\n\n**Inspiration from biological thinking patterns:** **1\\. Chaos and unpredictability**\n\nIf we pull the time scale from decades, hundreds of years to hundreds of millions, billions of years, we will find that, in fact, the world is unpredictable and chaotic.\n\nFor example, the sun rises from the east every day. Everyone will think, of course, the earth turns from west to east, so it will definitely rise from the east tomorrow. But what if a planet knocks the earth in half? What about collisions? It won't.\n\nThe big logic of biology is that the world is unpredictable. You don't know if a planet will come over. If there is n’t that asteroid, is the dinosaur still dominating, will there really be intelligent life?\n\n**The history of biological evolution shows a biological thinking model and worldview** . Our entire life begins with a single cell. Even the appearance of life was a coincidence. Until the emergence of humans, it was an accident. There are countless opportunities for us to be destroyed, and any unexpected factor may make you unable to grow.\n\n**All coincidences have made us who we are today.**\n\nTherefore, in a **chaotic world, only a firm belief in probability** , frankly facing the failure in the process, constantly throwing dice, looking for opportunities in change, the complexity of living systems, and the discontinuity of the external environment all make accurate prediction It's impossible. The entire nature has developed for billions of years, and the ups and downs of living things, accurate prediction is impossible.\n\n**2 ****, rational**\n\nThe second major revelation that biological thinking models bring to me is rationality.\n\nLao Tzu once said that \"the world is innocent, and all things are rude dogs; the saints are innocent, and the people are rude dogs.\" That is, you feel very bullish. In the entire history of biological evolution, you are just a small flat leaf boat, which is insignificant.\n\nI think very clearly. Nature does not care about individuals or populations. It only makes rules from the perspective of God, or the rule is **evolution—the survival of the fittest** . However, countless populations have become extinct, and nature is still alive. This is the rational side of nature.\n\nGive a chestnut. We all think that pandas are nagging. In fact, the reason why pandas are stingy is that they eat bamboo every day and they swell up on the last two cheeks. Eating bamboo for 10 hours a day, of course, seems very embarrassing. We are all used to emotional judgment. In fact, it is severely degraded.\n\nIf one day the pandas really go extinct, I don't feel sorry. The evolution of all populations has resulted from the extinction of individuals and races. This is a rational process. Therefore, don't let yourself fall into your own emotions.\n\nThere is a word called the perspective of God. An entrepreneur who wants to start a business must first look at things from the perspective of God. The so-called God's perspective is to penetrate oneself deeply and feel the internal changes keenly; to get out of one's own body can turn oneself into a spectator and observe the occurrence and result of many things. If you understand that this is a metabolism, a biological evolution, you will avoid many so-called sad emotions.\n\nWith this mentality, the prosperity and disgrace of nature, the gathering and disengagement of the world, and the change of personnel in the organization are all evolutionary processes.\n\n**3. ****Enjoy the process**\n\nFrom a biological perspective, what is the ultimate fate of the individual? What is the fate of the population? You will encounter a particularly contradictory logic called-individual death brings better power to the new birth. **For example, the fall of Nokia has allowed countless start-ups in Finland to flourish.**\n\n**Because the end of life is death** . There is a saying in \"Three Body\", \"Give civilization to the years, but not to the civilization\", that is, to give up everything.\n\nSo respect the process. What is the nature of the process? Simply put, it is to find your own sense of mission. As a living body, it will eventually disappear, but at least his insisted mission, shared thoughts, and changes to this world can be left behind. In this process, we promote ourselves, raise our awareness of the world, do things different from others, and do things that change the world. Having this process is enough.\n\n**3\\. Anti-fragility-benefit from uncertainty.**\n\nAnother important inspiration that biological thinking models bring to me is anti-fragility.\n\n**What is the core of anti-fragility? It means to continue to benefit in a volatile world.**\n\n**Some things can benefit from shocks. When exposed to volatility, randomness, chaos and stress, risk, and uncertainty, they can thrive and grow.**\n\n*Nietzsche famously said, \"Those who can't kill me will only make me stronger.\"*\n\n**Benefiting from big fluctuations is the ultimate ability of biology** . Genetics and mutation, in fact, are one end stable and one looking for opportunities in fluctuations; one is the death of an individual, the other is the evolution of a population; the other is the harsh environment and the more powerful life.\n\nFor example, the competition between humans and dinosaurs, when it becomes a fish, there are bigger fish to eat it. It had no choice but to run towards the water. After running, the fish could not swim, but there was no air and the water was muddy. Can only wait to climb up to the land, finally managed to survive, the volcano erupted again. Our ancestors had to hide underground, and when they came out, they found that dinosaurs ruled the world. At this point, our ancestors were quite large. But when it grows large, it is actually harmful and can be easily eaten by dinosaurs. Therefore, it needs to be smaller again, and it is easy to hide in the hole. But the overall competitive pressure of dinosaurs on humans has been great. Because it can't beat it, humans have to increase their sensory system and brain judgment system to make themselves run faster.\n\n![This is how the entire biological world survives. ](https://upload-images.jianshu.io/upload_images/18390058-20f3bee721e3f83b.jpg)\n\n<figcaption id=\"caption-attachment-2729\" class=\"wp-caption-text\" style=\"box-sizing: border-box; display: block; font-family: Verdana, Geneva, sans-serif; text-align: left; margin: 6px 0px 26px; font-size: 11px; font-style: italic; font-weight: normal; line-height: 17px; color: rgb(68, 68, 68);\">This is how the entire biological world survives.</figcaption>\n\nIt is precisely because of these extremely harsh environments that our ancestors have received great benefits through anti-fragility. It is also because of this opportunity that it has taken a different path from other creatures-by continuously increasing the ability to sense the world and developing intelligence, it has formed a population like ours today.\n\nThis is how the entire biological world survives.\n\n#### Biological thinking really opens up our perspective of God.\n","tags":["Biological","Thinking"],"categories":["Biological Thinking"]},{"title":"Metacognition Changing the stubborn thinking of the brain","url":"/2020-01-11/thinking/Metacognition Changing the stubborn thinking of the brain/","content":"\n\n![Metacognition](https://upload-images.jianshu.io/upload_images/18390058-d020b00b7238673f.jpg) \n\n\nIn this era, people's minds have reached a new height, but no one understands the process.\n\n-Aristotle\n\nUsually, you find that you hate a colleague, and it is easy to argue with him, but you can't understand why. Your negative emotions affect his work communication. Through introspection and thinking, you realize that this disgust is because he looks like a classmate who bullied you many years ago.\n\nThen you consciously adjusted your view of this colleague: \"He is actually a very good person, and my dislike for him has nothing to do with him.\" From then on he will never cause your anger and disgust again. Your work can proceed smoothly. This introspection and reflection on one's thought process is actually using \"metacognition\".\n\n**I. Metacognition**\n\nThe so-called metacognition refers to a person's cognition and monitoring of his own thinking process and learning activities. It is an individual's self-awareness, self-evaluation, and self-regulation of his own cognitive processing. In layman's terms, it is a reflection on one's thought process.\n\nFor example, students' recognizing, rethinking, and making active adjustments to their cognitive activities such as perception, memory, thinking, and imagination also belong to the scope of metacognition. **Metacognition is a unique ability of human beings. It helps us to get rid of the problem and re-examine the event itself from the perspective of an observer. The problem is often solved.**\n\n**The role of metacognition**\n\nAlthough the Bible says that everyone is equal, people's ability to use metacognition is indeed very different. Because it comes from the hard work of the day after tomorrow, if you want to master the skills, you must have continuous thinking training. Whenever we reflect on our own thinking process and acquired knowledge, we are actually using metacognition, which is the most powerful way to adjust our thinking and improve our thinking.\n\n**1\\. Impact on the feedback loop**\n\nIn recent years, the fusion and development of cognitive science and behavioral science have led to many new discoveries about brain research. Among them, the most important discovery is that the **human brain contains a series of never-ending \"feedback loops** \". **These \"feedback loops\" together Run, forming an engine that drives thought and behavior** .\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-497c49d7c52c1767.jpg)\n\nThe feedback loop is a necessary condition to ensure the operation of any system. The **human brain is the most complex system in the world. The feedback loop directs the operation of the entire body** . **Metacognition is the most powerful internal force we have that can change the feedback loop.**\n\nGoing back to the example at the beginning of the article, you have discovered the potential reason to hate colleagues after reflection. This kind of interpretation will be passed back to the brain feedback loop, and the brain emotion management module will be modified. You will not feel angry the next time you meet your colleagues Already.\n\n**2\\. Discovering \"stubborn thinking\" in the brain**\n\nEveryone has their own easy thinking mistakes. We can easily fall into the wrong automated thinking. If the wrong thinking persists for a long time, it will form a fixed feedback loop in the brain. We may unknowingly fall into negative emotions. Path, subjective speculation, or random labeling, or the pathway of emotional reasoning, because these pathways have formed neural structures in the brain, we are unconsciously led, thinking along these pathways, gradually becoming inertia Thinking mode.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-2227fef2e441bd6c.jpg)\n\nFor a long time, the inertial thinking mode has formed a stubborn closed cycle in your brain, which has always automatically led you to irrational behavior and negative emotions, hindering the achievement of goals and problem solving, and affecting your work and life.\n\nWhy do smart people make mistakes? **Often it is because people tend to accumulate a large number of rigid conclusions and attitudes, and they don't often reflect on reviews and are even more reluctant to make changes.**\n\nIn cognitive psychology, exaggerated or illogical ideas cause cognitive distortions. The process of refuting these ideas is called \"cognitive reconstruction.\" If you have a strong metacognitive ability, you can discover your own cognitive distortions and perform cognitive reconstruction through questioning and self-verification.\n\n**3\\. Promote changes in the nervous system**\n\nCognitive neuroscience believes that the axons of neurons are selectively connected with the axons of other neurons to form neural circuits. Different neural circuits have different effects on people. Metacognition can help us build benign neural circuits. system.\n\n**Methods to Improve Metacognitive Ability**\n\n**1\\. Daily reflection and meditation**\n\nKeeping a diary every day is a great way to reflect. The content of the diary can be the thinking process of making decisions on the day, the main content of learning, the knowledge points and the connections between them, the concepts that are easy to be confused and the errors that are easy to occur, and the progress and shortcomings of their own. Meditate: Free yourself, think about your day's gains and problems, think about how you can improve your learning and thinking processes, and rebuild your cognition.\n\n**2\\. Ask yourself questions often**\n\nThe self-questioning method is a very effective way in metacognitive training. Through a series of self-observation, self-monitoring, and self-evaluation question lists, such continuous questions, self-feedback, self-evaluation, and self-organization can help Optimize the thinking process to achieve the intended learning goals. Metacognitive monitoring capabilities have also been improved and developed.　\n\nFor example: When [reading a book](https://www.madewill.com/reading-thinking/%e6%8b%be%e9%98%b6%e8%80%8c%e4%b8%8a%ef%bc%9a%e9%ab%98%e9%98%b6%e5%ad%a6%e4%b9%a0%e8%80%85%e4%b8%8d%e6%96%ad%e8%bf%9b%e5%8c%96%e7%9a%84%e9%98%85%e8%af%bb%e8%83%bd%e5%8a%9b.html) , I can always ask questions. Why do I read this book? Do I really understand the author's point of view? What do I disagree with? What new concepts are there? What can I do? Monitor your learning at any time.　\n\n**3\\. Master more rational knowledge**\n\nMunger once said: \"If you want to be a rational thinker, you must cultivate a mind that crosses the boundaries of the discipline.\" The [grid](https://www.madewill.com/business-model/munger.html) thinking of different discipline combinations is a promoter of reason. Psychology, economics, etc. will greatly enhance our self-reflection and help us overcome the \"rational obstacles\" in our lives.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-4d91cce059ea3147.jpg)\n\nExercise the brain like muscles. If dumbbells and barbells are our machinery for muscle training, then high-quality books and articles are our tools for thinking training. And \"brain-building\" is the same as fitness. It is a visible and real change. The area of ​​your cerebral cortex will become larger and larger, and the sulci of your brain will increase.\n","tags":["Cognitive","Neuroscience"],"categories":["Cognitive Neuroscience"]},{"title":"Maze and Garden Simon ’s Secret Weapon for Interdisciplinary Learning","url":"/2020-01-09/thinking/Maze and Garden Simon ’s Secret Weapon for Interdisciplinary Learning/","content":"\n![image](https://upload-images.jianshu.io/upload_images/18390058-bed4c1ce8976de92.jpg) \n\nIn the process of continuous interdisciplinary learning, it is actually extremely easy to get lost. However, how did Simon, who has learned interdisciplinary and achieved outstanding achievements throughout his life, avoided getting lost? What secret weapon does he have?\n\nNobel laureate **Herbert Simon** (Herbert A. Simon) life two favorite metaphors, mazes and gardens, especially when writing his autobiography \"All the mode of my life.\"\n\nHe used the maze metaphor to describe scientific exploration, and wrote \"Hugo: The Story of the Apple,\" a simple understanding of the beauty of a scientific [model](https://www.madewill.com/category/thinking-model) ; he used the garden metaphor to describe life experience, and presented his life path crosswise, showing the surprises and surprises. The charm of the maze and garden to Simon is that they have no purpose.\n\n*   ![image](https://upload-images.jianshu.io/upload_images/18390058-9d166a8019a498ec.jpg)\n\n*   ![image](https://upload-images.jianshu.io/upload_images/18390058-4e654e1310655d8a.jpg)\n\n*   ![image](https://upload-images.jianshu.io/upload_images/18390058-88578a35e07352ce.jpg)\n\n*   ![image](https://upload-images.jianshu.io/upload_images/18390058-c1b2f6be27ad3f0f.jpg)\n\n*   ![image](https://upload-images.jianshu.io/upload_images/18390058-5d03bc08d513933b.jpg)\n\nThe charm of the maze lies in exploration, and the fun of the garden lies in excursion. If there is really an \"Ariadne line\", then the joy of thinking is lost.\n\n*Note: The line of Ariadne is derived from ancient Greek mythology. It is often used as a metaphor for the method and path to get out of the maze, and for clues to solve complex problems.*\n\n**But all the wise men's life is just to find that thin line, isn't it?**\n\nTherefore, Simon **used the \"principle** \" in the last chapter of **\"The** Patterns of My Life **\" to refer to \"Ariadne's Line\"** . It has helped Simon make a choice in the bifurcation of life and maintain the correct choice in the maze. It can also Provide an appropriate reason for your choice and explain it.\n\n**In short, this line can be used as a correct guide or as an excuse for error, so users often need to distinguish the wisdom of the former and acknowledge the wisdom of the latter.**\n\nSimon has a famous saying: One of the first rules of science is if somebody delivers a secret weapon to you, you better use it. In his autobiography, he also taught his students to use it this way: I advise my graduate students to have a secret weapon when choosing important topics. why? Because if this issue is really important, others will choose, then you have to master some knowledge or research methods that others don't have to get first.\n\nWhat you probably want to know most about this autobiography is the secret weapon of Simon's life research. where is it? I think it's in the last chapter, A Guide to Selection.\n\nAll human knowledge can be simply divided into two types:\n\n**The first is the worldview** -how do you view the world; the\n**second is the methodology** -how do you recognize, adapt and change the world.\n\nSimon's worldview can be reduced to two:\n\n**1) I ca n’t fully understand this world**\n**2) I still want to know this world**\n\nHow to explain it? In Simon's words: **As a limited rational being, I cannot understand the world completely and objectively. But I cannot ignore this world. I must do my best to understand it with the help of my scientific and philosophical partners and coordinate the relationship between the individual and the world. **At the same time, the world can still provide me with the deepest source of happiness. Whether it's outdoor at night, in the forest, or through a microscope, as long as I stare at it, I will find incredible changes, style and beauty, which are far beyond human artists.\n\nCorrespondingly, **Simon's methodology is:**\n\n**1) Limited reason, rather than pursuit of satisfaction,**\n**2) Use science to discover the hidden beauty of the world** ;\n\nSince the world is vast and human rationality is limited, \"I must avoid perfect goals ... I am an adaptive system. No matter what my goals are, my survival and success depend on being reasonably true to the environment around people and things Since my world image knowledge is approximately close to the real, I do n’t pursue perfection in everything, and at most I am satisfied with it.\n\n**The pursuit of the best will only waste valuable cognitive resources, and \"the best\" is the enemy of \"good\". \"**\n\nAt the same time, the beauty of the world does not always appear externally, and hidden beauty requires the use of science to decode it. The hidden beauty of the world is the intrinsic motivation of Simon's always engaged in science. He studied the works of past and present scientists, created new models through data, calculations, and laws, only for this most exciting experience.\n\n**The worldview and methodology are intertwined and weaved into a thread, strung the three brilliant beads of Simon's life: reason, society and science.**\n\nSimon's rational interest began with the decision-making behavior of people, and then extended to the decision-making behavior of the organization, always trying to find the optimal allocation of resources under uncertainty.\n\nThis process of searching is long and tortuous, and it keeps wandering through the various mazes: human decision-making behavior> budget decision-making> politics and economics> decision thinking> psychology> bounded rationality> computer science> artificial intelligence> human decision-making .\n\nA few years later, when Simon reviewed the maze layout again, he was surprised, but also reasonable. Is this a coincidence, an interest, or a must for science? How to look at this coincidence? Simon explained:\n\n**\"The need to solve problems and my passion for new tools have drawn me to a similar maze, allowing me to use my scientific life to pursue a core problem that I believe is an understanding of the state of man, and I can immerse myself in mathematics And the computer form system, for the latter, just the study itself brings me great happiness. \"**\n\nIs this the right path? Simon's meaning is that you might as well put the result in a long history, and you know that **the success of the winner is not based on superb debates, not on the ability to confuse everyone, or on the influence of politics, but on gradually accumulated data And factual support.**\n\nSun, moon and sky, rivers and rivers, finally see the true chapter.\n\nSimon's **social outlook is cooperation and responsibility** .\n\nAccording to the theory of satisfaction, living in society is, of course, looking forward to sharing progress with society.\n\nIf you want society to help, you must take responsibility for making it better-including ensuring sustainable use of resources, inheriting interesting and useful cultures, protecting values ​​and feelings that are worth protecting, and creating technologies that increase productivity .\n\nAlthough each generation has limited reason, civilization is, after all, accumulated from generation to generation. Discovering the hidden beauty of the world requires science. Science is divided into disciplines due to limited rationality. This allows humans to simplify their goals and narrow the choice space to a calculable range, which includes the physical range and the knowledge range.\n\n**Therefore, this world always needs a class of people. He transcends national boundaries and interdisciplinary learning, brings new knowledge from one place to another, and is most likely to have surprising discoveries at the intersection of the maze layout.**\n\nObviously, now that the Internet is so developed, you no longer have to travel around to gain new knowledge. What you need to do is to master more languages ​​that can communicate with the world, whether it is English or programming, but it is important to interdisciplinary research and stand on the maze Meeting point.\n\nIf you also ask Simon, which meeting point of the maze territory represents the future? Simon might answer: computer science and cognitive science, because their intersection is artificial intelligence.\n","tags":["Cognitive","Neuroscience"],"categories":["Cognitive Neuroscience"]},{"title":"不要因为焦虑而胡乱学习","url":"/2020-01-07/斜杠的忧虑/","content":"\n\n## **01**\n\n## **只有面子，**\n\n## **没有里子的斜杠青年**\n\n上周，和前同事木兰一起吃饭，她十分八卦地透露了一条信息：老陈又回来上班了。我一脸懵逼：“哪个老陈？”她一脸嫌弃地看着我：\n\n> 还有哪个老陈，就是那个说自己绘画天赋了得，上网学了几节插画就创业的那个老陈啊。结果创业失败了，就灰溜溜回来上班了，从头开始还是要按新人标准计算奖金。\n\n我想起来了，是有这么个人，有这么回事。之前做护士的时候，他有点闲暇就开始线上学习，后来慢慢有了起色，开始接简单的海报，每个月能多赚三千。于是，感觉人生从此起飞，后来一鼓作气辞职创业去了。没想到，他严重缺乏策划能力和美感体验，而且经验严重不足，最后的结果是：有上顿，没下顿，拿半路出家的技术谋生根本不成事儿。恕我直言，斜杠青年哪有那么好做，你看到的都是面子和标签，剩下的大多是名不副实、不上台面的技能。你凭什么拿速成的技能，去挑战别人吃饭的本事？表面的显性知识谁都容易学会，内里的隐性知识才最难掌握。\n\n## **02**\n\n## **职场的怪相：**\n\n## **上班摸鱼，回家学习，越学越迷茫**\n\n不知道你有没有发现一个现象：大多数人上班迷茫，下班后更迷茫。相当多一部分人不喜欢现在的工作，看到别人能写文章、拍抖音、做奶茶，于是按捺不住挣钱的欲望，也想做个斜杠青年。于是，他们开启了上班摸鱼、下班学习的状态，利用一切空闲时间进行碎片化的学习。比如，听了爆文大牛的写作课，把十几条写作心法铭记在心，但是依然写不好文章。这种现象实在是太常见了。但是过了一段时间，突然发现，效果真的出来了：自己更加迷茫了。别说无心上班了，甚至连碎片化学习都看着心烦了。如今，讲成功方法的书、号称“40分钟学会”的技能线上课、“15分钟就能看完”的经典名著俯拾即是。很多人都在争先恐后地学习新知识，但是完全派不上用场，工作上依然敷衍了事。就拿职场管理来说，有很多方面的内容要学习：\n\n*   傅盛讲的是小企业逆袭；\n*   彼得德鲁克用管理的哲学；\n*   吴伯凡老师讲的是中西方文化；\n*   马云老师灌的是毒鸡汤；\n*   刘强东主张兄弟们一起996；\n*   吴晓波老师讲的是财经管理。\n\n结果一股脑吞下去之后发现：不仅弄混了，还记不住，头脑还是一团浆糊。![每月多赚3000的副业，为什么毁掉了“斜杠青年”？](https://upload-images.jianshu.io/upload_images/18390058-4edc3a328d5c60d3.jpeg)\n\n有没有那么一瞬间，你觉得自己看了很多知识依旧一无所长？说实话，我有。明明觉得自己学了很多，成就感爆棚，但是冷静下来就会发现，自己还是像霜打的茄子，在一堆干货面前，我们学也焦虑，不学也焦虑。古典老师说过：\n\n> 一个人如果没有搜索能力，他不会知道背景；没有思考能力，不会知道为什么——这样单纯知道一句话，比不知道更糟糕。\n\n问题出在哪里呢？就出在干货太多了。吃不消，撑不下，都成了不能实践的一页空谈。生活的重击面前，干货也拯救不了我们。\n\n## **03**\n\n## **干货依赖症，**\n\n## **你是否也中招了？**\n\n前两天我和Andy讨论了一个话题：如何高效又快速地贴双眼皮贴？Andy说：“我知道啊。”然后她翻了半天微信收藏，甩给我一条教程链接，但是真的让她实际操作，她却完全不知道第一步是什么。这种感觉就好像，哈利波特说自己懂得魔法，但是真到要和伏地魔对峙的时候，却茫无头绪地狂翻教科书寻找魔咒。这样不但会错过可能的胜利机会，说不定还会小命不保。你见过、听过的干货太多了，但是都没有转化成自己的知识，只有量的积累，没有质的改变。网上有这么一个段子： \n\n> 如果你每天还在看耶鲁公开课，上3W咖啡听创业讲座，知乎果壳关注无数，36氪每日必读，对马云的创业史了如指掌，对张小龙的贪嗔痴如数家珍，喜欢罗振宇胜过乔布斯，逢人便谈互联网思维……那你应该还在每天挤地铁。\n\n这是干货依赖症：你听了，但是没听进去。这是病，得治。这就是为什么我们听了很多道理，却依然过不好这一生的原因。干货所代表的显性知识，只是一个骨架。而背后的隐形知识，才是血肉，没有血肉就不能盘活知识，就没有竞争力，你能很快地学到的干货，别人也能。你能很快地记住，也能很快地忘记。日本知识管理大师野中郁次郎说过：\n\n> 我们行动中蕴含的知识，属于不易用语言表达的隐性知识。而书本中的知识属于可用语言清晰表达的显性知识。知识创造的过程，其实是隐性知识和显性知识相互转化的过程。从显性知识到隐性知识的转化，叫做“知识的内化”。\n\n所谓核心竞争力，就是知识的内化能力。人跟人之间的差距，中间都藏着你对知识的转译、演变、深化。\n\n## **04**\n\n## **如何在工作中**\n\n## **修炼隐性知识？**\n\n就像开头提到的，创业失败又回来的老陈，学了画图软件，画了几张海报。但是，他并不了解设计和美感，所以失败也是意料之中。晚上想了千条路，一觉睡醒走老路。日渐焦虑的年轻人，真的无药可救了么？当然不是，我给你提3点建议：\n\n### **1、专注当下，把本职工作做到极致 **\n\n之前听《刘润5分钟商学院》，刘润老师讲了自己在微软时候的疑问：如何在2年内学会别人5年内掌握的知识？后来他发现只有一个办法：加班。一点点做好本职工作，才是最高效的学习方法。本职工作是你的饭碗，也是你精进的最好路径。\n\n### **2、形成自己的“经验学习圈”（Learning Cycle）**\n\n从身边人、前辈或者书中积累经验，然后对经验进行积极思考，把碎片化的东西进行梳理整合。再从思考中找到规律，最后把规律主动运用于实践，在实践中又可以得到更多经验，这样便形成正向循环。《5分钟商学院》的刘润老师曾经讲过一个费曼技巧：\n\n> 用你的语言，把你的模型，讲给别人听。你很可能会发现，过程中自己不明白或听众不明白的地方，就是自己理解的薄弱点。\n\n### **3、不要学见识，要掌握知识 **\n\n碎片化的信息很难成为真正的知识，它只能让人增长见识。见识与知识最大的区别在于，前者是满足了你的好奇心，后者是满足了你的真正需求。创造小米手机的雷布斯说过：\n\n> 任何人成功在任何的领域都需要一万个小时的苦练，谈飞猪上天真的是机会主义者。今天在空中飞的那些猪，他们都不止练了一万个小时，可能练了十万个小时以上。\n\n没有时间的积累，见识很难内化为真正的知识。这篇文章有点干，但是如果你读完就读完了，那么它也是一篇害人的“干货”，有价值但是无用。如果你从中思考到自己需要的东西，它就是值得的。\n","tags":["斜杠青年"],"categories":["认知升级"]},{"title":"Thinking modelBiological thinking Lollapalooza Effect-A collection of growth thinking about learning and life","url":"/2020-01-06/thinking/Lollapalooza Effect-A collection of growth thinking about learning and life/","content":"\n\n\"Life is a marathon, not a sprint.\"\n\n——Growth thinking\n\nEveryone is born with a strong desire to learn. Babies are expanding their skills every day. Not just ordinary skills, but the most difficult tasks in life, such as learning to walk and talk. They never think it is too difficult or worth the effort. Babies are not worried about making mistakes or being laughed at. They walk, they fall, they stand up, and they continue to move forward ...\n\nLife begins in this dynamic learning journey, which is the most precious portrayal of growth thinking.\n\n**Growth thinking is thinking that benefits people for life**\n\nGrowth Mindset was proposed by Carol S. Dweck, a professor of psychology at Stanford University, after years of research. It is one of the most influential psychological studies in recent decades. People with a growth mindset think that intelligence can be improved, pay more attention to their own efforts, generate the desire for continuous learning, and tend to work hard.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-7fdf26312b4d4cb3.jpg)\n\nRegrettably, most people in life are fixed minds, and there is no shortage of fatalists. They are doing habitual things every day, looking at the world with a fixed vision and thinking. Even reluctant learning is to cope with anxiety, rather than continuous, steady, long-term commitment to learning.\n\nThis is not difficult to understand from a biological perspective. **Biology usually prefers a fixed activity mode in order to save energy to the greatest extent. The brain is the body's largest energy consumption system,** accounting for 20% to 45% of the overall energy consumption. The distribution of cognitive resources is extremely rampant, and has a tendency to resist thinking naturally, so it is not surprising that most people's solidified thinking, but mental growth depends on long-term efforts, only in this way can we avoid life into narrow and rule-based Loss of ability to respond to environmental change.\n\nFrom the perspective of biopsychology, the brain can be trained like muscles. Every time new knowledge is acquired, new synapses are generated, and the synaptic connections are more consolidated when the existing knowledge is reviewed. The more frequently the learned knowledge is called, the more complicated and smooth the neuron circuit, which means that the brain of people who continue to learn is constantly remodeling, and their cognition will gradually improve.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-8be037a3fca5b8fd.jpg)\n\nFreed from the cage of fixed thinking and believing that he has the possibility of lifelong growth, it will be easier for him to absorb new things and make progress.\n\nLet's talk about critical point thinking. This is a concept derived from physics. It is used to understand the critical state before the phase transition of an object. I like to use it for learning. No matter what skills are learned, continue to strive to reach the critical point. Later learning It will move towards automation.\n\nFor example, in English, when your vocabulary reaches a certain value, the new context will be continuously absorbed by the knowledge you have mastered, and language learning will enter a hidden state. Think about how long we have n’t learned new Chinese vocabulary. , But this has not affected our understanding of new things.\n\nProgramming is also the same. 80% of programmers repeatedly use programming knowledge learned in the first year of work. When you want to learn something, please ignore the law of 10,000 hours, use it for a period of time, maybe half a year, Maybe it is one month, persist in working hard to push yourself to the tipping point of knowledge, and that's enough.\n\n[The critical point model] can help you understand interdisciplinary learning, and it may take a while to establish the basic framework of a discipline. Usually, you should read a few professional books, clarify the discipline context, and master the core basic concepts of the discipline. In the future, new knowledge will be automatically classified and organized by subject in your mind. After you have established a cross-disciplinary framework, you will enjoy the joy of implicit learning in multiple fields, thick snow, long slopes, and knowledge. Compound interest will continue to arise. In addition, I have developed a habit. When introducing a new concept, I will outline its source. Although it is slightly embarrassing, it is extremely useful for readers to absorb and understand.\n\nLet's talk about the self-catalysis model\n\nSelf-catalytic thinking comes from the \"catalyst\" concept in chemistry. When the product of a chemical reaction can further catalyze the rate of this reaction, allowing the rate of this reaction to increase at an extremely fast rate, this reaction is called an autocatalytic reaction.\n\nThe autocatalytic model is the way of thinking that accelerates growth.\n\nPersonal success often starts with small successes, such as Akiba's PPT, Laughing Friends of Time, Armored Learning Methodology. After achieving minor success in a certain field, they start an autocatalytic model and use it skillfully. initial local advantage, leveraging the network of other factors, such as other platforms and through cooperation, share uninterrupted speech, continued to enlarge its advantages, this **local advantage + muster agent** model also coincide commercial logic, because the two companies One of the most important functions is innovation and marketing. Marketing is the self-catalysis of the enterprise.\n\nUse the self-catalysis model to make yourself grow faster, and you can also find and build your auto-catalysis model to accelerate your goals.\n\n[Arrangements and combinations] also worth mentioning. Everything in the world is composed of basic elements, sometimes decomposed and sometimes aggregated. Permutation and combination thinking is used to look at things and derive new structures. Thinking is both a movement of knowledge and an arrangement of knowledge. Combination and trade-off; if it can produce a permutation and combination method that has never been before, it is innovation.\n\nIn recent years, in order to improve efficiency and stimulate organizational vitality, many large companies have introduced the Amoeba model. By breaking down the company into several self-managed small organizations, rearranging and combining personnel and resource elements to maximize efficiency, the company Permutation and combination of existing resources is an effective way to optimize redundancy.\n\nFor another example, in order to cater to the consumption upgrade, the fast-food restaurant combined the originally fixed single dishes into a multi-dish small-scale weighing model, which can meet people's needs for rich nutrition, and can be taken as needed to avoid waste. Bringing Innovation.\n\n**Multi-dimensional thinking, free growth**\n\nCharlie Munger often mentions his reverse thinking, \"think the other way, always think the other way around!\" Reverse thinking can often help us understand the meaning of a way of thinking more thoroughly. After all, an effective way to understand things is often to study them. On the opposite side, reverse thinking is also an activator of solidified thinking. Inadvertently, the thinking mode is X2.\n\nThere is a powerful Lollapalooza effect between thinking models, which will make us look at things more three-dimensionally, and see the possibility from different dimensions. Take life as an example. We can understand from at least four dimensions. There are at least three other possibilities for the road: the **[pursuit of wisdom] , love and relationship, and freedom, each of which can lead to happiness.**\n\nPeople often see only one dimension of possibility in their distress, and lack the perspective of seeing other opportunities. When they have a three-dimensional view of life, life has broad possibilities.\n\n[](http://xn--xgs52r1tiivv/)The power of [thinking mode](http://xn--xgs52r1tiivv/) will penetrate into every aspect of everyone's life, including your personality and your destiny. We may not be aware of these patterns, but they are critical to what we want and to achieve our goals successfully. Only when one has the right thinking can he fully enjoy the joy of life. With fun, it can stimulate intellectual curiosity and growth thinking, and then enter the ideal state of positive feedback.\n\nSource ( [Munger College] welcomes to share this article, reproduced, please keep the source!\n","tags":["Biological","Thinking"],"categories":["Biological Thinking"]},{"title":"Exploring the brain a panoramic cognitive human thinking model","url":"/2020-01-05/thinking/Exploring the brain a panoramic cognitive human thinking model/","content":"\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-dfa6191599236b86.jpg)\n\nOutline of this article\n\n*   How the human brain processes external things\n*   Several steps people think about\n*   Five stages of human brain development\n*   Several common thinking modes\n*   Thinking Features and Mind Maps\n*   The relationship between several thinking modes\n*   Human consciousness has the ability to travel through time and space\n*   Classification of human brain thinking patterns\n*   Comparison of Chinese and Western Thinking Modes\n*   The fundamental role of human thinking\n*   Masterpiece of human thinking\n\nFirst, the **human brain's processing of external things**\n\nIt can be roughly divided into the following processes:\n\n1.  There are sources of information.\n2.  Perceived by the five senses, a reflection is formed in the human brain.\n3.  The analysis and processing of the image by the human brain (from the formation of the concept to the analysis and reasoning).\n4.  Form your own opinion (either right or wrong) and guide practice.\n\n![How the human brain processes external things](https://upload-images.jianshu.io/upload_images/18390058-707760a5f7d9d4df.jpg)\n\n<figcaption style=\"box-sizing: border-box; display: block; margin: 6px 0px 26px; font-family: Verdana, Geneva, sans-serif; text-align: left; font-size: 11px; font-style: italic; font-weight: normal; line-height: 17px; color: rgb(68, 68, 68);\">**How the human brain processes external things**</figcaption>\n\nLeft and right brains are good at:\n\n*   **The right brain (perceptual) excels at:** rhythm rhythm, spatial awareness, overall concept Gestalt, imagination, daydreaming, color colour, size dimension.\n*   **Left brain (rational) excels:** text words, logic, numeric numbers, sequence, linearity, analysis, lists\n\nTwo **steps people think about**\n\n1.  Define Question\n2.  Set up Logical (Decision) Tree\n3.  Odom Razor Dirty Filter retains the core and boldly deletes irrelevant parts or parts with low relevance.\n4.  Propose hypothesis Build Hypothesis\n5.  Define data and sources, collect data\n6.  Test hypotheses.\n\nThree **stages of human brain thinking development**\n\nThe development of human brain thinking is divided into five stages and five forms, **namely: action thinking → image thinking → image logical thinking → abstract logical thinking → dialectical logical thinking. **People who have not studied developmental psychology are a very vague concept of thinking, and this ambiguity also brings about the ambiguity of life.\n\n**1\\. Action thinking is a form of thinking for infants and young children** : children's intelligence is in their hands. The movements of hands and feet directly stimulate the development of the central nervous system. Children's thinking and movement are synchronized.\n\n**2\\. Image thinking is the main form of thinking in early childhood: the** feature is that language symbols and physical images are linked together. This is a qualitative leap in thinking. Children begin to truly understand nature and internalize everything in nature into the nervous system. It is from this that we like to watch cartoon books in childhood.\n\n**3\\. Image logical thinking is the logical relationship between images** : if one apple plus two apples is equal to three apples, six pears are not as many as two pears. The emergence of logical thinking is another qualitative leap, that is, children begin to understand the connection between things. This is how we first started to understand mathematics.\n\n**4\\. Abstract logical thinking is a logical relationship separated from the image** : such as pure symbolic operation, generally 11 years old has complete abstract logical thinking. Abstract logical thinking is the most important form of thinking. All our education revolves around it. It is everything in the test-oriented education system. Once a person has poor logical and logical ability, he is sentenced. Under the current Chinese system, he It will never be accomplished.\n\n **5\\. Dialectical logical thinking is the highest level of thinking:** that the previous logical relationship may be incorrect, it does not matter whether it is right or wrong, Newton's law is sometimes wrong, killing is sometimes a hero, and Guan Jian is from which angle . Dialectical logical thinking can be said to be a real sign of a person's maturity. He directly determines the height of a person's growth and determines the success and failure of life. \n\nPsychological research has found that **some people with incomplete personality can't form dialectical logical thinking throughout their** lives, and can't look at problems from the perspective of others **throughout their** lives. But dialectical logical thinking is one thing, and willingness to use dialectical thinking to guide one's actions is another.\n\nDialectical logical thinking can be trained, that is, transposition thinking.\n\n**Fourth, several thinking modes commonly used by people**\n\n1.  **Empirical thinking** : This is the simplest. Most people in daily life are used to judging by their own experience.\n2.  **Analogical Thinking** : **Zhouyi is the earliest analogical reasoning system for human beings.**\n3.  **Logical thinking** refers to the rational cognitive process in which people actively reflect objective reality by means of thinking, concepts, judgments, and reasoning in the process of cognition. It is produced and developed as an analysis of the thinking and its structure and the laws of action. Only through logical thinking can people achieve the grasp of the essential provisions of specific objects, and then understand the objective world. Logical thinking has the characteristics of standardization, strictness, certainty, and repeatability. It is the advanced stage of human cognition, that is, the stage of rational cognition. Also called \"analytic thinking\", \"theoretical thinking\", \"abstract thinking\" or \"closed eyes thinking\".\n4.  **Dialectical thinking** refers to the correct reflection of the dialectical development process of objective things by thinking forms such as concept, judgment, and reasoning, and the thinking mode of understanding things from a perspective of change and development is a reflection of objective dialectics. The most basic feature of dialectical thinking is that the object as a whole is examined from its inherent contradictory movements, changes, and interrelationships in various aspects in order to understand the object systematically and completely in nature. Dialectical thinking is usually considered as a mode of thinking that is opposite to logical thinking.\n5.  In logical thinking, things are generally \"this is another\", \"not true is false\", and in dialectical thinking, things can be \"also this and another\" and \"true and false\" at the same time without hindrance Normal thinking activities. Everything in the world is interconnected and affects each other, and dialectical thinking is based on the objective connection between all things in the world, and further understands and perceives the world, and feels human and nature in the process of thinking. Relationship, and then a kind of thinking that leads to some conclusion.\n\n**V. Thinking Features and Mind Maps**\n\nScientific research has fully proved that: the characteristics of human thinking are radioactive, every piece of information, every feeling, memory or thought entering the brain (including every word, number, code, food, fragrance, line, color, image, Beats, notes, and lines) can be expressed as a branch of thinking, which shows a radioactive three-dimensional structure.\n\nThe process of discovery: Tony Buzan, a famous British psychologist (the \"Father of Memory\" in Britain. The inventor of the \"Mind Map\"), discovered the great artist Da Vinci in his process of studying the power and potential of the brain. Many drawings, symbols, and wires are used in the notes. He realized that this might be the secret of Da Vinci's super mind.\n\nTony Buzan also experienced the typical \"pilgrimage\" of students in college. After encountering difficulties and problems with information absorption, organization and memory, he went to the library for assistance, but was surprised to find that there was no relevant teaching about how to use the brain correctly and effectively. Books and materials, after these setbacks, also made Tony Buzan start to think that it is impossible to develop new ideas or methods to solve these common difficulties and problems.\n\nSo Tony Buzan began to study psychology, neurophysiology of the brain, linguistics, neurolinguistics, information theory, memory skills, comprehension, creative thinking and general science. Gradually Tony Buzan discovered every brain cell and Various techniques, if applied harmoniously and skillfully, will produce greater efficiency than working separately from each other. This seemingly tiny discovery has produced unexpectedly satisfying findings:\n\n**1\\. Changes in the method of taking notes can increase memory by at least more than 100%.**\n\n**2\\. Many \"learning impaired people\" are actually memory impairments, not thinking impairments, because memory impairments hinder the development of his brain thinking.**\n\n**3\\. The brain essentially remembers and thinks in an interrelated and integrated manner with keyword concepts.**\n\n**Mind map features:**\n\n1.  The biggest obstacle to thinking is complexity and chaos; mind mapping can make him simple, orderly, process-oriented, graphical, and clear.\n2.  Mind mapping is a process of presenting a mind-related process through a tree structure with sequential labels, and a process of materializing radiation thinking.\n3.  The mind map is mainly to promote the generation of inspiration and the formation of creative thinking by means of visualization.\n4.  Mind maps are an expression of radioactive thinking, and therefore a natural function of human thinking.\n5.  Mind maps navigate the entire range of cortical skills in a distinctive and uniquely effective way-vocabulary, graphics, numbers, logic, rhythm, color and sense of space.\n6.  The mind map is based on the simulation of the human brain. The entire picture is like a structure diagram of a human brain and can play the overall function of the human brain.\n7.  Mind maps are a key to unlocking the infinite potential of the human brain by using both graphic and textual skills.\n\nExample: Mind Map\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-694a9608178b4f39.jpg)\n\n<figcaption style=\"box-sizing: border-box; display: table-caption; margin: 6px 0px 26px; font-family: Verdana, Geneva, sans-serif; text-align: left; font-size: 11px; font-style: italic; font-weight: normal; line-height: 17px; color: rgb(68, 68, 68); caption-side: bottom;\">\n\n</figcaption>\n\n**The relationship between several thinking modes**\n\n1\\. (Tree-type thinking, space thinking, network thinking) are completely connected\n\n2.Each emphasis is different\n\n*   Tree thinking: describing objects\n*   Spatial thinking: describe objects and express spatial relationships\n*   Reticulated Thinking: Representing Relationships with Various Icons\n\n7\\. The **human mind has the ability to travel through time and space**\n\nThis can be seen everywhere in Chinese teaching: \"Northern scenery, thousands of miles of ice and snow, thousands of miles of snow drifting\", \"Qi Huang Han Wu, Tang Zong and Song Zu\" \"Yuan Yuanshan, swallowing the Yangtze River, vast vast soup soup, boundless horizon, Chaohui \"Xiyin, a lot of weather.\" These are unfolding in a specific space-time area. Without the author's creative thinking and time and space to travel, he would never create a well-known masterpiece.\n\n\"The good rain knows the season, when spring is happening, and the wind dives into the night, and the moist is silent.\" When the thinking consciousness enters a specific time and space with the expression of the words, the quiet words become active, and the words no longer appear in the mind It's language, but it's a scene of drizzle quietly moistening the earth on spring night. Entering the specific time and space defined by the work, you can change from static to dynamic.\n\nFrom careful description, all good narratives, argumentative papers, and expository texts, all good novels, poems, and prose all reflect the perfect combination of matter in a specific time and space, and reflect the author's deep understanding and understanding of the material movement in a specific time and space. The time span shown by the author's work is much smaller than the time and space span of the article in the idea. It is difficult to produce a good work without a broad and deep time and space background.\n\n**The classification of human brain thinking patterns**\n\nThere are many types of thinking patterns in the human brain, the most important of which are the following:\n\n1.  Linear thinking and non-linear thinking: Linear thinking is easy to go black all the way\n2.  Divergent thinking and convergent thinking: such as \"multiple solutions to one question\", \"write more about one thing\", \"multiple use of one thing\", etc., to cultivate the ability of divergent thinking. Psychologists believe that divergent thinking is the most important feature of \"creative thinking\" and one of the main indicators of creativity.\n3.  Point thinking, face thinking, and body thinking: To be able to think about problems from all directions, that is, \"stand up and think\". Such as three-dimensional greening (roof garden, indoor greening, backyard greening, balcony greening, etc.), three-dimensional planting (maize field growing mung beans, sorghum field growing peanuts, etc.),\n4.  Vertical thinking, lateral thinking and lateral thinking\n5.  Forward thinking and reverse thinking\n6.  Symmetric and asymmetric thinking\n7.  Static and dynamic thinking\n8.  Logical thinking and dialectical thinking\n9.  . . . . . .\n\n**Comparison of Chinese and Western Thinking Modes**\n\n**I. The West: Formal Analysis Thinking Mode**\n\nThales, the first western philosopher, made an outstanding contribution to the development of geometry while unveiling the prelude to the thinking model of formal analysis. The two are closely related. Heath, a well-known British historian of mathematical mathematics, pointed out when summarizing Thales' development of geometrical contributions (\"Geometrics has become a deductive science based on general propositions\").\n\nPitagora and his school promoted this model, considering numbers as both the substance of the ontology and the form of the ontology, thinking that everything is derived from imitated numbers. In particular, he is dedicated to linking numbers with geometry to explain the derivation of all things. It is under the impetus of this thinking mode that Pitagora has made outstanding contributions to the development of mathematics and natural sciences.\n\n**China: Overall Organic Thinking Mode**\n\n1\\. From the world view to the social history ethics view\n\n2\\. The theory of \"qi\" in the view of nature: In traditional Chinese philosophy, qi is understood as the material origin of a continuous intangible whole. An objective existence that can move and occupy space.\n\n3\\. Simple dialectical logic in thinking method. The dialectical logic of the pre-Qin period can be represented by the Book of Changes, which establishes the unifying principle of unity of opposites. In its mysterious form and idealist system, it contains rich thoughts on the use of dialectical thinking. It calls the law of the universe's change \"Easy\" and \"Dao\".\n\n**\"The Book of Changes\" was the earliest analogical reasoning system for human beings** .\n\n**3\\. India: Holistic One Form Intermediary Thinking Mode**\n\nTen, **the role of the human mind**\n\nThinking (thinking) is the indirect, generalized processing form of the human brain on real things, expressed in implicit or implicit language or action (Wikipedia).\n\nThinking processes multiple levels of objective relationships and connections, **revealing the intrinsic and essential characteristics of things,** and is a high-level form of psychological activity.\n\nThe thinking process is the process by which the **human brain processes information. Specifically, it includes a series of processes such as analysis, abstraction, synthesis, generalization, comparison, etc.** , and systematic and specific logical processing formed according to the development law of different things.\n\n[Thinking](https://www.madewill.com/category/thinking-model) (thinking model), it is the result of a variety of thinking and thought processes of the human brain collection; through human understanding and practice of the objective world, and is subject to the provisions of a series of basic concepts and constraints, has been the mode of **thinking Positioning** ; it is an internal formula for people's views, reference structures and beliefs, and plays a decisive role in people's words and actions (so as to affect the outside world).\n\nThe thinking mode itself is an organic system composed of many factors. The relative stability or major change of any factor will have an effect on the stability or change of the entire thinking mode. Among them, certain logical theories and principles that affect the logical form used in people's cognition activities also have an effect on the entire thinking mode of a particular subject.\n\nUsing a certain mode of thinking to think about a particular thing and produce a certain content of thought is the mode of thinking.\n\nAnother explanation is: the way of thinking is the sum of the thinking form, thinking method and thinking procedure in the thinking process formed by people in a certain era on the basis of certain ideas, knowledge and methods. It is a thinking framework. The above is a reflection of people's social production and life.\n\n**In many cases, the definitions of \"thinking mode\" and \"way of thinking\" are not much different.**\n\n**Eleven masterpieces of human thinking**\n\nHigh-level summary of ancient Chinese war theory in Sun Tzu's Art of War and Wuzi\n\nNatural sciences such as mathematics, physics, geography and astronomy focus on logical thinking\n\nHumanities such as history and politics focus on dialectical thinking\n\nNarrative, Analysis and Summary of The Romance of the Three Kingdoms\n","tags":["Cognitive","Neuroscience"],"categories":["Cognitive Neuroscience"]},{"title":"A golden key to understanding complex worlds in biological thinking","url":"/2020-01-03/thinking/A golden key to understanding complex worlds in biological thinking/","content":"\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-3d28983322cb8e97.jpg)\n\nQuestions worth thinking about in this article\nWhat is a complexity system?\nWhat is biological thinking?\nWhat is the relationship between biological thinking and the underlying logic of a complex world?\nHow to use biological thinking in business?\n\nMa Huateng mentioned in the open letter that Tencent has always adhered to the \"grayness rule\" for creating bio-type organizations when building organizations and product innovations, just as it allows mutations in evolution and allows \"imperfections\".\n\nToday's headline Zhang Yiming said when talking about enterprise systems, living things are rich in cell and ecology, and the species are diverse, but the rules behind them are very simple and elegant, which has many analogies for us to design enterprise systems.\n\nIn fact, not only Ma Huateng, Zhang Yiming, including Fu Sheng, Wang Xiaochuan, etc., more and more Internet front-line entrepreneurs have emphasized the inspiration that biology brings to them in interviews, and they have become obsessives and firm practices of [biological thinking] . Wang Xiaochuan even said that it is biological thinking that makes Sogou live to this day.\n\nWhy do Internet gangsters use biological thinking? In fact, the logic behind it is simple, **they have to solve the problems caused by [complex systems] .**\n\n**What is a complexity system?**\n\nOne of the most important changes facing Internet companies and our generation is: ultra-complex\n\nYes, we are in a new era and are building a new system, and this system cannot be mastered as a whole, or it is too complicated for anyone. In the modernization process of the past 200 years, complexity is a common thing, and human society has always tended to become more and more complicated.\n\nTake the machine, from the earliest dozens of parts to hundreds of tens of thousands of parts, and now a Boeing 747 aircraft has 6 million parts, 275 kilometers of various pipelines. No one in the world can fully figure out what is going on with a Boeing 747. \n\nAt the beginning, humans also evolved a method to deal with this complex system. The solution mentioned by Mr. Wu Jun is **modularity and hierarchical collaboration. **Each person is only responsible for one of these modules, and then they are layered on top of each other to form a huge system. Don't be afraid if something goes wrong, you can find the problem by breaking it down.\n\nSo, even if something as complex as a chip has hundreds of millions of transistors integrated on it, humans can still design it, make it, and control it. Complexity doesn't look that scary.\n\nNow, in the age of the critical point of technological complexity, human beings face the problem of **super complexity.**\n\n**A bunch of things are put there, even if many are messy, it's called \"complex\". But when a bunch of things reach a certain amount, they also affect each other, and a \"cascade effect\" occurs, which is called \"complexity\".**\n\nOnce this complexity is reached. It was an uncontrollable, even incomprehensible system.\n\nIn 2015, Google counted 2 billion lines of code for all its products.\nIn 2018, programmers at Alibaba wrote 1.2 billion lines of code a year. \n\nSounds okay? This number is about the same as the number of transistors on a chip. So a chip we humans can control. That software we humans can also control. Note, however, that their complexity is completely different.\n\nwhy? Because code and code are going to affect each other. This is called the \" **cascading effect** \". Once there are more than one billion lines of code that interact and interact with each other, and there are layers of interaction at different levels, how many kinds of situations are there? There must be countless more than the number of particles in the entire universe. \n\nOn March 6, 2019, the BNP Paribas system went down for more than 24 hours, and users could not perform online banking related operations and could not access his official website or applications. In January of this year, this happened once. \n\nIf you are familiar with the banking software system, you will know very well that with the development of banking business, the banking software system is continuously updated and has been constantly patched. Basically, the banking system is developed and built on the basic platform. It has become so complicated that no one can explain the system clearly.\n\nIn addition to the Internet software system, all aspects of human society are shrouded in complexity and fog. For example, the federal tax law of the United States has more than 74,000 pages. In fact, no one in the United States knows how this country collects taxes.\n\nAnother example is the financial industry, a financial wealth management product, after repeated packaging and sales, no one can understand what it is. No one knows when this product may ignite the financial crisis. Not even financial experts know.\n\nInternet companies that are at the forefront of the times, they have to face such a super complicated internal and external system environment.\n\nSamuel Abesman wrote in \"Why Biological Thinking Is Needed\":\n\n*Living things are complicated when they are alive, and at most they can be said to be complicated.*\n\nMan is also one of the creatures and the most complicated one to live.\n\n**Human beings are not only complex themselves, they have also created a complex world, and in the complex world have created various complex systems.**\n\nThere are inextricable relationships between people, between people and all living things in the world, between people and various systems created by themselves, and between systems. As long as one is alive, one has to face up to complex relationships. Because, as long as you are alive, there is no way to not be associated with other things.\n\nIt can be said that human technology, from websites to transaction systems, from urban infrastructure to scientific models, and even supply chain systems and logistics systems that provide supporting services for large enterprises, have become too complex and intertwined.\n\n**Why do we need biological thinking?**\n\nHuman cognition is ultimately limited. With the passage of time, the various technical systems we build have become more and more complex, and the relationships between the systems have become stronger and more difficult to understand. .\n\nNo matter how smart humans are and how good their memory is, it doesn't help, because these systems are constructed differently from the way humans [think] . Humans do not have the ability to cope with the millions of components and the numerous interactions between them at the same time, keeping all the results in their heads. Our brains are \"seriously overloaded\" and then fail.\n\nTherefore, while we enjoy the efficiency and convenience brought by technology, we are often troubled by these overly complex systems. The world is becoming more and more complex, and complexity continues to grow at a rapid rate. Paying attention to all designable, controllable, and predictable physics thinking becomes powerless, and the pursuit of adaptation, connection, and symbiotic biological thinking Has gradually been used as a cognitive framework for coping with complex variable environments.\n\nSimilarly, an advanced system cannot exist independently if it is to function normally. Among many system problems, biology is the most complicated system problem. If you accept the way you think about solving complex problems, you can develop the ability to solve complex problems.\n\nIn the process of creating complex machinery, mankind has repeatedly returned to nature to find guidance. We often lament the power of nature. Complex systems such as hive, ant colony, forest, and sea are formed naturally, cyclically, and constantly evolve.\n\nThere is a profound \"kinship\" between biological systems and technological systems, which means that we can learn a lot from biological thinking. Attention to detail and diversity of biological thinking will provide a vital perspective for understanding cluttered evolutionary systems.\n\n**Therefore, biological thinking is the golden key to understanding the complex world.**\n\nThe biological world is a world from 0 to 1\\. Biology emerges, lives, and evolves in a complex and changeable environment.\n\n**Biological thinking is about the coexistence of errors and the coexistence of risks. As long as it is effective, it does not require precision, or even understanding, which is the normal state of all successful species. **This is almost the same as the underlying logic of the enterprise, and the company's path is from nothing to existence, from survival to competition, and then to evolution.\n\nTherefore, Fu Shengcai of Cheetah lamented that biology is a discipline that can open up a lot of cross-border knowledge. Compared with natural sciences such as physics, biology reveals the underlying laws of the world more deeply, and its ideas are universal. Understanding the world with biological thinking is actually nothing more than returning to the state of human existence.\n\n**How to use biological thinking to solve problems?**\n\nSo, in the face of complex systems, how can we use biological thinking to solve problems?\n\n**1\\. Use to cope with risks.**\n\nThe physical method is to avoid risks by precision. The biological approach is redundant. **Redundancy means that instead of providing increased accuracy to ensure security, alternative methods are used to ensure stability. **Insects and fish have spawned a large number of offspring by spawning a large number, and few survived, but the genetic safety of the species is guaranteed.\n\nThe human body has two lungs, two kidneys, and two eyes. Any one of them has a problem. The other one can ensure the continued function. Even if there are problems, other functional organs will provide compensation methods to make up for the missing functions. . People with damaged left brains have particularly developed right brains, and blind people have particularly sensitive hearing.\n\n**The most direct strategy in the business world is: Don't put eggs in a basket. **We know the giants Kodak, Motorola, Nokia, their business is too \"focused\" is very easy to subvert. Therefore, the current actions of Internet giants after occupying traffic are often to seek investment in multiple fields, or to \"heavy\" the entire industry chain. \n\nAlibaba has Alipay, rookie, Ant Financial, and Flying Pig. It has developed Hema fresh products offline, and invested in hundreds of companies in artificial intelligence, travel, finance, social and other fields. In product innovation, Tencent also uses this kind of **biological thinking. In a chaotic world, only believe in probability, calmly face the failure in the process, keep throwing dice, and look for opportunities in the change. **\n\n\"Don't put eggs in a basket\" is not only manifested in competing for the layout of multiple industries, deepening and heavy operation, but also starting to seek a more dynamic and resilient team management method in terms of organization. \n\nEnterprises represented by Haier have gradually realized \"flattening\" and \"decentralization\" in management, which is actually an active transformation of \"anti-fragile\" organizational management. Inamori Kazuo's original \"Amoeba\" business model, by dividing the company organization into small \"Amoeba\" small collectives, while maintaining vitality, the profit center has sunk, and has never lost money for more than 50 years. \n\n**2\\. Gray rule: use \"imperfect\" rule to correct errors**\n\nThe physics method is to figure out the principle first, then correct the mistakes, and clear the original. The biological method is to survive a variety of environmental mutations. As long as you can pass through the evolutionary scissors, the fittest has survived. As for whether there are no errors at all and biology does not care, this is not important. \n\nMa Huateng has always adhered to the \"imperfect\" rule in Tencent's product innovation, that is, to achieve a single point of breakthrough, it is necessary to allow imperfections, but to quickly approach perfection. It is precisely some defects and imperfections of the product that have laid a very important foundation for the next development and evolution.\n\nCheetah has also emphasized \"continuing to benefit in a volatile world.\" Fu Sheng believes that benefiting from large fluctuations is the ultimate ability of biology. \n\nIn fact, genetics and mutations in biology are one that keeps stability and one looks for opportunities in fluctuations; one side is the death of an individual and the other is the evolution of a population; and the corresponding enterprise is that the environment is harsh on the one hand and the life is stronger on the other. These practices of all enterprises are actually imitating the nature, imitating the scissors of evolution, and creating various extreme environments, depending on who can survive. It's good to survive.\n\nAs for whether the company is completely free of bugs and defects, it is neither possible nor important. This is biological thinking. \n\n**3, humble heart + iterative perspective**\n\nIn the face of mistakes and failures, while admitting our ignorance and incompetence, we must also maintain an open and calm mind. For example, there is widespread synergy in ecosystems. It does not mean acting directly on the problem and influencing it, but rather making up for individual deficiencies by connecting others. \n\nExisting knowledge cannot solve contemporary problems and can be left to future generations to deal with. For a complex world, maintain a humility, awe, and generosity, and use continuous biological evolution theory to deal with it. The power to iterate with time and change will be endless.\n\n**to sum up**\n\nIf biological thinking is a golden key to understanding the complex world, biological thinking can be expressed as: don't be confused by appearances, chaos and unpredictability; enjoy the process, and look at things you do n’t understand with a developmental mindset; With a constant iterative vision, we firmly believe in probability and look for opportunities in change.\n\nWe must therefore strive to maintain two opposing states:\n\nThe first state requires us to work hard to overcome our ignorance, and we must not be obsessed with it; **(insist on exploration)**\nThe second state means that once we understand something, don't take it for granted. This is scientific thinking and biological thinking. It is our necessary ability and prerequisite for learning new things and solving problems. **(Keep iterating)**\n","tags":["Biological","Thinking"],"categories":["Biological Thinking"]},{"title":"开启个人职业规划","url":"/2020-01-02/职业规划/","content":"\n\n　　一、关于职业规划定义解读\n\n　　我们先看看什么是职业规划?\n\n　　百度百科上给出的定义是这样的：职业规划是对职业生涯乃至人生进行持续的系统的计划的过程，它包括职业定位、目标设定和通道设计三个要素。对于枯燥的定义，我们需要有一套方法论来分解一下：\n\n　　首先，职业规划是一个过程。我们从毕业走向社会，就开始了属于我们的职业之路，从正常的职业发展路径来看，原地踏步、停滞不前不是职场人应有的状态，毕竟随着自身年龄的增长、社会环境的变化、家庭责任的加重，我们的收入也要随之增长，要达到甚至高于当前必要支出才能保证有一个不错的生活环境。那么，收入的增长，靠什么实现?是职位升迁、是平台壮大、是自我综合素质提高、是趋势的超前把握、是机会的合适选择、还是?这就需要我们在职场当中，根据内外环境的变化、不同阶段的机会，不断进行学习成长，进而顺着自己的职业目标调整自身的职业路径，这是个不断设梦、追梦、圆梦的过程，而不是结果，也就是说，职业规划，没有对错之分，只有合适不合适、科学不科学而已。\n\n　　其次，职业规划的发展是持续的、系统的、有计划的。眼下，大多数刚步入社会的大学生或正在打拼的大部分普通职场人，是没有清晰的职业规划的或者是职业规划是面临困境、无奈时的一种被动选择。真正的职业规划，它应该是一种发自内心的、主动的自我规划，它应该是对自身职业发展不同阶段给予自己的一个交代(是好还是坏)，它应该是充满想象、值得憧憬、朝思暮想且不断为之努力的方向指引。它需要我们每一个职场人去给自己下定义：\n\n　　我要成为一个什么样的人?\n\n　　为了这个目标，我近三年要做什么?接下来五年我要如何调整?过了30岁没达到预期怎么办?\n\n　　设定的目标我如何达成?实现的路径和方法是什么?\n\n　　我的优势和特长是什么?如何在职场当中得到最大程度的发挥?\n\n　　眼下我存在哪些不足?不同的不足和短板如何快速成长?\n\n　　我有哪些可以借力的地方?有哪些资源哪些人能帮到我实现我的职业目标?\n\n　　这个时代的发展趋势是什么?我的职业发展路径和实现的方式，是顺势而为还是背道而驰?\n\n　　......\n\n　　所以，我们谈职业规划，它应该具备持续性，不能一阵一阵;它应该系统，从自身到公司内部到外部环境和资源;它应该有计划性，每一步都在自己的把控当中。\n\n\n\n　　二、理清职业规划的几个核心要点\n\n　　(一)没有目标的职业规划很难有未来。\n\n　　我们经常听到的职业规划告诉我们，需要有几步：明确就业方向(先就业后择业还是先择业后就业)、选行业(法律/教育/文化/影视)、选企业(华图 教育/新 东方/好 未来)、给自己职业定位(助理开始还是专员开始还是主管开始)，这个流程是没有错的，我想说，这只是职场的开始，可以说，对所有人都实用，大家的起点都是一样的，不是吗?\n\n　　然后呢?很多人，同样的时间，同样的起点，等过了几年，有的连升了3级当了经理，有的还原地不动(这样的鸡汤有好多，在这里不做阐述)，差别在哪里?当然，差别有很多，我认为最重要的一条是有没有清晰的职业目标并制定科学的实现方法。\n\n　　因为，设定了目标，就有了努力的方向，就有了要追求的高度。一个清晰的目标，可以让你自发的去做好各种计划，不断的总结自身的不足并改善，对自己的工作要求会不断的提高，会主动的做工作总结和思考，可以让你的职业成长围绕着一个点去积累经验，而这种日积月累的经验积累，也是你日后升职加薪的筹码。反之，没有目标呢?容易陷入什么都学、什么都做什么都不精、不断的尝试用跳槽来解决问题。\n\n　　所以，你要做的第一件事，就是找准一个可以为之奋斗3-5年的小目标，可以简单点但一定是明确的。如：3年后，我要做到主管级别，管5-10人的团队;1年后，我的工资翻一番等。当你完成一个又一个小目标的时候，你就比较容易形成好的职业习惯，如吃苦耐劳、勤奋好学、勤于总结、善于解决问题等。\n\n\n\n　　(二)如何科学的做职业规划?\n\n　　从我学习到的方法有擅长法则、喜好法则、价值发展法。如何理解?\n\n　　第一、擅长法则：从每个人的优势开始，可以是所学的专业、自己的特长、身边的资源开始梳理，比如，我是市场营销专业的，我的职业发展就是市场推广;我表达能力比较强，我的职业发展是做主持、销售或老师;我的师兄是在互联网公司做总监的，公司不错刚好需要招人等。好的职业起点，能节省试错的时间、少走弯路。\n\n　　第二、喜好法则：这点很容易理解，比如，我喜欢祖国的山河，所以我的职业是做一名导游;我喜欢篮球，我的职业是做一名篮球教练;我喜欢为人民服务，我的职业是做一名公务员等。心中喜爱的职业，是最长久的、最能激发个人潜能的。\n\n　　第三、价值发展法则：这一点对于很多人来说比较难，需要具备比较强的综合能力，比如：分析能力、人际关系能力、机会把握能力、系统规划能力等。此法则，是我们能有效识别行业、企业、部门在不同发展阶段的价值，然后顺应发展规律把握机会推动自己职业发展之路加速。如：雾霾严重，我们选择环保企业;公司在跑马圈地开发市场时期，你选择市场部;公司在大力发展分子公司的时候，你主动去开疆扩土;国家在倡导扶贫，你一马当先去做扶贫等。此法则，一般情况下建立在擅长法则或喜好法则基础之上。\n\n　　(三)争取在最短的时间成为某一方面的专家\n\n　　伟大是时间的函数，大家可能听说过10000小时定律：不论你想在任何一个领域成功，你都必须至少付出10000个小时的磨练。我们每个职场人留个心眼，去留意周围的职场伙伴，看看有哪些职场规律?\n\n　　第一、相对成功的公司高管，绝大多数是随着公司发展，一步一步成长并成为公司中流砥柱的;\n\n　　第二、在职场的前三年过于频繁跳槽的职场伙伴，一般不会有很好的发展;\n\n　　第三、不断更换行业、工种的职场伙伴，一般没有多大的想象空间;\n\n　　第四、猎头猎人，一般都要看行业工作经验;\n\n　　......\n\n　　大家想想，为什么?\n\n　　我观点是：对于大多数人而言，大家的智商是差不多的，都在中等水平。这样，就可以下定义，我们的知识是需要一步一步学习的，我们的能力是需要一步一步提高的，我们的视野是需要一步一步开阔的，而每一步，都是需要付出时间成本的，很少有一生下来的天才。\n\n　　那么，你要在职场当中快速增值，思路和方法是什么?在于聚焦，一点突破形成核心竞争力。如果你聚焦了，一切就会变得非常简单，比如我们学习效果最好的时期是哪个?是高三，因为，你的面前只有高考，你的任务只有学习，在职场当中也是一样的原理。\n\n　　举例，假设你是做市场工作的，你的任务很简单，就是找到你最强的那一点去吃透，比如你网络营销很厉害，那么你的工作方法论就是如何用网络营销优势来完成你的经营任务，穷尽网络营销一切手段做出业绩，你的每一步总结和规划也都是网络营销，你的成功经验也是基于网络营销来展开，这样，不断的尝试网络营销不同的手法形成自己的一套体系，进而去说服别人用你的方法论，不断的影响别人，慢慢的，你就成为了网络营销的高手、专家，你的职业发展之路，就有了核武器。\n\n　　给大家的建议是：不要随便的跳槽，如果要跳，也是往更高的平台跳，也是环绕着一个点去跳，也是不断的用沉淀的经验为自己背书去创造更大的价值。争取在最短的时间成为某一方面的专家。\n\n\n\n　　(四)如何做到在职场上弯道超车?\n\n　　很多职场人的心理，每天都在想，能不能有捷径升职加薪呢?能不能屌丝逆袭一下，一夜成高富帅白富美呢?我想，几率是比较小的，但是有方法是可以实现弯道超车的，关键词是主动、专业、换位思考，列出几条：\n\n　　1.学会自动报告你的工作内容和进度，主动的让领导知道，争取上级的支持和配合;\n\n　　2.养成对上司的询问有问必答而且清楚的习惯，让领导对你放心，争取上级交给你更重要的任务;\n\n　　3.虚心接受批评，不犯三次过错，让领导省事，争取让上级看到你的进步;\n\n　　4.毫无怨言接受任务，对自己业务主动提出改善计划，给领导分忧，争取上级给你升职加薪的理由;\n\n　　.........................\n\n　　总而言之，就是要养成正向的职业发展习惯，职场，最怕用心两字。\n\n　　反之，余世维老师关于《职业经理人常犯的11个错误》同样值得我们思考，看看我们是不是在日常的工作中犯了这样的错误，反过来，也是我们弯道超车的招啊：\n\n　　①拒绝承担个人责任; ②不去启发下属;③只强调结果，不强调思想;④一视同仁的管理方式：⑤忘了公司的命脉：利润：⑥只见问题，不看目标：⑦不当主管，只做哥们：⑧没有设定标准：⑨纵容能力不足的人：⑩眼中只有超级明星：最后，在公司内部形成对立。(本段摘自：余世维时代光华课堂核心观点)\n\n\n\n　　三、职场前五年决定未来职业的高度\n\n　　个人认为，职场前五年，你的选择、你的能力、你的职业习惯决定你的职业高度。\n\n　　(一)职场前两年，你需要把握机会选择一个好的行业和企业\n\n　　个人，是比较反感先就业后择业的观点的，没想清楚，就出发，很容易让一名刚刚步入职场的小鲜肉走偏了路，很容易失去择业的机会。不管是即将毕业的大学生也好，还是在职场当中的老兵也好，个人建议，谋定而后动，想清楚了你要什么?成为什么样的人?是否适合你?你去了是否有发展前景?正确的路径是什么?举例说明：\n\n　　你去任何企业面试前，你需要做的工作有哪些?\n\n　　①这个企业在行业中的地位、企业规模多大了?\n\n　　②这个企业处于什么样的发展阶段?是起步还是高速发展还是夕阳产业?\n\n　　③这个企业的招聘频率、招聘规模和重点招的职位是哪些?\n\n　　......\n\n　　当你去企业面试过程当中，你需要留意的地方在哪里?\n\n　　①面试官的能力水平，面试官是一个企业的缩影;\n\n　　②面试流程是规范按流程走，还是比较随意?\n\n　　③未来上级面试环节，上级是一个什么样的人?是否坦诚、乐观、自信和充满激情?\n\n　　.......\n\n　　当你进入企业开始工作的时候，你需要关注这些方面：\n\n　　①是否有完善的新员工培训制度?进入公司是否有人带?\n\n　　②企业文化怎么样?老板的人品、能力和管理模式，很大程度决定你的发展速度和高度;\n\n　　③公司的晋升通道是否开放?上级是否愿意培养你、是否提拔你?\n\n　　......\n\n　　当你从面试前，就开始为你的职业铺路的时候，你已经领先于别人了。对应上面曾老师给出的思路，来看看你现在随处的企业，是否符合你的职业发展要求吧。\n\n　　(二)职场前三年，你需要掌握最基本的职场技能\n\n　　首先，个人基本能力的掌握。我总结了七项重要的、共性的与大家分享：\n\n　　1)你的人际关系处理能力：是否可以和同级、上级和睦相处并慢慢形成工作默契;\n\n　　2)你的听说读写能力：你的表达能力、善于聆听能力、文案撰写能力，这是你的个人名片;\n\n　　3)你的执行力：所有工作任务，是否能按时、按质、按量完成任务;\n\n　　4)你的管理能力：时间管理、项目管理、目标管理等;\n\n　　5)你的学习能力：读书、培训、头脑风暴等方式，去养成爱学习的习惯，善于做笔记;\n\n　　6)你的工具应用能力：PPT、Word、excel、思维导图、VISO等是否能掌握基本应用;\n\n　　7)你的计划总结能力：主动规划你的工作，善于总结，这是一个人一辈子的好品质。\n\n　　基本上，这七项能力，是一个职场人，伴随一生的基础能力，我们所有职场人，需要付出行动去学会。\n\n　　其次，个人核心能力的把握。我引用了猎头一部分新的，与大家分享：\n\n　　1)解决问题的逆向思维能力与解决问题的方案制定能力;\n\n　　2)考虑问题的换位思考能力;\n\n　　3)信息资料收集与应用能力;\n\n　　4)目标实现与调整能力;\n\n　　5)超强的自我安慰与抗打击能力\n\n　　6)企业文化的适应与岗位变化的承受能力\n\n　　7)职业精神与不断渴望挑战的精神\n\n　　想一想，我们在职场前三年，修炼了这里面核心的几块能力，职业通道我们还需要担心吗?\n\n\n\n　　(三)职场前五年，你需要提炼并保持好的职业习惯\n\n　　我们经常在网络上看到，李克强总理如何艰苦朴素、华为的任正非总如何保持艰苦奋斗、万达的王健林总行程如何如何紧凑，我想说，这不奇怪，我认为，职业越发展到一定的高度，这些好的职业习惯不是可以装出来的，而是一个职场老兵真正由内而外散发出来的职业精神，比如：勤奋、吃苦、节约、平易近人、市场嗅觉、事物判断力等，这些好的职业习惯，我们在职场的前三年，至少要养成几条。\n\n　　四、画出你所在企业的职业规划路径图\n\n　　我觉得，一个好的企业，员工是可以知道有多少付出会有多少回报的，员工是可以知道自身的职业发展路径在哪里，员工是可以公平享受企业发展机会的。一年又一年，我给大家分享的第四方面是，我们每一个人，能不能勾画出属于你在所在企业的发展路径图，以华图教育为例，见下图，不展开陈述：\n\n\n\n　　五、结语\n\n　　如果你的人生还有风雨,那是因为你的心灵飞得还不够高。正确的做法是发现使你上升到云层之上的途径，那里的天空永远是碧蓝的。","tags":["个人发展"],"categories":["职业规划"]},{"title":"Cerebral neural plasticity-the underlying mechanism of lifelong growth","url":"/2020-01-01/thinking/Cerebral neural plasticity-the underlying mechanism of lifelong growth/","content":"\n\nIt used to be thought that the brain development would end in late adolescence and early adulthood. After the adulthood, the brain was basically stereotyped, and then it began to go downhill. Scientists now know that the brain still retains the potential for huge changes in adulthood. **This ability is called \"neural plasticity\" and refers to the ability to generate and modify neural connections. Our brains retain neural plasticity for life.**\n\nNeuroplasticity is reflected in the fact that the brain can be changed at any time by external stimuli. **When you practice a certain brain function for a long time, you can generate and strengthen the neural connections responsible for this function. **When you insist on practicing the piano every day, the brain area responsible for finger movements in the brain will grow more nerve fiber connections, and the “terrestrial” of your fingers in the brain will grow larger. The area of ​​the cerebral language cortex responsible for reading and writing English will also grow larger.\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-f70b562713e9e0e8.jpg)\n\nBut if you are occasionally lazy, have n’t practiced the piano for a few days, or have n’t learned English for a few days, the consolidation process of the “Piano Neural Network” or “English Neural Network” just set up in your brain will go on strike and become weaker and weaker. The nerve connections will even be trimmed, and when you regain your piano and English in a few days, you will feel a lot rusty. All in all, our brains can change throughout life and have a positive adaptation to the environment.\n\nAt birth you have almost all the neurons you can have in your life. Neurons will grow a lot of \"small hands\" connected with other neurons, these small hands are called \"synapses\". In the first 15 months of your life, the number of synaptic connections between brain neurons has reached its maximum. In the process, there will be a large number of neurons because nothing can be \"depressed\"-about half of the embryonic neurons will eventually die because they can not establish an effective connection with other neurons.\n\nIn addition, for those neurons that have survived because of their usefulness, their axons (longer synapses) will be surrounded by glial cells. This process is called **myelinization** . Why is the myelin sheath wrapped around the neuron axon? Because neurons in the brain need to transmit information over long distances, such as from the prefrontal lobe behind the forehead to the medial temporal lobe located in the middle of the brain, or from the occipital lobe located in the back of the head to the temporal lobe near the ear. Myelinating of nerve fibers is like wrapping a rubber insulation layer around the wires, which can greatly improve the transmission speed and quality of nerve signals in the brain.\n\nAfter that, the **brain will greatly trim the intricately developed neural connections, just like the construction of newly grown small saplings, trimming the rarely used neural connections, leaving only important, repeated neural connections, Make efficient use of brain energy and material. **The sharp trimming of the nerve fibers' bifurcations continues until the end of puberty.\n\nHow are distant neurons connected to each other? This seems to be a very incredible phenomenon, and scientists have not been able to tell why.\n\nA popular theory so far holds that **distant neurons learn about each other's existence by jointly generating synchronized electrical discharge activities, and reach out to each other's small hands of friendship-neurosynapses, and finally connect together** . However, scientists have not yet been able to understand how neural connections form.\n\nNot only can the brain network be modified for life and actively adapt to the environment, there is an important area in the brain that can generate new neurons throughout life. This magic area is the **hippocampus** .\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-41212d95fa84688d.jpg)\n\nThe hippocampus is the area of ​​the human brain that produces new neurons throughout life. This area is also the center of **spatial memory** formation. For example, when you learn to recognize the way in a new environment, the hippocampus in the brain will be excited to generate new neurons.\n\n**Once the new hippocampal neurons and synapses are integrated into the brain's original neural network, it can improve the brain's spatial memory and even promote further growth in the hippocampus. **A British study measured the brains of taxi drivers in London and found that their hippocampus on average was significantly larger than the hippocampus of ordinary people-thanks to the training on London's complicated roads.\n\nAlthough the brain is plastic throughout life, and can be modified at any time according to environmental changes, many people still find it a bit painful to learn new skills and knowledge. Why is this?\n\nWhen our brain is doing anything, it is almost impossible for a single neuron to complete, but it needs a group of neurons to show periodic activities to complete. This effect is similar to the wave-shaped crowds on the football stadium auditorium **.**\n\n**And the formation process of memory is also formed by the neuron groups of different regions periodically activated synchronously.**\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-08d02e834278dd13.jpg)\n\nSpecifically, distant neurons in the brain are synchronized and activated. This synchronization causes neurons in two places to grow new synapses toward each other because of reasons that some scientists have not yet understood, and eventually magically each other. Connected together to complete the encoding and consolidation of memory. **This process of memory consolidation usually requires repeated activations to achieve. **For example, when learning English words, we usually can't remember them. Instead, we need to memorize a word repeatedly, and remember it ten times or eight times.\n\nIf you feel that learning a certain knowledge skill is unattractive or even a bit painful, it is probably because the knowledge you are trying to learn is far from your actual ability, or your expectations for knowledge are far away. For example, when you learn English, when you first learn English, every word and grammar is a process from scratch. You need to establish a new circuit in your brain.\n\nThe establishment of a new circuit is a difficult process and is often forgotten after learning it. Every time you forget a word or grammar, you can easily feel frustrated, so when you learn English, it is easy to catch fish for three days and two days, and it is more difficult to stick to it. But when you learn English well, your brain's English circuit becomes more stable, and adding more bricks and tiles will become relatively simple. At this time, the difficulty of learning English is reduced, and I feel less painful.\n\nBrain plasticity does decrease with age, but the brain retains some plasticity until old age. As we get older, although our ability to learn new knowledge and adapt to the new environment will decline, but because the plasticity of the brain will always exist, the ability to learn will also always exist, it is completely feasible to live to learn.\n\n![Neural plasticity](https://upload-images.jianshu.io/upload_images/18390058-64cb392d693c2f95.jpg)\n\n**Ten principles of neural plasticity in the brain**\n\nNeuroplasticity indicates that the brain can constantly change during the course of life. For example, the neural circuit that completes a specific task can be moved from one place to another; the thickness of gray matter can be thicker or thinner; the synaptic connections can be stronger Or weakened.\n\nNeuroplasticity is the brain's native ability to adapt to an individual's development and growth, or to rebuild connections through plasticity after a brain injury. The developing brain is more plastic than the adult brain. Even the adult brain is plastic.\n\n**1\\. Use It Or Lose It — \"Use It Or Lose It\"**\n\nNeural circuits (neural connections) that are not actively involved in tasks for long periods of time degenerate. There are countless examples of this. After some musicians stop practicing for a while, the neural circuits responsible for music degenerate due to lack of use. When they play music again, they will feel rusty and will require extra time and attempts to re-strengthen the neural circuit.\n\n**2, the more you use the better — \"Use It and Improve It\"**\n\nTraining that enhances specific brain functions can improve this function. For example, after a stroke patient's body function is impaired, he will likely reduce the use of this side of the body. The physical dysfunction mentioned here comes from the result of damage to the neurons that control this side of the human brain, not from the muscles, ligaments, or bones on this side. In order to restore physical function on this side, a method called **restraint-induced exercise therapy (Cimt) is** often used . This method forces patients to use a paralyzed arm as much as possible by limiting healthy arms. This training can help restore the function of the human brain on the side injured by a stroke.\n\n**3\\. Targeting — \"Specificity\"**\n\nNeuroplasticity is directly related to the nature of the training the brain receives. From a therapeutic perspective, specific specific activities or patterns of movement are very important for forming specific neural circuits. For example, exercises that enhance swallowing may also be related to the brain areas and neural circuits that produce language, but exercises that enhance swallowing may not necessarily produce language.\n\n**4\\. Constant repetition is the best way to strengthen neural circuits — \"Repetition Matters\"**\n\nShaping the neural network through training requires enough repetitions. The challenge for therapists in a rehabilitation setting is often to quantify how long or how long a patient takes to recover certain skills. Whether it ’s the patient himself, a loved one, or an insurance company, they want to know, “How long does it take for patients to get better?” Although doctors do n’t know the exact time, they know that to make these changes, thousands of repetitions training.\n\n**5\\. Intensity — \"Intensity Matters\"**\n\nSufficient strength is required to induce plasticity. Sometimes strength and repetition are the same concept. Studies have also shown that the more intensive the treatment plan, the more likely a person is to achieve results, and the more likely these improvements will continue over time.\n\n**6\\. The sooner the easier — \"Timing Matters\"**\n\nTake rehabilitation as an example, in the process of recovery, different time points have different plasticity. The hypothesis here is that in the early stages of brain trauma, the brain itself has the strongest desire to recover. So the sooner the neurological changes are triggered by training, the better the chances of recovery. Conversely, if the opportunity for early recovery is missed, the brain adapts the trauma in its own way. And this adaptation may not be the way we want to go.\n\n**7\\. Significance — \"Salience Matters\"**\n\nTraining is best to have significant enough visible effects that can enhance plasticity. What did the patient get from training? What does it mean to him? The answers to these questions will affect their training effectiveness. It is important for the therapist to know what is important to the patient, because emotions can change the actual intensity of training and also strengthen memory. If the training effect is obvious and important to the patient, they are more likely to persevere and to remember the skills learned.\n\n**8\\. Age — \"Age Matters\"**\n\nTraining-induced plasticity is more likely to occur in the young brain. The young brain is already more plastic and adaptable than the aging brain.\n\n**9\\. Transferable — “Transference and Generalization”**\n\nAlthough training is generally targeted, the plasticity of the neural network generated for training for one task can improve the ability to complete similar tasks. During the training process, the therapist needs to focus on how a particular skill or activity is promoted or transferred to a real-world activity.\n\nFor example, short-term memory and navigation pathfinding are in the same brain area. The training of short-term memory may also affect the ability to navigate.\n\n**10\\. Compensation and interference — \"Interference\"**\n\nThe plasticity formed by brain compensation can hinder the desired effect of training. For example, when patients are able to perform neurotherapy without treatment, they will compensate for the function of some defects. Compensation is also a response to brain plasticity. Once they learn a different compensation method, it is difficult to change. Even the compensation method is not the best method. Compensation itself is also a manifestation of neural plasticity. It's just that the new changes are not necessarily what we want.\n\n\"Neural plasticity has broken people's inherent concepts and brought new inspirations in various aspects such as rehabilitation, learning and growth. In the past, scientists often thought that the brain structure does not change after the critical period of the baby. In fact, the brain is composed of neurons Cells and glial cells, these cells are interconnected, and by strengthening or weakening these connections, the structure of the brain changes. \"\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-4e2e79dd09fadaa7.jpg)\n\n**Neurons will always grow and change** , which **also shows that our life has infinite possibilities** .\n\nTherefore, \" **change** \" is very important. Only by changing yourself can neurons rejuvenate and grow new neurons. After consolidation, they are different.\n\nIf you don't change, you can only repeat the old neural circuit over and over again, live a repetitive life, and follow the old path as you repeat it again and again.\n","tags":["Cognitive","Neuroscience"],"categories":["Cognitive Neuroscience"]},{"title":"如何挣钱的35条建议","url":"/2019-12-12/How_to_Get_Rich/","content":"\n# 如何挣钱的35条建议\n1. 追寻财富，而不是金钱和地位(Seek Wealth, Not Money or Status) \n\nWealth is assets that earn while you sleep.\n财富是你睡觉时都能挣钱的资产。诸如工厂，机器人，计算机软件，作家的著作，被租赁的房产，被投资进入其它领域的钱等等。\nWealth buys your freedom\n财富的目标就是换取自由，你不必在不喜欢的地方呆着，不必在不喜欢的工作上消磨人生，仅此而已。它并不是让你能够买高级外套，或者开法拉利，或者开游艇，或者周游世界。这些物质享受会让你很快产生厌倦和愚蠢感。财富只是让你成为你自己的主宰。\nMoney is how we transfer wealth\n金钱是可以转换成资产财富的媒介。金钱是一种社会信用，它代表可以使用他人时间的一种能力。 \nStatus is your rank in the social hierarchy\n社会中有两大游戏，几乎每个人都参与其中。一个是金钱游戏，另外一个是身份地位游戏。在地位和财富之间始终上演着一种微妙的竞争，玩身份地位的人，经常会攻击或利用创造财富的人。\n创造财富是一种正和游戏(positive-sum)，你拥有一个房子，不会妨碍我也拥有一个房子。身份地位则是一种零和游戏(zero-sum)，老三要爬到老二的位置，那么老二的位子就必须让出来。\n2. 挣钱和运气无关(Making Money Isn't About Luck)\n\nMaking money isn't about luck. It's about becoming the kind of person who makes moeny.\n挣钱和运气无关，它是关于如何让自己变成注定挣钱的人。\nFour kinds of luck: 1). Blind Luck, 2). Luck from hustling, 3). Luck from preparation, 4). Luck from your unique character\n运气的四种分类：\n撞大运，纯运气\n奋力争取带来的好运，俗语：幸运眷顾勇敢的人，也就是积极行动大量做事后获得好运，经常折腾的人也会有好运。\n积极准备带来的好运，俗语：机会永远只给有准备的人。当你在某个领域积累很深，当大多数对这个领域不熟的人还没有觉察到的时候，你可以提前洞察这个领域的机会。\n独特个性带来的好运，你塑造了一个独特的个性，独特的品牌，独特的思维模式，然后运气就盯上了你(运气变成确定的事)。\nIn 1,000 parallel universes, you want to be wealthy in 999 of them\n在1000个平行宇宙中，你应该期望在其中的999个都是富有的，而不是只在其中的50个，靠运气才富有。通过追寻第四种运气，你可以消除运气因素。\n3. 让运气变成你的命运(Make Luck Your Destiny)\n\nBuild your character in a way so luck becomes deterministic.\n以特定方式塑造你的个性，让运气成为注定的事\nBuild your character so opportunity finds you\n塑造你的独特个性(专业，可信赖，正直诚实，勇于担责，具有长线思维)，让机会自己来找你\nYou have to be a little eccentric to be out on the frontier by yourself.\n第四种运气大都源于古怪的行事方式。想要非常收获，你必须要有非常表现。世界是一个高效场所，所有明显的地方都已经被挖掘过，为了找到新奇的尚未发现的事物，你必须要以非常规方式行事，你要有强烈意愿和兴趣，要比别人挖得更深，深到让人感觉非理性的地步。\nExtreme people get extreme results ~ Sam Altman\n极端的人获得极端的结果\nYou can't be normal and expect abnormal returns ~ Jeffrey Pfeffer\n你不可能表现平凡，但却期望不平凡的回报\nPlay stupid games win stupid prizes\n玩愚蠢的游戏，只会赢得愚蠢的奖励。\n4. 通过出租时间的方式，你不太可能致富(You Won't Get Rich Renting Out Your Time)\n\nYou won't get rich renting out your time, because you can't earn non-linearly.\n通过出租时间(比如打工)的方式，你不太可能致富，因为你的输入(时薪)和输出(工作产出)高度相关，你无法以非线性方式挣钱。\nRenting out your time means you're essentially replacable.\n出租你的时间意味着你根本上是可以被替代的。大部分输入和输出高度相关的工作，会逐步被机器人或AI所取代。\nYou must own equity to gain your financial freedom.\n你必须拥有资产(生意的一部分，如产品，生意，知识产权，股票期权等)，才可能以非线性方式挣钱，并赢得财务自由。\nYou want a career where your inputs don't match your outputs.\n你需要选择输入和输出不严格相关的，能以非线性方式挣钱的行业。要找高度创新，能够利用工具和杠杆的行业。\n5. 通过量入为出获得自由(Live Below Your Means for Freedom)\n\nPeople living far below their means enjoy a freedom that people busy upgrading their lifestyles just can't fathom\n量入为出的人能够体会到的自由，是那些忙于升级生活方式的人所无法理解的。\nThe most dangerous things are heroin and a monthly salary\n最危险的东西是海洛因和月光族。真正的致富之道是甘于朴素的生活+持续不断的努力积累。\nIdeally, you'll make your money in discret lumps\n以逐步增量的方式挣钱(而非暴发户)是最理想的。财富来得太快去得也快。\n\n6. 为社会创造它想要但是还不知道如何获取的东西(Give Society What It Doesn't Know How to Get)\n\nSociety will pay you for creating what it wants, but doen't know how to get, and delivering it at scale.\n如果你能够规模化创造和交付社会想要，但是还不知道如何获得的东西，那么社会就会回报你。\nFigure out what product you can provide and then figure out how to scale it.\n基于你的特质/能力，思考你能够提供什么产品，然后思考如何能够规模化地交付这种产品。\nEntrepreneur's job is to try to bring the high end to the mass market\n企业家的工作就将高端产品普及大众化。创造新事物 -> 预测社会需要这个新事物 -> 规模化生产 -> 普及到大众 -> 可盈利可持续\n7. 互联网极大扩展了职业的可能性(The Internet Has Massively Broadened Career Possibilities)\n\nThe Internet has massively broadened the possible space of careers, by allowing you to scale any niche obsession.\n互联网极大的扩展了可能的职业空间，它让你可以规模化任何你擅长和痴迷的细分市场。每个人都独一无二，互联网连接每一个人，通过它，你可以为你的产品/天赋/技能，找到足够听众，不管距离多么遥远。\nEscape competition through authenticity. No one can compete with you on being you.\n通过真诚避开竞争。不要模仿，不要复制，每个人都不同，每个人都各有所长，做你自己擅长的事，没有人可以和做自己的人竞争。\n8. 和具有长线思维的人玩长线游戏(Play Long-term Games With Long-term People)\n\nPick an industry where you can play long-term games with long-term people. All returns in life come from compound interest over many turns of the game.\n选择一个你能够和具有长线思维的人玩长线游戏的行业。生命中所有的回报(关系/挣钱/学习)都来自于反复游戏后产生的复利效应。\nWhen you switch industries, you're starting over from scratch\n如果你频繁换行业，那们你就要每次从零开始。\nLong-term players make each other rich. In short-term game, it seems like everybody is making themselves rich.\n长期玩家让彼此都致富，短期玩家只顾自己挣钱。\n9. 选择聪明，精力充沛和正直的合伙人(Pick Partners With Intelligence, Energy and Integrity)\n\nPicking partners with high intelligence, energy and integrity is the three-part checklist that you can't compromise on.\n选择合伙人如下三点是你不能将就的：\n聪明，保证做事的方向正确，勤奋的蠢人很多\n精力充沛，聪明的懒人也非常多\n正直，这点最重要，没有这点上面两点归零！\nMotivation has to come intrinsically. If you're trying to keep someone motivated for the long-term, that motivation has to come intrinsically.\n必须是自激励和自驱动的人。如果你想让某人对某事长期处于激励状态，那么这种激励必须是发自内在的。\nIntegrity is what someone does, despite what they say they do.\n考察一个人是否正直，主要看他真正做了什么，而不是他说了什么。注意细微的细节(subtle signals)，当他认为周围没人在看的时候，他的所言所行。\nPeople are oddly consistent.\n虽然可以短期隐瞒，但人的本性极具有一致性。如果一个人在餐厅对服务员不礼貌，那么他对你不礼貌只是时间问题。如果一个人有报复敌人倾向，那么他把你从朋友重新定义为敌人只是时间问题，你早晚会感受到。\n\nStatus Signalling: If you overtly bid for status, if you overtly talk about being high status, that is a low status move.\n如果你过度看重和强调身份地位，你越可能内心自卑。你越讲自己诚实/可靠/正直，情况越可能相反。已有的实在的东西没必要强调，缺乏的虚的东西才需要强调。\n\n10. 和理性乐观者为伍(Partner With Rational Optimists)\n\nDon't partner with cynics and pessimists. Their beliefs are self-fulfilling.\n不要和愤世嫉俗和悲观者为伍，他们的信条是自证预言。自证预言者：如果别人失败了，他会说我早就预言会失败，如果别人成功了，他会说这是撞大运。\n\nEither lead, follow, or get out of the way.\n要么领导大家，要么跟随领导，要么让开。愤世嫉俗和悲观者，他们不想领导，不想跟随，也不想让开，他们只是无休止散布悲观言论。所有的成功人士都是行动导向的，判断某事是否可行的最简单方式就是行动。\n\nPartner with rational optimists.\n乐观但是要理性，要清楚知道事物的艰难和不利面，但是依然乐观前行。\n\n11. 用特长知识武装自己(Arm Yourself With Specific Knowledge)\n\nArm yourself with specific knowledge. It can't be trained but it can be found by pursuing your genuine curiosity.\n用特长知识武装自己。特长知识无法通过培训获得，它只能通过追寻你的单纯的好奇而获得。对特长领域的好奇可能源于先天基因，也可能在儿童时期养成，它构成你后续职业的核心竞争。特长知识几乎无法后天培养，大致20岁以后，个人的特长领域已经初步显现，你需要意识到自己的特长领域，并基于它构建你的职业。很多时候，个人其实并不能清楚认识到自己的特长领域，周围亲近的人反而更清楚。特长知识无法通过培训简单获得，如果社会能够培训你，那么它也能培训其他人，然后取代你。\n12. 特长知识具有高度创新和技术性(Specific Knowledge Is Highly Creative or Technical)\n\nSpecific knowledge tends to be creative or technical. It's on the bleeding edge of technology, art and communication.\n特长知识具有高度创新和技术性。它存在于技术/艺术/沟通的前沿。\n\nSpecific knowledge can be taught through apprenticeships.\n通过学徒关系可以获得特长知识。巴菲特(Warren Buffett)和格雷厄姆(Benjamin Graham)是一个典型例子。\n\nSpecific knowledge is highly specific to the situation, it’s specific to the individual, it’s specific to the problem, and it can only be built as part of a larger obsession, interest, and time spent in that domain.\n特长知识和情景/个体/问题领域高度相关。它只能通过对某个领域的痴迷/兴趣和大量时间的投入才能获得。简单读一本书，上一门课无法获取特长知识，特长知识也无法被编程为一个算法(无法自动化)。\n\nYou can't be too deliberate about assembling specific knowledge. Build specific knowledge where you are a natural。\n不要刻意追求特长知识(过于目标导向)，找到你内在真正擅长和喜欢的事(你天生就是干这个的料)，然后基于此构建特长知识。\n\n13. 学习销售，学习制造(Learn to Sell, Learn to Build)\n\nLearn to sell. Learn to build. If you can do both, you will be unstoppable.\n学习销售，学习制造。如果你两样都会，你将不可阻挡。制造范畴：开发，制造，物流，采购，设计和运营服务。销售范畴：售卖，市场，沟通，招聘，筹钱，激励员工，公关\n\nThe Silicon Valley model is a builder and seller\n硅谷模式=世界级销售+世界级制造。例子，苹果创始人Steve Jobs + Steve Wozniak，微软创始人Gates + Allen，谷歌创始人Larry + Sergey。CEO + CTO组合。\n\nIf you can do both you will be unstoppable\n两者兼备可创造整个行业。例子埃隆·马斯克(Elon Musk)，史蒂夫·乔布斯(Steve Jobs), 拉里·埃里森(Larry Ellison/Oracle)。。。\n\nI'd rather teach an engineer marketing than a marketer engineering\n制造者容易转行为销售者(前提沟通能力好)，销售者不易转成制造者。职业早期建议从制造者开始(打下基础)，后期可转行为销售者。制造者需要大量聚焦时间的投入，而且新人新产品始终会推陈出新，所以制造技能难以规模化和保持前沿，销售技能则长期更易于规模化。\n\n14. 从读你喜欢的书开始，一直到你喜欢上阅读(Read What You Love Until You Love to Read)\n\n培养大量阅读和终身学习习惯。早期要读原创经典(基础+第一性原理)，以后各类学科都要广泛涉猎。\n\n15. 基础是数学和逻辑(The Foundations Are Math and Logic)\n\n现代最重要五项基础技能：阅读，写作，算术，沟通(说服力)，计算机编程。\n\n16. 没有所谓“商业\"技能(There's No Actual Skill Called \"Business\")\n\n通过实践锻炼你的做生意能力，而非纯理论学习。\n\nThe number of \"doing\" iterations drives the learning curve.\n试错迭代才能快速驱动学习曲线，迭代是不断尝试新的方法，而非重复劳动。\n\nIf you'are willing to bleed a little every day, you may win big later\n普通大众期望每天能挣一点，企业家则相反，他们愿意承受每天出一点血(比如损失金钱)，但最终可能会赢得更多。\n\n17. 通过勇于承担风险来获得杠杆(Embrace Accountability to Get Leverage)\n\nEmbrace accountability. Society will reward you with leverage.\n致富需要杠杆(人力/资金等)。只有建立勇于承担责任和风险的信誉，社会才会回报杠杆予你。\n\nPeople who can fail in public have a lot of power.\n敢于公开承担失败的人其实非常强大。\n\n18. 通过勇于承担风险来赢得资产(Take Accountability to Earn Equity)\n\nIf you have high accountability, you're less replaceable and you can get a piece of the business.\n如果你具有勇于承担风险的信誉，别人才会觉得你不可替代，才会把生意/资产交给你。位置越高，承担风险越大，同样受益也可能越大。船要沉没的时候，船长必须最后一个离开。公司破产的时候，员工最先得到工资补偿，然后是银行，最后才是资产投资人(风险最大，收益也可能越大)。\n\nAccountability is reputational skin in the game\nskin in the game是风险共担意思，来自黑天鹅的作者塔勒布的新书《非对称风险》。回报要和风险承担成正比，敢于压上你的声誉。\n\n19. 劳动力和资本是老式杠杆(Labor and Capital Are Old Leverage)\n\nWealth requires leverage. Labor and capital are older forms of leverage that everyone is fighting for.\n获得财富需要利用杠杆。劳动力和资本是几乎每个人都在竞争的老式杠杆。\n\nSociety overvalues labor leverage\n当前社会过度看重劳动力杠杆。看职位高低，就看带多少人，看公司实力，就看公司有多少人，实际上看的是能用多大的人力杠杆。\n\nYou want the minimum amount of labor that allows you to use the other forms of leverage.\n劳动力杠杆的劣势：管人并不容易，需要很强领导管理技能；劳动力杠杆竞争非常激烈，一不小心会成为政治斗争或政变中的牺牲品(历史上资本和劳动力之间的斗争从未停止)。所以应该尽量减少劳动力杠杆的使用，你只需少量劳动力，能够支持你使用其它形式的杠杆即可。\n\nCapital has been the dominant form of leverage in the last century.\n资本是上个世纪以来的一种主要的杠杆形式，少数人靠它获得巨量财富，大部分甚至还不懂如何利用它。资本是一种很强大的杠杆，可以转换成其它形式的杠杆(比如劳动力)，资本也易于规模化。运用好资本需要好的资本管理能力和分析技能。\n\nYou need specific knowledge and accountability to obtain capital\n你需要有特长知识和良好的信誉记录，社会才会放心把资本交到你手，以此作为杠杆，让你帮忙挣取更多资本。\n\n20. 产品和媒体是新杠杆(Product and Media are New Leverage)\n\nProduct and media are the leverage of new wealth. Create software and media that work for you while you sleep.\n产品和媒体几乎没有边际复制成本，是新财富的杠杆。从印刷出版开始 -> 到广播电视媒体开始加速 -> 互联网+代码大爆发。现在，你甚至可以不靠(或只是少量依靠)人力和资本杠杆，就可以放大你的努力。\n\nProduct leverage is where the new fortunes are made\n上一代财富主要靠资本杠杆，典型例子是巴菲特。新一代的财富主要靠代码和媒体杠杆，典型例子杰夫·贝佐斯(亚马逊创始人)，马克·艾略特·扎克伯格(Facebook创始人)，拉里·佩奇(谷歌创始人)，比尔·盖茨(微软创始人)，斯蒂夫·乔布斯(苹果创始人)。\n\nCombining all three forms of leverage is a magic combination\n劳动力杠杆：工程师+设计师+产品开发。\n资本杠杆: 市场，广告，规模化\n代码+媒体杠杆\n三者结合可以产生巨大杠杆效应。\n\nProduct and media leverage are permissionless\n对于劳动力杠杆，别人要愿意跟你才行。对于资本杠杆，要有人愿意投资给你才行。编程/写书/录制播客视频/写博客，所有这些都是不需要许可的，所以说它们是最公平的杠杆。\n\n21. 产品杠杆人人平等(Product Leverage is Egalitarian)\n\nLabor and capital are limited to the people who control those resources. But products reach global markets.\n劳动力和资本仅限于控制这些资源的人，但是产品可以触达全球市场。产品杠杆是一种正和游戏，如果你在乎要以道德的方式获取财富，你最好使用代码和媒体来创造财富，因为这类产品任何人都可以使用(具有平等属性)，而劳动力和资本则只有少数人能够使用(不平等)。\n\n22. 寻找一个能够利用杠杆的生意(Pick a Business Model With Leverage)\n\nIdeally, you should pick a business model with network effects, low marginal costs and scale economies.\n你最好选择一个具有网络效应的业务，低边际成本，并且是能够经济地规模化。\n\nScale economies: the more you produce, the cheaper it gets\n规模经济：生产越多，越便宜。造第12个产品比造第5个产品便宜，造第10000个产品比造前一个要便宜很多，这种方式易于商品化，并且能够自动给竞争者制造障碍。\n\nZero marginal cost of reproduction: producing more is free\n零边际复制成本：生产更多是免费的，典型例子是媒体产品。\n\nNetwork effects: value grows as the square of the customers. Network effect business are natual monopolies.\n网络效益：价值以客户的平方增长。典型例子Facebook/Uber/Google/Twitter/YouTube。具有网络效应的业务具有天然垄断(natual monopolies)和赢者通吃(winner-take-all)特性。语言具有网络效应，未来世界可能只有英语和中文两种语言。货币也具有网络效应，未来世界可能只有一种储备货币(目前是美元)。\n\nZero marginal cost business can pivot into network effect business.\n零边际成本的产品->易于规模化->易于产生网络效益->每增加一个用户都会增加整个网络的价值。网络效应是终极杠杆，你选择业务模式的时候一定要思考每增加一个客户，客户之间如何彼此增加价值，然后你躺在海滩上旅游的时候，客户都会自动增加价值。\n\n23. 从劳工到企业家的案例(Example: From Laborer to Entrepreneur)\n\nThe continuum from laborer to real estate tech company goes from low to high specific knowledge, accountability and leverage.\n从建筑工人到房地产技术公司的案例，展示特长知识、职责风险承担和杠杆，从低到高的发展历程。\n\nLaborers get paid hourly and have low accountability\n底层是建筑工人/水电工等，他们按小时计薪，但承担职责风险最小，社会地位低。他们需要掌握的特长知识很少，除了手头工具没有多少杠杆可用。\n\nGeneral contractors get equity, but they're also taking risk.\n第二层是总承包商，他们拥有资产，可以利用人力杠杆(承包工队)，可以获得项目收入的大头，但同时承担职责和风险也更多，如果项目失败，他们也要承担大头。\n\nProperty developers pocket the profit by applying capital leverage.\n第三层是房地产开发商，他们利用资本杠杆获取利润。经验丰富的承包商可能转型为房地产开发商，他们发展出了地产领域的商业敏锐性（知道哪块地皮会增值，政策经济层面因素等)，然后他们有能力找到资本投资(自己也可投资)。这层需要更多特长知识，风险承担和利用资本杠杆的能力。\n\nArchitects, large developers and REITs are even higher in the stack.\n建筑师，大开发商由于之前的成功带来的声誉，会让他们更加增值。也有部分转做房地产投资信托，他们同时熟悉房地产和金融/资本市场，他们不用去真正开发房地产，也不用管理很多人，他们可以充分利用资本杠杆。\n\nReal estate tech companies apply the maximum leverage.\n最上层是房地产技术公司(类似美国Trulia/RedFin/Zillow这样的公司)，他们可以最大程度利用杠杆。需要同时具备房地产、技术、投资领域的特长知识(一般需要互补团队)，承担职责和风险很高，可以利用的杠杆最大，同时潜在收益也可能最高。\n\n24. 判断就是决断技能(Judgment Is the Decisive Skill)\n\nIn an age of infinite leverage, judgment becomes the most important skill。Leverage is a force multipler for your judgment.\n在杠杆几乎无限的时代，判断力成为最重要的技能。判断力是基础，杠杆则是判断力的倍增器。职业早期你忙于追逐杠杆，一旦获得杠杆，你需要歇一歇，因为这个时候判断力就更加重要了(因为风险也更大了)。巴菲特如此富有主要是因为他的判断力，即便你把他的钱全部拿走，明天投资者仍然会給他1千亿，因为他的判断力还在。\n\nWithout experience, judgment is often less than useless.\n光有高智商还不够，判断力源于快速迭代+实战体验。需要切身投入和切肤之痛(skin in the game)才会产生真正的判断力。\n\nThe people with the best judgment are among the least emotional\n具有最好判断力的人是最不情绪化的。很多最好的投资人/企业家是近乎毫无情绪的机器人。情绪是真正阻碍你看清事物真相的东西。\n\nThe more outraged someone is, the worse their judgment.\n一个人越愤怒，他们的判断就越糟糕。\n\n25. 给自己设定一个超高时薪(Set an Aspirational Hourly Rate)\n\nIf outsourcing a task will cost less than your hourly rate, outsource it.\n如果外包某项工作所需费用比你的时薪少，那么就外包。对于大部分要花时间的事情，都要和你的时薪比较，再决定是否自己去做。如果致富你是的首要目标，那么你的主要时间都应该投在这件事情上。花费时间和别人争吵，买错小东西自己亲自去退货，都是浪费时间的蠢事。\n\nYou can't penny pinch your way to wealth\n对于基本的生活所需，你可以节俭，你也可以始终保持低开销，但是对于致富这件事情，你无法通过吝啬来实现。\n\nMy aspirational rate was $5,000/hr\n我(原文作者)给自己设定的超高时薪是每小时5000美金。如果你给自己设定的超高时薪，看起来还没有到荒谬的程度，那么你设得还不够高。\n\nYou should be working on your product and getting product-market fit. And you should be exercising and eating healthy. That's all you have time for while you're on wealth creation mission. ~ Paul Graham(保罗·格雷厄姆)，美国著名程序员、风险投资家、博客和技术作家。\n创业者只应该关注和花费时间在：产品，产品和市场契合，锻炼，健康饮食。\n\n26. 奋力工作(Work As hard As You Can)\n\nWork as hard as you can. Even Though what you work on and who you work with are more important.\n如果致富是你的目标(如果只是朝九晚五的上班则另当别论)，那么你必须奋力工作，同时要清楚意识到，做什么工作，和谁一起工作，比努力本身更重要。\n\n正确的工作领域最重要，选择职业或者做生意，先要弄清楚做什么，有没有市场，我能够开发出什么产品，能否发挥我的特长知识，我有没有兴趣能否持续投入？\n其次是和正确的人(聪明/精力充沛/正直)，和优秀的人一起工作，优秀的标准也要超高(now matter how high your bar is, raise your bar)。\n最后才是努力工作。\nNobody really works 80 hours a week\n没有人能真正工作80甚至120小时，这样说的人无非是在身份炫耀。没有人能持续工作80甚至120小时，同时还保持高产出和头脑清醒，你的脑袋会迟钝，灵感缺失。\n\n在知识领域的高效工作方式是：在灵感和兴趣来的时候，像冲刺一样奋力工作一段时间，然后休息一段更长的时间。这种方式更像猎捕狮子，而不是像马拉松，但长期看，致富的过程是一系列冲刺组成的一个马拉松。\n\nInspiration is perishable\n灵感容易腐烂。灵感稍纵即逝，灵感点子来的时候要及时抓住，及时动手去做。比如脑中突然来了写博文的灵感，马上动手去写，如果延迟或迟疑，它可能很快消失。\n\nImpatience with actions, patience with results.\n对行动可以没有耐性(积极行动)，但是对结果要有耐心。灵感来的时候，抓住行动，问题来的时候，不睡觉也要解决。但是产品被市场接受需要很长时间，与人合作磨合也需要很长时间，伟大的产品诞生需要不断的打磨，打磨，再打磨。\n\n27. 你应该足够忙，没有多少时间去社交(Be Too Busy to \"Do Coffee\")\n\nYou should be too busy to \"do coffee\", while still keeping an uncluttered calendar\n你应该足够忙，没有多少时间去社交，同时日程表最好空白，换句话说，你应该专注自己的要事，没有时间社交和开会。\n\n在职业早期的探索(exploring)阶段，你可以参与一些社交和建立一些关系。在职业的中后期利用(exploiting)阶段，你有更重要的事情要做，你必须无情地将会议从你的生活中剔除。\n\n如果某人要和你开会，你就问能否用电话代替。如果某人要和你电话沟通，你就问能否用电子邮件代替。如果某人想要发电子邮件给你，你就问能否用短信代替，实际上到这一步，大部分短信都是可以忽略的，除非是真正紧急的事情。\n\n你应该无情地拒绝会议。如果真的要开会，就边走边谈，或站着谈。让会谈保持简短，保持行动导向(actionable)。大部分超过8个人的圆桌会议，不会有任何产出，你基本上浪费至少一个小时。\n\nPeople will meet with you when you have proof of work\n如果你手头真有重要和有价值的产品，可以考虑找合适的人会谈合作。找重量级投资人谈，你必须先有工作证明(proof of work，区块链术语)，也就是你实际开发的产品或者项目进度，而不是ppt，更不是脑袋里的想法。\n\nNetworking is overrated even early in your career\n即便在职业早期，社交的作用也被夸大了。通过社交手段建立人脉获得机会，表面上看如此，实际这种机会顶多是第一(撞大运)和第二类(奋力争取带来的好运)。你应该把重点放在第三(积极准备带来的好运)和第四类(独特个性带来的好运)，专注建立声誉，开发产品，建立独特视角，善于发掘机会(在别人还没看到时)。\n\n忙碌的日程表和忙碌的脑瓜，做不了伟大的事情。自由的时间和思考，才可能。\n\n28. 不断重新定义你的工作(Keep Redefing What You Do)\n\nBecome the best in the world at what you do. Keep redefining what you do until this is true.\n在你专注的工作上做到世界最佳，不断重新定义和打磨你的工作，直到世界最佳成真。这个工作必须和你的特长知识/技能/职位/能力/位置/兴趣相匹配(遵循自己的内心，being authentic to yourself)，探索之旅会很漫长，但你始终要意识这点。\n\nFind founder-product-market fit\n企业家最重要的事：找到有市场的产品，而且你天生擅长干这事，三者缺一不可。\n\n29. 通过做自己避开竞争(Escape Competition Through Authenticity)\n\nCompetition will trap you in a lesser game\n受到社会上身份地位游戏的影响，我们容易追逐模仿而迷失自我。如果周围的人都是成功的商务人士，我也要成为商务人士。如果我周围的人都是成功的社会活动家，我也要成为社会活动家。如果我周围的人都是成功的架构师，我也要成为架构师。实际上这种追逐模仿更多是一种零和竞争游戏。\n\nNo one can complete with you on being you\n没有人能够和做自己的人竞争，做你最擅长的事，基于你的特长知识打造你的核心竞争。同时也要注意产品和市场匹配问题(product-market fit)，如果真实做自己，但是市场不大，你也要注意调整。\n\nIn entrepreneurship, the masses are never right\n从企业视角看，大众判断往往不正确。如果大众判断正确，那么大家早就发财致富了。如果大众都在谈论某个领域或产品，那么恰恰说明这个领域的竞争趋于饱和，已经没有多少机会了。相反，如果没人谈论某个领域，则可能这个领域的机会也不大。企业家需要在两者之间找到平衡。大部分人倾向模仿追逐热点，伟大的企业家倾向真实做自己。\n\nCombine your vocation and avocation\n最好将自己的事业和爱好结合起来，这样的人更容易做自己。经过不断尝试，很多人最终会找到自己最擅长的事业。\n\n30. 玩愚蠢的游戏，赢愚蠢的奖励(Play Stupid Games, Win Stupid Prizes)\n\nCompetition will blind you to greater games. You're one step away from a better market.\n热衷模仿竞争会蒙蔽你的双眼，陷入零和游戏，让你迷失自我。真实面对自己和客户才能找到更好市场。\n\n31. 最终你会获得你应得的（Eventually You Will Get What You Deserve）\n\nApply specific knowledge with leverge and eventually you will get what you deserve.\n如果满足特长知识，责任心，利用杠杆和真实做自己这些先决条件，那么从足够长的时间范围来看，你最终会获得回报。这个通常需要十年甚至二十年，也有快的三五年的，但是这种是例外。期间你会经历很多失败，但在企业界，有时往往做对一次就够了。\n\nWhat are you really good at that the market values?\n致富要素：\n特长知识的稀缺性\n能够利用多大的杠杆\n判断力的准确度\n责任心强度\n所做事情的社会价值多大(product market fit)\n持续投入时间+持续学习改进\n核心是：你擅长(特长知识)+社会需要(product market fit)，其它自然会来。\n32. 拒绝大部分建议(Reject Most Advice)\n\nMost advice is people giving you their winning lottery ticket numbers. The best founders listen to everyone but make up their own mind.\n如果你问那些速成人士是如何获得成功的，他们可能只是告诉你他们中奖的彩票号码(也就是说，你得到的建议往往只适用于特定的人和特定上下文环境)，可能完全不适用于你和你的环境。你应该关注系统，而非单一目标，也就是什么的系统或者说环境，才促成了某些人的成功。盲目生搬硬套只会适得其反。\n\n真正的创始人会聆听所有人的视角和建议，但是在最后做决策时，他会忽略所有人，而是根据自己的系统和上下文做决定。\n\nAdvice is maxims you can recall later, when you get your own experience.\n所谓建议可以认为是一种格言，也就是你有了自己体验之后，你能回忆起来并能对上号的格言。本文的35条建议也是35条格言，我(原文作者)碰到问题时也时常会回顾这些格言，作为我的处事指导，比如是否要和某人一起共事，如果我不能和他一起共事10年(长线游戏)，那么我何必要和他共事一天？如果你看过建议以后有体验共鸣，那么对你就要价值；如果你无感，那么请忽略，继续做自己的事就好了。\n\n33. 平和的内心，健康的身体和充满爱的家庭(A Calm Mind, a Fit Body, a House Full of Love)\n\nWhen you're finally wealthy, you'll realize it wasn't what you were seeking in the first place.\n当你某天最终变得富有，你会意识到这其实并不是你最初的追求。钱只能解决你钱的问题(获得一定的物质世界的自由度)，但是内心的平静，健康的身体，和谐有爱的家庭，这些靠钱买不来，这些也要靠你去挣取，不可偏废。\n\n34. 致富没有捷径(There Are No Get Rich Quick Schemes)\n\nGet rich quick schemes are just someone else getting rich off you.\n所谓的致富速成，往往是别人想从你身上挣钱的把戏。世界是一个高效之地，如果有容易挣钱的地方，那么这个地方早就被探索和利用过了。\n\n另一方面，作为专家或成功人士，可以给其他人一些(高质量和可执行的)致富建议，同时要坦承过程漫长且艰难，否则会毁掉自己的声誉。\n\n35. 将自己产品化(Productize Yourself)\n\nFigure out what you're uniquely good at and apply as much leverage as possible.\n找到你擅长的事业，并尽可能利用杠杆。Productize Yourself，把独特的你和你的特长知识进行产品化，勇于承担风险，真实面对自己，充分利用杠杆规模化你的产品。\n\nMaking money isn't even something you do, it's not a skill. It's who you are.\n做最真实的自己，挣钱并不是关于你做的事情，也不是一种技能，而是你到底是谁的问题。\n\nFind hobbies that make you rich, fit and creative\n挣钱是一个函数，输入是你的身份和你的爱好。找到你的三个爱好，一个让你挣钱，一个让你健康，一个让你富有创造力。\n\n附录\n\nHow to Get Rich: Every Episode https://nav.al/how-to-get-rich\n\nAngelList https://angel.co/\n\nHow to get rich(without getting lucky) https://twitter.com/naval/status/1002103360646823936","tags":["文摘","rich"],"categories":["财富自由"]},{"title":"2017年度中国互联网黑产报告","url":"/2019-12-05/black_production/","content":"\n**概述**\n\n2017年，纵观全球网络安全事件，从黑客组织Shadow Brokers泄露NSA的漏洞利用工具EternalBlue，到WannaCry勒索软件席卷全球，从国内58同城简历数据泄露，到国外信用机构Equifax被黑客入侵，黑灰产业蓬勃发展。\n\n只有事件爆发后才能察觉问题，这使得企业和用户的处境十分被动。企业对于黑产的行为逻辑、行动方式、利益和目的等都十分陌生。威胁猎人将根据平台第一线的攻击数据和深入访问调查的黑灰产现状，为大家揭开黑灰产的面纱。\n\n**目录**\n\n概述\n\n一、黑灰产事件举例分析\n\n1、东鹏特饮薅羊毛事件\n\n2、苹果36\n\n3、滴滴虚假注册\n\n4、Uber被黑客勒索\n\n5、教材涉黄案\n\n二、产业链分析\n\n1、上游资源提供者\n\na）黑卡\n\nb）黑IP\n\nc）账号\n\nd）账户认证\n\n2、下游变现细分产业\n\na）流量欺诈\n\nb）数据爬取采集\n\nc）薅羊毛\n\nd）引流\n\n三、对抗升级\n\n1、主流防控措施和黑产绕过方法\n\n2、新风控角度的思考\n\na）黑产大数据监控\n\nb）情报带来的针对性对抗\n\n结语\n\n**一、黑灰产事件举例分析**\n\n**1、东鹏特饮薅羊毛事件**\n\n营销活动大家都不陌生，通过奖励机制吸引用户。不过同时也会吸引来一群叫做“羊毛党”的人，他们依靠注册大量账号获取优惠券、争抢红包、奖品，再通过转卖等方式变现。大促、补贴、营销活动都是他们眼中一次次“捞钱”的机会，被叫做线报。\n\n缺乏业务安全意识、补贴又丰厚的活动是最容易被薅的。东鹏特饮是广东一家饮料公司，传统促销活动是瓶盖抽奖，随着互联网的普及，决定尝试新的方式——扫二维码领红包，想借力互联网省去繁琐的流转，顺便收集顾客信息，不料羊毛党却给了他们当头一棒。\n\n随着活动的升温，迅速出现了大量贩卖东鹏特饮CDK（码子）的人。所谓码子就是将活动二维码转换成的链接。购买码子后用微信点击便可以领取红包。渠道商和羊毛党手中的微信账号有限，但码却很多，他们以略低于最低额度红包的价格售卖，购买者也是稳赚不赔。\n\n而购买CDK的是普通用户吗，只能说比例太少，普通用户哪有渠道知道CDK的存在，大多是手中拥有很多微信账户的其他灰产从业人。他们平时的业务是用微信号加大量好友，再通过诈骗、微商等形式变现。东鹏特饮CDK只是顺便的行为之一罢了。\n\n总之在利益的促使下，迅速有人与废品回收站核心节点合作，低价大量收购瓶盖，提取二维码信息，市场上称为“废品码”，与之对应的是“必中码”，是打通关系后从生产瓶盖厂商、内部人员等处购买的，将二维码一键生成链接，转手卖给渠道商，渠道商再分发给各级下线，一套流程下来，层层都有利润，做活动的企业就成了冤大头。最终结果就是东鹏特饮发现实际兑换的奖金金额远远高于预期，而收获的只是营销效果为0的“僵尸用户”。\n\n不只是东鹏特饮，这类码子在市场上非常之多，蒙牛优益C、蒙牛冰淇淋、百事可乐、红牛、七喜、小茗、京东二维码等等，数不胜数。当活动发展到一定规模，下游还会有人以“收学费带赚钱”的形式大肆传播，整个过程犹如蝗虫过境，吃光企业的活动经费。\n\n羊毛党的基本行动方式就是以量取胜，用大量账号暴力争抢活动补贴、奖品，如新用户折扣券，然后转手低价卖出。事实上他们只是互联网黑色产业链的变现末端之一，有些直接称其为搬砖人，因其技术要求低，纯粹是体力活。\n\n他们的账号来源、行动模式都值得我们注意。比如瓜分新用户礼券的注册手机号从何而来？答案是手机黑卡。威胁猎人收集维护了海量数据的黑卡库，在下文产业链分析中会做出详细介绍。除去手机号，羊毛党作恶需要通过平台的IP、设备等检测，这些在黑产中都有着平台化、链条化的产业，羊毛党仅仅是它们的下游之一。详细产业链分析请参考上游资源提供者模块。\n\n**2、苹果36**\n\n同样遭遇薅羊毛的还有苹果。用户在iOS上消费后，苹果公司会按照比例与app服务提供方进行分账，以季度结算。结算时，大量商户发现苹果的分成和实际销售金额相差甚远。在查看之下，发现了真实原因：被薅。\n\n一些账户进行了6元和30元的小额消费后立即消失了，存在批量痕迹。原来苹果为了提升用户体验，设置了40元以下小额充值可以不验证，先派发商品的策略。对黑产来说，此举意味着每个小号36元的利润，立刻展开了行动。\n\n他们会首先通过脚本批量注册大量邮箱账号。国外一些邮箱注册不需要提供手机号，这一步操作几乎是“无成本”的。完成后，会利用软件，批量生成Apple ID，再批量激活。大部分厂商会在IP短时间注册量上进行判断，对黑产来说这一步的成本就是更换IP的成本。对此威胁猎人会在下述产业链部分详细阐述黑产逃过IP检测的方法。\n\n消费需要绑定银行卡，对于大量的银行卡需求，黑产的解决方案是家庭共享和注册虚拟银行卡。设置家庭共享后，每个账号可以有8个附属账号共享同一张银行卡，而这张银行卡是一张虚拟卡，当黑产持有一张银行卡后，可以线上向开卡行申请虚拟银行卡，卡号会和原卡不同，但都是属于同一个账户。\n\n当苹果发现盗刷行为会对该账号封号，当多个附属账号被封后，苹果会将主账号与其绑定的银行卡列入黑名单，这时，黑产会将虚拟卡注销，重新申请，完全不影响继续使用。苹果也会对设备进行检测，这时黑产会结合改机软件，在被锁机前刷新设备指纹，轻松解决。\n\n薅羊毛后，黑产就会利用低价优势，通过各种渠道销售虚拟商品进行变现。游戏和版权行业是受害的重灾区。\n\n针对36技术，苹果进行了策略调整，新注册用户限制使用先派发后收款的模式。然而此举对黑产来说只是提高了一点成本，还在接受范围中。造成的影响是黑产对老号的需求大幅增加，等待着苹果的问题将是盗号、撞库、养号等等。如上述变现环节，因为充值限制，会索要用户（购买黑灰产手中虚拟商品的人）的账号和密码，这个账户就可以“回收” 投入下一轮的利用。账号相关的产业链详细阐述可参考下文账号模块。\n\n**3、滴滴虚假注册**\n\n按照相关规定，网约车平台对注册司机需要进行相关考核审查，如有一定的驾驶年龄、北京要求“京人京车”等。很多不符合规定的人想完成注册，就会利用一种“代注册”的黑产业务。\n\n2017年9月，滴滴向广东省公安厅网警总队举报，发现发现几十万账户存在虚假注册、人车不符的问题。经查，发现了背后黑产大肆的牟利行为。驾龄不符、外地车不派单、车辆超龄都可以拿钱“解决”。\n\n首先黑产信息源通过行业内鬼等，查到真实符合规定的人车信息。一级中间商从信息源购买车辆人员信息。然后加价转卖给二级中间商，二级再加价转卖给代注册操作员。代注册操作人再通过PS等方式“加工信息”，与购买者信息结合，将分别合规的信息整合为一整套，完成注册操作，收费300-500元不等。而即使被发现，滴滴也只能对司机进行封号处理。\n\n有些操作人还会顺便薅一把滴滴的羊毛，如利用推荐机制，滴滴公司规定，每推荐成功一个司机，就能获得218元冲锋奖，和新司机前8个订单30%的流水。不难想象在各家网约车竞争期，活动不计成本，都只想着在大战中存活的时候，代注册一伙能够获得多么巨大的利润。\n\n事实上，在滴滴快的大战时，虚假司机账户就是主要是用来刷单，结合外挂牟利的。当网约车合并，国家监管变严后，代注册团伙转而向不符合规定的人售卖服务，部分团伙还会以出售“注册教程”的方式获取额外利润，这种教学收费模式往往是在本身利益降低时会产生的，当利益巨大时，掌握方法的人只会默默赚钱。\n\n这一系列牟利行为不只是对滴滴造成了伤害，也会对普通用户造成伤害。如滴滴外挂会通过修改定位等方式实现“挑单、抢单”。而滴滴不得不将距离最优算法，改成几公里内随机派单，而用户只能忍受明明看到身边有车，却需要在寒风中等待三公里外的一辆车。\n\n更令我们警醒的是，我们的个人信息，竟然是如此容易可以获得的。事实上，黑产的社工库也确实在不断完善，数据量越来越多，精准度越来越高，被广泛的用在撞库、诈骗等处，让人胆寒。滴滴这样的认证较为复杂，被应用更普遍的图形验证码、身份证认证、面部识别认证都有着发展稳定的服务产业链，将在下文账户认证部分作出介绍。\n\n**4、Uber被黑客勒索**\n\nUber在去年遭遇了大规模的数据泄露，包括5000万用户的姓名、邮箱、电话。和700万司机的个人信息及60万美国司机驾驶证号码。Uber称信用卡等信息数据并没有泄露。5700万数据，与雅虎、美国信用机构Enquifax泄露规模相比，本不值一提，在黑产中也不算惊天的数据。但Uber的做法引起了大家的关注——向黑客支付赎金。\n\n当时的CSO和助理，以支付10万美金的方式试图隐瞒此事，避免Uber数据在黑市流通。事后两人遭到了开除，CEO迫辞职，Uber最终声明并没有证据表示此次事件的数据被黑客利用，并将为信息泄露的司机提供免费的信息保护监控服务。\n\n黑客获取数据的方式令人好奇。事实上他们是从Uber工程师的私人GitHub库，获得了登录凭证，进而访问了Uber用以计算的亚马逊云服务账户，在账户中发现了用户数据，随即进行了勒索行为。我们不禁发现攻击有时只需要找到一处漏洞，而防守却需要全面严密。而除了防守还有另外一个问题需要我们面对——对已经泄露的数据该如何行动。Uber隐瞒的做法自然是不可取的。\n\n而面对这种问题一个暴力而有效的对抗方式是建立比黑产更庞大的泄露数据库，若能在黑产使用这些用户信息时判定出是已泄露账号，直接触发风控逻辑，便可以进行更严格的审核，绕过黑客的防护手段，对敌人造成无法回避的打击。而建立这样的数据库除了需要有效、实时的收集补充方案，也需要各大厂商的分享和参与，收集多方资料，构建更全面的数据源。\n\n**5、教材涉黄案**\n\n2017年2月，一则“高中教材涉黄”的新闻受到了疯狂转载，人教版高中语文选修教材中的诗词网址打开后竟然是黄色网站。实际上这个网站是遭到了篡改，实施者是一家名叫“雷胜科技”的公司。表面上它是一家互联网应用服务商，而背后却隐藏着一条完整的色情诱导诈骗产业链，“教材涉黄”将它拖出了水面。\n\n诈骗团伙开发色情网站和App，通过限制观看有色视频的时间，诱导用户付费获取完整视频。但事实上并没有所谓的“完整版”，盈利方式就是诈骗用户。这个产业链的每一个环节都是经过精心规划的。\n\n第一环节为开发，技术门槛极低，诈骗团伙能够以极低的价格购买到源码，有经验的开发者也可以在几天之内轻松完成。由于色情内容在我国的违法性，App展示的有色内容会经过精心编辑，能完全规避“淫秽色情”的法律界定。雷胜科技设置了研发、市场、编辑、财务和客服部门。编辑部就是负责剪辑擦边球类的有色视频的，甚至雇有专业律师审核图片和视频。\n\nApp上架之后就进入了推广环节，团伙会通过百度联盟、木马程序、修改网站内容链接等方式进行推广。雷胜科技就是修改了教育网站的内容链接被牵引出来的。\n\n之后就到了变现环节。诈骗团伙会从支付平台或者渠道商处申请获得支付接口。申请需要一套完整的公司三证信息（营业执照、税务登记证和组织机构代码证）及银行卡账户，这种在黑市上称为公司“壳”资料，有专人在收集贩卖，注册电商企业店、申请支付接口等都会向其购买。针对于设置了风控模型的第三方平台。诈骗团伙会通过准备多个支付接口，使用可以短时间切换接口的方式进行绕过。\n\n有些色情引流诈骗App还会在安装时获取权限（如发送短信等），之后向特定的SP号码发送短信进行扣费的方式进行盈利。这些app也会捆绑其他恶意业务，或是窃取用户隐私信息等，对用户造成更深的损害。雷胜科技是通过PC端和移动端流量分发引流，然后通过诈骗变现，而更为常见的方式是利用各大社交、视频等平台，引流至微信后变现，在引流模块我们会给出更详细的介绍。\n\n**二、产业链分析**\n\n**1、上游资源提供者**\n\n**a）黑卡**\n\n手机黑卡，指黑灰产从业者手中的大量非正常使用的手机卡。这些黑卡会提供给各个接码平台，用于接收发送验证码，进而进行各种虚假注册、认证业务。比如饿了么新用户有十几元的首单减免，羊毛党会从接码平台获取手机号批量注册，再通过下游将这些首单优惠以一半的价格卖给需要点外卖的人。注册成本是支付一毛钱给接码平台，收益是下游接单人的几元到十几元不等的收购价。而黑卡就是接码平台手机号的源头。\n\n被称为“史上最严”的手机卡实名制举措，确实在一段时间内打压了手机黑卡和接码市场，提供黑卡和接码服务的平台和个人一下子销声匿迹，但好景不长，仅仅几个月后，便出现了强劲的复苏态势，提供黑卡和接码服务的平台和个人如雨后春笋般涌现。至今，该市场已经极具规模，并且运行稳定，给甲方业务安全造成巨大压力。\n\n本着尽可能全面、精准的原则，猎人君从多个途径不遗余力的收集黑卡信息，从市场现存的黑卡，到曾经有恶意行为的黑卡，再到市场新增的黑卡，构建了庞大的黑卡数据库。对每个入库的黑卡号码经行多维度地评估，标注风险等级，可以有效帮助甲方完善基于手机号的风控策略。根据威胁猎人反向追踪调查，黑卡背后的产业链大概如下图所示：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-0cfa0e65cefecff7.png)\n\n**卡源卡商**\n\n卡源卡商指通过各种渠道（如开皮包公司、与代理商打通系等）从运营商或者代理商那里办理大量手机卡，通过加价转卖下游卡商赚取利润的货源持有者。卡源主要有：\n\n**· **物联网卡：主要用于工业、交通、物流等领域的手机卡。物联网卡无须实名认证，需要以企业名义办理，提供营业执照即可，营业执照可以以千元左右的价格买到。有些运营商对营业执照检测力度很低，甚至会为灰产定制专用的物联网卡套餐。这种卡多为0月租或者1月租，根据能否接听电话，分为短信卡（也称注册卡）和语音卡。\n\n**· **实名卡：这种多为联络运营商后，用网上收集的大量身份信息批量认证得到的。\n\n**· **海外卡：实名制实施后，卡商受到一定限制。从16年下半年开始，大量缅甸、越南、印尼等东南亚卡开始进入国内手机黑卡产业，这些卡支持GSM网络，国内可以直接使用，无需实名认证，基本是0月租，收短信免费，非常切合黑产利益。\n\n如上述东鹏特饮提到的薅羊毛事件中，我们只看到有人大量售卖账号，其实背后有个非常成熟的产业链，各级分工明确。了解了他们的经营方式后，我们再进一步分析黑卡数据可以发现运营商的比例甚至可以定位到犯罪团伙经常活动的城市。\n\n**手机黑卡运营商对比**\n\n下图展示了传统运营商和虚拟运营商黑卡的数量对比。来自传统运营商的黑卡数量要远多于来自虚拟运营商的黑卡数量，毕竟传统运营商和虚拟运营商的手机卡总量不在同一个数量级上。2017年8月份的新闻数据表明，全国虚拟运营商用户占移动用户总数的3.6%，3.6%的用户占比却贡献了20.17%的黑卡数量占比。相对传统运营商而言，虚拟运营商的手机卡中黑卡占比较高。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-7af61a4910b7d6ae.png)\n\n以下两张图展示了在非虚拟号段上和虚拟号段上三大运营商的黑卡数量对比。在非虚拟号段上，将近一半的手机黑卡来自于中国移动，约三分之一来自于中国联通，中国电信最少。在虚拟号段上，绝大多数是中国联通的手机黑卡，中国移动次之，中国电信依旧最少。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-d90f60ba4f72ccc2.png)\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-9b959df0a725b1d0.png)\n\n**手机黑卡归属地分布**\n\n依据归属地统计的数据，广东省十分抢眼，在黑卡归属地省份排名中遥遥领先，省内的广州、深圳、东莞和佛山也霸占了黑卡归属地城市排名中前五名中的四名。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-ccf024937a522807.png)\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-66679b96699bcf4c.png)\n\n**猫池厂家**\n\n猫池厂家负责生产猫池设备，并将设备卖给卡商使用。猫池是一种插上手机卡就可以模拟手机进行收发短信、接打电话、上网等功能的设备，在正常行业也有广泛应用，如邮电局、银行、证券商、各类交易所、各类信息呼叫中心等。猫池设备可以实现对多张手机卡的管理。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-1bf7864798c2ee6b.png)\n\n**卡商**\n\n卡商从卡源卡商那里大量购买手机黑卡，将黑卡插入猫池设备并接入卡商平台，然后通过卡商平台接各种验证码业务，根据业务类型的不同，每条验证码可以获得0.1元-3元不等的收入。\n\n黑卡数据库能够结合企业自身的后台数据，作为补充和参考，为企业筛选恶意用户提供账号维度上的支持。\n\n**b）黑IP**\n\nIP地址作为互联网的紧缺资源、一直是厂商最重要的风控方案之一。面对攻击，最主流防控措施之一就是封IP，企业根据黑IP库、同IP发起请求次数、密码错误率、是否有恶意行为等决定一段时间内禁止某IP的请求。\n\n而面对暴利，黑产不会轻易放弃，对待厂商的对抗，黑产积极主动寻求解决方案，甚至做到了平台化、链条化的反对抗。根据威胁猎人的长期监控，黑产主要有以下几种获取IP资源的方式：\n\n**· **扫描代理：通过全网扫描常见的代理服务端口，收集可用的代理IP地址，自行维护管理，成本高、效率低。\n\n**· **付费代理：代理商通过扫描、搭建、交换的方式，提供全球的代理服务器，有效降低自行收集的产品。代理IP平台非常之多，均可以提供API接口供黑产调用。\n\n**· **付费VPN：与代理相似，使用技术不同。\n\n**· **拨号VPS：这类VPS是一台虚拟服务器，通过ADSL拨号上网，每拨号一次换一次IP，使用者相当于拥有了整个城市的大量可用IP。更有相关供应商做到了打通全国多省市的拨号方式，俗称混拨。也就实现了在一台VPS中使用一个账号快速随机切换近百城市的ADSL线路拨入互联网。\n\n我们称这种用于网络攻击的IP为黑IP。威胁猎人通过大量渠道，在2017年采集并整理出全球范围内的黑IP，并做了详细分类。\n\n**黑IP类型排名**\n\n经统计，黑IP top 10类型比例如下。一个黑IP可能会有多个标签，整体看来，僵尸网络IP、机器人IP和代理IP的数量占据前三名。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-a6039edc6bf08a85.png)\n\n**黑IP地域分布**\n\n分析IP地域来源数据，全球黑IP分布图和top 20的国家如下。全球IPv4总数约为43亿，美国拥有30%以上，这一数据与图片相符，美国的黑IP数量占比36.39%，遥遥领先其他国家。发达国家的黑IP数量要多于发展中国国家，可以简单理解为，发达国家拥有更多的互联网设备，也就拥有更多的IP资源，所以黑IP的数量与互联网设备的数量成正比。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-4003f25754e38705.png)\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-065bcd8f9d349155.png)\n\n以下两张图片为全球黑IP来源城市top 20和全球黑IP所属运营商top 10。从来源城市数据看来，top榜单中大多数是美国城市，中国城市数量紧随其后，其中北京更是占据了榜首。上榜的城市都是经济较为发达的城市。从所属运营商数据看来，top 10中一半是美国的运营商。\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-39d01c2c0e9b1ccb.png)\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-3cb1a49fce45ba1a.png)\n\n**c）账号**\n\n批量注册和养号：\n\n在互联网灰产中，无论是行迹匆匆的羊毛党，还是猥琐发育的养号者，都需要大量账号作为牟利的支撑。因此，注册环节也就成了互联网公司和灰产的最前沿战场。各公司的注册页面看似平淡，实则暗流涌动。\n\n灰产的逐利本性决定他们非常强调投入产出比。灰产会雇佣开发人员开发针对注册环节的自动化攻击工具。这种注册软件大抵有两类：\n\n**· **模拟操作类：通过控件操作浏览器元素实现，真实加载注册页面，模拟用户操作。\n\n**· **协议破解类：通过HTTPS协议实现，破解注册接口协议，直接带参数调用注册接口实现注册。\n\n除了批量注册外，灰产也会根据平台特色，使用其他平台第三方登录的方式跳转成小号，批量产出，例如有一种微博账号叫做授权号，因为注册流程等原因，在微博平台受到风控限制，很难进行后续变现业务，就只用作授权其他平台账号，在其他平台上完成变现。这种授权号成本低于手机号注册，每个只需要几分钱。\n\n针对这类账号，很多厂商会对新注册账号进行监控，于是产生了号商养号的行为，注册后模仿真实用户进行一些操作，将号码从监控列表剔除之后再进行业务。\n\n薅羊毛的新号、刷量的小号都是通过这些方式得到的，但针对苹果风控被灰产需要的老号就需要通过盗号、养号、撞库获得了。当各个平台增加风控后，这类老号需求就会出现，如微信满月号、陌陌半年号等等属于养号，几年扫号老号等属于盗号或撞库所得。\n\n**撞库**\n\n撞库，即攻击者通过收集各个网站的泄露的用户数据等方式，生成用户名和密码字典，批量去其他网站登录，尝试撞出目标网站的可用账户密码。近年来，随着频繁出现的数据库泄露事件，撞库攻击取代了木马盗号成为了主流的盗号方式。\n\n下图为猎人君统计的2017年撞库攻击量走势图：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-ea994909eeccca2a.png)\n\n以下是2017年撞库攻击者“钟爱”的一些攻击目标和接口：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-9f6aa0e52d84e23c.png)\n\n游戏行业在地上互联网公司也是盈利最为可观的，在地下自然也聚集了大量相关从业人员，拥有众多的细分变现产业链。能否直接获得游戏账号的撞库方案自然是受黑客欢迎与关注的，因此，游戏公司向来是撞库攻击的高发地。国内外各大游戏公司在2017年都持续受到大量的撞库攻击。\n\n版权行业和社交行业也是深受其害，随着正版化的推进以及带宽的增加，许多相关资源需要付费观看，存在不愿意花高价购买会员，而愿意用低价购买一个账号使用的人，就会存在这些会员账号变现的途径，进而这些账号也就是对黑产有价值的。\n\n社交行业也拥有数量众多的变现方式，主要的灰产有刷量（点赞、播放量、榜单等）、私信引流、色情社交引流、诈骗等。社交平台对抗的风控策略不断升级，社交平台的老账号也就成了某些圈内富有价值的资源，如某陌交友平台的老号价格在30元以上。老号资源意味着封杀率低、生意可持续。因此，社交账号也是黑产的重要目标。\n\n**撞库数据来源**\n\n（1）信封号产业链\n\n信封号，是QQ号产业链中的黑话，每一万个或者一千个被盗取的QQ号，称为一个信封。信封号产业链就是QQ号盗取、销赃的产业链。当QQ号中的Q币、游戏虚拟装备等被清洗一空、压榨干净后。就会将大量的账号密码贩卖给黑客完善社工库，或者制作密码字典。由于QQ邮箱在国内的市场占有率很高，以及很多用户习惯直接用QQ号对应的QQ邮箱和密码作为第三方平台的账号。大量QQ号被直接用来进行网站撞库。\n\n（2）网站泄露数据库\n\n网站泄露数据库的标志性事件是2011年CSDN 600万用户数据泄露，引领了当年一波数据泄露高峰，数十个网站的用户数据被公开，大量只在地下流通的数据被抛上台面。平时不关注此道的黑客也掌握了足够的数据源切入，某种程度上点燃了撞库攻击的热潮。而且被爆出的数据泄露其实也只是冰山一角，更多的再地下黑市中交易流通。\n\n（3）地下黑市流通\n\n数据窃取与交易是地下产业链隐藏最深的部分，也常有一些定制性的交易，不少黑客通过数据交易来构建庞大的社工库。黑客间的私下交易，我们无法得知，到底有多少网站数据已经被窃取也无法客观的评估。但通过半公开渠道也可管中窥豹，以下是暗网某地下数据交易市场的截图：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-5414451aa4440854.png)\n\n**攻击方法和主流防控**\n\n通过对海量攻击行为的监控和分析，我们发现黑客攻击方法如下：\n\n（1）判断账号是否存在\n\n**· **注册接口快速验证：很多网站在填写注册信息时，会通过AJAX对账户名可用性做实时验证，这个接口就可以被黑客利用做账户存在的筛选。\n\n**· **登录接口返回信息：部分网站账号密码错误时，会返回敏感信息暴露账号存在情况，如返回“账号不存在”或“密码错误”。现越来越多的厂商返回“账号或密码错误”，可以有效避免被利用。\n\n**· **找回密码接口：部分网站，在找回密码流程中，也会有一次提示信息，也常会被黑客用来验证账户存在。\n\n（2）业务安全集中管理问题突出\n\n从TH-Karma统计的数据来看，许多网站的主要入口有比较严格的审计措施，会根据登录IP、频率等触发验证码或者封锁IP。但当公司业务增多，安全管理复杂度大幅增加，不同子站各用一套自己登录验证。这些没有接入审计功能的边缘业务接口就称为了黑客攻击的温床。\n\n（3）攻击效果\n\n根据威胁猎人对大量撞库数据的统计，能够成功绕过风控的攻击占供攻击量的83%，撞库的成功率则在0.4%左右浮动。\n\n对此威胁猎人建立维护了一个高危账号库。高危账号指的是已被黑灰产从业者恶意利用的账号，大多来自泄露的数据库。对于甲方而言，看到这些账号要多一份心眼，很有可能背后暗藏着不轨动机。威胁猎人根据在2017年高危账号，做出了一些统计。\n\n（1）高危邮箱账号域名排名\n\nTop 20的高危邮箱账号域名如下：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-54048d875cc40590.png)\n\n国内邮箱域名占据60%以上，其中以163.com、qq.com和game.sohu.com为主。国外主流邮箱域名（例如yahoo.com、gmail.com和hotmail.com），以及一些俄罗斯邮箱域名（例如mail.ru和yandex.ru）和德国邮箱域名（例如web.de）也位列top 20之内。基本可以看出，top 20的高危邮箱账号域名的至少满足以下条件之一：\n\n**· **邮箱服务用户基数大；\n\n**· **来自于黑灰产活动活跃的地区。\n\n（2）高危账号关联密码排名\n\n此外，猎人君也统计了与高危账号关联的密码，数量排名top 20都是一些常见的弱密码，列表如下： \n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-5dfefeebec3f5da1.png)\n\n**d）账户认证**\n\n账户认证产业链属于地下产业链中的服务型产业链。几乎所有的互联网企业都会要求用户手机认证，有些还要求实名认证、人脸识别验证，配合技术或人工审核。这必然给各个地下产业链都带来了障碍，账户认证产业链自然就应运而生了。\n\n**手机接码、听码**\n\n短信验证是建立在手机和手机号成本上的真人验证，被广泛的应用于注册等场景。如上述黑卡产业链的介绍，黑产的对抗方案并不依赖于手机和办卡成本，而是接码平台，黑产从业者从该类平台接收一个验证码需要支付1-3毛钱。\n\n接码平台是负责连接卡商和羊毛党、号商等有手机验证码需求的群体，提供软件支持、业务结算等平台服务，通过业务分成获利。一般会提供给使用者客户端、API、有些还会提供手机客户端。手机客户端用以支持各种手机业务。而API能够对接到自动化工具、脚本中，实现批量注册。\n\n使用者首先要“收藏”自己要做的项目后才可以收取验证码，这样做的好处是避免手机号在相同注册场景的重复使用，同时也便于应对新形式的对抗，比如，整个注册过程可能需要接收多次验证码，并发送一次验证码。平台会将收发集成一个流程，供使用者批量化操作。\n\n有些厂商选择了语音验证码，而接码平台也产生了相应收取语音验证码的服务，同时也产生了“听码”网赚。接码平台很多，活跃的有数十家，比较知名的接码平台有：爱乐赞、玉米（现菜众享）、Thewolf、星辰等，其中Thewolf和星辰可以接语音验证码。\n\n2016年11月当时最大的平台爱码被警方查处，随后很多平台转入地下。如爱乐赞因为非常稳定，卡商众多，是最受黑产欢迎的接码平台之一。现已不支持在线注册，在有老客户介绍情况下，联系客服充值1000元才可以开新账户，另一种解决方式是与别人共用一个账号，且每次充值不能低于5元，否则会被封号。\n\n**打码**\n\n验证码是风控最广泛的一种部署方案。普通厂商会直接接入，有后台分析的厂商会在后台审计异常时触发验证码以不影响普通用户体验。而在黑产中，撞库、注册等都需要进行大量验证码识别。所以带动了另一个服务产业链——打码平台。\n\n作为一种最简单、应用最广泛的图灵测试方案，大量公司和团队不断尝试自动化破解，以至于验证码升级到了人类也需要多次才能识别的境地。国内的黑产，依靠低廉的劳动力解决了问题。他们对无法技术解决的验证码使用率暴力的方式——人工打码进行破解。这种方式广泛传播到了大量第三世界国家，导致全球有近百万人以此为生。打码工人平均每码收入1-2分钱，熟练工每分钟可以打码20个左右，每小时收入10-15元。\n\n随着技术的发展，黑产也与时俱进，逐渐产生了使用AI打码的平台。如警方在17年打击的“快啊答题”平台，使用了伯克利大学的数据模型，引入大量验证码数据对识别系统训练，将机器识别验证码的能力提高了2000倍，价格降低到了每千次15-20元。为撞库等需要验证的业务提供了极大的便利。\n\n**身份证认证及过脸**\n\n人脸识别技术发展逐渐成熟，“刷脸”在近两年成为新时期生物识别技术应用的主要场景。进入2017年后，在通关、金融、电信、公证等很多领域都需要对人和证件进行一致性的验证。2016年6月国家网信办发布《移动互联网应用程序信息服务管理规定》，明确要求移动互联网应用程序按照“后台实名、前台自愿”的原则，对注册用户进行基于移动电话号码等真实身份信息认证。\n\n互联网厂商面对法规以及某些业务上的需求，纷纷推出账号强制实名认证，并将人脸认证环节放到App中完成。实名让互联网时代更加规范的同时，也给由于某些原因无法实名或者需要大量实名账号完成黑灰色业务的人群造成了障碍，于是“过脸产业”应运而生，为别人批量完成认证获取利益。\n\n厂商认证时经常会要求用户拍摄身份证正反面照片及手持身份证照片等。黑产获取此类身份证“料”的方式有但不限于以下几种：\n\n**· **收料人偏远地区收集：他们会到偏远地区以几十元的价格大量购买拍摄一整套的照片，没有网络安全意思的民众很多为了一点的利益愿意配合。\n\n**· **有些收料人甚至会假扮社区工作人员等在社区中进行收集，相对前一种，几乎没有成本。\n\n**· **还有一种纯粹通过网络收集他人泄露出的照片。\n\n收集后会以5-10元的价格卖给下一级使用者。对于需要过人脸认证的场景，从业者会利用PS等工具处理好一张带背景的人脸图，再利用Crazy Talk生成动态视频的软件，录制“眨眼”、“摇头”、“说话”等动作，完成后将摄像头对准视频，完成认证，过脸服务收费10元到100元不等。\n\n过脸产业最开始被用在网络借贷薅羊毛上，如今已经广泛使用在各种实名认证的业务上。今日头条头条号、58同城、移动“任我行”卡、腾讯大王卡等都是其盈利的途径。\n\n账号认证增加难度和用户体验优化之间找到平衡点，对各个厂商来说都是不小的难点。在苹果36事件中，就是为了提升用户体验给羊毛党留下了可乘之机。苹果若能对筛选出的恶意用户提高认证成本，就可以找到平衡点。而做到这点需要对用户行为和恶意行为进行分析。用户行为厂商可以进行记录，恶意行为需要情报的配合，包括恶意用户的行动模式、流程、最终目的等。\n\n**2、下游变现细分产业**\n\n**a）流量欺诈**\n\n流量欺诈已经发展成了成熟的产业链，刷量可通过人为的操作提高网页访问量、视频播放量、广告点击量、搜索引擎搜索量等等。市场充斥着大量刷量工具和服务，几元就可以买到数千IP的访问。或是使用大量代理IP刷流量，或是基于P2P互刷原理（即挂机访问别人的网站，得到点数后可以用来发布任务，为自己的网站刷量），刷量可以高度模拟真实用户的行为轨迹，使得视频网站、直播平台、广告联盟、搜索引擎、电商等甲方难以有效加以区分。猎人君通过分析在2017年捕获的流量刷量数据，得出以下流量刷量黑灰产业中目标厂商的top 10：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-1aea75b62915d5c4.png)\n\n**刷量行为主要集中在以下几个场景**\n\n（1）刷搜索引擎关键词排名\n\n搜索引擎排名对网站的流量影响巨大。市场上有提供很多提高关键词排名的服务，原理是利用大量IP在搜索引擎搜索指定关键词，然后到指定网站，点击进入，甚至进一步模仿用户浏览、点击，欺骗搜索引擎，使其认为该站与该关键词关联度很高。百度，作为国内最大的流量出入口，榜首位置实至名归。针对百度的流量刷量类型有多种，主要类型包括刷搜索流量和点击百度网盟广告。2017年底，百度推出“惊雷算法”，旨在打击以作弊的方式提升网站搜索排序的行为，究竟效果如何，2018年我们拭目以待。Top 10榜单中还出现了360搜索和中国搜索，刷搜索流量在整个流量刷量产业中的比重可见一斑。\n\n（2）刷视频播放量\n\n另一个流量刷量产业的大头是刷视频播放量，目标厂商包括榜单中的优酷、搜狐、龙珠视频/直播、爱奇艺、腾讯等，以及不在榜单中的触手直播、风行网等。很多视频有夸张的播放量，点赞和回复却寥寥无几。视频网站依据视频人气付给视频作者酬劳，虚假的播放量可直接导致视频网站蒙受金钱上的损失。对于用户来说，人气很高的热门视频，内容质量却名不副实，用户体验下降。\n\n（3）刷广告展示量和点击量\n\n通常告主会和广告联盟或站长合作，进行推广，按照CPM、CPC的方式结算广告费用给站长。一些无良的站长会使用软件或者购买服务恶意刷CPM、CPC，获取不正当利益。广告联盟存在一些广告反欺诈机制，刷量有可能面临封号，但依旧有很多人通过刷量技巧和网站数量来大规模获利。\n\n（4）电商和网站访问量\n\n此外，刷页面的访问量，包括刷社交站点的内容曝光量和电商商品浏览量，也是流量刷量产业中相当活跃的一个分支，比如新浪博客的访问量，以及淘宝和天猫商品的浏览量等。总而言之，当今的互联网世界中，充满了障眼法，眼见不一定为实，所谓的“人气排名”，所谓的“热门列表”，不可完全相信。\n\n** b）数据爬取采集**\n\n爬虫就是收集信息，“爬虫写的好，拥有整个互联网的数据不是梦”。数据分析本身并没有善恶标签，方法和目的却可以将之定性。黑灰产如今规模庞大，分支众多，从猎人君观察到的攻击流量来看，黑灰产从业者的需求比较分散，快递、媒体、电商、账号有效性等等都是攻击者的目标。黑灰产从业者做爬虫的目的多种多样，比如：\n\n**· **用作产品化上游的数据支撑，比如某些针对电商的秒杀、抢购软件。\n\n**· **用作分析竞争对手的产品和业务策略，比如爬取竞争对手的产品信息和用户论坛。\n\n**· **爬取竞争对手的用户数据，尤其是有效的手机号或邮箱格式的用户名，之后可用于定向的推广营销。\n\n**· **爬取有效的用户名，可用于生成用户名字典，实施撞库攻击。\n\n**· **爬取个人信息，恶意利用，甚至实施诈骗。\n\n以下是猎人君统计的2017年较为热门的一些爬虫攻击目标和接口：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-95a110eeb83d68c2.png)\n\n**c）薅羊毛**\n\n薅羊毛，简单理解就是，以不正当的方式获取互联网上的各种福利，如新用户注册红包。这些人不以“利小而不为”，只要是看到福利，能薅则薅，使得互联网公司的推广经费中很大一笔部分都打了水漂。薅羊毛入门门槛极低，如今，薅羊毛规模之大，足以称之为一个行业。薅羊毛行业紧紧依附互联网行业，与互联网行业的以等同的速度发展。2017年，薅羊毛活动如火如荼，主要针对各类金融平台、电商平台以及O2O平台。\n\n威胁猎人总结了一份2017羊毛热词云图，如下所示：\n\n![威胁猎人：2017年度中国互联网黑产报告](https://upload-images.jianshu.io/upload_images/18390058-368a07bb007ebecb.png)\n\n词云图的中央，是大大的两个字“会员”，各类会员，包括低价会员甚至是免费会员，深得众羊毛党的喜爱。其他福利，比如优惠券、红包、商品秒杀、激活码、各类低价QQ钻等，也有较高的词频。认领福利需要账号，账号相关的关键词，比如注册、老号、白号、小号等，也是榜上有名。既然有账号，就有连带的账号实名业务，比如认证、绑定、实名等。另外，不出意外的是，“骗子”的词频相当高，黑灰产市场本来就不受法律保护，“黑吃黑”的现象也较为普遍。 \n\n**d）引流**\n\n有一些不适合直接变现却坐拥巨大流量的平台，比如短视频平台、社交平台等，黑产也不会放弃，采用引流方式进行变现。一个简单的引流变现操作是这样的：操作者在头像、昵称、个人资料等任何可以被平台曝光的地方留下联系方式，比如微信号，再通过发送诱惑性的内容吸引用户前往添加好友，之后通过诈骗、微商等形式深度变现。\n\n常见的社交平台引流方法，是通过软件批量关注、发送私信等方式。一些引流操作可以带来巨大的流量，个人无法消耗，会以“出粉”形式卖出，即买家根据成功添加微信的“人头”数，付给引流者报酬。\n\n引流人往往会结合目标用户的心理以及引流平台的特点，进行操作，如到美拍的美妆视频下写“前100人免费送XXX化妆水”，吸引可以通过微商变现的“女粉”。在陌陌等平台上通过诱惑性图片、视频加上“想交男朋友”等话术，吸引“色粉”（“男粉”），在微信中骗取红包或是销售一些男性用品。\n\n诸如此类，还有“保健粉”（可用于销售医疗用品）、“连信粉”（中年有消费力的）、“股民粉”、“宝妈粉”、“女大学生粉”等等。在业内叫做精准引流，用户群体越精准，价格越高。\n\n而购买者有两类，一种是真实微商，另一种就是我们在东鹏特饮中提到的，用微信作为变现出口的黑产，如引来色粉后撸包，即诈骗，用微信机器人伪装成女性，通过发送诱惑图片视频的方式索要红包。\n\n这种方式只能骗一次，所以他们需要引流人给他们源源不断的粉，称为“火车站流量”，而微信被举报后账号就报销了，所以他们会向号商购买账号，做到最后，变现可以用量化标准来计算收益，微信号平均多久会死，谁家引来的粉平均每个人头几块钱……单从这一条往下看，引流和号商一直都有市场，会持续存在，而他们需要绕过厂商的风控，又需要一系列的服务型产业链，他们都会持续的与厂商对抗，只要利益不消失，对抗就会持续升级。\n\n**三、对抗升级**\n\n**1、主流防控措施和黑产绕过方法**\n\n面对恶意行为，除去IP等规则判断，厂商也会从行为和设备角度进行判断。如用户登录过程的行为，包括停留时间、鼠标焦点、页面访问流程、csrf-token等。再通过客户端上报机器信息，识别判定是否存在伪造设备。\n\n而面对对抗，黑产也在不断升级，主要会从以下几个方面进行绕过：\n\n**· **边缘业务与新业务处寻找可利用接口：黑灰产不断寻找审计不严格的边缘业务接口，找到后便能绕过所有的防护措施，如入无人之境。而厂商在这个维度上很难有行之有效的监控，因为本来就是被疏忽的接口。这里可以从第三方视角进行监控。威胁猎人对黑产流量进行大数据分析，可以是这种伎俩暴露在阳光下，何人何时攻击了新的接口，从攻击出发分析检测，可极大增强厂商对漏洞的反应速度。\n\n**· **模仿真实用户：规避后台行为分析模型方面，黑卡提交请求时不再是仅仅填写User-Agent，而是尽可能全的完成整个流程，包括：完整的页面打卡流程代替仅仅向关键接口提交请求；携带csrf-token等完备的参数；页面停留时间采用函数随机化；HTTP header严格遵守浏览器特征；随机化所有其他不重要的参数等。\n\n**2、新风控角度的思考**\n\n企业制定安全策略往往存在两个问题：\n\n**· **是安全策略面向所有客户，灰产可以不断尝试摸清规律，设法绕过。\n\n**· **对最新的攻击方式不了解，导致制定防御策略无法有效打击黑产，反而容易误伤正常用户。\n\n**· **面对后台数据，只知道自己拦截了多少恶意用户，不知道有多少没有拦截。\n\n因此威胁猎人从行业出发，针对电商、社交、游戏、云计算等不同行业的不同特点，逐一分析，还原真实攻击场景，以期望解决企业面临攻防信息不对等的问题，为企业精准防御灰产攻击提供数据补充、情报支撑。\n\n**a）黑产大数据监控**\n\n基于黑产攻击的资源建立持续监控机制，对已经泄露和已经在使用的黑IP、黑卡、批量注册账号、盗取账号、恶意流量等进行积累和实时更新。就能结合风控系统，从多个维度判断，有效筛选出可疑用户。\n\n**b）情报带来的针对性对抗**\n\n情报收集和分析工作可以有效的还原出某个针对企业的攻击方式，用于针对性打击。如通过情报和数据结合分析，得出攻击者的目的、攻击流程和行动模式后，厂商就可以多维度的打击，如A场景检测到却在B场景打击，让攻击者摸不着头脑，测试不出套路。在入口处有所遗漏时，还可以在出口处再次进行打击，如注册处或许没有全部拦截，当检测到注册后立即绑卡抢红包提现一气呵成的用户，标记高级别危险标签，提高提现门槛等。\n\n**结语**\n\n传统的“你来我往”、“亡羊补牢式”的攻防策略已无法有效与现在的黑灰产势力抗衡，作为防守方的甲方应当将战场向前推进，步步逼近黑灰产大本营，以争取更多的主动权。情报收集、风险侦测和威胁感知将是新型对抗模型中的三把利刃，能够帮助甲方做到“知彼知己，百战不殆”。\n","tags":["文摘","互联网实事"],"categories":["文摘"]},{"title":"IP地址、子网掩码、网络号、主机号、网络地址、主机地址以及ip段/数字含义","url":"/2019-11-23/computer-basic/ip地址相关知识点/","content":"\n#### IP地址、子网掩码、网络号、主机号、网络地址、主机地址以及ip段/数字含义\n1\\. ip地址概述\n----------\n\n互联网中有数百万台的主机和路由器，为了确切的标识它们，TCP/IP建立了一套编址方案，为每台主机和路由器分配一个全网唯一的地址，即IP地址。任何两台主机的ip地址不能相同，但是允许一个主机拥有多个ip地址。\n\n2\\. ip地址结构及分类\n-------------\n\n这里介绍的是ipv4版本的ip地址，也是大家现阶段都在用的ip地址。\n\nip地址是由32位二进制数，即4个字节组成的，由网络号和主机号两个字段组成。总的来说，寻址的过程是：先按网络号把网络找到，再按主机号把主机找到。\n\n为了便于对ip地址进行管理，同时还考虑到网络的差异很大，ip地址分为5类，即A类到E类，如下所示\n\n[![ip address](https://saucer-man.com/usr/uploads/sina/5cc5cfbd8cbb6.jpg \"ip address\")](https://saucer-man.com/usr/uploads/sina/5cc5cfbd8cbb6.jpg \"ip address\")\n\n这些32位的地址通常写成四个二进制的数，其中每个整数对应一个字节，这就是点分十进制记法，例如我的ip地址就是171.48.163.10。\n\n关于上图中的网络号范围：\n\n-   A类地址网络号占用一个字节，但是由于有一位是类别位，只有7位可供使用，但是由于规定，网络字段全0是个保留字段表示本网络，而127（01111111）是另外一个保留字段，作为本地软件的环回测试。我们常见的127.0.0.1表示本机，原因也是出自这里。所以A类地址可以指派的网络号个数为（2^7-2）.\n-   B类网络地址网络号有两个字节，前两位为10已经固定，只剩下14位可用，由于这14位无论怎么取值都不会使得网络号为全0或者全1，但是实际上规定，B类不指派128.0.0.0，所以最小网络地址为128.1.0.0。因此B类地址可指派的网络号个数为（2^14-1）.\n-   C类地址有3个字节的网络字段号，前三位固定110，只有剩下21位可用，同样C类地址192.0.0.0也不指派，可指派的最小网络地址也是192.0.1.0。因此C类地址可指派的网络号个数为（2^21-1）.\n\n可以看出区分各类地址最简单的方法就是看它的第一个十进制整数:\n\n| 类型 | 范围 |\n| --- | --- |\n| A | 0.0.0.0到127.255.255.255 |\n| B | 128.0.0.0到191.255.255.255 |\n| C | 192.0.0.0到123.255.255.255 |\n| D | 224.0.0.0到239.255.255.255 |\n| E | 240.0.0.0到247.255.255.255 |\n\n目前大量使用的是A、B、C三类地址，当某单位申请一个ip地址时，实际上只是申请到了一个网络号，具体主机号由本单位自行分配。\n\n3\\. 建立子网\n--------\n\n现在所有的主机都支持子网编址，不是把ip地址看成单纯的网络号+主机号，而是把主机号再分成一个子网号和一个主机号。这个分配的过程就是管理员建立子网的过程。\n\n举个例子，假设这里有个A类网络地址（120.252），在剩下的16bit中，8bit用于子网号，8bit用于主机号，格式如下所示。\n\n| 16位 | 8位 | 8位 |\n| --- | --- | --- |\n| 网络号=120.252 | 子网号 | 主机号 |\n\n这样就允许有254个子网，每个子网有254台主机。\n\n子网对于外部来说隐藏了内部网络组织的细节。在我们的网络例子中，所有的ip地址都有一个B类网络号120.252。但是其中有超过30多个子网，多于400台主机分配在这些子网中，由一台路由器提供Internet的接，在各子网之间用路由器互连。\n\n4\\. 子网掩码\n--------\n\n上例中对于主机号的分配我们是把16位拆分成8位子网号和8位主机号，那么为什么不是7位子网号9位主机号呢，这里就涉及到子网掩码的作用了。\n\nTCP/IP体系规定用一个32位的子网掩码来表示子网号字段对应的长度。具体的做法是：子网掩码由一连串的'1'和一连串的'0'组成，'1'对应于网络号和子网号字段，而'0'对应主机号。\n\n现在我们来看一个例子。假设还是分配到一个A类地址120.252。其子网掩码是`11111111.11111111.11111111.00000000`。可以看出前24位为'1'，后'8'位是'0'，说明网络号+子网号为前24位，子网号为24-16=8位，主机号为后8位。划分的主机地址格式为\n\n| 16位 | 8位 | 8位 |\n| --- | --- | --- |\n| 网络号=120.252 | 子网号 | 主机号 |\n\n如果子网掩码为`11111111.11111111.11111110.00000000`，那么子网号有23-16=7位，主机号有9位。对应的主机地址格式就变成：\n\n| 16位 | 7位 | 9位 |\n| --- | --- | --- |\n| 网络号=120.252 | 子网号 | 主机号 |\n\n通过上述例子我们便可以知道子网掩码的作用就是用来确定对主机号的划分。\n\nA、B、C类ip地址默认的子网掩码分别为:`255.0.0.0`、`255.255.0.0`、`255.255.255.0`。\n\n有了子网掩码，总路由器才可以确定把数据转发到哪个子网。举个例子：\n\n假设一个公司网络的网段为120.252。子网掩码`11111111.11111111.11111111.11000000`(前26位是网络号+子网号，后6位是主机号)。\n\n这时外界计算机要将一个数据传送给ip地址为120.252.16.98的主机，那么这个数据会先到公司的总路由器，进入120.252网络中。总路由器接收到了这个数据该转发到哪个子网中呢？\n\n首先将120.252.16.98转化为2进制为`1111000.11111100.00010000.01100010`,根据子网掩码便可以确定网段为`1111000.11111100`(120.252),子网号为`0001000001`(65),主机号为`100010`(34)。\n\n最终路由器把数据转发到第65个子网上，在由子网路由器转发到第34个主机上。\n\n5\\. 同一网段\n--------\n\n想在同一网段，必须做到网络标识相同。各类IP的网络标识算法都是不一样的，需要根据子网掩码的位数来判断。\n\n算法只要把IP和子网掩码的每位数AND就可以了。（AND方法：0和1=0　0和0=0　1和1=1）\n\n如：And　192.168.0.1，255.255.255.0，先转换为二进制，然后AND每一位。\\\nIP：　11000000.10101000.00000000.00000001\\\n子网掩码：　11111111.11111111.11111111.00000000\\\n得出AND结果：　 11000000.10101000.00000000.00000000\\\n转换为十进制192.168.0.0，这就是网络标识。\n\n其实很容易理解，只要两个ip地址的网络号和子网号都一样就是同一网段。 而网络号和子网号的位数是需要根据子网掩码才可以知道了，上述算法的原理也是基于此。\n\n举例： 假设我们的主机地址是140.252.1.1（一个B类地址），而子网掩码是255.255.255.0（子网号8位，主机号8位主机号）\n\n-   如果目的ip地址为140.252.1.5，那么我们就知道b类网络号是相同的（140.252），子网号是相同的（1），属于同一网段\n-   如果目的ip地址为140.252.4.5，那么我们就知道b类网络号是相同的（140.252），但是子网号是不同的（1和4）。不属于同一网段\n-   如果目的ip地址为140.251.4.5，那么我们就知道b类网络号是不相同的（140.252和140.251），后面比较就没必要了，不属于同一网段。\n\n一般来说，如果主机没有设置防火墙禁止别人访问的话，位于同一网段是主机是可以直接访问的。\n\n6\\. 私有地址\n--------\n\nA、B、C三类地址是我们常见的IP地址段。在这三类地址中，绝大多数的IP地址都是公有地址，需要向国际互联网信息中心申请注册。但是在IPv4地址协议中预留了3个IP地址段，作为私有地址，供组织机构内部使用。这三个地址段分别位于A、B、C三类地址内：\n\n-   A类地址：10.0.0.0--10.255.255.255\n-   B类地址：172.16.0.0--172.31.255.255\n-   C类地址：192.168.0.0--192.168.255.255\n\n这些地址是不会被Internet分配的，它们在Internet上也不会被路由。也就是说在外界公网上，不可能有ip地址是上述范围之中的。\n\n绝大部分计算机都是在一个内网中，而不是直接分配一个公网ipv4地址，我们可以用ipconfig查看一下本地的吧ip地址，然后对比一下公网ip地址：\n\n[![内网ip](https://saucer-man.com/usr/uploads/sina/5cc5cfbda9d58.jpg \"内网ip\")](https://saucer-man.com/usr/uploads/sina/5cc5cfbda9d58.jpg \"内网ip\")\\\n[![公网ip](https://saucer-man.com/usr/uploads/sina/5cc5cfbde88c0.jpg \"公网ip\")](https://saucer-man.com/usr/uploads/sina/5cc5cfbde88c0.jpg \"公网ip\")\n\n> ipconfig查出来的是你本机的IP地址，也就是内网私有地址，此类地址仅在局域网使用，不能联通外网。\n>\n> 百度查出来的地址是你上网的公网地址。\n\n为什么要使用这种机制呢？\n\nipv4的数目看起来很多，实际上全世界分一分就捉襟见肘了，因此我们使用私有ip，大量的内网ip地址转换为一个或少量的公网IP地址，就减少对公网IP地址的占用；有时候一家公司不需要连接Internet，只需要内部之间相互通信，便可以建立一个局域网，分配私有ip给公司的每个电脑，这样可以防止hacker的入侵。\n\n实际上这个公网地址就是内网连接Internet的身份。外界只会知道你的公网ip地址，内网的地址可以根据需要自行分配，对外界是不可知的。\n\n公网ip的数量稀少，导致一般一个公司或者单位才会分到一个公网ip，一般企业内部或者学校的计算机都是只分配了内网ip。\n\n那如果我们的ip是私有地址，那怎么连接Internet呢？\n\n既然我们都是用的私有ip，那么我们应该只可以在内网内部进行通信，你可能会疑问那么我们是怎么连接上Internet的呢?\n\n这是使用了NAT技术。NAT英文全称是\"Network Address Translation\"，中文意思是\"网络地址转换\"，它是一个IETF(Internet Engineering Task Force, Internet工程任务组)标准，允许一个整体机构以一个公用IP（Internet Protocol）地址出现在Internet上。顾名思义，它是一种把内部私有网络地址（IP地址）翻译成合法网络IP地址的技术。\n\n简单的说，NAT就是在内部网络中使用私有地址，而当内部计算机连接Internet时，就在网关（可以理解为出口，打个比方就像院子的门一样）处，将内部地址替换成公用地址，从而在外部公网（internet）上正常使用，NAT可以使多台计算机共享Internet连接。\n\n实现NAT转换的地方一般是路由器。我们也可以通过对路由器的设置来让外界对公网的访问映射成对局域网某一主机的访问，当我们要将计算机做成服务器时，需要用到此技术。\n\n最后让我们来模拟一下我们上网的流程。\n\n小明是个大学生，电信公司分给他宿舍楼一个ip是118.168.14.117，宿舍楼网络管理员给小明同学分配的私有ip是192.168.1.150。这天小明打开了浏览器想要搜索明天天气怎么样，这时的数据发给了路由器，然后路由器把数据发送给了百度的服务器（私有ip转换为公网ip）。百度接收到之后返回了一段数据到了路由器（百度以为是路由器的公网ip请求的），然后路由找到小明的私有ip（公网ip转换到私有ip），将数据发给了小明，这就是小明真实上网的流程。\n\n7\\. 网关\n------\n\n上述小明的例子中，路由器作为公网和私网的桥梁，这时路由器便充当了网关的角色。实际上路由器并不是内外网的连接点，网关才是，这里路由器可以完成网关的功能完成网络的互连罢了。\n\n网关就是一个网络连接到另一个网络的\"关口\"，也就是网络关卡，又称网间连接器、协议转换器。网关是一个网络层上的概念，既可以用于局域网互连，也可以用于广域网互连。\n\n由于历史的原因，许多有关TCP/IP的文献曾经把网络层使用的路由器称为网关，在今天很多局域网采用都是路由来接入网络，因此通常指的网关就是路由器的IP！\n\n网关\\\n网关(Gateway)又称网间连接器、协议转换器。默认网关在网络层上以实现网络互连，是最复杂的网络互连设备，仅用于两个高层协议不同的网络互连。网关的结构也和路由器类似，不同的是互连层。网关既可以用于广域网互连，也可以用于局域网互连\n\n网关实质上是一个网络通向其他网络的IP地址。\n\n比如有网络A和网络B，网络A的IP地址范围为\"192.168.1.1~192. 168.1.254\"，子网掩码为255.255.255.0；网络B的IP地址范围为\"192.168.2.1~192.168.2.254\"，子网掩码为255.255.255.0。\n\n在没有路由器的情况下，两个网络之间是不能进行TCP/IP通信的，即使是两个网络连接在同一台交换机（或集线器）上，TCP/IP协议也会根据子网掩码（255.255.255.0）判定两个网络中的主机处在不同的网络里。\n\n而要实现这两个网络之间的通信，则必须通过网关。如果网络A中的主机发现数据包的目的主机不在本地网络中，就把数据包转发给它自己的网关，再由网关转发给网络B的网关，网络B的网关再转发给网络B的某个主机。\n\n所以说，只有设置好网关的IP地址，TCP/IP协议才能实现不同网络之间的相互通信。那么这个IP地址是哪台机器的IP地址呢？网关的IP地址是具有路由功能的设备的IP地址，具有路由功能的设备有路由器、启用了路由协议的服务器（实质上相当于一台路由器）、代理服务器（也相当于一台路由器）。\n\n广播地址(Broadcast Address)\\\n广播地址(Broadcast Address)是专门用于同时向网络中所有工作站进行发送的一个地址。\n\n在使用TCP/IP 协议的网络中，主机标识段host ID 为全1 的IP 地址为广播地址，广播的分组传送给host ID段所涉及的所有计算机。例如，对于10.1.1.0 （255.255.255.0 ）网段，其广播地址为10.1.1.255 （255 即为2 进制的11111111 ），当发出一个目的地址为10.1.1.255 的分组（封包）时，它将被分发给该网段上的所有计算机。\n\n根据IP地址和子网掩码求 网络地址 和 广播地址\\\n将IP地址和子网掩码换算为二进制，子网掩码连续全1的是网络地址，后面的是主机地址，虚线前为网络地址，虚线后为主机地址\n\nIP地址和子网掩码进行与运算，结果是网络地址（即主机号全0是网络地址）\n\n将运算结果中的网络地址不变，主机地址变为1，结果就是广播地址\n\n地址范围就是含在本网段内的所有主机\n\n网络地址+1即为第一个主机地址，广播地址-1即为最后一个主机地址，\\\n由此可以看出地址范围是： 网络地址+1 至 广播地址-1\n\n主机的数量=2^二进制位数的主机-2\\\n减2是因为主机不包括网络地址和广播地址。\n\n示例\\\n一个主机的IP地址是202.112.14.137，掩码是255.255.255.224，要求计算这个主机所在网络的网络地址和广播地址\n\n根据子网掩码可以分割网络号+主机号\\\n255.255.255.224 转二进制：\n\n11111111 11111111 11111111 11100000\n\n网络号有27位，主机号有5位\n\n网络地址就是：把IP地址转成二进制和子网掩码进行与运算\n\n11001010 01110000 00001110 10001001\n\nIP地址&子网掩码\n\n11001010 01110000 00001110 10001001\n\n11111111 11111111 11111111 11100000\n\n------------------------------------------------------\n\n11001010 01110000 00001110 10000000\\\n1\\\n2\\\n3\\\n4\\\n5\\\n6\\\n7\\\n即：202.112.14.128\n\n计算广播地址\\\n广播地址：网络地址的主机位全部变成1 ，10011111 即159 即：202.112.14.159\n\n主机数\\\n主机号有5位，那么这个地址中，就只能有25-2=3025-2=30个主机\n\n因为其中全0作为网络地址，全1作为广播地址\n\n根据每个网络的主机数量进行子网地址的规划和计算子网掩码\\\n这也可按上述原则进行计算。\n\n比如一个子网有10台主机，那么对于这个子网需要的IP地址是\\\n10＋1＋1＋1＝13\\\n注意：加的第一个1是指这个网络连接时所需的网关地址，接着的两个1分别是指网络地址和广播地址。\\\n因为13小于16（16等于2的4次方），所以主机位为4位。\n\n而 256－16＝240 所以该子网掩码为255.255.255.240。\n\n如果一个子网有14台主机，不少人常犯的错误是：依然分配具有16个地址空间的子网，而忘记了给网关分配地址。这样就错误了，因为：\\\n14＋1＋1＋1＝17\\\n17.大于16，所以我们只能分配具有32个地址（32等于2的5次方）空间的子网。这时子网掩码为：255.255.255.224\n\n5） 主机的数量\n\n206 110 4 0/18被划分成16个子网，每个子网掩码？\\\n（划分成16个子网，根据子网掩码/18就表示有18个1，就要从的IP地址的主机位借4位来用作网络位！）\n\n子网掩码是255.255.252.0\n\n每个子网可以容纳的主机数是1024台。\n\n下面我来给你详细解答：\n\n206.110.1.0 /18 由最后的那个/18，我们可以知道这个IP已经规定了它的网络位是18位，它默认的子网掩码就是11111111.11111111.11 | 000000.00000000(其中1代表网络位,0代表主机位)\n\n可以看出我们可以操作的位数就是后面的14个0，也就是说我们可以在地面划分出几位作为子网的网络位，进而来划分子网。要求是切分成16个子网，我们知道2的4次方刚好等于16，这就说明子网网络位的位数是4位，那14-4=10就是子网的主机位。所以上面我写的那串二进制就可以变成：11111111.11111111.111111 | 00.00000000(其中1代表网络位,0代表主机位)\n\nip段/数字-如192.168.0.1/24是什么意思?\\\n后面这个数字标示了我们的网络号的位数，也就是子网掩码中前多少号为1\n\n129.168.1.1 /24 这个24就是告诉我们网络号是24位\n\n也就相当于告诉我们了\n\n子网掩码是：11111111 11111111 11111111 00000000\n\n即：255.255.255.0\n\n172.16.10.33/27 中的/27\n\n也就是说子网掩码是255.255.255.224 即27个全1\n\n11111111 11111111 11111111 11100000","tags":["IP划分","IP地址"],"categories":["计算机基础"]},{"title":"个人改变命运的关键方式","url":"/2019-10-09/financial_freedom_change_destiny/","content":"\n\n专注做好一件事，锚定稳定的市场需求、做出真正的高质量，是以不变应万变、立于不败之地的一条正道。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-9c74fca868c1b356)\n\n**今天的热门职业**\n\n**十年之后可能变“惨”业**\n\n**1.今天的金领职业可能是明天的“惨业”**\n\n我先讲几个现象。第一个现象，大学里有个很热门的专业，很多学霸奔这个专业去，那就是金融专业。因为金融专业毕业后找工作，收入高。尤其是证券业，是典型的金领职业。所以高材生趋之若鹜。\n\n哈佛、耶鲁、哥伦比亚这类顶级学校的毕业生，华尔街行业是他们的首选，因为起薪就非常高。中国目前也是这样，我相信找工作时能找到比这个行业收入更高的工作是不太容易的，当然，这两年互联网行业也可以，但证券业肯定是收入最高的行业之一。\n\n1980年代后期，台湾股市繁荣，证券业兴旺发达，出首富、出暴发户，顶级聪明的人才涌进证券业。这很正常，哪个行业收入高人才就往哪个方向涌。1993年我南京大学毕业到深圳，进证券公司工作，那时候大陆证券人都羡慕和学习台湾证券业。但是前两年我出差去台北，看报纸看到一条新闻，令我很感慨，新闻大标题写着：证券业已成为台湾十大Can业之一，哪个Can？悲惨的惨，惨业。**现在台湾的年轻人找工作，据说是实在没地方去了才勉强去证券公司工作。**\n\n为什么？因为台湾的证券市场已经衰落了，很冷清，没多少交易量，而且台湾经济没有纵深腹地，股市很难再看到大牛市回春了。当年的盛况一去不返，金领职业就变成了惨业。\n\n今天中国的金融业仍然是炙手可热的行业，但大家有没有想过十年后，**会不会有哪天，也变成惨业了呢？说不准。**\n\n如果你把眼光放到未来15-20年的长周期来看，今天的金融热现象其实存在着很大的问题。\n\n**第一，国家的总体发展方向是脱虚向实，实体经济是实，而金融是虚**，再不搞脱虚向实，国家的经济就要发生危险了。想想看，如果大家都不去干实业、搞生产，倒倒钱、搞搞金融交易就能发财，我们的国家还能有前途吗？所以脱虚向实是必由之路。\n\n**第二，这么多年来，大量优秀人才都读金融专业，金融行业人才的供求关系是严重供过于求。**\n\n今天你毕业能进金融行业工作，说明你还是挺有竞争力的，胜出了。如果10年或者15年后，你35、40岁了，正是年富力强的时候，发现自己处在了一个“惨业”当中，怎么办？那时候你再想转到一个热门行业，估计就很难了。因为那时候你已经是中年了，除了金融经验，你不会干别的，也很难再重新学习了。\n\n我说这些话，并不是说我对金融业的未来发展有这么笃定的判断，这个我没把握，我不是神仙，也不是算命的，未必有这个远见。我只是想告诉大家一个观念：**在你二十几岁到三十岁前后的时间里，你一定要思考未来中国社会将如何演变，从而根据这个演变来确定你个人的职业生涯和财富生涯。**\n\n简单来说，**在你职业年龄的这个时间窗口，你要看到未来十年二十年的产业兴衰和社会变迁**，如果你没这个眼光，你的职业选择可能是盲目的或者不自觉的。\n\n**2.今天的好生意，明天可能做不下去了**\n\n我来说第二个现象，汽车与出行。去年有个非常震撼的消息，不知道大家看到了没有，华尔街日报出了一个很刺激的新闻：\n\n*2018年无人驾驶汽车公司Waymo开始出租车商业化运营服务，最新估值1750亿美元，相当于传统汽车三巨头的市值之和。*\n\n要知道现在中国3500多家上市公司里，超过500亿人民币市值的公司才200多家，超过一千亿市值的就更少了，而这家Waymo公司最新估值1750亿美元，一万多亿人民币啊，这是一个令人震撼的数字。Waymo的估值，预示着无人驾驶时代的真正到来。 \n\n滴滴出行的老板程维说过一句话，我印象非常深，他说：**以后，你如果听到一个朋友买了一辆私家车，你会觉得他很奇怪，就像今天听一个朋友说他买了一匹马。**为什么？因为以后全部是智能出行了，人们不再需要私家车。如果谁家的车上还有个司机，那就更奇怪了，因为将来都是无人驾驶。\n\n据说将来政府还可能立法，禁止人为开车，因为人为开车会疲劳、酒驾、情绪影响等，会发生交通事故，而无人驾驶安全度更高。\n\n现在看来，关于汽车和出行，这是大趋势，必然走向。相应的，一系列与此相关的生意、职业，将来都要大变化。比如办驾校一直是个好生意，将来就会办不下去了，因为很少人需要考驾照。比如4S店、汽车维修和服务、汽车保险等等生意，都会变得面目全非。\n\n移动互联网刚出现时，我就有预感，如果冒出一些人用移动互联网的商业模式进入出租车行业，传统的出租车行业将被颠覆，传统的出租车行业一直靠牌照和管制赚取垄断利润，长期以来这是个难以撼动的好生意，移动互联网会把它搞掉。果然，后来冒出了滴滴快的这类企业，出租车行业的垄断地位就被搞掉了，如果不是政府保护，滴滴这样的公司是会彻底颠覆掉出租车行业的。\n\n社会的变化其实很快，十年时间就足以天翻地覆。证券业、汽车业、出行业是这样，其他行业未来又怎么可能不变呢？**现在热衷的行业、生意和职业，未来会不会继续存在，都是一个问题。**\n\n**3.行业的趋势决定了你未来的收入**\n\n我来说第三个现象，最近有消息说北京离婚率达到48%，是不是言过其实我不知道，反正现在离婚的人确实很多。**2018年中国成年单身人口超过2.4亿，预计未来会达到4亿左右**，现在越来越多的精英和高素质人才都不想结婚了，或者结了婚赶快离掉过单身日子。\n\n这就造成了一个很有意思的消费现象：过去一年，天猫迷你款商品销售增速比普通款高出15%，迷你款微波炉销售增长了970%，宠物经济的规模接近2000亿了。单身生活，什么用品都仅需满足一个人的用量，所以迷你款就够了。\n\n多说一句，目前如果要找一个百分之百看好、必然会增长的行业，那宠物行业必居其一，单身人士，宠物成为必需品。同理，为单身人群提供各种服务和产品的行业及相关职业，大趋势向好。 \n\n另一个我很看好的产业是养老。虽然目前做养老行业挣钱的不多，也非常辛苦，但**15年之后，养老这个行业一定类似当年的寿险、保险行业一样非常火爆，从职业选择上讲，是很好的行业。**\n\n当然，与养老相关的不单是开养老院，我昨天看新闻，说**上海、大连这些城市的老年大学特别火**。以前我们讲入学难，说的是小孩入学难，但现在老年大学入学比小孩入学还难，有的老人排队排了五年都没轮到上老年大学。这是一个缩影，将来，与养老相关的事、相关的职业都会变的很好，不要把目光只局限在养老院上。\n\n昨天央视报道说，北京月嫂起薪是八千块，熟练的月嫂月薪两万五，而且提前半年预约才能排到队，否则没档期。说到这里，我也想问一下各位目前的薪资如何，一个985、211毕业的研究生，现在的起薪能不能达到这个标准？所以，**能不能跟上社会的变化、行业的趋势，直接决定了你职业的收入水平。**\n\n总体来说，**未来十年，物联网+大数据+人工智能+各种新科技，将聚变出全新的产业、全新的商业和全新的职业**，在这个过程中，一批夕阳职业会衰败，一批朝阳职业会兴起，职场命运，财富丰瘠，首先系于此。\n\n在你的职场第一个十年，什么是夕阳职业，什么是朝阳职业？这个判断，很重要。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-693bc1b5972fe4ba)\n\n**35岁之前****必须想清楚4个问题**\n\n讲完大的趋势，我们回过头再来探讨，职场的第一个十年到底该怎么过？我不知道大家有没有系统的思考过自己的职业，或者有没有考虑过究竟应该怎么思考职业？这是我总结的几点经验，希望大家听完后可以走出职业的迷茫期，把握住25岁到35岁这个重要的时间窗口。\n\n**1.了解社会、适应社会**\n\n今天来听课的人，我看你们都很年轻。在你们年轻的这个阶段，有一个任务就是要了解和适应社会。\n\n我经常跟年轻人讲读书有一个危险，就是一口气读到博士，**从高中、本科到研究生再到博士，一口气读下来，是不好的**。等你博士毕业，基本上也在30岁附近了，你在书本和校园里一呆就呆了30年，想想看，你对社会还能有多少认识？你的视野已经局限在你所攻读的专业领域了，等你毕业时都不知道自己适不适合干这行。这是很可怕的。\n\n读书要读读停停、停停读读，我自己是无意中这样读过来的。本科毕业后我去工作了两年，工作两年后我觉得不满意，然后就去考了南京大学的研究生，研究生毕业后去深圳工作了几年，然后去芝加哥留学了解了解美国，最后又到人民大学读了博士，最终了却了自己的学历心愿。\n\n读读停停、停停读读的过程，使我两边不耽误，一边是增长知识、解决学历，一边是了解社会、了解职业。根据对社会对职业的了解，来调整和补充自己的学习。\n\n当然，我这个观点，是针对文科说的。如果你学的是理工科，那就另说，比如学医的、搞科研的，你需要硕博连读，需要持续呆在实验室不受外界干扰，否则你很难在本专业里有突破和发展。 \n\n另外，我也希望大家大学毕业后不要选择留校工作，因为留校工作往往也就意味着你一辈子都要待在校园里，一生都走不出校门，那你还怎么去了解这个社会、了解其他职业？\n\n所以，职场的第一个十年，哪怕你没升官、没发财甚至好像什么都没做成，但**只要你明白了原来社会就是这么回事儿，知道了该怎么在这里面混，发现了原来还有这样的行业能让你在里面干，那你这个十年就不算白过，你做了你应该做的事，我觉得这也是年轻人第一个十年应该完成的刚性任务。**\n\n**2.认识自己、调整自己、修理自己、磨炼自己**\n\n认识自己、调整自己、修理自己、磨炼自己，这几个字的道理大家都懂，但要真正做到，却需要用一生的时间。我已经是一个50几岁的人了，现在还经常修理自己，因为我经常能发现自己的毛病，而且有些毛病很顽固，修理了十年都没修理过来。 \n\n我有个非常好的企业家朋友，云南人，成就很大也非常有境界，他抽烟抽的厉害。有一次我说，你抽那么多烟干吗？戒了它吧。他不说话，我说你戒不了吧！他说我就没想戒，我想开了，想抽就抽吧，人生就这么活吧。他说，想戒怎么会戒不了？**一个男人，如果连烟都戒不了，还能干成啥事？**这句话给我留下了非常深的印象。连烟都戒不掉，连游戏都戒不掉，连熬夜都戒不掉，连懒散都戒不掉，就这么一些很简单的事都做不到，那还谈什么追求成功？ \n\n但事实上，这些小事，确实很不容易做到。**认识自己、调整自己和修理自己是非常难的事，比了解社会都难。**如果你在调整自己、管理自己方面不当真，放任自己，那你就少扯谈什么人生追求和成功了。喜欢打游戏你就打吧，愿意喝王者农药就喝吧，总是懒散就由着自己的性子懒散去吧，玩一辈子玩到死拉倒。如果你真的是这么选择了，倒是可以，你想明白你的人生就是要这样，那没什么好说的。 \n\n所以，**我希望大家在探讨成功前，首先要想一想该如何管理自己。李嘉诚说，很多人的人生失败，本质上是自我管理上的失败造成的。**养成自我管理的习惯，是职场第一个十年的重要任务。\n\n**3.补课、补证、补学历**\n\n**第一，你要补课，补你的知识。**不知道你们有没有这个感觉，发现自己大学学的知识在工作中基本没用，一方面可能是你没有从事与本专业相关的工作，另一方面是大学教学的知识跟实践脱节太远，所以工作后的第一个十年，需要补课，**一定要把自己在职场能用的着的一系列专业、行业知识补上去。** \n\n**第二，要补证。**你想从事的职业都需要哪些证书，做律师要考司法考试，做会计师要考CPA，想从事基金业就要考基金从业资格，想做投行就要考保荐代表人，职场的前几年要把工作所需的证书补上。 \n\n**第三，补学历。**如果你的行业入门学历门槛很高，非要985硕士以上学历，那你就要考985硕士，甚至要考博士，当然学习的过程中也不要忘了你所需要的职业技能。\n\n**4.建立良好的工作生活形态**\n\n最后是一个很现实的问题， 就是工作十年左右时间，**35岁之前，你要尽量解决基本生活问题，要有位子（职务）、票子（收入）、房子、车子，建立工作、婚姻和家庭生活的基本形态。**职场的第一个十年，在经济收入上，你**最好要实现收支上的大体平衡**，不要过度负债，否则债务和供房的压力，会让你变得急功近利，急于成功，心态和精神都可能变得扭曲。 \n\n总之，了解社会，修理自己，补课补证，位子票子，这4个方面的问题，是职场的第一个十年、大约35岁之前必须解决的。\n\n![image.gif](https://upload-images.jianshu.io/upload_images/18390058-e3e207b2a732b923.gif)\n\n**人的一生有3次机会**\n\n**最近的一次在2019年**\n\n我先讲一个人，他叫周金涛，中信建投的首席经济学家，他在2015年12月份做过一场演讲，2016年3月又在上海清算所重新讲了12月那场演讲的PPT，后来整理成了一篇著名的文章，叫做“人生就是一场康波”。\n\n这篇文章的观点后来红遍了整个资本市场，为什么？因为他的观点和预言，特别明确、锋利，而且都被后来发生的事实验证了，让人很震撼。 \n\n所谓康波，是前苏联经济学家康德拉季耶夫在1925年提出的一个世界经济运动长周期规律，经济学上称为康德拉季耶夫周期。康波理论认为：**资本主义世界的经济体系是以45-60年作为一个周期而循环波动的，随着生产和科技的演进，经济趋势通常会在45-60年的时间内发生一次由兴到衰的转变。**具体的理论在这里我们就不展开了，大家只要知道这是一个非常经典的宏观经济理论或者说一个分析方法就行了。\n\n![image.gif](https://upload-images.jianshu.io/upload_images/18390058-2b74daefd329060b.gif)\n\n周金涛作为经济学家，对康波周期很有领悟和心得，他把这个理论应用在分析世界经济和中国经济当中。在2015年12月的那次演讲中，他说我们处在这样的一个60年康波周期当中：1982-1990年是回升期；1991-2004年为繁荣期；2004-2015年是衰退期；**2015年之后就进入到了本次康波的萧条期****，**这个阶段将会持续到2025年，也就是说**未来十年，我们注定将在萧条中度过。**在这个萧条期，房地产会转跌、互联网+的热潮会退去、全球资产价格全面回落。一个个都应验了。\n\n2015年底，周金涛的原话，他说未来的4年里，2016年将是最好的年景，**2018年到2019年将是康波周期的万劫不复之年，是六十年中最差的。**现在就是2019年了，我们接下来面临的挑战可想而知。他说：“一个康波周期是60年，而每隔20年却会出现一个波动性的转折点，所以从我们工作开始算起，**人的一生只有三次机会，只要抓住其中一次机会，至少可以成为中产，如果一次机会也没抓住，那么变富无望，终生贫穷。**”\n\n他说最近的3次机会是：\n\n第一次机会是2008年，如果你在那时候买股票、买地产，现在肯定很富裕。 \n\n第二次是2019年，就是今年，因为今年会是历史上最低的低点，今年下半年所有的资产都有可能触底反弹。我们现在看到了春节过后股票也开始暴涨了，这也是一个明显的迹象。\n\n第三次是在2030年附近。\n\n根据周金涛的观点，在座的各位，三次机会中你们还能赶上两次，2019年和2030年附近。我希望你们都能抓住这个机会。\n\n今年春节后，股市开始活跃，大家又蠢蠢欲动，做起了发财梦，想抓住2019年的这次机会。很多人都希望我讲讲怎么看今年的股市。虽然这个点上我不方便说这个话题，但既然你们来了，我还是要告诉你们，**股市有风险，投资需谨慎，否则冒然入市，结果很可能就是“亲人两行泪”。** \n\n告诉你们个事实，**如果你不是职业做证券的人，不是从事投资职业的人，你想通过炒股票赚钱，赢的概率很难超过20%，输的概率80%以上，不管行情好坏，你炒股票大概率是亏钱的，要不然怎么叫你“韭菜”？统计规律逃不出二八定律，炒股票，20%的人可能挣钱，80%的人要亏钱。**你会是那20%吗？你指望一个80%概率要亏钱的事来让你发财致富吗？\n\n大家别忘了周金涛是证券公司的首席经济学家，他的服务对象，是机构投资者、职业投资者。你如果不是职业投资者，他所说的机会对你的适用性，你就要小心，应用之妙，存乎于心。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-676ef2ec341fd332)\n\n**房价会大跌吗？****或者继续涨？**\n\n钱和房子，是我们大多数人的一生中两个无法逃避的刚性需求，我称之为两个硬核刚需。钱的背后是金融问题，而房子的背后是房地产问题。我接下来就跟大家聊聊金融和房地产。\n\n中国的金融和房地产，大趋势怎么看？\n\n**1.什么是金融？**\n\n首先讲个笑话，说有个人在美国上大学，选课时发现一门课叫“选择与未来”，便果断的选择了它，希望通过这门课来学习些人生的经验和智慧，指导自己的未来选择。然而上课后他发现不知所云，一点也听不懂，最主要的是老师讲的话题跟人生话题一点都不沾边。 \n\n为什么？因为这门课叫《Option  and  Future》，翻译过来是《期权与期货》，这是一门金融课程而不是什么选择与未来。他不懂金融专业术语，所以弄出了这样一个笑话。\n\n我为什么讲这个笑话？我是想说，金融是什么，金融是一套自己的话语系统和知识体系，如果你不在这个圈子里，那套话语系统像是黑话，你听不懂。**学习金融专业就是学习一套话语系统和知识体系，如果不了解这套话语系统，你就无法谈论金融话题。**\n\n那金融到底是个什么事呢？金融在经济运行系统里非常复杂，你很难一时半会说清楚，而且你终其一生研究这个事也未必能搞的明白，但金融其实也很简单，简单到就五件事：\n\n第一，利率。利率是货币供应决定的，发多少钞票，市场上供应多少钱，有多少人需要资金，供求关系决定利率，利率是资金的价格。\n\n第二，股票债券，即资本市场。\n\n第三，期货，主要是大宗商品。\n\n第四，汇率，外汇供求决定汇率，汇率是外汇的价格。\n\n第五，房地产。以前是黄金和石油，现在主要是房地产，尤其在中国。\n\n下面，再和大家讲讲中国金融体系的三大特点：\n\n**第一，银行系统庞大，资本市场很小，这是造成中国经济高杠杆系数运行的主要原因。**\n\n货币供应到银行，银行贷款给企业、给实体部门，都是贷款，结果高杠杆造成了整个中国经济在高杠杆和高风险系数上运行，这是中国经济非常大的一个问题，这个问题的解决必须靠金融改革。\n\n**第二，基于房地产金融。**\n\n中国所有的金融、所有的信用都是基于房地产抵押的，纯信用的贷款很少，中国的银行系统是基于房地产的。\n\n**第三，基于美元的外汇。**\n\n全世界各个国家的外汇都是基于美元，否则没戏，虽然现在人民币好一点了，但整体上还是基于美元的外汇。\n\n这五件事和三大特点决定着中国金融改革发展的三大方向：\n\n**（1）****扩大资本市场，增加直接融资，使中国经济总体去杠杆，风险系数降下来。**\n\n换句话说，在金融体系里，银行体系将来会保持稳定甚至慢性萎缩，资本市场体系、证券业，还是扩张的趋势。找金融工作的同学们注意啦，要尽量去资本市场体系找工作，次选银行系统。\n\n**（2）****从****基于房地产的金融过渡到基于产业结构效率的金融，从基于抵押的信用过渡到基于大数据的信用。**\n\n数据资产，成为金融业的核心资源和竞争力。 \n\n**（3）人民币国际化，逐步成为世界各国储备货币。**\n\n**2.怎么看待房地产？**\n\n讲完金融我们再来聊聊房地产， 大家先来看看下面这张图表。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-6cda0c53996b8f68)\n\n你们看，美国股票总市值和房地产总市值都是30万亿美元，欧盟是20万亿美元和20万亿美元，日本的房产市值多一点是10万亿美元，股票市值6万亿美元。\n\n美国、欧盟、日本都是成熟的、发达的市场经济国家，它们的股票总市值和房产总市值，大致是相当的。假定这种状态是成熟的、发达的市场经济国家的正常态的话，那么我们可以推测，**中国只要走市场经济道路，总是要不断地走向成熟的、发达的市场经济形态，最终会变得股票总市值和房产总市值，大致相当。**\n\n目前，中国的房产总市值65万亿美元，股票总市值6万亿美元。按上述推测，未来的趋势，无非是三个变化方向：\n\n1.资本市场增长，向上赶上房地产市值。\n\n2.房地产市值萎缩，向下靠拢股票总市值。\n\n3.股票市值增长与房地产市值下滑同时进行，走向平衡。\n\n据此，我们可以得出一个明确的答案，就是**房地产不可能像过去15年那样狂飙突进了。总体上是放缓脚步、保持稳定，不可能再突飞猛进，而资本市场还大有增长空间。**\n\n当然，大家也不必为房地产下跌感到紧张。去年很多地产大佬来问我房地产会不会下跌、崩盘？我说你一万个放心，**房地产不可能大跌更不可能崩盘。**原因很明显，**中国金融体系三大特点中的第二点决定了房地产不可能大跌，因为中国金融是基于房地产的金融，中国的银行体系，安危系于房地产价格。**\n\n**如果中国房地产崩盘，中国银行体系就会跟着崩盘**，进而整个中国金融体系都可能土崩瓦解，金融危机，就要大爆发。这种事怎么可能出现？这种事怎么可能允许出现？如果出现了，那就是“大地震”一样的天灾，人为控制不了的情况，那就另说。\n\n所以，中国的房地产会在相当长的一段时间内保持稳定，房价长时间继续涨，很难；普遍大跌，更不可能。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-3ca29cca73be6cb4)\n\n**可叹多少聪明人**\n\n**一生没有做好一件事**\n\n社会底层出身的人，不成功就没有出路，所以人生的责任就是必须追求成功。我说人人都可以成功，你信不信？\n\n做好一罐辣椒酱，你会不会？陶华碧专心致志做好辣椒酱，就成了老干妈，每年几十亿营业收入，几亿元的纯利。\n\n做好一个火锅，你会不会？张勇专心致志做好普通口味的火锅，就成了海底捞，海底捞上市了，现在市值超过一千亿元。\n\n做好一个鸭脖子，你会不会？绝味鸭脖现在市值100多亿元。 \n\n做好一个打火机，你会不会？普通打火机几块钱一个，ZIPPO打火机几千块一个，成为男人的至爱和标配。\n\n做好一瓶酱油，你会不会？海天酱油，现在市值2000亿元。\n\n做好寿司，你会不会？小野二郎终其一生，专心致志做寿司，做成了享誉世界的寿司之神。\n\n做好避孕套，就成了杜蕾斯；做好棉衬衫，就成了香港溢达；做好运动服装的代工厂，就成了申洲国际，每年纯利润几十亿元，市值一千多亿元。\n\n我想问问你们，**你们上了名牌大学，受了高等教育，成了高素质人才，让你一辈子做好一件事、做好一个产品、做好一个服务，你能不能做的好？**\n\n能做得好，就能成功。做不好，就别扯淡。\n\n大家应该明白了，我说这些，就是想向你们传递一个道理：“心心在一艺，其艺必工；心心在一职，其职必举。”只要你能够倾一生的时光与精力，倾一生的思维与智慧，把一件事做到极致，那么你就能成功。可叹世上不知多少聪明人，一生没有做好一件事。\n\n孟子和王阳明说：人人皆可成圣贤！如果有人跟你说“人人皆可成功”，你千万别把它当作庸俗的成功学，当然更不要把说者看作是孟子王阳明那样的圣贤。\n\n清静一念，即刻就是菩提道场；你一正心，朗然见得成功之道。人人皆可成功，只是一个常识常理而已。\n","tags":["税后收入","睡后成长"],"categories":["财务自由"]},{"title":"MySQL 磁盘","url":"/2019-10-06/mysql/MySQL学习笔记（Day018：磁盘）/","content":"\nMySQL学习笔记（Day018：磁盘）\n=============================\n@(MySQL学习)\n\n[TOC]\n\n## 一. iostat\n\n```bash\n#\n# 安装 iostat\n#\nshell> yum install sysstat \n# debian 系： apt-get install sysstat\n\n# 使用\nshell> iostat -xm 3 # x表示显示扩展统计信息，m表示以兆为单位显示，3表示每隔3秒显示\n# 输出如下：\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n           0.58    0.00    0.33    0.00    0.00   99.08\n\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsda               0.00     0.00    0.00    0.67     0.00     0.00     8.00     0.00    2.00    0.00    2.00   1.00   0.07\nsdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\n```\n\n| CPU属性| 说明|\n|:------:|:---:|\n|%user|CPU处在用户模式下的时间百分比|\n|%nice|CPU处在带NICE值的用户模式下的时间百分比|\n|%sys|CPU处在系统模式下的时间百分比|\n|%iowait|CPU等待IO完成时间的百分比|\n|%steal|管理程序维护另一个虚拟处理器时，虚拟CPU的无意的等待时间的百分比|\n|%idle|闲置cpu的百分比|\n>**提示：**\n如果%iowait的值过高，表示硬盘存在I/O瓶颈;\n如果%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。\n如果%idle值如果`持续`很低，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。\n\n|Device属性 | 说明 |\n|:----------:|:----:|\n|rrqm/s\t    |每秒进行 merge 的读操作数目 |\n|wrqm/s\t    |每秒进行 merge 的写操作数目|\n|r/s\t    |每秒完成的读 I/O 设备次数|\n|w/s\t    |每秒完成的写 I/O 设备次数|\n|rsec/s\t    |每秒读扇区数|\n|wsec/s\t    |每秒写扇区数|\n|rkB/s\t    |每秒读K字节数|\n|wkB/s\t    |每秒写K字节数|\n|avgrq-sz\t|平均每次设备I/O操作的数据大小 (扇区)|\n|avgqu-sz\t|平均I/O队列长度|\n|await\t    |平均每次设备I/O操作的等待时间 (毫秒)|\n|svctm\t    |平均每次设备I/O操作的服务时间 (毫秒)|\n|%util\t    |一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比|\n>**提示：**\n如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。\n如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；\n如果 await 远大于 svctm，说明I/O队列太长，io响应太慢，则需要进行必要优化。\n如果avgqu-sz比较大，也表示有当量io在等待。\n\n-----\n\n## 二. 磁盘\n### 1. 磁盘的访问模式\n* 顺序访问\n    - 顺序的访问磁盘上的块；\n    - 一般经过测试后，得到该值的单位是`MB/s`，表示为磁盘`带宽`，普通硬盘在 50~ 100 MB/s\n* 随机访问\n    - 随机的访问磁盘上的块\n    - 也可以用MB/s进行表示，但是通常使用`IOPS`（每秒处理IO的能力），普通硬盘在 100-200 IOPS\n\n![磁盘的访问模式](/images/mysql/Selection_001.png)\n\n>拷贝文件属于顺序访问，`数据库`中访问数据属于`随机访问`。\n数据库对数据的访问做了优化，把随机访问转成顺序访问。\n\n\n### 2. 磁盘的分类\n* HDD\n    - 盘片通过旋转，磁头进行定位，读取数据；\n    - 顺序性较好，随机性较差；\n    - 常见转速\n        - 笔记本硬盘：5400转/分钟；\n        - 桌面硬盘：7200转/分钟；\n        - 服务器硬盘：10000转/分钟、15000转/分钟；\n        - SATA：120 ~ 150 IOPS\n        - SAS ：150 ~ 200 IOPS\n    \n    >从理论上讲，15000转/分钟，最高是 15000/60 约等于250IOPS\n    >由于机械盘片需要旋转，转速太高无法很好的散热\n\n    >如果一个HDD对4K的块做随机访问是0.8MB/s，可通过`0.8 *（1 / 4）= 200` 或者 `（0.8 * 1000） / 4=200`得到`IOPS`，但是这个值存在部分干扰因素，如cache等\n\n* SSD\n    - 纯电设备\n    - 由FLash Memory组成\n    - 没有读写磁头\n    - MLC闪存颗粒对一般企业的业务够用。目前SLC闪存颗粒价格较贵\n    - IOPS高\n        - 50000+ IOPS\n        - 读写速度非对称 以 [INTEL SSD DC-S3500](http://www.intel.com/content/www/us/en/solid-state-drives/ssd-dc-s3500-spec.html)为例子：\n            - Random 4KB3 Reads: Up to 75,000 IOPS \n            - Random 4KB Writes: Up to 11,500 IOPS\n            - Random 8KB3 Reads: Up to 47,500 IOPS\n            - Random 8KB Writes: Up to 5,500 IOPS\n        \n        - 当写入数据时，要先擦除老数据，再写入新数据\n        - 擦除数据需要擦除整个区域（128K or 256K）一起擦除（自动把部分有用的数据挪到别的区域）\n            \n            > 对比发现4K性能要优于8K的性能，几乎是2倍的差距，当然16K就更明显，所以当使用SSD时，建议数据库页大小设置成4K或者是8K，`innodb_page_size=8K`）\n            > 上线以前，SSD需要经过严格的压力测试（一周时间），确保性能平稳\n        \n    - Endurance Rating\n        - 表示该SSD的寿命是多少\n        - 比如450TBW，表示这个SSD可以反复写入的数据总量是450T（包括添加和更新）\n    \n    \n    - SSD线上参数设置\n        - 磁盘调度算法改为Deadline\n\n            ```bash\n            echo deadline > /sys/block/sda/queue/scheduler  # deadline适用于数据库，HDD也建议改成Deadline\n            ```\n            \n        - MySQL参数\n            - `innodb_log_file_size=4G`  该参数设置的尽可能大\n            - `innodb_flush_neighbors=0`\n            \n            > 性能更平稳，且至少有15%的性能提升\n        \n    - SSD 品牌推荐\n        - Intel\n        - FusionIO\n        - 宝存\n        \n    - 不是很建议使用PCI-E的Flash卡（PCI-E插槽的SSD） \n        - 性能过剩\n        - 安装比较麻烦\n\n### 3. 提升IOPS性能的手段\n* 通过 RAID 技术\n    - 功耗较高\n    - IOPS在2000左右\n\n* 通过购买共享存储设备\n    - 价格非常昂贵\n    - 但是比较稳定\n    - 底层还是通过RAID实现\n\n* 直接使用SSD\n    - 性能较好的SSD可以达到 `万级别的IOPS`\n    - 建议可以用SSD + RAID5，RAID1+0太奢侈\n\n\n### 4. RAID类别\n\n* RAID0\n![RAID0](/images/mysql/130px-RAID_0.svg.png)\n    - 速度最快\n    - 没有冗余备份\n\n* RAID1\n![RAID1](/images/mysql/130px-RAID_1.svg.png)\n    - 可靠性高\n    - 读取速度理论上等于硬盘数量的倍数\n    - 容量等于一个硬盘的容量\n\n* RAID5\n![RAID5](/images/mysql/220px-RAID_5.svg.png)\n     - 至少要3块硬盘\n     - 通过对数据的奇偶检验信息存储到不同的磁盘上，来恢复数据，最多只能坏一块\n     - 属于折中方案\n\n* RAID6\n![RAID6](/images/mysql/270px-RAID_6.svg.png)\n    - 至少是4块硬盘\n    - 和RAID5比较，RAID6增加第二个独立的奇偶校验信息，写入速度略受影响\n    - 数据可靠性高，可以同时坏两块\n    - 由于使用了双校验机制，恢复数据速度较慢\n\n* RAID1+0\n![RAID 1+0](/images/mysql/220px-RAID_10.svg.png)\n\n* RAID5+0\n![Alt text](/images/mysql/RAID_50.png)\n\n\n### 5. RAID卡\n* BBU\n    - Battery Backup Unit\n    - 目前几乎所有RAID卡都带BBU\n    - 需要电池保证写入的可靠性（在断电后，将RAID卡`内存`中的缓存的数据刷入到磁盘）\n    - 电池有充放电时间 (30天左右一个周期，充放电会切换成 Write Through，导致性能下降)\n        - 使用`闪存（Flash）`的方式，就不会有充放电性能下降的问题\n\n* RAID卡缓存\n    - Write Backup （`强烈建议开启缓存`）\n    - Write Through (不使用缓存，直接写入)\n\n\n* LSI-RAID卡相关命令\n    - 查看电量百分比\n    \n        ```bash\n        [root@test_raid ~]# megacli -AdpBbuCmd -GetBbuStatus -aALL |grep \"Relative State of Charge\"\n        Relative State of Charge: 100 %\n        ```\n\n    - 查看充电状态\n    \n        ```bash\n        [root@test_raid ~]# megacli -AdpBbuCmd -GetBbuStatus -aALL |grep \"Charger Status\"\n        Charger Status: Complete\n        ```\n    - 查看缓存策略\n    \n        ```bash\n        [root@test_raid ~]# megacli -LDGetProp -Cache -LALL -a0\n        Adapter 0-VD 0(target id: 0): Cache Policy:WriteBack, ReadAdaptive, Direct, No Write Cache if bad BBU\n        ```\n\n### 6. 文件系统和操作系统\n\n* 文件系统\n    - XFS/EXT4\n    - noatime (不更新文件的atime标记，减少系统的IO访问)\n    - nobarrier （禁用barrier，可以提高性能，前提是使用write backup和使用BBU）\n    \n    > mount -o noatime,nobarrier /dev/sda1 /data\n\n* 操作系统\n    - 推荐Linux\n    - 关闭SWAP\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL数据类型","url":"/2019-10-06/mysql/MySQL学习笔记(Day008)/","content":"\nMySQL学习笔记（Day008：数据类型）\n================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. INT类型\n### 1. INT类型的分类\n* **TINYINT**\n    - 存储空间 ： 1 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-128, 127]\n        - 无符号(unsigned) ：[0, 255]\n        \n* **SMALLINT**\n    - 存储空间 ： 2 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-32768, 32767]\n        - 无符号(unsigned) ：[0, 65535]\n        \n* **MEDIUMINT**\n    - 存储空间 ： 3 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-8388608, 8388607]\n        - 无符号(unsigned) ：[0, 16777215]\n        \n* **INT**\n    - 存储空间 ： 4 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-2147483648, 2147483647]\n        - 无符号(unsigned) ：[0, 4294967295]\n        \n* **BIGINT**\n    - 存储空间 ： 8 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-9223372036854775808, 9223372036854775807]\n        - 无符号(unsigned) ：[0, 18446744073709551615]\n\n### 2. INT类型的使用\n* **自增长ID**\n    - `推荐`使用`BIGINT`，而不是INT；\n\n* **unsigned or signed**\n    - 根据实际情况使用，一般情况下推荐`默认`的`sigend`\n    - unsigned 的注意事项\n\n```sql\nmysql> create table test_unsigned(a int unsigned, b int unsigned);\nQuery OK, 0 rows affected (0.14 sec)\n\nmysql> insert into test_unsigned values(1, 2);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a - b from test_unsigned;\nERROR 1690 (22003): BIGINT UNSIGNED value is out of range in '(`burn_test`.`test_unsigned`.`a` - `burn_test`.`test_unsigned`.`b`)'\n\nmysql> select b - a from test_unsigned;   \n+-------+\n| b - a |\n+-------+\n|     1 |\n+-------+\n1 row in set (0.00 sec)\n\nmysql> set sql_mode = 'no_unsigned_subtraction'; -- 这样就可以得到负数\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select a - b from test_unsigned;\n+-------+\n| a - b |\n+-------+\n|    -1 |\n+-------+\n1 row in set (0.00 sec) \n```\n\n>一般情况下使用`int`时，推荐有符号数`(signed)`， 使用无符号数只是比原来多一倍的取值，数量级上没有改变。\n\n>如果需要取值范围很大，直接选择用`BIGINT`\n\n    \n### 3. INT(N) \n\n```sql\nmysql> show create table  test_unsigned;\n+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table         | Create Table                                                                                                                                    |\n+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n| test_unsigned | CREATE TABLE `test_unsigned` (\n  `a` int(10) unsigned DEFAULT NULL, \n  `b` int(10) unsigned DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n* **int(N) 和 zerofill**\n    - int(**N**)中的`N`是显示宽度，`不表示`存储的数字的`长度`的上限。\n    - `zerofill`表示当存储的数字`长度 < N`时，用`数字0`填充左边，直至补满长度`N`\n    - 当存储数字的长度`超过N时`，按照`实际存储`的数字显示\n    \n```sql\nmysql> create  table  test_int_n(a int(3) zerofill);  -- 显示宽度N=3\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> insert into test_int_n values(1);\nQuery OK, 1 row affected (0.04 sec)\n\nmysql> select * from test_int_n;\n+------+\n| a    |\n+------+\n|  001 |   -- 不满 N=3时，左边用0填充\n+------+\n1 row in set (0.00 sec)\n\nmysql> insert into test_int_n values(1111);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_int_n;\n+------+\n| a    |\n+------+\n|  001 |\n| 1111 |  -- 超过N=3的长度时，是什么数字，显示什么数字\n+------+\n2 rows in set (0.00 sec)\n\nmysql> select a, HEX(a) from test_int_n\\G\n*************************** 1. row ***************************\n     a: 001\nHEX(a): 1    -- 实际存储的还是1\n*************************** 2. row ***************************\n     a: 1111\nHEX(a): 457  -- 1111对应的16进制就是457\n2 rows in set (0.00 sec)\n```\n\n> int(N)中的`N`和`zerofill`配合才有意义，且仅仅是显示的时候才有意义，和实际存储没有关系，不会去截取数字的长度。\n\n### 4. AUTO_INCREMENT\n* 自增\n* 每张表一个\n* 必须是索引的一部分\n\n```sql\nmysql> create table test_auto_increment(a int auto_increment);\nERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key\n-- 没有指定为key，报错了\n\nmysql> create table test_auto_increment(a int auto_increment primary key);  -- 指定为key后有效\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> insert into test_auto_increment values(NULL);  -- 插入NULL值\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_auto_increment;\n+---+\n| a |\n+---+\n| 1 |  -- 插入NULL值，便可以让其自增，且默认从1开始\n+---+\n1 row in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(0);  -- 插入 0\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_auto_increment;\n+---+\n| a |\n+---+\n| 1 |\n| 2 |  -- 插入 0 ，自增长为2\n+---+\n2 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(-1);  -- 插入 -1\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_auto_increment;\n+----+\n| a  |\n+----+\n| -1 |   -- 刚刚插入的-1\n|  1 |\n|  2 |\n+----+\n3 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(NULL);  -- 继续插入NULL\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_auto_increment;\n+----+\n| a  |\n+----+\n| -1 |\n|  1 |\n|  2 |\n|  3 |  -- 刚刚插入NULL， 自增为3\n+----+\n4 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values('0'); -- 插入字符0\nQuery OK, 1 row affected (0.04 sec)\n\nmysql> select * from test_auto_increment;\n+----+\n| a  |\n+----+\n| -1 |\n|  1 |\n|  2 |\n|  3 |\n|  4 |  -- 插入字符'0' 后， 自增长为4\n+----+\n5 rows in set (0.00 sec)\n\nmysql> update test_auto_increment set a = 0 where a = -1;  -- 更新为0\nQuery OK, 1 row affected (0.03 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from test_auto_increment;\n+---+\n| a |\n+---+\n| 0 |  -- 原来的 -1 更新为0\n| 1 |\n| 2 |\n| 3 |\n| 4 |\n+---+\n5 rows in set (0.00 sec)\n\n--\n--  数字 0 这个值比较特殊， 插入0和插入NULL的效果是一样的，都是代表自增\n--\n\n-----\n\nmysql> insert into test_auto_increment values(NULL), (100), (NULL); \nQuery OK, 3 rows affected (0.02 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_auto_increment;\n+-----+\n| a   |\n+-----+\n|   0 |\n|   1 |\n|   2 |\n|   3 |\n|   4 |\n|   5 | -- 第一个NULL\n| 100 | -- 100\n| 101 | -- 第二个NULL, 按当前最大的值 +１来设置，之前是100，所以这里101\n+-----+\n8 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(99); -- 插入99\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_auto_increment;\n+-----+\n| a   |\n+-----+\n|   0 |\n|   1 |\n|   2 |\n|   3 |\n|   4 |\n|   5 |\n|  99 |  -- 刚刚插入的 99\n| 100 |\n| 101 |\n+-----+\n9 rows in set (0.00 sec)\n```\n> `AUTO_INCREMENT`是实例启动时，取当前表的最大值，然后 +1 即为下次自增的值。`（MAX + 1)`\n\n> **TIPS:**\n`insert into tablename select NULL;` 等价与 `insert into tablename values (NULL);`\n\n-----\n\n## 二. 数字类型\n### 1. 数字类型的分类\n* 单精度类型：FLOAT\n    - 存储空间：4 字节\n    - 精确性：低\n    \n* 双精度类型：DOUBLE\n    - 占用空间：8 字节\n    - 精确性：低，比FLOAT高\n    \n* 高精度类型：DECIMAL\n    - 占用空间：变长\n    - 精确性：非常高\n\n**注意：财务系统`必须使用DECIMAL`**\n    \n-----    \n    \n## 三. 字符串类型\n\n### 1. 字符串类型介绍\n\n| 类型 | 说明 | N的含义 |  是否有字符集 | 最大长度 |\n|:------:|:------:|:---------:|:------------:|:---:|\n| `CHAR(N)` | 定长字符 | 字符 | 是 | 255 | \n| `VARCHAR(N)` | 变长字符 | 字符 | 是 | 16384 | \n| BINARY(N) | 定长二进制字节 | 字节 | 否 | 255 | \n| VARBINARY(N) | 变长二进制字节 | 字节 | 否 | 16384 | \n| TINYBLOB(N) | 二进制大对象 | 字节 | 否 | 256 | \n| BLOB(N) | 二进制大对象 | 字节 | 否 | 16K | \n| MEDIUMBLOB(N) | 二进制大对象 | 字节 | 否 | 16M | \n| LONGBLOB(N) | 二进制大对象 | 字节 | 否 | 4G | \n| TINYTEXT(N) | 大对象 | 字节 | 是 | 256 | \n| TEXT(N) | 大对象 | 字节 | 是 | 16K | \n| MEDIUMTEXT(N) | 大对象 | 字节 | 是 | 16M | \n| LONGTEXT(N) | 大对象 | 字节 | 是 | 4G | \n\n    \n### 2. N和字符集\n* **char(N)**\n    - 假设当前table的字符集的`最大长度`为`W`, 则`char(N)`的最大存储空间为 `(N X W)Byte`;假设使用`UTF-8`，则char(10)可以最小存储10个字节的字符，最大存储30个字节的字符，其实是另一种意义上的`varchar`\n    - 当存储的字符数`小于N`时，尾部使用`空格`填充，并且填充最小字节的空格\n    \n```sql\nmysql> create table test_char(a char(10));\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql> show create table test_char;\n+-----------+------------------------------------------------------------------------------------------------+\n| Table     | Create Table                                                                                   |\n+-----------+------------------------------------------------------------------------------------------------+\n| test_char | CREATE TABLE `test_char` (\n  `a` char(10) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n+-----------+------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> insert into test_char values('abc');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_char values('你好吗');\nQuery OK, 1 row affected (0.05 sec)\n\nmysql> insert into test_char values('大家好ab');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_char values('大家ab好');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_char values('大家ab好吗');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a, length(a) from test_char;\n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n+----------------+-----------+\n5 rows in set (0.00 sec)\n\nmysql> select a, hex(a) from test_char;\n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |    -- 注意这里，以及下面的16进制值，一会可以对比\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n+----------------+------------------------------+\n5 rows in set (0.00 sec)\n\nmysql> select hex(' ');\n+----------+\n| hex(' ') |\n+----------+\n| 20       |   -- 注意 空格 空格对应的16进制数字是 20\n+----------+\n1 row in set (0.00 sec)\n```\n\n**`test_char`表实际二进制存储文件**\n\n```sql\n-- \n-- shell> hexdump -C test_char.idb\n--\n\n-- 1:abc\n-- 2:你好吗\n-- 3:大家好ab\n-- 4:大家ab好\n-- 5:大家ab好吗\n\n-- ---省略---\n00006070  73 75 70 72 65 6d 75 6d  0a 00 00 00 10 00 24 00  |supremum......$.|\n00006080  00 00 00 02 03 00 00 00  00 1f 33 a8 00 00 00 26  |..........3....&|\n00006090  01 10 61 62 63 20 20 20  20 20 20 20 0a 00 00 00  |..abc       ....| -- 1:后面补了7个空格\n000060a0  18 00 24 00 00 00 00 02  04 00 00 00 00 1f 34 a9  |..$...........4.|\n000060b0  00 00 00 25 01 10 e4 bd  a0 e5 a5 bd e5 90 97 20  |...%........... | -- 2:补充了1个空格\n000060c0  0b 00 00 00 20 00 25 00  00 00 00 02 05 00 00 00  |.... .%.........|\n000060d0  00 1f 39 ac 00 00 00 26  01 10 e5 a4 a7 e5 ae b6  |..9....&........| -- 3:没有补充空格\n000060e0  e5 a5 bd 61 62 0b 00 00  00 28 00 25 00 00 00 00  |...ab....(.%....|  -- \n000060f0  02 06 00 00 00 00 1f 3a  ad 00 00 00 28 01 10 e5  |.......:....(...| --\n00006100  a4 a7 e5 ae b6 61 62 e5  a5 bd 0e 00 00 00 30 ff  |.....ab.......0.|  -- 4：没有补充空格\n00006110  5f 00 00 00 00 02 07 00  00 00 00 1f 3f b0 00 00  |_...........?...|\n00006120  00 29 01 10 e5 a4 a7 e5  ae b6 61 62 e5 a5 bd e5  |.)........ab....|--\n00006130  90 97 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................| -- 5：没有补充空格\n00006140  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n-- ---省略---\n```\n\n* **varchar(N)**\n```sql\nmysql> create table test_varchar(a varchar(10));\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql> show create table test_varchar;\n+--------------+------------------------------------------------------------------------------------------------------+\n| Table        | Create Table                                                                                         |\n+--------------+------------------------------------------------------------------------------------------------------+\n| test_varchar | CREATE TABLE `test_varchar` (\n  `a` varchar(10) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n+--------------+------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> insert into test_varchar values('abc');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_varchar values('你好吗');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_varchar values('大家好ab');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_varchar values('大家ab好');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_varchar values('大家ab好吗');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a, hex(a) from test_varchar;\n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n+----------------+------------------------------+\n5 rows in set (0.00 sec)\n\nmysql> select a, length(a) from test_varchar;\n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n+----------------+-----------+\n5 rows in set (0.00 sec)\n```\n\n\n**`test_varchar`表实际二进制存储文件**\n\n```sql\n-- \n-- shell> hexdump -C test_char.idb\n--\n\n-- 1:abc\n-- 2:你好吗\n-- 3:大家好ab\n-- 4:大家ab好\n-- 5:大家ab好吗\n\n-- 和char一样观察，都没有进行空格的填充\n\n00006070  73 75 70 72 65 6d 75 6d  03 00 00 00 10 00 1d 00  |supremum........|\n00006080  00 00 00 02 08 00 00 00  00 1f 44 b5 00 00 00 29  |..........D....)|\n00006090  01 10 61 62 63 09 00 00  00 18 00 23 00 00 00 00  |..abc......#....| \n000060a0  02 09 00 00 00 00 1f 45  b6 00 00 00 2b 01 10 e4  |.......E....+...|\n000060b0  bd a0 e5 a5 bd e5 90 97  0b 00 00 00 20 00 25 00  |............ .%.|\n000060c0  00 00 00 02 0a 00 00 00  00 1f 4a b9 00 00 00 2c  |..........J....,|\n000060d0  01 10 e5 a4 a7 e5 ae b6  e5 a5 bd 61 62 0b 00 00  |...........ab...|\n000060e0  00 28 00 25 00 00 00 00  02 0b 00 00 00 00 1f 4b  |.(.%...........K|\n000060f0  ba 00 00 00 2c 01 10 e5  a4 a7 e5 ae b6 61 62 e5  |....,........ab.|\n00006100  a5 bd 0e 00 00 00 30 ff  67 00 00 00 00 02 0c 00  |......0.g.......|\n00006110  00 00 00 1f 50 bd 00 00  00 2d 01 10 e5 a4 a7 e5  |....P....-......|\n00006120  ae b6 61 62 e5 a5 bd e5  90 97 00 00 00 00 00 00  |..ab............|\n00006130  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n```\n\n* **插入数据尾部带空格**\n\n```sql\nmysql> insert into test_char values('好好好   ');  -- 后面有3个空格\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_varchar values('好好好   '); -- 后面有3个空格\nQuery OK, 1 row affected (0.02 sec)\n\n-- \n-- test_char 表\n--\nmysql> select a, length(a) from test_char;   \n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n| 好好好         |         9 |  -- 只有9个字节\n+----------------+-----------+\n6 rows in set (0.00 sec)\n\nmysql> select a, hex(a) from test_char;\n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n| 好好好         | E5A5BDE5A5BDE5A5BD           | -- 无填充空格\n+----------------+------------------------------+\n6 rows in set (0.00 sec)\n\n\n--\n-- test_varchar表\n--\nmysql> select a, length(a) from test_varchar;\n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n| 好好好         |        12 |  -- (好好好)9个字节 +  3个字节的空格\n+----------------+-----------+\n7 rows in set (0.00 sec)\n\nmysql> select a, hex(a) from test_varchar;      \n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n| 好好好         | E5A5BDE5A5BDE5A5BD202020     |  -- 后面有20 20 20 ，表示3个自己的空格\n+----------------+------------------------------+\n7 rows in set (0.00 sec)\n```\n\n>上面的现象无法用统一的规则进行表述，但是[官方文档](http://dev.mysql.com/doc/refman/5.7/en/innodb-physical-record.html)给出的解释是，这样的安排是为了避免索引页的碎片 \n\n### 3.BLOB和TEXT\n* 在BLOB和TEXT上创建索引时，必须指定索引前缀的长度\n```sql\nmysql> create table test_text(a int primary key, b text, key(b));\nERROR 1170 (42000): BLOB/TEXT column 'b' used in key specification without a key length\n\nmysql> create table test_text(a int primary key, b text, key(b(64)));\nQuery OK, 0 rows affected (0.13 sec)\n```\n\n* BLOB和TEXT列不能有默认值\n* BLOB和TEXT列排序时只使用该列的前max_sort_length个字节\n```sql\nmysql> select @@max_sort_length;\n+-------------------+\n| @@max_sort_length |\n+-------------------+\n|              1024 |\n+-------------------+\n1 row in set (0.00 sec)\n```\n>不建议在MySQL中存储大型的二进制数据，比如歌曲，视频\n\n-----\n\n## 四. 字符集\n### 1. 常见的字符集\n* utf8\n* utf8mb4\n* gbk\n* gb18030\n\n```sql\nmysql> show character set;\n+----------+---------------------------------+---------------------+--------+\n| Charset  | Description                     | Default collation   | Maxlen |\n+----------+---------------------------------+---------------------+--------+\n| big5     | Big5 Traditional Chinese        | big5_chinese_ci     |      2 |\n| dec8     | DEC West European               | dec8_swedish_ci     |      1 |\n| cp850    | DOS West European               | cp850_general_ci    |      1 |\n| hp8      | HP West European                | hp8_english_ci      |      1 |\n| koi8r    | KOI8-R Relcom Russian           | koi8r_general_ci    |      1 |\n| latin1   | cp1252 West European            | latin1_swedish_ci   |      1 |\n| latin2   | ISO 8859-2 Central European     | latin2_general_ci   |      1 |\n| swe7     | 7bit Swedish                    | swe7_swedish_ci     |      1 |\n| ascii    | US ASCII                        | ascii_general_ci    |      1 |\n| ujis     | EUC-JP Japanese                 | ujis_japanese_ci    |      3 |\n| sjis     | Shift-JIS Japanese              | sjis_japanese_ci    |      2 |\n| hebrew   | ISO 8859-8 Hebrew               | hebrew_general_ci   |      1 |\n| tis620   | TIS620 Thai                     | tis620_thai_ci      |      1 |\n| euckr    | EUC-KR Korean                   | euckr_korean_ci     |      2 |\n| koi8u    | KOI8-U Ukrainian                | koi8u_general_ci    |      1 |\n| gb2312   | GB2312 Simplified Chinese       | gb2312_chinese_ci   |      2 |\n| greek    | ISO 8859-7 Greek                | greek_general_ci    |      1 |\n| cp1250   | Windows Central European        | cp1250_general_ci   |      1 |\n| gbk      | GBK Simplified Chinese          | gbk_chinese_ci      |      2 | -- gbk，表示的字符有限\n| latin5   | ISO 8859-9 Turkish              | latin5_turkish_ci   |      1 |\n| armscii8 | ARMSCII-8 Armenian              | armscii8_general_ci |      1 |\n| utf8     | UTF-8 Unicode                   | utf8_general_ci     |      3 | -- utf8，最长3字节\n| ucs2     | UCS-2 Unicode                   | ucs2_general_ci     |      2 |\n| cp866    | DOS Russian                     | cp866_general_ci    |      1 |\n| keybcs2  | DOS Kamenicky Czech-Slovak      | keybcs2_general_ci  |      1 |\n| macce    | Mac Central European            | macce_general_ci    |      1 |\n| macroman | Mac West European               | macroman_general_ci |      1 |\n| cp852    | DOS Central European            | cp852_general_ci    |      1 |\n| latin7   | ISO 8859-13 Baltic              | latin7_general_ci   |      1 |\n| utf8mb4  | UTF-8 Unicode                   | utf8mb4_general_ci  |      4 | -- utf8 + mobile端字符\n| cp1251   | Windows Cyrillic                | cp1251_general_ci   |      1 |\n| utf16    | UTF-16 Unicode                  | utf16_general_ci    |      4 |\n| utf16le  | UTF-16LE Unicode                | utf16le_general_ci  |      4 |\n| cp1256   | Windows Arabic                  | cp1256_general_ci   |      1 |\n| cp1257   | Windows Baltic                  | cp1257_general_ci   |      1 |\n| utf32    | UTF-32 Unicode                  | utf32_general_ci    |      4 |\n| binary   | Binary pseudo charset           | binary              |      1 |\n| geostd8  | GEOSTD8 Georgian                | geostd8_general_ci  |      1 |\n| cp932    | SJIS for Windows Japanese       | cp932_japanese_ci   |      2 |\n| eucjpms  | UJIS for Windows Japanese       | eucjpms_japanese_ci |      3 |\n| gb18030  | China National Standard GB18030 | gb18030_chinese_ci  |      4 | -- gb18030,最长4个字节\n+----------+---------------------------------+---------------------+--------+\n41 rows in set (0.00 sec)\n```\n\n### 2. collation\ncollation的含义是指排序规则，`ci（case insensitive）`结尾的排序集是不区分大小写的\n```sql\nmysql> select 'a' = 'A';\n+-----------+\n| 'a' = 'A' |\n+-----------+\n|         1 |  -- 因为大小写无关，所以返回1\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> create table test_ci (a varchar(10), key(a));\nQuery OK, 0 rows affected (0.13 sec)\n\nmysql> insert into test_ci values('a');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_ci values('A');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_ci where a = 'a';\n+------+\n| a    |\n+------+\n| a    |  \n| A    |  -- A也被我们查到了\n+------+\n2 rows in set (0.00 sec)\n```\n> 上面的情况如果从业务的角度上看，可以很好理解，比如创建一个用户叫做Tom，你是不希望再创建一个叫做tom的用户的\n\n* **修改默认的collation**\n```sql\nmysql> set names utf8mb4 collate utf8mb4_bin;  -- 当前会话有效\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select 'a' = 'A';\n+-----------+\n| 'a' = 'A' |\n+-----------+\n|         0 |\n+-----------+\n1 row in set (0.00 sec)\n```\n\n>字符集的指定，可以在创建数据库的时候指定，也可以在创建表的时候单独指定，也可以创建列的时候进行指定\n\n-----\n\n## 五. 集合类型\n* 集合类型ENUM 和 SET\n* ENUM类型最多允许65536个值\n* SET类型最多允许64个值\n* 通过sql_mode参数可以用户约束检查\n\n```sql\nmysql> create table test_col (\n    -> user varchar(10),\n    -> sex enum('male', 'female')  -- 虽然写的是字符串，单其实存储的整型，效率还是可以的\n    -> );\n    \nmysql> insert into test_col values (\"tom\", \"male\");\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_col values (\"tom\", \"xmale\");  -- 不是male 和 female\nQuery OK, 1 row affected, 1 warning (0.03 sec)  -- 有warning\n\nmysql> set sql_mode='strict_trans_tables';  -- 设置为严格模式\nQuery OK, 0 rows affected, 2 warnings (0.00 sec)\n\nmysql> insert into test_col values (\"tom\", \"xmale\");\nERROR 1265 (01000): Data truncated for column 'sex' at row 1\n```\n>强烈建议新业务上都设置成严格模式\n\n\n----\n\n## 六. 日期类型\n\n| 日期类型 | 占用空间 | 表示范围 |\n|----------|----------|----------|\n| `DATETIME` | 8 | 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 |\n| DATE| 3 | 1000-01-01 ~ 9999-12-31 |\n| `TIMESTAMP` | 4 | 1970-01-01 00:00:00UTC ~ 2038-01-19 03:14:07UTC |\n| YEAR | 1 | YEAR(2):1970-2070, YEAR(4):1901-2155 |\n| TIME | 3 | -838:59:59 ~ 838:59:59 |\n\n>TIMESTAMP 带时区功能\n\n```sql\nmysql> create table test_time(a timestamp, b datetime);\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql> insert into test_time values (now(), now());\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_time;\n+---------------------+---------------------+\n| a                   | b                   |\n+---------------------+---------------------+\n| 2015-11-28 10:00:39 | 2015-11-28 10:00:39 |\n+---------------------+---------------------+\n1 row in set (0.00 sec)\n\nmysql> select @@time_zone; \n+-------------+\n| @@time_zone |\n+-------------+\n| SYSTEM      |\n+-------------+\n1 row in set (0.00 sec)\n\nmysql> set time_zone='+00:00';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @@time_zone;\n+-------------+\n| @@time_zone |\n+-------------+\n| +00:00      |\n+-------------+\n1 row in set (0.00 sec)\n\nmysql> select * from test_time;\n+---------------------+---------------------+\n| a                   | b                   |\n+---------------------+---------------------+\n| 2015-11-28 2:00:39 | 2015-11-28 10:00:39  |  -- 时区的差别体现出来了\n+---------------------+---------------------+\n1 row in set (0.00 sec)\n```\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"AI算法","url":"/2019-10-06/AI算法分类/","content":"\n\n一、人工智能学习算法分类\n人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning)\n\n总的来说，在sklearn中机器学习算法大概的分类如下：\n\n1. 纯算法类\n- .回归算法\n- .分类算法\n- .聚类算法\n- 降维算法\n- 概率图模型算法\n- 文本挖掘算法\n- 优化算法\n- 深度学习算法\n\n2.建模方面\n- .模型优化\n- .数据预处理\n\n二、详细算法\n1.分类算法\n- .LR (Logistic Regression，逻辑回归又叫逻辑分类)\n- .SVM (Support Vector Machine，支持向量机)\n- .NB (Naive Bayes，朴素贝叶斯)\n- .DT (Decision Tree，决策树)\n\n- - .C4.5\n- - .ID3\n- - .CART\n- .集成算法\n\n- - .Bagging\n- - .Random Forest (随机森林)\n- - .GB(梯度提升,Gradient boosting)\n- - .GBDT (Gradient Boosting Decision Tree)\n- - .AdaBoost\n- - .Xgboost\n- .最大熵模型\n\n2.回归算法\n- .LR (Linear Regression，线性回归)\n- .SVR (支持向量机回归)\n- . RR (Ridge Regression，岭回归)\n\n3.聚类算法\n- .Knn\n- .Kmeans 算法\n- .层次聚类\n- .密度聚类\n\n4.降维算法\n- .SGD (随机梯度下降)\n\n5.概率图模型算法\n- .贝叶斯网络\n- .HMM\n- .CRF (条件随机场)\n\n6.文本挖掘算法\n- .模型\n\n- - .LDA (主题生成模型，Latent Dirichlet Allocation)\n- - .最大熵模型\n- .关键词提取\n\n- - .tf-idf\n- - .bm25\n- - .textrank\n- - .pagerank\n- - .左右熵 :左右熵高的作为关键词\n- - .互信息：\n- .词法分析\n\n- - .分词\n– ①HMM (因马尔科夫)\n– ②CRF (条件随机场)\n- - .词性标注\n- - .命名实体识别\n- .句法分析\n\n- - .句法结构分析\n- - .依存句法分析\n- .文本向量化\n\n- - .tf-idf\n- - .word2vec\n- - .doc2vec\n- - .cw2vec\n- .距离计算\n\n- - .欧氏距离\n- - .相似度计算\n7.优化算法\n- .正则化\n\n- - .L1正则化\n- - .L2正则化\n8.深度学习算法\n- .BP\n- .CNN\n- .DNN\n- .RNN\n- .LSTM\n\n三、建模方面\n1.模型优化·\n- .特征选择\n- .梯度下降\n- .交叉验证\n- .参数调优\n- .模型评估：准确率、召回率、F1、AUC、ROC、损失函数\n2.数据预处理\n- .标准化\n- .异常值处理\n- .二值化\n- .缺失值填充： 支持均值、中位数、特定值补差、多重插补","tags":["AI","机器学习"],"categories":["MachineLearning"]},{"title":"MySQL SELECT","url":"/2019-10-05/mysql/MySQL学习笔记（Day011：SELECT）/","content":"MySQL学习笔记（Day011：SELECT）\n=====================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. SELECT语法介绍\n\n>[SELECT语法官方文档](http://dev.mysql.com/doc/refman/5.7/en/select.html)\n\n```sql\nSELECT\n-- -------------------------不推荐使用--------------------------\n    [ALL | DISTINCT | DISTINCTROW ]\n      [HIGH_PRIORITY]\n      [MAX_STATEMENT_TIME = N]\n      [STRAIGHT_JOIN]\n      [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT]\n      [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS]\n-- -------------------------------------------------------------\n    select_expr [, select_expr ...]\n    [FROM table_references  \n      [PARTITION partition_list]\n    [WHERE where_condition] \n    [GROUP BY {col_name | expr | position}  \n      [ASC | DESC], ... [WITH ROLLUP]] \n    [HAVING where_condition] \n    [ORDER BY {col_name | expr | position} \n      [ASC | DESC], ...]\n    [LIMIT {[offset,] row_count | row_count OFFSET offset}]  \n    [PROCEDURE procedure_name(argument_list)]\n    [INTO OUTFILE 'file_name'\n        [CHARACTER SET charset_name]\n        export_options\n      | INTO DUMPFILE 'file_name'\n      | INTO var_name [, var_name]]\n    [FOR UPDATE | LOCK IN SHARE MODE]]\n```\n\n## 二. LIMIT 和 ORDER BY\n```sql\nmysql> select * from employees limit 1;  -- 从employees中 随机 取出一条数据，结果是不确定的\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  10001 | 1953-09-02 | Georgi     | Facello   | M      | 1986-06-26 |\n+--------+------------+------------+-----------+--------+------------+\n1 row in set (0.00 sec)\n\n--\n-- order by col_name 根据某列的值进行排序\n-- asc ： 升序(default)\n-- desc： 降序\n--\n\nmysql> select * from employees order by emp_no asc  limit 1;   -- 使用order by col_name asc进行升序排序\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  10001 | 1953-09-02 | Georgi     | Facello   | M      | 1986-06-26 |\n+--------+------------+------------+-----------+--------+------------+\n1 row in set (0.00 sec)\n\nmysql> select * from employees order by  emp_no limit 1;       -- 默认就是升序的\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  10001 | 1953-09-02 | Georgi     | Facello   | M      | 1986-06-26 |  -- 结果和上面一致\n+--------+------------+------------+-----------+--------+------------+\n1 row in set (0.00 sec)\n\nmysql> select * from employees order by emp_no desc limit 1;    -- desc表示降序\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n| 499999 | 1958-05-01 | Sachin     | Tsukuda   | M      | 1997-11-30 |  -- 降序显示\n+--------+------------+------------+-----------+--------+------------+\n-- 通过order by排序后 limit 1 才是确定的\n1 row in set (0.00 sec)\n\nmysql> show create table employees\\G\n*************************** 1. row ***************************\n       Table: employees\nCreate Table: CREATE TABLE `employees` (\n  `emp_no` int(11) NOT NULL,\n  `birth_date` date NOT NULL,\n  `first_name` varchar(14) NOT NULL,\n  `last_name` varchar(16) NOT NULL,\n  `gender` enum('M','F') NOT NULL,\n  `hire_date` date NOT NULL,\n  PRIMARY KEY (`emp_no`)    -- emp_no 是主键，order by 主键 不会创建临时表的，主键(索引)本身有序\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n1 row in set (0.00 sec)\n\nmysql> select * from employees order by emp_no asc limit 5,5;  -- limit start, offset\n                                                               -- 从第5条 开始取，取5条出来\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  10006 | 1953-04-20 | Anneke     | Preusig   | F      | 1989-06-02 |\n|  10007 | 1957-05-23 | Tzvetan    | Zielinski | F      | 1989-02-10 |\n|  10008 | 1958-02-19 | Saniya     | Kalloufi  | M      | 1994-09-15 |\n|  10009 | 1952-04-19 | Sumant     | Peac      | F      | 1985-02-18 |\n|  10010 | 1963-06-01 | Duangkaew  | Piveteau  | F      | 1989-08-24 |\n+--------+------------+------------+-----------+--------+------------+\n5 rows in set (0.00 sec)\n\n-- 以上这个语法有一种分页的效果，但是会随着start的增加，性能会下降，因为会扫描表(从 1 到 start)\n\n-- 相对比较推荐的方法\nmysql> select * from employees where emp_no > 20000 order by emp_no limit 5;\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  20001 | 1962-05-16 | Atreye     | Eppinger  | M      | 1990-04-18 |\n|  20002 | 1955-12-25 | Jaber      | Brender   | M      | 1988-01-26 |\n|  20003 | 1953-04-11 | Munehiko   | Coors     | F      | 1991-02-07 |\n|  20004 | 1952-03-07 | Radoslaw   | Pfau      | M      | 1995-11-24 |\n|  20005 | 1956-02-20 | Licheng    | Przulj    | M      | 1992-07-17 |\n+--------+------------+------------+-----------+--------+------------+\n5 rows in set (0.00 sec)\n-- （当然推荐把热数据放cache里,比如Redis）\n```\n>`ORDER BY` 是把已经查询好的结果集进行排序\n\n## 三. WHERE\n`WHERE`是将查询出来的结果，通过`WHERE`后面的条件(condition)，对结果进行过滤\n```sql\nmysql> select * from employees  where emp_no > 30000  emp_no limit 4; -- 不加order by的limit是不确定的SQL\n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  30001 | 1953-03-27 | Izaskun    | Morton    | M      | 1988-05-21 |\n|  30002 | 1960-08-23 | Branimir   | Snedden   | M      | 1998-09-24 |\n|  30003 | 1952-11-25 | Takahito   | Vilarrasa | M      | 1990-08-22 |\n|  30004 | 1957-11-26 | Lucian     | Penttonen | F      | 1992-10-08 |\n+--------+------------+------------+-----------+--------+------------+\n4 rows in set (0.00 sec)\n\nmysql> select * from employees  where emp_no > 40000  order by emp_no limit 4; \n+--------+------------+------------+-----------+--------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  |\n+--------+------------+------------+-----------+--------+------------+\n|  40001 | 1956-03-28 | Akemi      | Maliniak  | F      | 1987-08-06 |\n|  40002 | 1960-03-15 | Nakhoon    | Badr      | M      | 1990-02-13 |\n|  40003 | 1960-01-26 | Jacopo     | Marshall  | F      | 1991-09-30 |\n|  40004 | 1955-09-09 | Anneke     | Stiles    | F      | 1986-03-05 |\n+--------+------------+------------+-----------+--------+------------+\n4 rows in set (0.02 sec)\n\nmysql> select * from employees  \n    -> where emp_no > 40000\n    -> and hire_date > '1991-01-01'   -- 可以用 and 进行 逻辑与 \n    -> order by emp_no limit 4;\n+--------+------------+------------+------------+--------+------------+\n| emp_no | birth_date | first_name | last_name  | gender | hire_date  |\n+--------+------------+------------+------------+--------+------------+\n|  40003 | 1960-01-26 | Jacopo     | Marshall   | F      | 1991-09-30 |\n|  40005 | 1961-02-27 | Zsolt      | Fairtlough | F      | 1991-07-08 |\n|  40012 | 1955-02-07 | Chinhyun   | Ozeri      | F      | 1995-08-12 |\n|  40015 | 1964-10-08 | Ioana      | Lemarechal | M      | 1997-08-07 |\n+--------+------------+------------+------------+--------+------------+\n4 rows in set (0.00 sec)\n\nmysql> select * from employees\n    -> where (emp_no > 40000 and birth_date > '1961-01-01') -- 使用()明确条件的逻辑规则\n    ->    or (emp_no > 40000 and hire_date > '1991-01-01')  -- 可以使用 or 做 逻辑或\n    -> order by emp_no limit 5;\n+--------+------------+------------+------------+--------+------------+\n| emp_no | birth_date | first_name | last_name  | gender | hire_date  |\n+--------+------------+------------+------------+--------+------------+\n|  40003 | 1960-01-26 | Jacopo     | Marshall   | F      | 1991-09-30 |\n|  40005 | 1961-02-27 | Zsolt      | Fairtlough | F      | 1991-07-08 |\n|  40006 | 1962-11-07 | Basim      | Panienski  | F      | 1986-12-27 |\n|  40012 | 1955-02-07 | Chinhyun   | Ozeri      | F      | 1995-08-12 |\n|  40015 | 1964-10-08 | Ioana      | Lemarechal | M      | 1997-08-07 |\n+--------+------------+------------+------------+--------+------------+\n5 rows in set (0.00 sec)\n```\n\n## 四. JOIN\n![Alt text](/images/mysql/123.png)\n\n### 4.1. INNER JOIN\n\n```sql\n--\n-- ANSI SQL 89\n-- 关联employees表和titles表\n-- 要求是 employees的emp_no 等于 titles的emp_no\n--\nmysql> select * from employees,titles where employees.emp_no = titles.emp_no limit 5;\n+--------+------------+------------+-----------+--------+------------+--------+-----------------+------------+------------+\n| emp_no | birth_date | first_name | last_name | gender | hire_date  | emp_no | title           | from_date  | to_date    |\n+--------+------------+------------+-----------+--------+------------+--------+-----------------+------------+------------+\n|  10001 | 1953-09-02 | Georgi     | Facello   | M      | 1986-06-26 |  10001 | Senior Engineer | 1986-06-26 | 9999-01-01 |\n|  10002 | 1964-06-02 | Bezalel    | Simmel    | F      | 1985-11-21 |  10002 | Staff           | 1996-08-03 | 9999-01-01 |\n|  10003 | 1959-12-03 | Parto      | Bamford   | M      | 1986-08-28 |  10003 | Senior Engineer | 1995-12-03 | 9999-01-01 |\n|  10004 | 1954-05-01 | Chirstian  | Koblick   | M      | 1986-12-01 |  10004 | Engineer        | 1986-12-01 | 1995-12-01 |\n|  10004 | 1954-05-01 | Chirstian  | Koblick   | M      | 1986-12-01 |  10004 | Senior Engineer | 1995-12-01 | 9999-01-01 |\n+--------+------------+------------+-----------+--------+------------+--------+-----------------+------------+------------+\n5 rows in set (0.00 sec)\n\n--\n-- 在上面的基础上只显示emp_no，名字，性别和职位名称\n--\nmysql> select emp_no, concat(last_name,' ', first_name), gender, title \n    -> from employees,titles\n    -> where employees.emp_no = titles.emp_no limit 5;\nERROR 1052 (23000): Column 'emp_no' in field list is ambiguous  -- 报错了，原因是emp_no两个表都有\n\nmysql> select employees.emp_no, -- 指定了employees\n    -> concat(last_name,' ', first_name), gender, title\n    -> from employees,titles\n    -> where employees.emp_no = titles.emp_no limit 5;\n+--------+-----------------------------------+--------+-----------------+\n| emp_no | concat(last_name,' ', first_name) | gender | title           |\n+--------+-----------------------------------+--------+-----------------+\n|  10001 | Facello Georgi                    | M      | Senior Engineer |\n|  10002 | Simmel Bezalel                    | F      | Staff           |\n|  10003 | Bamford Parto                     | M      | Senior Engineer |\n|  10004 | Koblick Chirstian                 | M      | Engineer        |\n|  10004 | Koblick Chirstian                 | M      | Senior Engineer |\n+--------+-----------------------------------+--------+-----------------+    \n\nmysql> select employees.emp_no,\n    -> concat(last_name,' ', first_name) as emp_name, gender, title  -- 对名字的列取一个别名叫emp_name\n    -> from employees,titles\n    -> where employees.emp_no = titles.emp_no limit 5;\n+--------+-------------------+--------+-----------------+\n| emp_no | emp_name          | gender | title           |  -- 这里就显示了emp_name\n+--------+-------------------+--------+-----------------+\n|  10001 | Facello Georgi    | M      | Senior Engineer |\n|  10002 | Simmel Bezalel    | F      | Staff           |\n|  10003 | Bamford Parto     | M      | Senior Engineer |\n|  10004 | Koblick Chirstian | M      | Engineer        |\n|  10004 | Koblick Chirstian | M      | Senior Engineer |\n+--------+-------------------+--------+-----------------+\n5 rows in set (0.00 sec)\n\nmysql> select e.emp_no,  -- 使用表的别名\n    -> concat(last_name,' ', first_name) as emp_name, gender, title\n    -> from employees as e,titles as t      -- 对表做别名\n    -> where e.emp_no = t.emp_no limit 5;   -- 使用报表的别名\n+--------+-------------------+--------+-----------------+\n| emp_no | emp_name          | gender | title           |\n+--------+-------------------+--------+-----------------+\n|  10001 | Facello Georgi    | M      | Senior Engineer |\n|  10002 | Simmel Bezalel    | F      | Staff           |\n|  10003 | Bamford Parto     | M      | Senior Engineer |\n|  10004 | Koblick Chirstian | M      | Engineer        |\n|  10004 | Koblick Chirstian | M      | Senior Engineer |\n+--------+-------------------+--------+-----------------+\n5 rows in set (0.00 sec)\n\n--\n-- ANSI SQL 92\n-- inner join ... on ...语法\n--\nmysql> select e.emp_no,\n    -> concat(last_name,' ', first_name) as emp_name, gender, title\n    -> from employees as e inner join titles as t  -- inner join 可以省略inner关键字\n    -> on e.emp_no = t.emp_no limit 5;             -- 配合join使用on\n+--------+-------------------+--------+-----------------+\n| emp_no | emp_name          | gender | title           |\n+--------+-------------------+--------+-----------------+\n|  10001 | Facello Georgi    | M      | Senior Engineer |\n|  10002 | Simmel Bezalel    | F      | Staff           |\n|  10003 | Bamford Parto     | M      | Senior Engineer |\n|  10004 | Koblick Chirstian | M      | Engineer        |\n|  10004 | Koblick Chirstian | M      | Senior Engineer |\n+--------+-------------------+--------+-----------------+\n5 rows in set (0.00 sec)\n\n--\n-- 上面两种语句在效率上其实是一样的，只是语法上的区别\n--\n--- 第一种\nmysql> explain select e.emp_no,\n    -> concat(last_name,' ', first_name) as emp_name, gender, title\n    -> from employees as e,titles as t\n    -> where e.emp_no = t.emp_no limit 5;\n+----+-------------+-------+------------+------+---------------+---------+---------+--------------------+--------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys | key     | key_len | ref                | rows   | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------+---------+---------+--------------------+--------+----------+-------------+\n|  1 | SIMPLE      | e     | NULL       | ALL  | PRIMARY       | NULL    | NULL    | NULL               | 298124 |   100.00 | NULL        |\n|  1 | SIMPLE      | t     | NULL       | ref  | PRIMARY       | PRIMARY | 4       | employees.e.emp_no |      1 |   100.00 | Using index |\n+----+-------------+-------+------------+------+---------------+---------+---------+--------------------+--------+----------+-------------+\n2 rows in set, 1 warning (0.00 sec)\n\n--- 第二种\nmysql> explain select e.emp_no,\n    -> concat(last_name,' ', first_name) as emp_name, gender, title\n    -> from employees as e inner join titles as t\n    -> on e.emp_no = t.emp_no limit 5;\n+----+-------------+-------+------------+------+---------------+---------+---------+--------------------+--------+----------+-------------+\n| id | select_type | table | partitions | type | possible_keys | key     | key_len | ref                | rows   | filtered | Extra       |\n+----+-------------+-------+------------+------+---------------+---------+---------+--------------------+--------+----------+-------------+\n|  1 | SIMPLE      | e     | NULL       | ALL  | PRIMARY       | NULL    | NULL    | NULL               | 298124 |   100.00 | NULL        |\n|  1 | SIMPLE      | t     | NULL       | ref  | PRIMARY       | PRIMARY | 4       | employees.e.emp_no |      1 |   100.00 | Using index |\n+----+-------------+-------+------------+------+---------------+---------+---------+--------------------+--------+----------+-------------+\n2 rows in set, 1 warning (0.00 sec)\n\n-- 通过explain看两条语句的执行计划，发现是一样的，所以性能上是一样的，只是语法的不同\n```\n\n### 4.2. OUTER JOIN\n```sql\n--\n-- 左连接 left join\n--\nmysql> use burn_test\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> create table test_left_join_1(a int);\nQuery OK, 0 rows affected (0.16 sec)\n\nmysql> create table test_left_join_2(b int); \nQuery OK, 0 rows affected (0.14 sec)\n\nmysql> insert into test_left_join_1 values (1);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_left_join_1 values (2);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_left_join_2 values (1); \nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_left_join_1;\n+------+\n| a    |\n+------+\n|    1 |\n|    2 |\n+------+\n2 rows in set (0.00 sec)\n\nmysql> select * from test_left_join_2;\n+------+\n| b    |\n+------+\n|    1 |\n+------+\n1 row in set (0.00 sec)\n\nmysql> select * from \n    -> test_left_join_1 as t1 \n    -> left join   -- 使用left join\n    -> test_left_join_2 as t2 \n    -> on t1.a = t2.b;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    1 |  -- 满足条件的，显示t2中该条记录的值\n|    2 | NULL |  -- 不满足条件的，用NULL填充\n+------+------+\n2 rows in set (0.00 sec)\n-- left join ： 左表 left join 右表 on 条件；\n--              左表全部显示，右表是匹配表，\n--              如果右表的某条记录满足 [on 条件] 匹配，则该记录显示\n--              如果右表的某条记录 不 满足 匹配，则该记录显示NULL\n\n--\n-- 右连接 right join （继续使用test_left_join_1和2两张表）\n--\nmysql> select * from\n    -> test_left_join_1 as t1\n    -> right join   -- 使用right join\n    -> test_left_join_2 as t2\n    -> on t1.a = t2.b;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    1 |   -- 右表（t2）全部显示\n+------+------+\n1 row in set (0.00 sec)\n-- right join ： 左表 right join 右表 on 条件\n--               右表全部显示，左边是匹配表\n--               同样和left join，满足则显示，不满足且右表中有值，则填充NULL\n\nmysql> insert into test_left_join_2 values (3);  -- t2 中再增加一条记录\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from\n    -> test_left_join_1 as t1\n    -> right join\n    -> test_left_join_2 as t2\n    -> on t1.a = t2.b;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    1 |\n| NULL |    3 | -- 右表存在，左表没有，用NULL填充\n+------+------+\n2 rows in set (0.00 sec)\n\n--\n-- 查找在t1表，而不在t2表的数据\n--\nmysql> select * from \n    -> test_left_join_1 as t1 \n    -> left join\n    -> test_left_join_2 as t2 \n    -> on t1.a = t2.b where t2.b is null;\n+------+------+\n| a    | b    |\n+------+------+\n|    2 | NULL |  -- 数据1 在t1和t2中都有，所以不显示\n+------+------+\n1 row in set (0.00 sec)\n\n\n-- left join ： left outer join , outer关键字可以省略\n-- right join： right outer join , outer 关键字可以省略\n\n-- join无论inner还是outer，列名不需要一样，甚至列的类型也可以不一样，会进行转换。\n-- 一般情况下，表设计合理，需要关联的字段类型应该是一样的\n\n\n--\n-- 查找哪些员工不是经理\n--\nmysql> select e.emp_no,\n    -> concat(last_name,' ', first_name) as emp_name, gender, d.dept_no\n    -> from employees as e left join dept_manager as d\n    -> on e.emp_no = d.emp_no\n    -> where d.emp_no is null limit 5;\n+--------+-------------------+--------+---------+\n| emp_no | emp_name          | gender | dept_no | -- dept_no是dept_manager的字段\n+--------+-------------------+--------+---------+\n|  10001 | Facello Georgi    | M      | NULL    |\n|  10002 | Simmel Bezalel    | F      | NULL    |\n|  10003 | Bamford Parto     | M      | NULL    |\n|  10004 | Koblick Chirstian | M      | NULL    |\n|  10005 | Maliniak Kyoichi  | M      | NULL    |\n+--------+-------------------+--------+---------+\n5 rows in set (0.00 sec)\n\n-- 在 inner join中，过滤条件放在where或者on中都是可以的\n-- 在 outer join中 条件放在where和on中是不一样的\nmysql> select * from \n    -> test_left_join_1 as t1\n    -> left join\n    -> test_left_join_2 as t2\n    -> on t1.a = t2.b\n    -> where t2.b is null;\n+------+------+\n| a    | b    |\n+------+------+\n|    2 | NULL |\n+------+------+\n1 row in set (0.00 sec)\n\nmysql> select * from \n    -> test_left_join_1 as t1\n    -> left join\n    -> test_left_join_2 as t2\n    -> on t1.a = t2.b\n    -> and t2.b is null;  -- 除了a=b, 还要找到b=null的，但是b里面没有null，所有a全部显示，b全为null\n+------+------+\n| a    | b    |\n+------+------+\n|    1 | NULL |\n|    2 | NULL |\n+------+------+\n2 rows in set (0.00 sec)\n\n-- ON 参与outer join的结果的生成，而where只是对结果的一个过滤\n\n--\n-- 作业：查处普通员工的title，部门名称，薪资\n--\n\n\n\n```\n\n### 4.3. GROUP BY\n```sql\n--\n-- 找出同一个部门的员工数量\n--\nmysql> select dept_no, count(dept_no)  -- count是得到数量，这里就是分组函数\n    -> from dept_emp\n    -> group by dept_no;  -- 通过 dept_no 分组\n+---------+----------------+\n| dept_no | count(dept_no) |\n+---------+----------------+\n| d001    |          20211 |\n| d002    |          17346 |\n| d003    |          17786 |\n| d004    |          73485 |\n| d005    |          85707 |\n| d006    |          20117 |\n| d007    |          52245 |\n| d008    |          21126 |\n| d009    |          23580 |\n+---------+----------------+\n9 rows in set (0.10 sec)\n\n--\n-- 选出部门人数 > 50000 \n-- \nmysql> select dept_no, count(dept_no)\n    -> from dept_emp\n    -> group by dept_no\n    -> having count(dept_no) > 50000;  -- 如果是对分组的聚合函数做过滤，使用having，用where报语法错误\n+---------+----------------+\n| dept_no | count(dept_no) |\n+---------+----------------+\n| d004    |          73485 |\n| d005    |          85707 |\n| d007    |          52245 |\n+---------+----------------+\n3 rows in set (0.09 sec)\n\n--\n-- 每个用户每个月产生的订单数目\n--\nmysql> desc orders;\n+-----------------+-------------+------+-----+---------+-------+\n| Field           | Type        | Null | Key | Default | Extra |\n+-----------------+-------------+------+-----+---------+-------+\n| o_orderkey      | int(11)     | NO   | PRI | NULL    |       |  -- 订单ID\n| o_custkey       | int(11)     | YES  | MUL | NULL    |       |  -- 客户ID\n| o_orderstatus   | char(1)     | YES  |     | NULL    |       |\n| o_totalprice    | double      | YES  |     | NULL    |       |\n| o_orderDATE     | date        | YES  | MUL | NULL    |       |  -- 订单日期\n| o_orderpriority | char(15)    | YES  |     | NULL    |       |\n| o_clerk         | char(15)    | YES  |     | NULL    |       |\n| o_shippriority  | int(11)     | YES  |     | NULL    |       |\n| o_comment       | varchar(79) | YES  |     | NULL    |       |\n+-----------------+-------------+------+-----+---------+-------+\n9 rows in set (0.00 sec)\n\nmysql> select  o_orderkey, o_custkey, o_orderDATE from orders limit 3; \n+------------+-----------+-------------+\n| o_orderkey | o_custkey | o_orderDATE |\n+------------+-----------+-------------+\n|          1 |     36901 | 1996-01-02  |\n|          2 |     78002 | 1996-12-01  |\n|          3 |    123314 | 1993-10-14  |\n+------------+-----------+-------------+\n3 rows in set (0.00 sec)\n\n\n--\n-- 查找客户每年每月产生的订单数\n--\n\nmysql> select o_custkey, count(o_orderkey),                                                                  -> year(o_orderDATE), month(o_orderDATE)\n    -> from orders\n    -> group by o_custkey, year(o_orderDATE), month(o_orderDATE)\n    -> limit 10;\n+-----------+-------------------+-------------------+--------------------+\n| o_custkey | count(o_orderkey) | year(o_orderDATE) | month(o_orderDATE) |\n+-----------+-------------------+-------------------+--------------------+\n|         1 |                 1 |              1992 |                  4 |\n|         1 |                 1 |              1992 |                  8 |\n|         1 |                 1 |              1996 |                  6 |\n|         1 |                 1 |              1996 |                  7 |\n|         1 |                 1 |              1996 |                 12 |\n|         1 |                 1 |              1997 |                  3 |\n|         2 |                 1 |              1992 |                  4 |\n|         2 |                 1 |              1994 |                  5 |\n|         2 |                 1 |              1994 |                  8 |\n|         2 |                 1 |              1994 |                 12 |\n+-----------+-------------------+-------------------+--------------------+\n10 rows in set (8.97 sec)\n\n-- 使用 date_format 函数\n\nmysql> select o_custkey, count(o_orderkey),\n    -> date_format(o_orderDATE, '%Y-%m') as date\n    -> from orders\n    -> group by o_custkey, date_format(o_orderDATE, '%Y-%m')\n    -> limit 10;\n+-----------+-------------------+---------+\n| o_custkey | count(o_orderkey) | date    |\n+-----------+-------------------+---------+\n|         1 |                 1 | 1992-04 |\n|         1 |                 1 | 1992-08 |\n|         1 |                 1 | 1996-06 |\n|         1 |                 1 | 1996-07 |\n|         1 |                 1 | 1996-12 |\n|         1 |                 1 | 1997-03 |\n|         2 |                 1 | 1992-04 |\n|         2 |                 1 | 1994-05 |\n|         2 |                 1 | 1994-08 |\n|         2 |                 1 | 1994-12 |\n+-----------+-------------------+---------+\n10 rows in set (11.46 sec)\n\n\n-- 作业：查找客户每周（以年，月，周 显示）产生的订单量\n\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL索引/B+树/Explain","url":"/2019-10-05/mysql/MySQL学习笔记Explain/","content":"MySQL学习笔记（Day015-Day016：索引/B+树/Explain）\n=========================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 索引\n### 1. 索引的定义\n\n>索引是对记录按照一个或者多个字段进行排序的一种方式。对表中的某个字段建立索引会创建另一种数据结构，其中保存着字段的值，每个值又指向与它相关的记录。这种索引的数据结构是经过排序的，因而可以对其执行二分查找。且性能较高\n    \n### 2. 二分查找\n>**摘自《MySQL技术内幕：InnoDB存储引擎（第2版）》5.2.1 小节**\n>`二分查找法（binary search）`也称为折半查找法，用来查找一组有序的记录数组中的某一记录，其基本思想是：将记录按有序化（递增或递减）排列，在查找过程中采用跳跃式方式查找，即先以有序数列的中点位置作为比较对象，如果要找的元素值小于该中点元素，则将待查序列缩小为左半部分，否则为右半部分。通过一次比较，将查找区间缩小一半。*（O(logN)的时间复杂度）*\n\n```bash\n# 给出一个例子，注意该例子已经是升序排序的，且查找 数字 48\n数据：5， 10， 19， 21， 31， 37， 42， 48， 50， 52\n下标：0，  1，  2，  3，  4，  5，  6，  7，  8，  9\n```\n- 步骤一：设`low`为下标最小值`0`，`high`为下标最大值`9`;\n- 步骤二：通过`low`和`high`得到`mid`，mid=（low + high） / 2，初始时`mid`为下标`4`*(也可以=5，看具体算法实现)*；\n- **`步骤三`**：`mid=4`对应的数据值是31，31 < 48（我们要找的数字）；\n- 步骤四：通过二分查找的思路，将`low`设置为31对应的下标`4`，`high`保持不变为`9`，此时`mid`为`6`；\n- **`步骤五`**：`mid=6`对应的数据值是42，42 < 48（我们要找的数字）；\n- 步骤六：通过二分查找的思路，将`low`设置为42对应的下标`6`，`high`保持不变为`9`，此时`mid`为`7`；\n- **`步骤七`**：`mid=7`对应的数据值是48，48 == 48（我们要找的数字），查找结束；\n\n    **通过3次`二分查找`就找到了我们所要的数字，而顺序查找需8次。**\n\n-----\n\n## 二. 二叉树(Binary Tree)\n>[二叉树 wiki](https://en.wikipedia.org/wiki/Binary_tree)\n\n### 1. 二叉树的定义\n* 每个节点至多只有二棵子树；\n* 二叉树的子树有左右之分，次序不能颠倒；\n* 一棵深度为k，且有 $2^k-1$个节点，称为满二叉树(Full Tree)；\n* 一棵深度为k，且root到k-1层的节点树都达到最大，第k层的所有节点都`连续集中`在最左边，此时为完全二叉树（Complete Tree）\n\n    ![complete/Full Tree](/images/mysql/400px-FullBT_CompleteBT.jpg)\n\n\n### 2. 平衡二叉树（AVL-树）\n>[平衡二叉树 wiki](https://en.wikipedia.org/wiki/AVL_tree)\n\n* 左子树和右子树都是平衡二叉树；\n* 左子树和右子树的高度差绝对值不超过1；\n\n    - 平衡二叉树\n    ![平衡二叉查找树](/images/mysql/Binary_Tree_Order.jpg)\n    <br/>\n    - 非平衡二叉树\n    ![Alt text](/images/mysql/NoBalance_Binary_Tree.jpg)\n        \n        \n* **平衡二叉树的遍历**\n以上面平衡二叉树的图例为样本，进行遍历：\n    - `前序`：`6`, 3, 2, 5, 7, 8（ROOT节点在开头, `中`-左-右 顺序）\n    - `中序`：2, 3, 5, `6`, 7, 8（中序遍历即为升序，左-`中`-右 顺序）\n    - `后序`：2, 5, 3, 8, 7, `6`（ROOT节点在结尾，左-右-`中` 顺序）\n\n    >1：可以通过`前序`和`中序` 或者是 `后序`和`中序`来推导出一个棵树\n    >2：`前序`或者`后序`用来得到ROOT节点，`中序`可以区分左右子树\n\n* **平衡二叉树的旋转**\n\n    ![二叉树的旋转](/images/mysql/Tree_Rotation.jpg)\n\n\n    >需要通过旋转（左旋，右旋）来维护平衡二叉树的平衡，在添加和删除的时候需要有额外的开销。\n\n-----\n\n## 三. B树/B+树\n>[B树  wiki介绍](https://en.wikipedia.org/wiki/B-tree)  \n>[B+树 wiki介绍](https://en.wikipedia.org/wiki/B%2B_tree)\n\n>**注意:B树和B+树开头的`B`不是Binary，而是`Balance`**\n\n### 1. B树的定义\n阶为M *(节点上关键字(Keys)的个数)* 的B树的定义：\n\n* 每个节点最多有M个孩子；\n* 除了root节点外，每个非叶子(non-leaf)节点至少含有(M/2)个孩子；\n* 如果root节点不为空，则root节点至少要有两个孩子节点；\n* 一个非叶子(non-leaf)节点如果含有K个孩子，则包含k-1个keys；\n* 所有叶子节点都在同一层；\n* B树中的非叶子(non-leaf)节点也包含了数据部分；\n\n### 2. B+树的定义\n在B树的基础上，B+树做了如下改进\n\n* 数据只存储在叶子节点上，非叶子节点只保存索引信息；\n    - 非叶子节点（索引节点）存储的只是一个Flag，不保存实际数据记录；\n    - 索引节点指示该节点的左子树比这个Flag小，而右子树大于等于这个Flag\n* 叶子节点本身按照数据的升序排序进行链接(串联起来)；\n    - 叶子节点中的数据在`物理存储上是无序`的，仅仅是在`逻辑上有序`（通过指针串在一起）；\n\n### 3. B+树的作用\n\n* 在块设备上，通过B+树可以有效的存储数据；\n* 所有记录都存储在叶子节点上，非叶子(non-leaf)存储索引(keys)信息；\n* B+树含有非常高的扇出（fanout），通常超过100，在查找一个记录时，可以有效的减少IO操作；\n\n\n### 3. B+树的操作\n\n* **B+树的插入**\nB+树的插入必须保证插入后叶子节点中的记录依然排序。\n\n    - 插入操作步骤*（引用自姜老师的书《MySQL技术内幕：InnoDB存储引擎（第2版）》 第5.3.1小节）*\n    ![B+ Insert](/images/mysql/B+_insert.jpg)\n\n    >B+树总是会保持平衡。但是为了保持平衡对于新插入的键值可能需要做大量的拆分页（split）操作；部分情况下可以通过B+树的旋转来替代拆分页操作，进而达到平衡效果。\n\n* **B+树的删除**\nB+树使用填充因子（fill factor）来控制树的删除变化，50%是填充因子可设的最小值。B+树的删除操作同样必须保证删除后叶子节点中的记录依然排序。与插入不同的是，删除根据填充因子的变化来衡量。\n    - 删除操作步骤*（引用自姜老师的书《MySQL技术内幕：InnoDB存储引擎（第2版）》 第5.3.2小节）*\n    ![B+ Delete](/images/mysql/B+_delete.jpg)\n\n\n### 3. B+树的扇出(fan out)\n* **B+树图例**\n![B+树的例子](/images/mysql/B+Example.jpg)\n\n    * 该 B+ 树高度为 2\n    * 每叶子页（LeafPage）4条记录\n    * `扇出数为5`\n    * 叶子节点(LeafPage)由小到大（有序）串联在一起\n\n>`扇出`是每个索引节点(Non-LeafPage)指向每个叶子节点(LeafPage)的指针\n>`扇出数` = 索引节点(Non-LeafPage)可存储的最大关键字个数 + 1\n>图例中的索引节点(Non-LeafPage)最大可以存放4个关键字，但实际使用了3个；\n\n### 4. B+树存储数据举例\n假设B+树中页的大小是16K，每行记录是200Byte大小，求出树的高度为1，2，3，4时，分别可以存储多少条记录。\n\n* 查看数据表中每行记录的平均大小\n\n    ```sql\n    mysql> show table status like \"%employees%\"\\G\n    *************************** 1. row ***************************\n               Name: employees\n             Engine: InnoDB\n            Version: 10\n         Row_format: Dynamic\n               Rows: 298124\n     Avg_row_length: 51   -- 平均长度为51个字节\n        Data_length: 15245312\n    Max_data_length: 0\n       Index_length: 0\n          Data_free: 0\n     Auto_increment: NULL\n        Create_time: 2015-12-02 21:32:02\n        Update_time: NULL\n         Check_time: NULL\n          Collation: utf8mb4_general_ci\n           Checksum: NULL\n     Create_options: \n            Comment: \n    1 row in set (0.00 sec)\n    ```\n\n* **高度为1**\n    - 16K/200B 约等于 80 个记录（数据结构元信息如指针等忽略不计）\n\n* **高度为2**\n非叶子节点中存放的仅仅是一个索引信息，包含了`Key`和`Point`指针；`Point`指针在MySQL中固定为`6Byte`。而`Key`我们这里假设为`8Byte`，则单个索引信息即为14个字节，`KeySize = 14Byte`\n    - 高度为2，即有一个索引节点（索引页），和N个叶子节点\n    - 一个索引节点可以存放 16K / KeySize = 16K / 14B = 1142个索引信息，即有（1142 + 1）个扇出，以及有（1142 + 1）个叶子节点（数据页）  *（可以简化为1000）*\n    - 数据记录数 = （16K / KeySize + 1）x (16K / 200B) 约等于 80W 个记录\n\n* **高度为3**\n高度为3的B+树，即ROOT节点有1000个扇出，每个扇出又有1000个扇出指向叶子节点。每个节点是80个记录，所以一共有 8000W个记录\n\n\n* **高度为4**\n同高度3一样，高度为4时的记录书为（8000 x 1000）W\n\n>上述的8000W等数据只是一个理论值。线上实际使用单个页的记录数字要乘以70%，即第二层需要70% x 70% ，依次类推。\n\n>因此在数据库中，B+树的高度一般都在2～4层，这也就是说查找某一键值的行记录时最多只需要2到4次IO，2～4次的IO意味着查询时间只需0.02～0.04秒（假设IOPS=100，当前SSD可以达到50000IOPS）。\n\n从5.7开始，页的预留大小可以设置了，以减少split操作的概率（空间换时间）\n\n-----\n\n\n## 四. MySQL索引\n### 1. MySQL 创建索引\n>[ALTER TABLE 方式](http://dev.mysql.com/doc/refman/5.7/en/alter-table.html)\n>[CREATE INDEX 方式](http://dev.mysql.com/doc/refman/5.7/en/create-index.html)\n\n```sql\n--\n-- ALTER TABLE\n--\nmysql> create table test_index_1(a int, b int , c int);\nQuery OK, 0 rows affected (0.20 sec)\n\nmysql> show create table test_index_1\\G\n*************************** 1. row ***************************\n       Table: test_index_1\nCreate Table: CREATE TABLE `test_index_1` (\n  `a` int(11) DEFAULT NULL,\n  `b` int(11) DEFAULT NULL,\n  `c` int(11) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n1 row in set (0.00 sec)\n\nmysql> insert into test_index_1 values\n    -> (1,10,100),(2,20,200),\n    -> (3,30,300),(4,40,400);\nQuery OK, 4 rows affected (0.03 sec)\nRecords: 4  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_index_1;\n+------+------+------+\n| a    | b    | c    |\n+------+------+------+\n|    1 |   10 |  100 |\n|    2 |   20 |  200 |\n|    3 |   30 |  300 |\n|    4 |   40 |  400 |\n+------+------+------+\n4 rows in set (0.00 sec)\n\nmysql> explain select * from test_index_1 where a=3\\G  -- 看执行计划，使用的是扫描整张表的方式\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_1\n   partitions: NULL\n         type: ALL\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 4\n     filtered: 25.00\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n\n-- 给字段a 增加索引\nmysql> alter table test_index_1 add index idx_a (a);  -- 给字段a添加索引。索引名为idx_a\nQuery OK, 0 rows affected (0.15 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> explain select * from test_index_1 where a=3\\G  -- 看执行计划，使用的key为idx_a，走了索引\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_1\n   partitions: NULL\n         type: ref\npossible_keys: idx_a\n          key: idx_a\n      key_len: 5\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\n-- 使用create index\nmysql> explain select * from test_index_1 where b=30\\G  -- 同样b字段也没有索引\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_1\n   partitions: NULL\n         type: ALL\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 4\n     filtered: 25.00\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n\n-- 给b字段增加索引\nmysql> create index idx_b on test_index_1 (b);\nQuery OK, 0 rows affected (0.14 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> explain select * from test_index_1 where b=30\\G  -- 查看执行计划，使用的key为idx_b，走了索引\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_1\n   partitions: NULL\n         type: ref\npossible_keys: idx_b\n          key: idx_b\n      key_len: 5\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n```\n\n### 2. MySQL 查看索引\n```sql\n--\n-- 方式一\n--\nmysql> desc orders;\n+-----------------+-------------+------+-----+---------+-------+\n| Field           | Type        | Null | Key | Default | Extra |\n+-----------------+-------------+------+-----+---------+-------+\n| o_orderkey      | int(11)     | NO   | PRI | NULL    |       | -- 索引\n| o_custkey       | int(11)     | YES  | MUL | NULL    |       | -- 索引\n| o_orderstatus   | char(1)     | YES  |     | NULL    |       |\n| o_totalprice    | double      | YES  |     | NULL    |       |\n| o_orderDATE     | date        | YES  | MUL | NULL    |       | -- 索引\n| o_orderpriority | char(15)    | YES  |     | NULL    |       |\n| o_clerk         | char(15)    | YES  |     | NULL    |       |\n| o_shippriority  | int(11)     | YES  |     | NULL    |       |\n| o_comment       | varchar(79) | YES  |     | NULL    |       |\n+-----------------+-------------+------+-----+---------+-------+\n9 rows in set (0.00 sec)\n\n--\n-- 方式二\n--\nmysql> show create table orders\\G\n*************************** 1. row ***************************\n       Table: orders\nCreate Table: CREATE TABLE `orders` (\n  `o_orderkey` int(11) NOT NULL,\n  `o_custkey` int(11) DEFAULT NULL,\n  `o_orderstatus` char(1) DEFAULT NULL,\n  `o_totalprice` double DEFAULT NULL,\n  `o_orderDATE` date DEFAULT NULL,\n  `o_orderpriority` char(15) DEFAULT NULL,\n  `o_clerk` char(15) DEFAULT NULL,\n  `o_shippriority` int(11) DEFAULT NULL,\n  `o_comment` varchar(79) DEFAULT NULL,\n  PRIMARY KEY (`o_orderkey`),  -- 索引\n  KEY `i_o_orderdate` (`o_orderDATE`), -- 索引\n  KEY `i_o_custkey` (`o_custkey`),  -- 索引\n  CONSTRAINT `orders_ibfk_1` FOREIGN KEY (`o_custkey`) REFERENCES `customer` (`c_custkey`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n1 row in set (0.00 sec)\n\n\n--\n-- 方式三\n--\nmysql> show index from orders\\G\n*************************** 1. row ***************************\n        Table: orders\n   Non_unique: 0        -- 表示唯一的 \n     Key_name: PRIMARY  -- key的name是primary\n Seq_in_index: 1\n  Column_name: o_orderkey\n    Collation: A\n  Cardinality: 1306748  -- 基数，这个列上不同值的记录数\n     Sub_part: NULL\n       Packed: NULL\n         Null: \n   Index_type: BTREE    -- 索引类型是BTree\n      Comment: \nIndex_comment: \n*************************** 2. row ***************************\n        Table: orders\n   Non_unique: 1       -- Non_unique为True，表示不唯一\n     Key_name: i_o_orderdate\n Seq_in_index: 1\n  Column_name: o_orderDATE\n    Collation: A\n  Cardinality: 2405\n     Sub_part: NULL\n       Packed: NULL\n         Null: YES\n   Index_type: BTREE\n      Comment: \nIndex_comment: \n*************************** 3. row ***************************\n        Table: orders\n   Non_unique: 1\n     Key_name: i_o_custkey\n Seq_in_index: 1\n  Column_name: o_custkey\n    Collation: A\n  Cardinality: 95217\n     Sub_part: NULL\n       Packed: NULL\n         Null: YES\n   Index_type: BTREE\n      Comment: \nIndex_comment: \n3 rows in set (0.00 sec)\n\nmysql>  select count(*) from orders; \n+----------+\n| count(*) |\n+----------+\n|  1500000 |  -- orders中有150W条记录，和Cardinality 是不一致的\n+----------+\n1 row in set (0.25 sec)\n```\n\n### 3. Cardinality（基数）\n\n`Cardinality`表示该索引列上有多少`不同的记录`，这个是一个`预估的值`，是采样得到的（由InnoDB触发，采样20个页，进行预估），该值`越大越好`，即当`Cardinality / RowNumber 越接近1越好`。表示该列是`高选择性的`。\n\n- 高选择性：身份证 、手机号码、姓名、订单号等\n- 低选择性：性别、年龄等\n\n**即该列是否适合创建索引，就看该字段是否具有高选择性**\n\n\n```sql\nmysql>  show create table lineitem\\G\n*************************** 1. row ***************************\n       Table: lineitem\nCreate Table: CREATE TABLE `lineitem` (\n  `l_orderkey` int(11) NOT NULL,\n  `l_partkey` int(11) DEFAULT NULL,\n  `l_suppkey` int(11) DEFAULT NULL,\n  `l_linenumber` int(11) NOT NULL,\n  `l_quantity` double DEFAULT NULL,\n  `l_extendedprice` double DEFAULT NULL,\n  `l_discount` double DEFAULT NULL,\n  `l_tax` double DEFAULT NULL,\n  `l_returnflag` char(1) DEFAULT NULL,\n  `l_linestatus` char(1) DEFAULT NULL,\n  `l_shipDATE` date DEFAULT NULL,\n  `l_commitDATE` date DEFAULT NULL,\n  `l_receiptDATE` date DEFAULT NULL,\n  `l_shipinstruct` char(25) DEFAULT NULL,\n  `l_shipmode` char(10) DEFAULT NULL,\n  `l_comment` varchar(44) DEFAULT NULL,\n  PRIMARY KEY (`l_orderkey`,`l_linenumber`),  -- 两个列作为primary\n  KEY `i_l_shipdate` (`l_shipDATE`),\n  KEY `i_l_suppkey_partkey` (`l_partkey`,`l_suppkey`),\n  KEY `i_l_partkey` (`l_partkey`),\n  KEY `i_l_suppkey` (`l_suppkey`),\n  KEY `i_l_receiptdate` (`l_receiptDATE`),\n  KEY `i_l_orderkey` (`l_orderkey`),\n  KEY `i_l_orderkey_quantity` (`l_orderkey`,`l_quantity`),\n  KEY `i_l_commitdate` (`l_commitDATE`),\n  CONSTRAINT `lineitem_ibfk_1` FOREIGN KEY (`l_orderkey`) REFERENCES `orders` (`o_orderkey`),\n  CONSTRAINT `lineitem_ibfk_2` FOREIGN KEY (`l_partkey`, `l_suppkey`) REFERENCES `partsupp` (`ps_partkey`, `ps_suppkey`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n1 row in set (0.00 sec)\n\nmysql>  show index from lineitem\\G  -- 省略其他输出，只看PRIMARY\n*************************** 1. row ***************************\n        Table: lineitem\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 1  -- 索引中的顺序，该列的顺序为1\n  Column_name: l_orderkey\n    Collation: A\n  Cardinality: 1416486  -- 约140W\n     Sub_part: NULL\n       Packed: NULL\n         Null: \n   Index_type: BTREE\n      Comment: \nIndex_comment: \n*************************** 2. row ***************************\n        Table: lineitem\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 2  -- 索引中的顺序，该列的顺序为2\n  Column_name: l_linenumber\n    Collation: A\n  Cardinality: 5882116  -- 约580W\n     Sub_part: NULL\n       Packed: NULL\n         Null: \n   Index_type: BTREE\n      Comment: \nIndex_comment:\n```\n>**对应当前例子**\n第一个索引（Seq_in_index = 1）的`Cardinality`的值表示`当前列（l_orderkey）`的不重复的值，\n第二个索引（Seq_in_index = 2）的`Cardinality`的值表示`前两列（l_orderkey）和（l_linenumber）`不重复的值\n\n```sql\n--\n-- SQL-1\n--\nmysql> select * from lineitem limit 10;                                                                 \n+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+----------------------------------------+\n| l_orderkey | l_partkey | l_suppkey | l_linenumber | l_quantity | l_extendedprice | l_discount | l_tax | l_returnflag | l_linestatus | l_shipDATE | l_commitDATE | l_receiptDATE | l_shipinstruct    | l_shipmode | l_comment                              |\n+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+----------------------------------------+\n|          1 |    155190 |      7706 |            1 |         17 |        21168.23 |       0.04 |  0.02 | N            | O            | 1996-03-13 | 1996-02-12   | 1996-03-22    | DELIVER IN PERSON | TRUCK      | blithely regular ideas caj             |\n|          1 |     67310 |      7311 |            2 |         36 |        45983.16 |       0.09 |  0.06 | N            | O            | 1996-04-12 | 1996-02-28   | 1996-04-20    | TAKE BACK RETURN  | MAIL       | slyly bold pinto beans detect s        |\n|          1 |     63700 |      3701 |            3 |          8 |         13309.6 |        0.1 |  0.02 | N            | O            | 1996-01-29 | 1996-03-05   | 1996-01-31    | TAKE BACK RETURN  | REG AIR    | deposits wake furiously dogged,        |\n|          1 |      2132 |      4633 |            4 |         28 |        28955.64 |       0.09 |  0.06 | N            | O            | 1996-04-21 | 1996-03-30   | 1996-05-16    | NONE              | AIR        | even ideas haggle. even, bold reque    |\n|          1 |     24027 |      1534 |            5 |         24 |        22824.48 |        0.1 |  0.04 | N            | O            | 1996-03-30 | 1996-03-14   | 1996-04-01    | NONE              | FOB        | carefully final gr                     |\n|          1 |     15635 |       638 |            6 |         32 |        49620.16 |       0.07 |  0.02 | N            | O            | 1996-01-30 | 1996-02-07   | 1996-02-03    | DELIVER IN PERSON | MAIL       | furiously regular accounts haggle bl   |\n|          2 |    106170 |      1191 |            1 |         38 |        44694.46 |          0 |  0.05 | N            | O            | 1997-01-28 | 1997-01-14   | 1997-02-02    | TAKE BACK RETURN  | RAIL       | carefully ironic platelets against t   |\n|          3 |      4297 |      1798 |            1 |         45 |        54058.05 |       0.06 |     0 | R            | F            | 1994-02-02 | 1994-01-04   | 1994-02-23    | NONE              | AIR        | blithely s                             |\n|          3 |     19036 |      6540 |            2 |         49 |        46796.47 |        0.1 |     0 | R            | F            | 1993-11-09 | 1993-12-20   | 1993-11-24    | TAKE BACK RETURN  | RAIL       | final, regular pinto                   |\n|          3 |    128449 |      3474 |            3 |         27 |        39890.88 |       0.06 |  0.07 | A            | F            | 1994-01-16 | 1993-11-22   | 1994-01-23    | DELIVER IN PERSON | SHIP       | carefully silent pinto beans boost fur |\n+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+----------------------------------------+\n10 rows in set (0.00 sec)\n\n--\n-- SQL-2\n--\nmysql> select l_orderkey, l_linenumber from lineitem limit 10; \n+------------+--------------+\n| l_orderkey | l_linenumber |\n+------------+--------------+\n|     721220 |            2 |\n|     842980 |            4 |\n|     904677 |            1 |\n|     990147 |            1 |\n|    1054181 |            1 |\n|    1111877 |            3 |\n|    1332613 |            1 |\n|    1552449 |            2 |\n|    2167527 |            3 |\n|    2184032 |            5 |\n+------------+--------------+\n10 rows in set (0.00 sec)\n\n--- SQL-1和SQL-2其实都是在没有排序的情况下，取出前10条数据。但是结果不一样\n\n--\n-- SQL-3\n--\nmysql> select l_orderkey, l_linenumber from lineitem order by l_orderkey limit 10;  -- 和上面的sql相比，多了一个order by的操作\n+------------+--------------+\n| l_orderkey | l_linenumber |\n+------------+--------------+\n|          1 |            1 | -----\n|          1 |            2 | -- 看orderkey为1，对应的linenumber有6条\n|          1 |            3 | -- 这就是orderkey的Cardinality仅为140W\n|          1 |            4 | -- 而(orderkey + linenumber)就有580W\n|          1 |            5 | \n|          1 |            6 | -----\n|          2 |            1 |\n|          3 |            1 |\n|          3 |            2 |\n|          3 |            3 |\n+------------+--------------+\n10 rows in set (0.01 sec)\n\n--- SQL-3 和SQL-2 不同的原因是 他们走了不同的索引\nmysql> explain select l_orderkey, l_linenumber from lineitem limit 10\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: lineitem\n   partitions: NULL\n         type: index\npossible_keys: NULL\n          key: i_l_shipdate  -- 使用了shipdate进行了索引\n      key_len: 4\n          ref: NULL\n         rows: 5882306\n     filtered: 100.00\n        Extra: Using index\n1 row in set, 1 warning (0.00 sec)\n\n\nmysql> explain select l_orderkey, l_linenumber from lineitem order by l_orderkey limit 10\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: lineitem\n   partitions: NULL\n         type: index\npossible_keys: NULL\n          key: i_l_orderkey  -- 使用了orderkey进行了查询\n      key_len: 4\n          ref: NULL\n         rows: 10\n     filtered: 100.00\n        Extra: Using index\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select * from lineitem limit 10\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: lineitem\n   partitions: NULL\n         type: ALL   -- SQL-1进行了全表扫描\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 5882306\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\n-- 所以，不使用order by取出的结果，可以理解为不是根据主键排序的结果。\n```\n>`innodb_on_state = off `\n在MySQL5.5之前，执行`show create table`操作会触发采样，而5.5之后将该参数off后，需要主动执行`analyze table`才会去采样。采样不会锁表或者锁记录。\n\n\n### 4. 复合索引\n\n\n```sql\nmysql>  show create table lineitem\\G\n*************************** 1. row ***************************\n       Table: lineitem\nCreate Table: CREATE TABLE `lineitem` (\n  `l_orderkey` int(11) NOT NULL,\n  `l_partkey` int(11) DEFAULT NULL,\n  `l_suppkey` int(11) DEFAULT NULL,\n  `l_linenumber` int(11) NOT NULL,\n  `l_quantity` double DEFAULT NULL,\n  `l_extendedprice` double DEFAULT NULL,\n  `l_discount` double DEFAULT NULL,\n  `l_tax` double DEFAULT NULL,\n  `l_returnflag` char(1) DEFAULT NULL,\n  `l_linestatus` char(1) DEFAULT NULL,\n  `l_shipDATE` date DEFAULT NULL,\n  `l_commitDATE` date DEFAULT NULL,\n  `l_receiptDATE` date DEFAULT NULL,\n  `l_shipinstruct` char(25) DEFAULT NULL,\n  `l_shipmode` char(10) DEFAULT NULL,\n  `l_comment` varchar(44) DEFAULT NULL,\n  PRIMARY KEY (`l_orderkey`,`l_linenumber`),  -- 两个列作为primary，这个就是复合索引\n  KEY `i_l_shipdate` (`l_shipDATE`),\n  KEY `i_l_suppkey_partkey` (`l_partkey`,`l_suppkey`),\n  KEY `i_l_partkey` (`l_partkey`),\n  KEY `i_l_suppkey` (`l_suppkey`),\n  KEY `i_l_receiptdate` (`l_receiptDATE`),\n  KEY `i_l_orderkey` (`l_orderkey`),\n  KEY `i_l_orderkey_quantity` (`l_orderkey`,`l_quantity`),\n  KEY `i_l_commitdate` (`l_commitDATE`),\n  CONSTRAINT `lineitem_ibfk_1` FOREIGN KEY (`l_orderkey`) REFERENCES `orders` (`o_orderkey`),\n  CONSTRAINT `lineitem_ibfk_2` FOREIGN KEY (`l_partkey`, `l_suppkey`) REFERENCES `partsupp` (`ps_partkey`, `ps_suppkey`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n1 row in set (0.00 sec)\n\n--\n-- 复合索引举例\n-- \nmysql> create table test_index_2(a int, b int , c int);\nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> alter table test_index_2 add index  idx_mul_ab (a, b);        \nQuery OK, 0 rows affected (0.11 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> insert into test_index_2 values\n    -> (1,1,10),\n    -> (1,2,9),\n    -> (2,1,8),\n    -> (2,4,15),\n    -> (3,1,6),\n    -> (3,2,17);\nQuery OK, 6 rows affected (0.04 sec)\nRecords: 6  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_index_2 where a = 1;\n+------+------+------+\n| a    | b    | c    |\n+------+------+------+\n|    1 |    1 |   10 |\n|    1 |    2 |    9 |\n+------+------+------+\n2 rows in set (0.00 sec)\n\nmysql> explain select * from test_index_2 where a = 1\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ref\npossible_keys: idx_mul_ab\n          key: idx_mul_ab   -- 走了索引\n      key_len: 5\n          ref: const\n         rows: 2\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\nmysql> select * from test_index_2 where a = 1 and b = 2;\n+------+------+------+\n| a    | b    | c    |\n+------+------+------+\n|    1 |    2 |    9 |\n+------+------+------+\n1 row in set (0.00 sec)\n\nmysql> explain select * from test_index_2 where a = 1 and b = 2\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ref   -- 此时也是走了索引\npossible_keys: idx_mul_ab\n          key: idx_mul_ab  \n      key_len: 10\n          ref: const,const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select * from test_index_2 where b = 2\\G  -- 只查询b          \n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ALL  -- 没有使用索引\npossible_keys: NULL  \n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 6\n     filtered: 16.67\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n\n\nmysql> explain select * from test_index_2 where a=1 or b = 2\\G  -- 使用or，要求结果集是并集\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ALL  -- 没有使用索引，因为b没有索引，所以b是走全表扫描的，既然走扫描，a的值也可以一起过滤\n                    -- 就没有必要在去查一次 a 的索引了\npossible_keys: idx_mul_ab\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 6\n     filtered: 30.56\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n\n--\n-- 特别的例子\n--\n---- 还是只使用b列去做范围查询，发现是走索引了\n---- 注意查询的是 count(*)\nmysql> explain select count(*) from test_index_2 where  b >1 and b < 3\\G \n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: index  -- 走了索引 \npossible_keys: NULL\n          key: idx_mul_ab\n      key_len: 10\n          ref: NULL\n         rows: 6\n     filtered: 16.67\n        Extra: Using where; Using index  -- 覆盖索引\n1 row in set, 1 warning (0.00 sec)\n\n-- 因为要求的是count(*), 要求所有的记录的和，\n-- 那索引a是包含了全部的记录的，即扫描（a,b)的索引也是可以得到count(*)的\n\nmysql> explain select * from test_index_2 where  b >1 and b < 3\\G        \n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ALL  -- 查询 * 就没法使用（a，b）索引了，需要全表扫描b的值。\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 6\n     filtered: 16.67\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n\n\nmysql> explain select * from test_index_2 where  a = 1 and c = 10\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ref  -- 也是走索引的，先用走a索引得到结果集，再用c=10去过滤\npossible_keys: idx_mul_ab\n          key: idx_mul_ab\n      key_len: 5\n          ref: const\n         rows: 2\n     filtered: 16.67\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n\nmysql> explain select * from test_index_2 where  b = 2 and c = 10\\G   \n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ALL  -- 而b和c是不行的，(b，c)不是有序的\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 6\n     filtered: 16.67\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)\n```\n\n-----\n\n## 五. information_schema（一）\n\n```sql\nmysql> use information_schema;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\n\nmysql> show tables;\n+---------------------------------------+\n| Tables_in_information_schema          |\n+---------------------------------------+\n| CHARACTER_SETS                        |\n| COLLATIONS                            |\n| COLLATION_CHARACTER_SET_APPLICABILITY |\n| COLUMNS                               |\n| COLUMN_PRIVILEGES                     |\n| ENGINES                               |\n| EVENTS                                |\n| FILES                                 |\n| GLOBAL_STATUS                         |\n| GLOBAL_VARIABLES                      |\n| KEY_COLUMN_USAGE                      |\n| OPTIMIZER_TRACE                       |\n| PARAMETERS                            |\n| PARTITIONS                            |\n| PLUGINS                               |\n| PROCESSLIST                           |\n| PROFILING                             |\n| REFERENTIAL_CONSTRAINTS               |\n| ROUTINES                              |\n| SCHEMATA                              |\n| SCHEMA_PRIVILEGES                     |\n| SESSION_STATUS                        |\n| SESSION_VARIABLES                     |\n| STATISTICS                            |\n| TABLES                                |\n| TABLESPACES                           |\n| TABLE_CONSTRAINTS                     |\n| TABLE_PRIVILEGES                      |\n| TRIGGERS                              |\n| USER_PRIVILEGES                       |\n| VIEWS                                 |\n| INNODB_LOCKS                          |\n| INNODB_TRX                            |\n| INNODB_SYS_DATAFILES                  |\n| INNODB_FT_CONFIG                      |\n| INNODB_SYS_VIRTUAL                    |\n| INNODB_CMP                            |\n| INNODB_FT_BEING_DELETED               |\n| INNODB_CMP_RESET                      |\n| INNODB_CMP_PER_INDEX                  |\n| INNODB_CMPMEM_RESET                   |\n| INNODB_FT_DELETED                     |\n| INNODB_BUFFER_PAGE_LRU                |\n| INNODB_LOCK_WAITS                     |\n| INNODB_TEMP_TABLE_INFO                |\n| INNODB_SYS_INDEXES                    |\n| INNODB_SYS_TABLES                     |\n| INNODB_SYS_FIELDS                     |\n| INNODB_CMP_PER_INDEX_RESET            |\n| INNODB_BUFFER_PAGE                    |\n| INNODB_FT_DEFAULT_STOPWORD            |\n| INNODB_FT_INDEX_TABLE                 |\n| INNODB_FT_INDEX_CACHE                 |\n| INNODB_SYS_TABLESPACES                |\n| INNODB_METRICS                        |\n| INNODB_SYS_FOREIGN_COLS               |\n| INNODB_CMPMEM                         |\n| INNODB_BUFFER_POOL_STATS              |\n| INNODB_SYS_COLUMNS                    |\n| INNODB_SYS_FOREIGN                    |\n| INNODB_SYS_TABLESTATS                 |\n+---------------------------------------+\n61 rows in set (0.00 sec)\n\n-- information_schema 数据库相当于一个数据字典。保存了表的元信息。\n\nmysql> select * from key_column_usage limit  3\\G  -- 显示了哪个索引使用了哪个列\n*************************** 1. row ***************************\n           CONSTRAINT_CATALOG: def\n            CONSTRAINT_SCHEMA: burn_test\n              CONSTRAINT_NAME: PRIMARY\n                TABLE_CATALOG: def\n                 TABLE_SCHEMA: burn_test\n                   TABLE_NAME: Orders -- 表名\n                  COLUMN_NAME: order_id  -- 索引的名称\n             ORDINAL_POSITION: 1\nPOSITION_IN_UNIQUE_CONSTRAINT: NULL\n      REFERENCED_TABLE_SCHEMA: NULL\n        REFERENCED_TABLE_NAME: NULL\n       REFERENCED_COLUMN_NAME: NULL\n*************************** 2. row ***************************\n           CONSTRAINT_CATALOG: def\n            CONSTRAINT_SCHEMA: burn_test\n              CONSTRAINT_NAME: product_name\n                TABLE_CATALOG: def\n                 TABLE_SCHEMA: burn_test\n                   TABLE_NAME: Orders_MV\n                  COLUMN_NAME: product_name\n             ORDINAL_POSITION: 1\nPOSITION_IN_UNIQUE_CONSTRAINT: NULL\n      REFERENCED_TABLE_SCHEMA: NULL\n        REFERENCED_TABLE_NAME: NULL\n       REFERENCED_COLUMN_NAME: NULL\n*************************** 3. row ***************************\n           CONSTRAINT_CATALOG: def\n            CONSTRAINT_SCHEMA: burn_test\n              CONSTRAINT_NAME: child_ibfk_1\n                TABLE_CATALOG: def\n                 TABLE_SCHEMA: burn_test\n                   TABLE_NAME: child\n                  COLUMN_NAME: parent_id\n             ORDINAL_POSITION: 1\nPOSITION_IN_UNIQUE_CONSTRAINT: 1\n      REFERENCED_TABLE_SCHEMA: burn_test\n        REFERENCED_TABLE_NAME: parent\n       REFERENCED_COLUMN_NAME: id\n3 rows in set (0.04 sec)\n\n-- 作业:  \n-- 1:Cardinality 在那张表里面\n-- 2:线上的索引是否可以优化\n```\n\n-----\n\n## 六. EXPLAIN （一）\n\n> [EXPLAIN 官方文档](http://dev.mysql.com/doc/refman/5.7/en/explain.html)\n\n* explain是解释SQL语句的执行计划，即显示该SQL语句怎么执行的\n    - 使用`explain`的时候，也可以使用`desc`\n* 5.6 版本支持DML语句进行explain解释\n* 5.6 版本开始支持`JSON格式`的输出\n\n    **注意：EXPLAIN查看的是执行计划，做SQL解析，不会去真的执行；且到5.7以后子查询也不会去执行。**\n    <br>\n\n\n* **参数extended**\n\n```sql\nmysql> explain extended  select * from test_index_2 where  b >1 and b < 3\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ALL\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 6\n     filtered: 16.67\n        Extra: Using where\n1 row in set, 2 warnings (0.00 sec)  有 warnings，这里相当于提供一个信息返回\n\nmysql> show warnings\\G \n*************************** 1. row ***************************\n  Level: Warning\n   Code: 1681\nMessage: 'EXTENDED' is deprecated and will be removed in a future release. -- 即将被弃用\n*************************** 2. row ***************************  -- 显示真正的执行语句\n  Level: Note\n   Code: 1003\nMessage: /* select#1 */ select `burn_test`.`test_index_2`.`a` AS `a`,`burn_test`.`test_index_2`.`b` AS `b`,`burn_test`.`test_index_2`.`c` AS `c` from `burn_test`.`test_index_2` where ((`burn_test`.`test_index_2`.`b` > 1) and (`burn_test`.`test_index_2`.`b` < 3))\n2 rows in set (0.00 sec)\n```\n\n\n* **参数FORMAT**\n    - 使用`FORMART=JSON`不仅仅是为了格式化输出效果，还有其他有用的显示信息。\n    - 且当5.6版本后，使用`MySQL Workbench`，可以使用`visual Explain`方式显示详细的图示信息。\n\n```sql\nmysql> explain format=json  select * from test_index_2 where  b >1 and b < 3\\G          \n*************************** 1. row ***************************\nEXPLAIN: {\n  \"query_block\": {\n    \"select_id\": 1,\n    \"cost_info\": {\n      \"query_cost\": \"2.20\"  -- 总成本\n    },\n    \"table\": {\n      \"table_name\": \"test_index_2\",\n      \"access_type\": \"ALL\",\n      \"rows_examined_per_scan\": 6,\n      \"rows_produced_per_join\": 1,\n      \"filtered\": \"16.67\",\n      \"cost_info\": {\n        \"read_cost\": \"2.00\",\n        \"eval_cost\": \"0.20\",\n        \"prefix_cost\": \"2.20\",\n        \"data_read_per_join\": \"16\"\n      },\n      \"used_columns\": [\n        \"a\",\n        \"b\",\n        \"c\"\n      ],\n      \"attached_condition\": \"((`burn_test`.`test_index_2`.`b` > 1) and (`burn_test`.`test_index_2`.`b` < 3))\"\n    }\n  }\n}\n1 row in set, 1 warning (0.00 sec)\n```","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL Explain_2","url":"/2019-10-05/mysql/MySQL学习笔记（Day017：Explain_2）/","content":"MySQL学习笔记（Day017：Explain_2）\n=========================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 作业解析\n* **哪张原数据表中记录了Cardinality信息**\n\n```sql\n--\n-- 在information_schema.STATISTICS中记录了相关的信息\n--\nmysql> use information_schema;\nDatabase changed\n\nmysql> show create table STATISTICS\\G\n*************************** 1. row ***************************\n       Table: STATISTICS\nCreate Table: CREATE TEMPORARY TABLE `STATISTICS` (\n  `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '',\n  `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '',  -- 表所在的库\n  `TABLE_NAME` varchar(64) NOT NULL DEFAULT '',  -- 表名\n  `NON_UNIQUE` bigint(1) NOT NULL DEFAULT '0',\n  `INDEX_SCHEMA` varchar(64) NOT NULL DEFAULT '', \n  `INDEX_NAME` varchar(64) NOT NULL DEFAULT '',  -- 索引名\n  `SEQ_IN_INDEX` bigint(2) NOT NULL DEFAULT '0', -- 索引的序号\n  `COLUMN_NAME` varchar(64) NOT NULL DEFAULT '',\n  `COLLATION` varchar(1) DEFAULT NULL,\n  `CARDINALITY` bigint(21) DEFAULT NULL,   -- 这里我们找到了Cardinality\n  `SUB_PART` bigint(3) DEFAULT NULL,\n  `PACKED` varchar(10) DEFAULT NULL,\n  `NULLABLE` varchar(3) NOT NULL DEFAULT '',\n  `INDEX_TYPE` varchar(16) NOT NULL DEFAULT '',\n  `COMMENT` varchar(16) DEFAULT NULL,\n  `INDEX_COMMENT` varchar(1024) NOT NULL DEFAULT ''\n) ENGINE=MEMORY DEFAULT CHARSET=utf8\n1 row in set (0.00 sec)\n\n\n--\n-- 之前我们可以通过 show index from table_name的方式查看索引\n--\n\nmysql>  show index from employees.salaries\\G\n*************************** 1. row ***************************\n        Table: salaries\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 1       -- 索引序号为1\n  Column_name: emp_no\n    Collation: A\n  Cardinality: 286271  -- Cardinality值\n     Sub_part: NULL\n       Packed: NULL\n         Null: \n   Index_type: BTREE\n      Comment: \nIndex_comment: \n*************************** 2. row ***************************\n        Table: salaries\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 2         -- 索引序号为2\n  Column_name: from_date\n    Collation: A\n  Cardinality: 2760952   -- Cardinality值\n     Sub_part: NULL\n       Packed: NULL\n         Null: \n   Index_type: BTREE\n      Comment: \nIndex_comment: \n2 rows in set (0.00 sec)\n\n--\n-- 现在可以通过STATISTICS表查看某张表的信息\n--\nmysql> select * from STATISTICS where table_name='salaries'\\G\n*************************** 1. row ***************************\nTABLE_CATALOG: def\n TABLE_SCHEMA: employees\n   TABLE_NAME: salaries\n   NON_UNIQUE: 0\n INDEX_SCHEMA: employees\n   INDEX_NAME: PRIMARY\n SEQ_IN_INDEX: 1        -- 索引序号为1\n  COLUMN_NAME: emp_no\n    COLLATION: A\n  CARDINALITY: 286271   -- Cardinality值\n     SUB_PART: NULL\n       PACKED: NULL\n     NULLABLE: \n   INDEX_TYPE: BTREE\n      COMMENT: \nINDEX_COMMENT: \n*************************** 2. row ***************************\nTABLE_CATALOG: def\n TABLE_SCHEMA: employees\n   TABLE_NAME: salaries\n   NON_UNIQUE: 0\n INDEX_SCHEMA: employees\n   INDEX_NAME: PRIMARY\n SEQ_IN_INDEX: 2          -- 索引序号为2\n  COLUMN_NAME: from_date\n    COLLATION: A\n  CARDINALITY: 2760952    -- Cardinality值\n     SUB_PART: NULL\n       PACKED: NULL\n     NULLABLE: \n   INDEX_TYPE: BTREE\n      COMMENT: \nINDEX_COMMENT: \n2 rows in set (0.00 sec)\n\n---\n---  可以看出，上面两个方法得到的Cardinality的值是相等\n---  结论就是information_schema.STATISTICS这张表记录了Cardinality信息\n---\n```\n\n* **检查表的索引创建的情况，判断该索引是否有创建的必要**\n\n```sql\n--\n-- 1. 表的信息如table_schema, table_name, table_rows等\n--    在information_schema.TABLES中\n--\nmysql> show create table TABLES\\G  \n*************************** 1. row ***************************\n       Table: TABLES\nCreate Table: CREATE TEMPORARY TABLE `TABLES` (\n  `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '',\n  `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '',   -- 表所在的库\n  `TABLE_NAME` varchar(64) NOT NULL DEFAULT '',     -- 表名\n  `TABLE_TYPE` varchar(64) NOT NULL DEFAULT '',\n  `ENGINE` varchar(64) DEFAULT NULL,\n  `VERSION` bigint(21) unsigned DEFAULT NULL,\n  `ROW_FORMAT` varchar(10) DEFAULT NULL,\n  `TABLE_ROWS` bigint(21) unsigned DEFAULT NULL,    -- 表的记录数\n  `AVG_ROW_LENGTH` bigint(21) unsigned DEFAULT NULL,\n  `DATA_LENGTH` bigint(21) unsigned DEFAULT NULL,\n  `MAX_DATA_LENGTH` bigint(21) unsigned DEFAULT NULL,\n  `INDEX_LENGTH` bigint(21) unsigned DEFAULT NULL,\n  `DATA_FREE` bigint(21) unsigned DEFAULT NULL,\n  `AUTO_INCREMENT` bigint(21) unsigned DEFAULT NULL,\n  `CREATE_TIME` datetime DEFAULT NULL,\n  `UPDATE_TIME` datetime DEFAULT NULL,\n  `CHECK_TIME` datetime DEFAULT NULL,\n  `TABLE_COLLATION` varchar(32) DEFAULT NULL,\n  `CHECKSUM` bigint(21) unsigned DEFAULT NULL,\n  `CREATE_OPTIONS` varchar(255) DEFAULT NULL,\n  `TABLE_COMMENT` varchar(2048) NOT NULL DEFAULT ''\n) ENGINE=MEMORY DEFAULT CHARSET=utf8\n1 row in set (0.00 sec)\n\n--\n-- 2. information.STATISTICS中存在 table_schema 和 table_name 信息\n--\nmysql> show create table STATISTICS\\G\n*************************** 1. row ***************************\n       Table: STATISTICS\nCreate Table: CREATE TEMPORARY TABLE `STATISTICS` (\n  `TABLE_CATALOG` varchar(512) NOT NULL DEFAULT '',\n  `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT '',  -- 表所在的库\n  `TABLE_NAME` varchar(64) NOT NULL DEFAULT '',  -- 表名\n  `NON_UNIQUE` bigint(1) NOT NULL DEFAULT '0',\n  `INDEX_SCHEMA` varchar(64) NOT NULL DEFAULT '', \n  `INDEX_NAME` varchar(64) NOT NULL DEFAULT '',   -- 索引名\n  `SEQ_IN_INDEX` bigint(2) NOT NULL DEFAULT '0', \n  `COLUMN_NAME` varchar(64) NOT NULL DEFAULT '',\n  `COLLATION` varchar(1) DEFAULT NULL,\n  `CARDINALITY` bigint(21) DEFAULT NULL, \n  `SUB_PART` bigint(3) DEFAULT NULL,\n  `PACKED` varchar(10) DEFAULT NULL,\n  `NULLABLE` varchar(3) NOT NULL DEFAULT '',\n  `INDEX_TYPE` varchar(16) NOT NULL DEFAULT '',\n  `COMMENT` varchar(16) DEFAULT NULL,\n  `INDEX_COMMENT` varchar(1024) NOT NULL DEFAULT ''\n) ENGINE=MEMORY DEFAULT CHARSET=utf8\n1 row in set (0.00 sec)\n\n\n--\n-- 3. 将TABLES 和 STATISTICS 表中的table_schema和table_name相关联\n--    通过Cardinality和table_rows 计算，即可得到对应索引名的 选择性\n--\n\n--\n-- 3.1 因为存在复合索引，所以我们要取出复合索引中seq最大的哪个值\n--     这样取出的cardinality值才是最大的\n--\nmysql> select \n    ->     table_schema, table_name, index_name,\n    ->     max(seq_in_index)  -- 取出最大的seq号后，选出index_name等信息\n    -> from\n    ->     STATISTICS\n    -> group by table_schema , table_name , index_name\\G\n\n--  -----------省略其他输出-----------\n*************************** 10. row ***************************\n     table_schema: burn_test\n       table_name: test_index_2\n       index_name: idx_mul_ab   -- 这个是上次测试复合索引建立的index\nmax(seq_in_index): 2            -- 取出了最大的seq\n--  -----------省略其他输出-----------\n\n--\n--  3.2 得到了最大的seq，从而可以取出对应的cardinality\n--\n\nmysql> select \n    ->     table_schema, table_name, index_name, cardinality\n    -> from\n    ->     STATISTICS\n    -> where\n    ->     (table_schema , table_name, index_name, seq_in_index) in \n    ->        (select \n    ->             table_schema, table_name, \n    ->             index_name, max(seq_in_index)\n    ->         from\n    ->             STATISTICS\n    ->         group by table_schema , table_name , index_name)\\G\n\n*************************** 1. row ***************************\ntable_schema: burn_test\n  table_name: Orders\n  index_name: PRIMARY\n cardinality: 5\n*************************** 2. row ***************************\ntable_schema: burn_test\n  table_name: Orders_MV\n  index_name: product_name\n cardinality: 3\n*************************** 3. row ***************************\ntable_schema: burn_test\n  table_name: child\n  index_name: par_ind\n cardinality: 0\n*************************** 4. row ***************************\ntable_schema: burn_test\n  table_name: parent\n  index_name: PRIMARY\n cardinality: 1\n*************************** 5. row ***************************\ntable_schema: burn_test\n  table_name: t4\n  index_name: PRIMARY\n cardinality: 4\n\n--  -----------省略其他输出-----------\n\n--\n-- 3.3 最后通过table_schema和table_name 让上述的信息和TABLES表进行关联\n--\n\nSELECT \n     t.TABLE_SCHEMA,t.TABLE_NAME,INDEX_NAME, CARDINALITY, TABLE_ROWS, \n     CARDINALITY/TABLE_ROWS AS SELECTIVITY  -- 得到选择性\nFROM\n    TABLES t,  -- 查询的表一，TABLES\n\t(\n\t\tSELECT     \n\t\t\ttable_schema,\n\t\t\ttable_name,\n\t\t\tindex_name,\n\t\t\tcardinality\n\t\tFROM STATISTICS \n\t\tWHERE (table_schema,table_name,index_name,seq_in_index) IN (\n\t\tSELECT \n\t\t\ttable_schema,\n\t\t\ttable_name,\n\t\t\tindex_name,\n\t\t\tMAX(seq_in_index)\n\t\tFROM\n\t\t\tSTATISTICS\n\t\tGROUP BY table_schema , table_name , index_name )\n\t) s   -- 查询的表二，就是上面3.2的查询结果\nWHERE\n    t.table_schema = s.table_schema   -- 通过库关联\n        AND t.table_name = s.table_name  -- 再通过表变量\n        AND t.table_schema = 'employees'   -- 指定某一个库名\nORDER BY SELECTIVITY;\n\n+--------------+--------------+------------+-------------+------------+------------+\n| TABLE_SCHEMA | TABLE_NAME   | index_name | cardinality | TABLE_ROWS | SELECTIVITY |\n+--------------+--------------+------------+-------------+------------+------------+\n| employees    | dept_emp     | dept_no    |           8 |     330400 |     0.0000 |\n| employees    | salaries     | PRIMARY    |      286271 |    2760952 |     0.1037 |\n| employees    | dept_manager | dept_no    |           9 |         24 |     0.3750 |\n| employees    | titles       | PRIMARY    |      296887 |     440887 |     0.6734 |\n| employees    | dept_emp     | PRIMARY    |      298761 |     330400 |     0.9042 |\n| employees    | titles       | PRIMARY    |      440166 |     440887 |     0.9984 |\n| employees    | salaries     | PRIMARY    |     2760952 |    2760952 |     1.0000 |\n| employees    | dept_manager | PRIMARY    |          24 |         24 |     1.0000 |\n| employees    | titles       | PRIMARY    |      440887 |     440887 |     1.0000 |\n| employees    | departments  | PRIMARY    |           9 |          9 |     1.0000 |\n| employees    | employees    | PRIMARY    |      298124 |     298124 |     1.0000 |\n| employees    | dept_emp     | PRIMARY    |      330400 |     330400 |     1.0000 |\n| employees    | dept_manager | PRIMARY    |          24 |         24 |     1.0000 |\n| employees    | departments  | dept_name  |           9 |          9 |     1.0000 |\n+--------------+--------------+------------+-------------+------------+------------+\n\n--\n--  通过最后一列的SELECTIVITY是否接近1，判断该索引创建是否合理\n--  注意：\n--  Cardinality和table_rows的值，都是通过随机采样，预估得到的\n--  当analyze前后，Cardinality值相差较多，说明该索引是不应该被创建的(页上的记录数值分布不平均)\n--\n--  推荐 SELECTIVITY 15% 以上是适合的\n\n--\n-- 索引使用情况\n--\n\nmysql> select * from x$schema_index_statistics limit 1\\G\n*************************** 1. row ***************************\n  table_schema: employees\n    table_name: employees\n    index_name: PRIMARY       --  索引名字\n rows_selected: 300024        --  读取的记录数\nselect_latency: 370177723990  --  使用该索引读取时总的延迟时间370毫秒（单位是皮秒）\n rows_inserted: 0             --  插入的行数\ninsert_latency: 0\n  rows_updated: 0             --  更新的行数\nupdate_latency: 0\n  rows_deleted: 0\ndelete_latency: 0\n1 row in set (0.00 sec)\n\n-- 结合 之前的SELECTIVITY和这里的数值，可以更好的判断索引是否合理\n-- 重启后数据归0\n```\n\n>索引是要排序的，建立索引越多，排序以及维护成本会很大，插入数据的速度会变慢，所以索引建立的多，不是仅仅是浪费空间，还会降低性能，增加磁盘IO\n\n**注意：MySQL5.6的版本`STATISTICS数据存在问题`，截止5.6.28仍然存在，官方定性为Bug**\n\n>作业一：在`MySQL5.6`中使用`mysql.innodb_index_stats`得到索引的选择性（SELECTIVITY）\n\n-----\n\n## 二. MySQL5.6安装sys库\n\n```bash\nshell > git clone https://github.com/mysql/mysql-sys.git\nshell > ls | grep sys_56.sql\nsys_56.sql # 这个就是我们要安装的到mysql5.6的sys\n\nshell> mysql -u root -S /tmp/mysql.sock_56 < sys_56.sql  # 直接导入即可\n```\n```sql\nmysql> select version();\n+------------+\n| version()  |\n+------------+\n| 5.6.27-log |\n+------------+\n1 row in set (0.00 sec)\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| burn_test          |\n| burn_test_56       |\n| mysql              |\n| performance_schema |\n| sys                |  -- 新安装的sys库，但是这个里面只有88个记录，因为5.7中增加了几张表，有101个记录\n| test               |\n+--------------------+\n7 rows in set (0.00 sec)\n```\n\n---\n\n## 三. Explain（二）\n\n### 1. Explain输出介绍\n\n|  列  | 含义 |\n|:----:|:----:|\n|id|执行计划的id标志|\n|select_type|SELECT的类型|\n|table|输出记录的表|\n|partitions|符合的分区，[PARTITIONS]|\n|type|JOIN的类型|\n|possible_keys|优化器可能使用到的索引|\n|key|优化器实际选择的索引|\n|key_len|使用索引的字节长度|\n|ref|进行比较的索引列|\n|rows|优化器`预估`的记录数量|\n|filtered|根据条件过滤得到的记录的百分比[EXTENDED]|\n|extra|额外的显示选项|\n\n![Explain示例](/images/mysql/Selection_001.png)\n\n\n\n#### （1）. id\n`从上往下理解`，不一定 id 序号大的先执行\n> 可以简单的理解为 id 相等的从上往下看，id 相等的从下往上看。但是在某些场合也`不一定适用`\n\n\n#### （2）. select_type\n\n| select_type | 含义 |\n|:------------:|:-----:|\n| SIMPLE | 简单SELECT(不使用UNION或子查询等)  |\n| PRIMARY | 最外层的select |\n| UNION | UNION中的第二个或后面的SELECT语句 |\n| DEPENDENT UNION | UNION中的第二个或后面的SELECT语句，依赖于外面的查询  |\n| UNION RESULT | UNION的结果  |\n| SUBQUERY | 子查询中的第一个SELECT  |\n| DEPENDENT SUBQUERY | 子查询中的第一个SELECT，依赖于外面的查询 |\n| DERIVED | 派生表的SELECT(FROM子句的子查询)  |\n| MATERIALIZED | 物化子查询 |\n| UNCACHEABLE SUBQUERY | 不会被缓存的并且对于外部查询的每行都要重新计算的子查询  |\n| UNCACHEABLE UNION | 属于不能被缓存的 UNION中的第二个或后面的SELECT语句 |\n\n\n* **MATERIALIZED**\n    - 产生中间临时表（实体）\n    - 临时表自动创建索引并和其他表进行关联，提高性能\n    - 和子查询的区别是，优化器将可以进行`MATERIALIZED`的语句自动改写成`join`，并自动创建索引\n\n\n#### （3）. table\n* 通常是用户操作的用户表\n* <unionM, N> UNION得到的结果表\n* <derivedN> 排生表，由id=N的语句产生\n* <subqueryN> 由子查询物化产生的表，由id=N的语句产生\n\n\n####（4）. type\n>**摘自姜老师的PDF，按照图上箭头的顺序来看，成本（cost）是从小到大**\n\n![TYPE](/images/mysql/Selection_002.png)\n\n\n####（5）. extra\n\n![Extra](/images/mysql/Selection_003.png)\n\n* Using filesort：可以使用复合索引将filesort进行优化。提高性能\n* Using index：比如使用覆盖索引\n* Using where: 使用where过滤条件\n\n> Extra的信息是可以作为优化的提示，但是更多的是优化器优化的一种说明","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL体系结构","url":"/2019-10-04/mysql/MySQL学习笔记-体系结构2/","content":"MySQL学习笔记（Day004：权限拾遗/Role模拟/Workbench/体系结构）\n============================================\n@(MySQL学习)\n\n[TOC]\n\n##一. 权限拾遗\n### 1. GRANT与创建用户\n```sql\nmysql> grant select on sys.* to 'perf'@'127.0.0.1' identified by '123';\nQuery OK, 0 rows affected, 1 warning (0.01 sec) -- 这里有一个warning\n\nmysql> show warnings;\n-- 输入warning的Message如下：\n-- Using GRANT for creating new user is deprecated and will be removed in future release. Create new user with CREATE USER statement.\n```\n**上面的这个例子使用`GRANT`赋权限的同时创建了`'perf'@'127.0.0.1'`这个用户，但是出现了`warning`，从给出的提示看来，以后的MySQL版本会废弃掉这种方式**\n\n* **正确的创建用户并赋权的方式**\n```sql\nmysql> create user 'pref'@'127.0.0.1' identified by '123';\nQuery OK, 0 rows affected (0.00sec)\nmysql> grant select on sys.* to 'perf'@'127.0.0.1';\nQuery OK, 0 rows affected (0.00sec)\n```\n\n### 2. 查看某一个用户的权限\n```sql\nmysql> show grants for 'perf'@'127.0.0.1';   \n+-----------------------------------------------+\n| Grants for perf@127.0.0.1                     |\n+-----------------------------------------------+\n| GRANT USAGE ON *.* TO 'perf'@'127.0.0.1'      | -- USAGE表示用户可以登录\n| GRANT SELECT ON `sys`.* TO 'perf'@'127.0.0.1' | -- 对sys库的所有表有select权限\n+-----------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n### 3. 删除某一个用户\n```sql\nmysql> drop user 'perf'@'127.0.0.1';\nQuery OK, 0 rows affected (0.00sec)\n```\n\n### 4. MySQL权限信息\n```sql\nmysql> select * from mysql.user where user='perf'\\G\n*************************** 1. row ***************************\n                  Host: 127.0.0.1\n                  User: perf\n           Select_priv: N   ---由于perf用户是对sys库有权限，所以这里（USER）全是N\n           Insert_priv: N\n           Update_priv: N\n           Delete_priv: N\n           Create_priv: N\n             Drop_priv: N\n           Reload_priv: N\n         Shutdown_priv: N\n          Process_priv: N\n             File_priv: N\n            Grant_priv: N\n       References_priv: N\n            Index_priv: N\n            Alter_priv: N\n          Show_db_priv: N\n            Super_priv: N\n Create_tmp_table_priv: N\n      Lock_tables_priv: N\n          Execute_priv: N\n       Repl_slave_priv: N\n      Repl_client_priv: N\n      Create_view_priv: N\n        Show_view_priv: N\n   Create_routine_priv: N\n    Alter_routine_priv: N\n      Create_user_priv: N\n            Event_priv: N\n          Trigger_priv: N\nCreate_tablespace_priv: N\n              ssl_type: \n            ssl_cipher: \n           x509_issuer: \n          x509_subject: \n         max_questions: 0\n           max_updates: 0\n       max_connections: 0\n  max_user_connections: 0\n                plugin: mysql_native_password\n authentication_string: *23AE809DDACAF96AF0FD78ED04B6A265E05AA257\n      password_expired: N\n password_last_changed: 2015-11-18 12:20:13\n     password_lifetime: NULL\n        account_locked: N    -- 如果这里为Y，该用户就无法使用了\n1 row in set (0.00 sec)\n\n------------------------我是分割线-----------------------------\n\nmysql> select * from mysql.db where user='perf'\\G    \n*************************** 1. row ***************************\n                 Host: 127.0.0.1\n                   Db: sys  -- sys 数据库\n                 User: perf\n          Select_priv: Y    -- 有select权限，和我们赋予perf的权限一致\n          Insert_priv: N\n          Update_priv: N\n          Delete_priv: N\n          Create_priv: N\n            Drop_priv: N\n           Grant_priv: N\n      References_priv: N\n           Index_priv: N\n           Alter_priv: N\nCreate_tmp_table_priv: N\n     Lock_tables_priv: N\n     Create_view_priv: N\n       Show_view_priv: N\n  Create_routine_priv: N\n   Alter_routine_priv: N\n         Execute_priv: N\n           Event_priv: N\n         Trigger_priv: N\n1 row in set (0.00 sec)\n```\n>**注意：**\n**`不建议`使用`INSERT`或者`GRANT`对元数据表进行修改，来达到修改权限的目的**\n\n### 5. information_schema\n```sql\nmysql> select user();\n+----------------+\n| user()         |\n+----------------+\n| perf@127.0.0.1 |\n+----------------+\n1 row in set (0.00 sec)\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |  -- 这是一个特殊的数据库，我们虽然可以看见，但其实没有权限\n| sys                |\n+--------------------+\n2 rows in set (0.00 sec)\n\nmysql> use information_schema;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> select * from  INNODB_CMP;\nERROR 1227 (42000): Access denied; you need (at least one of) the PROCESS privilege(s) for this operation\n```\n\n-----\n\n## 二. MySQL模拟角色\n### 1. 角色的定义：\n>`角色`(Role)可以用来批量管理用户，同一个角色下的用户，拥有相同的权限。\n`MySQL5.7.X`以后可以模拟角色(Role)的功能，通过`mysql.proxies_priv`模拟实现。\n`mysql.proxies_priv`在`5.5.X`和`5.6.X`的时候就存在，但是`无法模拟`角色(Role)功能。\n\n### 2. 模拟角色操作：\n```sql\n-- 感谢 @M062 郭释 同学 对这里提出的修正\n-- 查看当前proxy是否代开\nmysql> show variables like \"%proxy%\";\n+-----------------------------------+-------+\n| Variable_name                     | Value |\n+-----------------------------------+-------+\n| check_proxy_users                 | OFF   |\n| mysql_native_password_proxy_users | OFF   |\n| proxy_user                        |       |\n| sha256_password_proxy_users       | OFF   |\n+-----------------------------------+-------+\n4 rows in set (0.00 sec)\n\nmysql> set global check_proxy_users=ON;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> set global mysql_native_password_proxy_users=ON;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> exit; -- 以上设置参数，对当前会话无效，需要退出后重新登录\n             -- 如果有需要，可以设置到my.cnf中去\n\nmysql> show variables like \"%proxy%\";\n+-----------------------------------+-------+\n| Variable_name                     | Value |\n+-----------------------------------+-------+\n| check_proxy_users                 | ON    |\n| mysql_native_password_proxy_users | ON    |\n| proxy_user                        |       |\n| sha256_password_proxy_users       | OFF   |\n+-----------------------------------+-------+\n4 rows in set (0.00 sec)\n\n-- ---------------至此，下面的权限映射才有意义---------------\n\nmysql> create user 'junior_dba'@'127.0.0.1';  -- 相当于定于一个 角色(Role),省略密码，仅为演示\n                                              -- 但这只是个普通的用户，名字比较有(Role)的感觉\n                                              -- 有点类似用户组\nQuery OK, 0 rows affected (0.00sec)\n\nmysql> create user 'tom'@'127.0.0.1';         -- 用户1，省略密码，仅为演示\nQuery OK, 0 rows affected (0.02sec)\n\nmysql> create user 'jim'@'127.0.0.1';         -- 用户2，省略密码，仅为演示\nQuery OK, 0 rows affected (0.02sec)\n\nmysql> grant proxy on 'junior_dba'@'127.0.0.1' to 'tom'@'127.0.0.1';  -- 将junior_dba的权限映射(map)到tom\nQuery OK, 0 rows affected (0.02sec)\n\nmysql> grant proxy on 'junior_dba'@'127.0.0.1' to 'jim'@'127.0.0.1';  -- 然后映射(map)给jim\nQuery OK, 0 rows affected (0.01sec)\n\nmysql> grant select on *.* to 'junior_dba'@'127.0.0.1';  -- 给junior_dba（模拟的Role）赋予实际权限\nQuery OK, 0 rows affected (0.01 sec)\n\n\nmysql> show grants for 'junior_dba'@'127.0.0.1';        -- 查看 junior_dba的权限\n+-------------------------------------------------+\n| Grants for junior_dba@127.0.0.1                 |\n+-------------------------------------------------+\n| GRANT SELECT ON *.* TO 'junior_dba'@'127.0.0.1' |\n+-------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> show grants for 'jim'@'127.0.0.1';               -- 查看jim的权限\n+--------------------------------------------------------------+\n| Grants for jim@127.0.0.1                                     |\n+--------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'jim'@'127.0.0.1'                      |\n| GRANT PROXY ON 'junior_dba'@'127.0.0.1' TO 'jim'@'127.0.0.1' |\n+--------------------------------------------------------------+\n2 rows in set (0.00 sec)\n\nmysql> show grants for 'tom'@'127.0.0.1';               -- 查看tom的权限 \n+--------------------------------------------------------------+\n| Grants for tom@127.0.0.1                                     |\n+--------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'tom'@'127.0.0.1'                      |\n| GRANT PROXY ON 'junior_dba'@'127.0.0.1' TO 'tom'@'127.0.0.1' |\n+--------------------------------------------------------------+\n2 rows in set (0.00 sec)\n\nmysql> select * from mysql.proxies_priv;    --  查看 proxies_priv的权限\n+-----------+------+--------------+--------------+------------+----------------------+---------------------+\n| Host      | User | Proxied_host | Proxied_user | With_grant | Grantor              | Timestamp           |\n+-----------+------+--------------+--------------+------------+----------------------+---------------------+\n| localhost | root |              |              |          1 | boot@connecting host | 0000-00-00 00:00:00 |\n| 127.0.0.1 | tom  | 127.0.0.1    | junior_dba   |          0 | root@localhost       | 0000-00-00 00:00:00 |\n| 127.0.0.1 | jim  | 127.0.0.1    | junior_dba   |          0 | root@localhost       | 0000-00-00 00:00:00 |\n+-----------+------+--------------+--------------+------------+----------------------+---------------------+\n3 rows in set (0.00 sec)\n\n-- 测试jim的权限\n-- 如果不按照@M062 郭释同学给出的修正，\n-- 即便映射了junior_dba的select权限，但是jim仍然无法select\n-- 甚至show databases; 都无法得到所有数据库信息。\nmysql> select user();\n+---------------+\n| user()        |\n+---------------+\n| jim@127.0.0.1 |\n+---------------+\n1 row in set (0.00 sec)\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n4 rows in set (0.00 sec)\n\nmysql> select host,user from mysql.user;\n+-----------+------------+\n| host      | user       |\n+-----------+------------+\n| 127.0.0.1 | jim        |\n| 127.0.0.1 | junior_dba |\n| 127.0.0.1 | perf       |\n| 127.0.0.1 | tom        |\n| localhost | mysql.sys  |\n| localhost | root       |\n+-----------+------------+\n6 rows in set (0.00 sec)\n\nmysql> create database aaa;\nERROR 1044 (42000): Access denied for user 'junior_dba'@'127.0.0.1' to database 'aaa'\n```\n>**`mysql.proxies_priv`仅仅是对Role的`模拟`，和Oracle的角色还是有所不同.官方称呼为`Role like`**\n\n>`MySQL5.6.X` 模拟Role功能需要安装插件，具体方法请参考[论坛帖子](http://www.innomysql.net/AskInside/topic/mysql5-6-%E9%85%8D%E7%BD%AEproxy%E7%94%A8%E6%88%B7-%EF%BC%88%E6%B3%A8%E6%84%8F%E7%94%A8mysql5-6%EF%BC%89) [官方文档1](http://dev.mysql.com/doc/refman/5.6/en/proxy-users.html) [官方文档2](http://dev.mysql.com/doc/refman/5.6/en/pluggable-authentication.html)\n\n-----\n\n## 三. Workbench与Utilities介绍\n### 1. 下载\n* [Workbench-win32下载](http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-workbench-community-6.3.5-win32.msi)\n* [Workbench-win64下载](http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-workbench-community-6.3.5-winx64.msi)\n* [Utilities下载](http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-utilities-1.5.6.tar.gz)\n\n\n### 2. Workbench功能概述\n* **SQL语句格式化**\n* **SQL关键字upcase**\n* **MySQL Dashboard**\n* **SQL语法提示**\n* **ER图**\n* **Forward Engine**  `//ER图 --> DB表结构`\n* **Reverse**  `//DB表结构 --> ER图`\n\n### 3. Utilities安装\n```bash\nshell> python setup.py install  # 如果安装不成功，查看一下python的版本。推荐2.7.X\n```\n-----\n\n## 四. MySQL体系结构\n### 1. 数据库\n>数据库（数据库文件）是一个或者一组二进制文件，通常来说存在与文件系统之上。\n### 2. 数据库实例\n>由数据库后台进程/线程以及一个共享区域组成(程序的概念),数据库实例是用来操作数据库文件的\n\n**`注意：MySQL中，数据库实例和数据库是一一对应的。没有Oracle的一对多(RAC)的机制。`**\n\n### 3. MySQL体系结构\n1. 单进程多线程结构\n    - 不会影响MySQL的性能，看程序如何写。（多进程程序，进程间通信开销大于多线程）\n\n\n2. 存储引擎的概念\n    - 可以理解成文件系统，例如FAT32, NTFS, EXT4。 一个表是一个分区，引擎就是分区的文件系统\n    - `存储引擎的对象就是表`\n    - show tables; 可以看到每个表对应的是上面引擎(Engine)\n    - 除了特殊情况，我们现在就只考虑`INNODB`\n\n\n3. 体系结构图\n![MySQL体系结构提](/images/mysql/MySQL体系结构.jpg)\n\n4. 逻辑存储结构\n\n    |MySQL逻辑存储结构|\n    |:---------------:|\n    |instance|\n    |database|  \n    |schema|\n    |table|\n    |view|\n\n- **一个DB对应一个schema**\n- **一个DB对应一个文件夹**\n- **一个表对应一组文件**\n\n```\n                                                 |--> table1 --- | view1 |\nMySQL Instance -----> Database ----> Schema ---> |--> table2 --- | view2 |\n                                                 |--> table3 --- | View3 |\n```\n\n\n>**注意：**\nMySQL中一个`Database`对应一个`Schema`,之所以要有这个`schema`, 是为了兼容其他数据库\n`information_schema`数据库不是文件夹，存在于内存中，在启动时创建\n\n\n### 4. MySQL物理存储结构\n1. MySQL配置文件\n* `datadir`\n        - 存储数据二进制文件的路径\n\n2. 表结构的组成\n    - frm：表结构定义文件\n    - MYI：索引文件\n    - MYD：数据文件\n    - 可以用`hexdump -c XXX.frm`查看二进制文件(意义不大)\n    - `show create table tablename;`\n    - mysqlfrm (utilities工具包)\n\n        ```bash\n        shell> mysqlfrm --diagnostic /data/mysql_data/aaa/.a.frm  #可将frm文件转成create table的语句\n        ```\n    \n\n\n3. 错误日志文件\n* `log_err`\n        - 建议配置成统一的名字\n* 方便定位错误\n\n4. 慢查询日志文件\n* 将运行超过某一个时间`阈`(*yu四声*)`值`的SQL语句记录到文件\n        - MySQL < 5.1 ：以秒为单位\n        - MySQL >= 5.1 : 以毫秒为单位\n        - MySQL >= 5.5 : 可以将慢查询日志记录到表\n        - MySQL >= 5.6 : 以更细的粒度记录慢查询\n        - MySQL >= 5.7 : 增加timestamps支持\n* `slow_query_log_file`\n        - 建议配置成统一的名字\n* 用于优化查询\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL权限拾遗/Role模拟/Workbench","url":"/2019-10-04/mysql/MySQL学习笔记-体系结构1/","content":"\nMySQL学习笔记（Day004：权限拾遗/Role模拟/Workbench/体系结构）\n============================================\n@(MySQL学习)\n\n[TOC]\n\n##一. 权限拾遗\n### 1. GRANT与创建用户\n```sql\nmysql> grant select on sys.* to 'perf'@'127.0.0.1' identified by '123';\nQuery OK, 0 rows affected, 1 warning (0.01 sec) -- 这里有一个warning\n\nmysql> show warnings;\n-- 输入warning的Message如下：\n-- Using GRANT for creating new user is deprecated and will be removed in future release. Create new user with CREATE USER statement.\n```\n**上面的这个例子使用`GRANT`赋权限的同时创建了`'perf'@'127.0.0.1'`这个用户，但是出现了`warning`，从给出的提示看来，以后的MySQL版本会废弃掉这种方式**\n\n* **正确的创建用户并赋权的方式**\n```sql\nmysql> create user 'pref'@'127.0.0.1' identified by '123';\nQuery OK, 0 rows affected (0.00sec)\nmysql> grant select on sys.* to 'perf'@'127.0.0.1';\nQuery OK, 0 rows affected (0.00sec)\n```\n\n### 2. 查看某一个用户的权限\n```sql\nmysql> show grants for 'perf'@'127.0.0.1';   \n+-----------------------------------------------+\n| Grants for perf@127.0.0.1                     |\n+-----------------------------------------------+\n| GRANT USAGE ON *.* TO 'perf'@'127.0.0.1'      | -- USAGE表示用户可以登录\n| GRANT SELECT ON `sys`.* TO 'perf'@'127.0.0.1' | -- 对sys库的所有表有select权限\n+-----------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n### 3. 删除某一个用户\n```sql\nmysql> drop user 'perf'@'127.0.0.1';\nQuery OK, 0 rows affected (0.00sec)\n```\n\n### 4. MySQL权限信息\n```sql\nmysql> select * from mysql.user where user='perf'\\G\n*************************** 1. row ***************************\n                  Host: 127.0.0.1\n                  User: perf\n           Select_priv: N   ---由于perf用户是对sys库有权限，所以这里（USER）全是N\n           Insert_priv: N\n           Update_priv: N\n           Delete_priv: N\n           Create_priv: N\n             Drop_priv: N\n           Reload_priv: N\n         Shutdown_priv: N\n          Process_priv: N\n             File_priv: N\n            Grant_priv: N\n       References_priv: N\n            Index_priv: N\n            Alter_priv: N\n          Show_db_priv: N\n            Super_priv: N\n Create_tmp_table_priv: N\n      Lock_tables_priv: N\n          Execute_priv: N\n       Repl_slave_priv: N\n      Repl_client_priv: N\n      Create_view_priv: N\n        Show_view_priv: N\n   Create_routine_priv: N\n    Alter_routine_priv: N\n      Create_user_priv: N\n            Event_priv: N\n          Trigger_priv: N\nCreate_tablespace_priv: N\n              ssl_type: \n            ssl_cipher: \n           x509_issuer: \n          x509_subject: \n         max_questions: 0\n           max_updates: 0\n       max_connections: 0\n  max_user_connections: 0\n                plugin: mysql_native_password\n authentication_string: *23AE809DDACAF96AF0FD78ED04B6A265E05AA257\n      password_expired: N\n password_last_changed: 2015-11-18 12:20:13\n     password_lifetime: NULL\n        account_locked: N    -- 如果这里为Y，该用户就无法使用了\n1 row in set (0.00 sec)\n\n------------------------我是分割线-----------------------------\n\nmysql> select * from mysql.db where user='perf'\\G    \n*************************** 1. row ***************************\n                 Host: 127.0.0.1\n                   Db: sys  -- sys 数据库\n                 User: perf\n          Select_priv: Y    -- 有select权限，和我们赋予perf的权限一致\n          Insert_priv: N\n          Update_priv: N\n          Delete_priv: N\n          Create_priv: N\n            Drop_priv: N\n           Grant_priv: N\n      References_priv: N\n           Index_priv: N\n           Alter_priv: N\nCreate_tmp_table_priv: N\n     Lock_tables_priv: N\n     Create_view_priv: N\n       Show_view_priv: N\n  Create_routine_priv: N\n   Alter_routine_priv: N\n         Execute_priv: N\n           Event_priv: N\n         Trigger_priv: N\n1 row in set (0.00 sec)\n```\n>**注意：**\n**`不建议`使用`INSERT`或者`GRANT`对元数据表进行修改，来达到修改权限的目的**\n\n### 5. information_schema\n```sql\nmysql> select user();\n+----------------+\n| user()         |\n+----------------+\n| perf@127.0.0.1 |\n+----------------+\n1 row in set (0.00 sec)\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |  -- 这是一个特殊的数据库，我们虽然可以看见，但其实没有权限\n| sys                |\n+--------------------+\n2 rows in set (0.00 sec)\n\nmysql> use information_schema;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> select * from  INNODB_CMP;\nERROR 1227 (42000): Access denied; you need (at least one of) the PROCESS privilege(s) for this operation\n```\n\n-----\n\n## 二. MySQL模拟角色\n### 1. 角色的定义：\n>`角色`(Role)可以用来批量管理用户，同一个角色下的用户，拥有相同的权限。\n`MySQL5.7.X`以后可以模拟角色(Role)的功能，通过`mysql.proxies_priv`模拟实现。\n`mysql.proxies_priv`在`5.5.X`和`5.6.X`的时候就存在，但是`无法模拟`角色(Role)功能。\n\n### 2. 模拟角色操作：\n```sql\nmysql> create user 'junior_dba'@'127.0.0.1';  -- 相当于定于一个 角色(Role),\n                                               -- 但这只是个普通的用户，名字比较有(Role)的感觉\n                                               -- 有点类似用户组\nQuery OK, 0 rows affected (0.00sec)\n\nmysql> create user 'tom'@'127.0.0.1';         -- 用户1\nQuery OK, 0 rows affected (0.02sec)\n\nmysql> create user 'jim'@'127.0.0.1';         -- 用户2\nQuery OK, 0 rows affected (0.02sec)\n\nmysql> grant proxy on 'junior_dba'@'127.0.0.1' to 'tom'@'127.0.0.1';  -- 将junior_dba的权限映射(map)到tom\nQuery OK, 0 rows affected (0.02sec)\n\nmysql> grant proxy on 'junior_dba'@'127.0.0.1' to 'jim'@'127.0.0.1';  -- 然后映射(map)给jim\nQuery OK, 0 rows affected (0.01sec)\n\nmysql> grant select on *.* to 'junior_dba'@'127.0.0.1';  -- 给junior_dba（模拟的Role）赋予实际权限\nQuery OK, 0 rows affected (0.01 sec)\n\n\nmysql> show grants for 'junior_dba'@'127.0.0.1';        -- 查看 junior_dba的权限\n+-------------------------------------------------+\n| Grants for junior_dba@127.0.0.1                 |\n+-------------------------------------------------+\n| GRANT SELECT ON *.* TO 'junior_dba'@'127.0.0.1' |\n+-------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> show grants for 'jim'@'127.0.0.1';               -- 查看jim的权限\n+--------------------------------------------------------------+\n| Grants for jim@127.0.0.1                                     |\n+--------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'jim'@'127.0.0.1'                      |\n| GRANT PROXY ON 'junior_dba'@'127.0.0.1' TO 'jim'@'127.0.0.1' |\n+--------------------------------------------------------------+\n2 rows in set (0.00 sec)\n\nmysql> show grants for 'tom'@'127.0.0.1';               -- 查看tom的权限 \n+--------------------------------------------------------------+\n| Grants for tom@127.0.0.1                                     |\n+--------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'tom'@'127.0.0.1'                      |\n| GRANT PROXY ON 'junior_dba'@'127.0.0.1' TO 'tom'@'127.0.0.1' |\n+--------------------------------------------------------------+\n2 rows in set (0.00 sec)\n\nmysql> select * from mysql.proxies_priv;    --  查看 proxies_priv的权限\n+-----------+------+--------------+--------------+------------+----------------------+---------------------+\n| Host      | User | Proxied_host | Proxied_user | With_grant | Grantor              | Timestamp           |\n+-----------+------+--------------+--------------+------------+----------------------+---------------------+\n| localhost | root |              |              |          1 | boot@connecting host | 0000-00-00 00:00:00 |\n| 127.0.0.1 | tom  | 127.0.0.1    | junior_dba   |          0 | root@localhost       | 0000-00-00 00:00:00 |\n| 127.0.0.1 | jim  | 127.0.0.1    | junior_dba   |          0 | root@localhost       | 0000-00-00 00:00:00 |\n+-----------+------+--------------+--------------+------------+----------------------+---------------------+\n3 rows in set (0.00 sec)\n```\n>**`mysql.proxies_priv`仅仅是对Role的`模拟`，和Oracle的角色还是有所不同.官方称呼为`Role like`**\n\n-----\n\n## 三. Workbench与Utilities介绍\n### 1. 下载\n* [Workbench-win32下载](http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-workbench-community-6.3.5-win32.msi)\n* [Workbench-win64下载](http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-workbench-community-6.3.5-winx64.msi)\n* [Utilities下载](http://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-utilities-1.5.6.tar.gz)\n\n\n### 2. Workbench功能概述\n* **SQL语句格式化**\n* **SQL关键字upcase**\n* **MySQL Dashboard**\n* **SQL语法提示**\n* **ER图**\n* **Forward Engine**  `//ER图 --> DB表结构`\n* **Reverse**  `//DB表结构 --> ER图`\n\n### 3. Utilities安装\n```bash\nshell> python setup.py install  # 如果安装不成功，查看一下python的版本。推荐2.7.X\n```\n-----\n\n## 四. MySQL体系结构\n### 1. 数据库\n>数据库（数据库文件）是一个或者一组二进制文件，通常来说存在与文件系统之上。\n### 2. 数据库实例\n>由数据库后台进程/线程以及一个共享区域组成(程序的概念),数据库实例是用来操作数据库文件的\n\n**`注意：MySQL中，数据库实例和数据库是一一对应的。没有Oracle的一对多(RAC)的机制。`**\n\n### 3. MySQL体系结构\n1. 单进程多线程结构\n    - 不会影响MySQL的性能，看程序如何写。（多进程程序，进程间通信开销大于多线程）\n\n\n2. 存储引擎的概念\n    - 可以理解成文件系统，例如FAT32, NTFS, EXT4。 一个表是一个分区，引擎就是分区的文件系统\n    - `存储引擎的对象就是表`\n    - show tables; 可以看到每个表对应的是上面引擎(Engine)\n    - 除了特殊情况，我们现在就只考虑`INNODB`\n\n\n3. 体系结构图\n![MySQL体系结构提](/images/mysql/MySQL体系结构.jpg)\n\n4. 逻辑存储结构\n\n    |MySQL逻辑存储结构|\n    |:---------------:|\n    |instance|\n    |database|  \n    |schema|\n    |table|\n    |view|\n\n- **一个DB对应一个schema**\n- **一个DB对应一个文件夹**\n- **一个表对应一组文件**\n\n```\n                                                 |--> table1 --- | view1 |\nMySQL Instance -----> Database ----> Schema ---> |--> table2 --- | view2 |\n                                                 |--> table3 --- | View3 |\n```\n\n\n>**注意：**\nMySQL中一个`Database`对应一个`Schema`,之所以要有这个`schema`, 是为了兼容其他数据库\n`information_schema`数据库不是文件夹，存在于内存中，在启动时创建\n\n\n### 4. MySQL物理存储结构\n1. MySQL配置文件\n* `datadir`\n        - 存储数据二进制文件的路径\n\n2. 表结构的组成\n    - frm：表结构定义文件\n    - MYI：索引文件\n    - MYD：数据文件\n    - 可以用`hexdump -c XXX.frm`查看二进制文件(意义不大)\n    - `show create table tablename;`\n    - mysqlfrm (utilities工具包)\n\n        ```bash\n        shell> mysqlfrm --diagnostic /data/mysql_data/aaa/.a.frm  #可将frm文件转成create table的语句\n        ```\n    \n\n\n3. 错误日志文件\n* `log_err`\n        - 建议配置成统一的名字\n* 方便定位错误\n\n4. 慢查询日志文件\n* 将运行超过某一个时间`阈`(*yu四声*)`值`的SQL语句记录到文件\n        - MySQL < 5.1 ：以秒为单位\n        - MySQL >= 5.1 : 以毫秒为单位\n        - MySQL >= 5.5 : 可以将慢查询日志记录到表\n        - MySQL >= 5.6 : 以更细的粒度记录慢查询\n        - MySQL >= 5.7 : 增加timestamps支持\n* `slow_query_log_file`\n        - 建议配置成统一的名字\n* 用于优化查询\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"Employees/临时表的创建/外键约束","url":"/2019-10-03/mysql/MySQL学习笔记(Day010)/","content":"\nMySQL学习笔记（Day010：Employees/临时表的创建/外键约束）\n=====================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. Employees数据库安装\n### 1. Employees数据库介绍\n`Employees`数据库是一个用于学习和测试的数据库，大约`160MB`，`4百万条记录`\n\n### 2. Employees的安装\n>[官方安装文档](https://dev.mysql.com/doc/employee/en/employees-installation.html)\n\n#### 2.1. 下载\n根据官方文档的连接，我们可以找到下载该数据库的两种方式\n\n* [employees_db-full-1.0.6.tar.bz2](https://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2)\n* [github-test_db](https://github.com/datacharmer/test_db) 使用`git clone` 进行仓库拉取\n\n#### 2.2. 解压和拉取\n```bash\n[root@MyServer Downloads]> tar jxf employees_db-full-1.0.6.tar.bz2 \n# 老师讲解时说是解压有问题，但是我这里却可以解压\n[root@MyServer Downloads]> cd employees_db\n[root@MyServer employees_db]> ll\ntotal 164492\n-rw-r--r--. 1 501 games       752 Mar 30  2009 Changelog\n-rw-r--r--. 1 501 games      6460 Oct  9  2008 employees_partitioned2.sql\n-rw-r--r--. 1 501 games      7624 Feb  6  2009 employees_partitioned3.sql\n-rw-r--r--. 1 501 games      5660 Feb  6  2009 employees_partitioned.sql\n-rw-r--r--. 1 501 games      3861 Nov 28  2008 employees.sql # 主要文件\n-rw-r--r--. 1 501 games       241 Jul 30  2008 load_departments.dump\n-rw-r--r--. 1 501 games  13828291 Mar 30  2009 load_dept_emp.dump\n-rw-r--r--. 1 501 games      1043 Jul 30  2008 load_dept_manager.dump\n-rw-r--r--. 1 501 games  17422825 Jul 30  2008 load_employees.dump\n-rw-r--r--. 1 501 games 115848997 Jul 30  2008 load_salaries.dump\n-rw-r--r--. 1 501 games  21265449 Jul 30  2008 load_titles.dump\n-rw-r--r--. 1 501 games      3889 Mar 30  2009 objects.sql\n-rw-r--r--. 1 501 games      2211 Jul 30  2008 README\n-rw-r--r--. 1 501 games      4455 Mar 30  2009 test_employees_md5.sql\n-rw-r--r--. 1 501 games      4450 Mar 30  2009 test_employees_sha.sql\n\n\n[root@MyServer github_employees]> git clone https://github.com/datacharmer/test_db.git\nInitialized empty Git repository in /root/Downloads/github_employees/test_db/.git/\nCloning into 'test_db'...\nremote: Counting objects: 94, done.\nremote: Total 94 (delta 0), reused 0 (delta 0), pack-reused 94\nUnpacking objects: 100% (94/94), done.\nChecking connectivity... done.\n\n[root@MyServer test_db]> ll\ntotal 168340\n-rw-r--r--. 1 root root      964 Dec  2 21:25 Changelog\n-rw-r--r--. 1 root root     7948 Dec  2 21:25 employees_partitioned_5.1.sql\n-rw-r--r--. 1 root root     6276 Dec  2 21:25 employees_partitioned.sql\n-rw-r--r--. 1 root root     4193 Dec  2 21:25 employees.sql  # 主要文件\ndrwxrwxr-x. 2 root root     4096 Dec  2 21:25 images\n-rw-r--r--. 1 root root      250 Dec  2 21:25 load_departments.dump\n-rw-r--r--. 1 root root 14159880 Dec  2 21:25 load_dept_emp.dump\n-rw-r--r--. 1 root root     1090 Dec  2 21:25 load_dept_manager.dump\n-rw-r--r--. 1 root root 17722832 Dec  2 21:25 load_employees.dump\n-rw-r--r--. 1 root root 39806034 Dec  2 21:25 load_salaries1.dump\n-rw-r--r--. 1 root root 39805981 Dec  2 21:25 load_salaries2.dump\n-rw-r--r--. 1 root root 39080916 Dec  2 21:25 load_salaries3.dump\n-rw-r--r--. 1 root root 21708736 Dec  2 21:25 load_titles.dump\n-rw-r--r--. 1 root root     4568 Dec  2 21:25 objects.sql\n-rw-r--r--. 1 root root     4007 Dec  2 21:25 README.md\ndrwxrwxr-x. 2 root root     4096 Dec  2 21:25 sakila\n-rw-r--r--. 1 root root      272 Dec  2 21:25 show_elapsed.sql\n-rwxr-xr-x. 1 root root     1800 Dec  2 21:25 sql_test.sh\n-rw-r--r--. 1 root root     4878 Dec  2 21:25 test_employees_md5.sql\n-rw-r--r--. 1 root root     4882 Dec  2 21:25 test_employees_sha.sql\n```\n**解压出来的主要文件大小是不一样的，且根据网页上发布和更新的时间上看，github中的时间比较新，所以这里使用github中源作为的安装文件**\n\n#### 2.3. 安装\n```bash\n[root@MyServer test_db]> mysql -uroot -S /tmp/mysql.sock_57 -p < employees.sql \nEnter password: \nINFO\nCREATING DATABASE STRUCTURE\nINFO\nstorage engine: InnoDB\nINFO\nLOADING departments\nINFO\nLOADING employees\nINFO\nLOADING dept_emp\nINFO\nLOADING dept_manager\nINFO\nLOADING titles\nINFO\nLOADING salaries\ndata_load_time_diff\n00:01:51\n```\n\n#### 2.4. 验证\n```bash\n[root@MyServer test_db]> time mysql -uroot -S /tmp/mysql.sock_57 -p -t < test_employees_sha.sql \nEnter password: \n+----------------------+\n| INFO                 |\n+----------------------+\n| TESTING INSTALLATION |\n+----------------------+\n+--------------+------------------+------------------------------------------+\n| table_name   | expected_records | expected_crc                             |\n+--------------+------------------+------------------------------------------+\n| employees    |           300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |\n| departments  |                9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |\n| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |\n| dept_emp     |           331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |\n| titles       |           443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |\n| salaries     |          2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |\n+--------------+------------------+------------------------------------------+\n+--------------+------------------+------------------------------------------+\n| table_name   | found_records    | found_crc                                |\n+--------------+------------------+------------------------------------------+\n| employees    |           300024 | 4d4aa689914d8fd41db7e45c2168e7dcb9697359 |\n| departments  |                9 | 4b315afa0e35ca6649df897b958345bcb3d2b764 |\n| dept_manager |               24 | 9687a7d6f93ca8847388a42a6d8d93982a841c6c |\n| dept_emp     |           331603 | d95ab9fe07df0865f592574b3b33b9c741d9fd1b |\n| titles       |           443308 | d12d5f746b88f07e69b9e36675b6067abb01b60e |\n| salaries     |          2844047 | b5a1785c27d75e33a4173aaa22ccf41ebd7d4a9f |\n+--------------+------------------+------------------------------------------+\n+--------------+---------------+-----------+\n| table_name   | records_match | crc_match |\n+--------------+---------------+-----------+\n| employees    | OK            | ok        |\n| departments  | OK            | ok        |\n| dept_manager | OK            | ok        |\n| dept_emp     | OK            | ok        |\n| titles       | OK            | ok        |\n| salaries     | OK            | ok        |\n+--------------+---------------+-----------+\n+------------------+\n| computation_time |\n+------------------+\n| 00:00:18         |\n+------------------+\n+---------+--------+\n| summary | result |\n+---------+--------+\n| CRC     | OK     |\n| count   | OK     |\n+---------+--------+\n\nreal    0m19.406s\nuser    0m0.005s\nsys     0m0.004s\n\n\n[root@MyServer test_db]> time mysql -uroot -S /tmp/mysql.sock_57 -p -t < test_employees_md5.sql    \nEnter password: \n+----------------------+\n| INFO                 |\n+----------------------+\n| TESTING INSTALLATION |\n+----------------------+\n+--------------+------------------+----------------------------------+\n| table_name   | expected_records | expected_crc                     |\n+--------------+------------------+----------------------------------+\n| employees    |           300024 | 4ec56ab5ba37218d187cf6ab09ce1aa1 |\n| departments  |                9 | d1af5e170d2d1591d776d5638d71fc5f |\n| dept_manager |               24 | 8720e2f0853ac9096b689c14664f847e |\n| dept_emp     |           331603 | ccf6fe516f990bdaa49713fc478701b7 |\n| titles       |           443308 | bfa016c472df68e70a03facafa1bc0a8 |\n| salaries     |          2844047 | fd220654e95aea1b169624ffe3fca934 |\n+--------------+------------------+----------------------------------+\n+--------------+------------------+----------------------------------+\n| table_name   | found_records    | found_crc                        |\n+--------------+------------------+----------------------------------+\n| employees    |           300024 | 4ec56ab5ba37218d187cf6ab09ce1aa1 |\n| departments  |                9 | d1af5e170d2d1591d776d5638d71fc5f |\n| dept_manager |               24 | 8720e2f0853ac9096b689c14664f847e |\n| dept_emp     |           331603 | ccf6fe516f990bdaa49713fc478701b7 |\n| titles       |           443308 | bfa016c472df68e70a03facafa1bc0a8 |\n| salaries     |          2844047 | fd220654e95aea1b169624ffe3fca934 |\n+--------------+------------------+----------------------------------+\n+--------------+---------------+-----------+\n| table_name   | records_match | crc_match |\n+--------------+---------------+-----------+\n| employees    | OK            | ok        |\n| departments  | OK            | ok        |\n| dept_manager | OK            | ok        |\n| dept_emp     | OK            | ok        |\n| titles       | OK            | ok        |\n| salaries     | OK            | ok        |\n+--------------+---------------+-----------+\n+------------------+\n| computation_time |\n+------------------+\n| 00:00:16         |\n+------------------+\n+---------+--------+\n| summary | result |\n+---------+--------+\n| CRC     | OK     |\n| count   | OK     |\n+---------+--------+\n\nreal    0m18.452s\nuser    0m0.007s\nsys     0m0.005s\n```\n至此，`Employees`测试数据库就安装完成了\n\n-----\n\n## 二. 表(TABLE)\n### 1. 表的介绍\n* 表是关系数据库的核心\n* 表 = 关系\n* 表是记录的集合\n* 二维表格模型易于人的理解\n* MySQL默认存储引擎都是基于行(记录)存储\n* 每行记录都是基于列进行组织的\n\n### 2. 表是数据的集合\n```sql\nselect * from table_name limit 1;  \n```\n集合是无序的，上面的SQL语句的意思是 **从表(集合)中`随机`选出一条数据，结果是不确定的**, 不能简单的认为是取出第一条数据。\n\n```sql\nselect * from table_name order by col_name limit 1;\n```\n只有通过`order by`排序之后取出的数据，才是确定的。\n\n\n### 3. 创建表 \n#### 3.1. 临时表\n>[官方文档 表创建的语法](https://dev.mysql.com/doc/refman/5.7/en/create-table.html)\n\n* **临时表的创建**\n```sql\n--\n--  mysql 5.7.9\n--\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> use burn_test;\nDatabase changed\n\nmysql> create temporary table temp_a(a int);\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> show  create table temp_a\\G\n*************************** 1. row ***************************\n       Table: temp_a\nCreate Table: CREATE TEMPORARY TABLE `temp_a` (\n  `a` int(11) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4   -- 使用的是innodb \n1 row in set (0.00 sec)\n\n--\n-- mysql 5.6.27\n--\nmysql> select version();\n+------------+\n| version()  |\n+------------+\n| 5.6.27-log |\n+------------+\n1 row in set (0.00 sec)\n\nmysql> create temporary table  temp_a_56(a int);\nQuery OK, 0 rows affected (0.06 sec)\n\nmysql> show create table temp_a_56\\G\n*************************** 1. row ***************************\n       Table: temp_a_56\nCreate Table: CREATE TEMPORARY TABLE `temp_a_56` (\n  `a` int(11) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4  -- 5.6 也是innodb\n1 row in set (0.00 sec)\n\n-----\n\n--\n-- 终端1 MySQL 5.7.9\n--\n\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> show processlist\\G\n*************************** 1. row ***************************\n     Id: 10  -- 当前ID 是 10\n   User: root\n   Host: localhost\n     db: burn_test\nCommand: Query\n   Time: 0\n  State: starting\n   Info: show processlist   -- 当前终端执行\n*************************** 2. row ***************************\n     Id: 12\n   User: root\n   Host: localhost\n     db: NULL\nCommand: Sleep\n   Time: 328\n  State: \n   Info: NULL\n*************************** 3. row ***************************\n     Id: 13\n   User: root\n   Host: localhost\n     db: burn_test\nCommand: Sleep\n   Time: 16\n  State: \n   Info: NULL\n3 rows in set (0.00 sec)\n\nmysql> insert into temp_a values(123);\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> select * from temp_a;      \n+------+\n| a    |\n+------+\n|  123 |\n+------+\n1 row in set (0.00 sec)\n\n--\n-- 终端2 MySQL 5.7.9\n--\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> show processlist\\G\n*************************** 1. row ***************************\n     Id: 10\n   User: root\n   Host: localhost\n     db: burn_test\nCommand: Sleep\n   Time: 75\n  State: \n   Info: NULL\n*************************** 2. row ***************************\n     Id: 12\n   User: root\n   Host: localhost\n     db: NULL\nCommand: Sleep\n   Time: 403\n  State: \n   Info: NULL\n*************************** 3. row ***************************\n     Id: 13   -- 当前 ID 是 13\n   User: root\n   Host: localhost\n     db: burn_test\nCommand: Query\n   Time: 0\n  State: starting\n   Info: show processlist  -- 当前终端执行\n3 rows in set (0.00 sec)\n\nmysql> use burn_test;\nDatabase changed\n\nmysql> show tables;\nEmpty set (0.00 sec)   -- 从其他终端登录的用户(session)无法看到temp_a这个临时表\n\n----\n\n--\n-- 临时表 和 普通表 同名问题\n--\nmysql> create table test_1 (a int);  -- 创建一张普通的表叫做 test_1\nQuery OK, 0 rows affected (0.16 sec)\n\nmysql> insert into test_1 values(23);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_1 values(24);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_1;\n+------+\n| a    |\n+------+\n|   23 |   -- 可以看到插入的数据\n|   24 |\n+------+\n2 rows in set (0.00 sec)\n\nmysql> create temporary table test_1 (a int);   -- 创建一种和test_1 同名的临时表\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> insert into test_1 values(1000);  -- 插入一个 不一样的值\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> select * from test_1;\n+------+\n| a    |\n+------+\n| 1000 |   -- 只能搜索到 临时表中的数据\n+------+\n1 row in set (0.00 sec)\n\nmysql> create temporary table if not exists table_name  (a int);  -- 使用if not exists进行判断\n```\n\n>**1：临时表是`SESSION`级别的, 当前用户logout或者其他用户登录上来，是无法看到这张表的**\n>**2：当临时表和普通表同名时，当前用户只能看到同名的临时表**\n>**3：创建表时带上`if not exists`进行表的存在性检查；同时建议在临时表的表名前面加上统一的`prefix`**\n\n\n* **临时表的作用**\n    - 临时表主要的作用是给当前登录的用户存储临时数据或者临时结果的。\n    - 不要和SQL优化器在排序过程中内部帮你创建的临时表相混淆。\n\n\n* **临时表的存储引擎**\n```sql\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"default%tmp%\";\n+----------------------------+--------+\n| Variable_name              | Value  |\n+----------------------------+--------+\n| default_tmp_storage_engine | InnoDB |  -- 5.7.9 的临时表默认存储引擎就是 InnoDB\n+----------------------------+--------+\n1 row in set (0.00 sec)\n\n\nmysql> select version();\n+------------+\n| version()  |\n+------------+\n| 5.6.27-log |\n+------------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"default%tmp%\";\n+----------------------------+--------+\n| Variable_name              | Value  |\n+----------------------------+--------+\n| default_tmp_storage_engine | InnoDB |  -- 5.7.26 的临时表默认存储引擎也是 InnoDB\n+----------------------------+--------+\n1 row in set (0.00 sec)\n\n-- 5.6 之前用的是MyISAM\n```\n\n* **临时表存储位置**\n```bash\n#\n# MySQL 5.7\n#\nmysql> system ls -l /tmp  # 使用system 可以解析执行linux shell命令\ntotal 20\ndrwxr-xr-x. 4 mysql mysql 4096 Dec  2 10:06 mysql_data\nsrwxrwxrwx. 1 mysql mysql    0 Dec  2 21:20 mysql.sock_56\nsrwxrwxrwx. 1 mysql mysql    0 Dec  2 20:51 mysql.sock_57\n-rw-------. 1 mysql mysql    5 Dec  2 20:51 mysql.sock_57.lock\n-rw-r-----. 1 mysql mysql 8554 Dec  2 22:04 #sqlf18_a_0.frm  -- temp_1 的表结构\n\nmysql> system ls -l /data/mysql_data/5.7/ | grep ib\n-rw-r-----. 1 mysql mysql       879 Dec  2 20:47 ib_buffer_pool\n-rw-r-----. 1 mysql mysql  12582912 Dec  2 22:21 ibdata1\n-rw-r-----. 1 mysql mysql 134217728 Dec  2 22:20 ib_logfile0\n-rw-r-----. 1 mysql mysql 134217728 Dec  2 21:33 ib_logfile1\n-rw-r-----. 1 mysql mysql  12582912 Dec  2 22:33 ibtmp1  -- 这个是我们的表结构对应的数据\n\nmysql> show variables like \"innodb_temp%\";\n+----------------------------+-----------------------+\n| Variable_name              | Value                 |\n+----------------------------+-----------------------+\n| innodb_temp_data_file_path | ibtmp1:12M:autoextend |\n+----------------------------+-----------------------+\n1 row in set (0.00 sec)\n\n#-----\n\n#\n# MySQL 5.6\n#\nmysql> system ls -l /tmp\ntotal 68\ndrwxr-xr-x. 4 mysql mysql  4096 Dec  2 10:06 mysql_data\nsrwxrwxrwx. 1 mysql mysql     0 Dec  2 21:20 mysql.sock_56\nsrwxrwxrwx. 1 mysql mysql     0 Dec  2 20:51 mysql.sock_57\n-rw-------. 1 mysql mysql     5 Dec  2 20:51 mysql.sock_57.lock\n-rw-rw----. 1 mysql mysql  8554 Dec  2 22:38 #sql13f3_7_0.frm   -- 表结构\n-rw-rw----. 1 mysql mysql 49152 Dec  2 22:38 #sql13f3_7_0.ibd   -- 表数据\n\n# 5.6.27 中没有 innodb_temp_data_file_path 变量\nmysql> show variables like \"innodb_temp%\";\nEmpty set (0.00 sec)\n\nmysql> show variables like \"%innodb%temp%\";\nEmpty set (0.00 sec)\n```\n\n>MySQL5.7.9 把临时`表结构`放在`tmpdir`，而数据`表数据`放在`datadir`\n>MySQL5.6.27 把临时`表结构`和`表数据`都放在`tmpdir`\n\n\n### 4. 查看表结构\n```sql\nmysql> show create table test_1\\G   -- 表结构\n*************************** 1. row ***************************\n       Table: test_1\nCreate Table: CREATE TABLE `test_1` (\n  `a` int(11) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n1 row in set (0.00 sec)\n\nmysql> desc test_1\\G  -- 表的描述,描述二维表信息\n*************************** 1. row ***************************\n  Field: a\n   Type: int(11)\n   Null: YES\n    Key: \nDefault: NULL\n  Extra: \n1 row in set (0.00 sec)\n\nmysql> show table status like \"test_1\"\\G    -- 看表结构的元数据信息\n*************************** 1. row ***************************\n           Name: test_1\n         Engine: InnoDB\n        Version: 10\n     Row_format: Dynamic\n           Rows: 2\n Avg_row_length: 4096\n    Data_length: 8192\nMax_data_length: 0\n   Index_length: 0\n      Data_free: 0\n Auto_increment: NULL\n    Create_time: 2015-12-02 22:20:19\n    Update_time: 2015-12-02 22:20:44\n     Check_time: NULL\n      Collation: utf8mb4_general_ci\n       Checksum: NULL\n Create_options: \n        Comment: \n1 row in set (0.00 sec)\n\n```\n\n### 5. ALTER TABLE\n>[ALTER TABLE语法官方文档](http://dev.mysql.com/doc/refman/5.7/en/alter-table.html)\n```sql\nmysql> alter table test_1 add column b char(10);  -- 添加列 b\nQuery OK, 0 rows affected (0.25 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_1;\n+------+------+\n| a    | b    |\n+------+------+\n|   23 | NULL |\n|   24 | NULL |\n+------+------+\n2 rows in set (0.00 sec)\n\nmysql> alter table test_1 drop column b;  -- 删除列 b\nQuery OK, 0 rows affected (0.27 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_1;\n+------+\n| a    |\n+------+\n|   23 |\n|   24 |\n+------+\n2 rows in set (0.00 sec)\n```\n>注意，当表记录很大的时候，`alter table`会很耗时，影响性能\n\n* **ONLINE DDL**\n5.6以后对在线DDL操作进行了优化，以提高性能。[官方文档](http://dev.mysql.com/doc/refman/5.7/en/innodb-create-index-overview.html)     \n\n-----\n\n## 三. 外键约束\n### 1. 外键的介绍\n>[官方文档](https://dev.mysql.com/doc/refman/5.7/en/create-table-foreign-keys.html)\n```sql\n--\n-- 摘自 MySQL官方文档\n--\n\nCREATE TABLE product (         -- 商品表\n    category INT NOT NULL,     -- 商品种类\n    id INT NOT NULL,           -- 商品id\n    price DECIMAL, \n    PRIMARY KEY(category, id)  -- 主键是 (category, id)\n)   ENGINE=INNODB;\n\nCREATE TABLE customer (   -- 客户表\n    id INT NOT NULL,      -- 客户id\n    PRIMARY KEY (id)      -- 主键是 id\n)   ENGINE=INNODB;\n\nCREATE TABLE product_order (         -- 订单表\n    no INT NOT NULL AUTO_INCREMENT,  -- number，自增长\n    product_category INT NOT NULL,   -- 商品种类\n    product_id INT NOT NULL,         -- 商品id \n    customer_id INT NOT NULL,        -- 客户id\n\n    PRIMARY KEY(no),                        -- 主键是  no \n    INDEX (product_category, product_id),   -- 对 (product_category, product_id) 做索引\n    INDEX (customer_id),                    -- 对 customer_id 做索引\n\n    FOREIGN KEY (product_category, product_id)  -- 两个外键约束\n    REFERENCES product(category, id)   -- 字段 product_category 引用自 product表的category\n                                       -- 字段 product_id 引用自 product表的id\n      ON UPDATE CASCADE ON DELETE RESTRICT,     -- 级联跟新  和  严格模式删除\n\n    FOREIGN KEY (customer_id)\n      REFERENCES customer(id)\n)   ENGINE=INNODB;\n```\n\n### 2. 外键操作\n```sql\n--\n-- 表结构 摘自  MySQL 官方文档\n--\n\nmysql> create table parent (\n    ->     id int not null,\n    ->     primary key (id)\n    -> ) engine=innodb;\nQuery OK, 0 rows affected (0.14 sec)\n\nmysql> create table child (\n    ->     id int, \n    ->     parent_id INT,\n    ->     index par_ind (parent_id),\n    ->     foreign key (parent_id) \n    ->         references parent(id)\n    ->         on delete cascade on update cascade  -- 比官网例子增加 update cascade \n    -> ) engine=innodb;\nQuery ok, 0 rows affected (0.15 sec)\n\nmysql> insert into child values(1,1);  -- 我们插入一条数据，id=1，parent_id=1\nERROR 1452 (23000): Cannot add or update a child row: a foreign key constraint fails (`burn_test`.`child`, CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON DELETE CASCADE)  \n-- 直接报错了，因为此时parent表中没有任何记录\n\nmysql> insert into parent values(1);   -- 现在parent中插入记录\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into child values(1,1);  -- 然后在child中插入记录，且parent_id是在parent中存在的\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into child values(1,2);  -- 插入parent_id=2的记录，报错。因为此时parent_id=2的记录不存在\nERROR 1452 (23000): Cannot add or update a child row: a foreign key constraint fails (`burn_test`.`child`, CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON DELETE CASCADE)\n\nmysql> select * from  child;     \n+------+-----------+\n| id   | parent_id |\n+------+-----------+\n|    1 |         1 |  -- parent_id = 1\n+------+-----------+\n1 row in set (0.00 sec)\n\nmysql> select * from  parent;\n+----+\n| id |\n+----+\n|  1 |  -- 根据表结构的定义（Foreign_key），这个值就是 child表中的id\n+----+\n1 row in set (0.00 sec)\n\nmysql> update parent set id=100 where id=1;       \nQuery OK, 1 row affected (0.04 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from parent;\n+-----+\n| id  |\n+-----+\n| 100 |  -- 已经设置成了100\n+-----+\n1 row in set (0.00 sec)\n\nmysql> select * from child;\n+------+-----------+\n| id   | parent_id |\n+------+-----------+\n|    1 |       100 |  -- 自动变化，这是on update cascade的作用，联级更新，parent更新，child也跟着更新\n+------+-----------+\n1 row in set (0.00 sec)\n\nmysql> delete from parent where id=100;  -- 删除这条记录\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from parent;  -- id=100的记录已经被删除了\nEmpty set (0.00 sec)\n\nmysql> select * from child;  -- id=1，parent_id=100的记录跟着被删除了。on delete cascade的作用\nEmpty set (0.00 sec)\n\nmysql> alter table child drop foreign key child_ibfk_1;  -- 删除 之前的外键\nQuery OK, 0 rows affected (0.07 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> alter table child add foreign key(parent_id) \n    -> references parent(id) on update cascade on delete restrict;  -- 使用严格模式\nQuery OK, 0 rows affected (0.27 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> insert into parent values(50);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into child values(3,50); \nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into child values(3,51);  -- 和之前一样会提示错误\nERROR 1452 (23000): Cannot add or update a child row: a foreign key constraint fails (`burn_test`.`child`, CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON UPDATE CASCADE)\n\nmysql> delete from parent where id=50;  -- 删除失败了，因为是restrict模式\nERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`burn_test`.`child`, CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON UPDATE CASCADE)\n\n-- 注意，delete 后面说明都不写表示 no action == restrict\n```\n> 外键约束，可以让数据进行一致性更新，但是会有一定的`性能损耗`，线上业务使用不多。通常上述级联更新和删除都是由应用层业务逻辑进行判断并实现。\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"Mysql升级/参数/连接/权限","url":"/2019-10-02/mysql/MySQL学习笔记(Day003)/","content":"\nMySQL学习笔记（Day003：升级/参数/连接/权限）\n============================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 数据库升级\n###1. **环境说明：**\n一般说来，MySQL数据库的二进制数据文件，也就是`my.cnf`中的配置项`datadir`所在的位置，和我们MySQL应用程序安装的位置，是分开的，仅仅通过配置项告诉MySQL，数据库的数据存在`datadir`这个目录下。当程序和数据分离以后，方便我们对数据库应用程序做版本的升级或者回退。\n\n###2. **环境举例：**\n* *MySQL安装目录：*\n    - **MySQL 5.6.27:** /usr/local/mysql-5.6.27-linux-glibc2.5-x86_64\n    - **MySQL 5.7.9 :** /usr/local/mysql-5.7.9-linux-glibc2.5-x86_64\n         \n* *datadir目录：*\n    - /data/mysq_data/\n\n* *初始环境：*\n \n    ```bash\n    shell> ll | grep mysql\n    lrwxrwxrwx   1 root root    34 Nov 16 13:40 mysql -> mysql-5.6.27-linux-glibc2.5-x86_64\n    drwxr-xr-x  13 root mysql  4096 Nov 16 13:37 mysql-5.6.27-linux-glibc2.5-x86_64\n    drwxr-xr-x   9 7161 wheel 4096 Oct 12 00:29 mysql-5.7.9-linux-glibc2.5-x86_64\n    \n    shell> ll /data/mysql_data/\n    total 13540\n    -rw-rw---- 1 mysql mysql    65468 Nov 16 13:50 bin.000001\n    -rw-rw---- 1 mysql mysql  1176237 Nov 16 13:50 bin.000002\n    -rw-rw---- 1 mysql mysql       26 Nov 16 13:50 bin.index\n    -rw-rw---- 1 mysql mysql     6882 Nov 16 13:50 error.log\n    -rw-rw---- 1 mysql mysql      865 Nov 16 13:50 ib_buffer_pool\n    -rw-rw---- 1 mysql mysql 12582912 Nov 16 13:50 ibdata1\n    drwx------ 2 mysql mysql     4096 Nov 16 13:50 mysql\n    drwx------ 2 mysql mysql     4096 Nov 16 13:50 performance_schema\n    drwx------ 2 mysql mysql     4096 Nov 16 13:49 test\n    ```\n\n### 3. 版本升级\n```bash\nshell> /etc/init.d/mysqld stop  #安全的停止数据库的运行\nshell> cd /usr/local/\nshell> unlink mysql\nshell> ln -s mysql-5.7.9-linux-glibc2.5-x86_64 mysql \n        #此时，MySQL的应用程序版本已经升级完成\n        #/etc/init.d/mysqld\n        #/etc/profile中PATH增加的/usr/local/mysql/bin\n        #都不需要做任何的改变，即可将当前系统的mysql版本升级完成\n        #注意：此时只是应用程序升级完成，系统表仍然还是5.6的版本\n        \n\nshell> cd /usr/local/mysql\nshell> chown root.mysql . -R\n#5.7.x -> 5.6.X 降级存在问题，这里暂且注释掉\n#shell> cp /data/mysql_data/mysql /你的备份路径/mysql_5_6_27.backup -r\n       #该步骤将mysql5.6.27版本的系统表进行了备份，以便将来可以回退\n       \nshell> /etc/init.d/mysqld start \n#此时 /etc/init.d/mysqld start  # 可以启动\n#     且可以使用 mysql -u root -p （原密码） 进入数据库\n#     show databases;存在test表，而没有sys表（数据的二进制文件兼容）\n#     但是如果去看error.log会发现好多的WARNNING\n#     所以，这个时候我们要去 upgrade 去升级\n\n       \nshell> mysql_upgrade -p -s  \n        #参数 -s 一定要加,表示只更新系统表，-s: upgrade-system-tables\n        #如果不加-s,则会把所有库的表以5.7.9的方式重建，线上千万别这样操作\n        #因为数据库二进制文件是兼容的，无需升级\n        #什么时候不需要-s ? 当一些老的版本的存储格式需要新的特性，\n        #                 来提升性能时，不加-s\n        #即使通过slave进行升级，也推荐使用该方式升级，速度比较快\n   \nEnter password: \nThe --upgrade-system-tables option was used, databases wont be touched.\nChecking if update is needed.\nChecking server version.\nRunning queries to upgrade MySQL server.\nUpgrading the sys schema.\nUpgrade process completed successfully.\nChecking if update is needed.\n    \nshell> mysql -u root -p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 4\nServer version: 5.7.9-log MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |  # 这个就是升级后的系统库，如果回退，将备份的拷贝回来覆盖即可\n| performance_schema |\n| sys                |  # 5.7 新的sys库\n| test               |  # 5.6 中的test库\n+--------------------+\n5 rows in set (0.00 sec)\n```\n> `5.1.X`、`5.5.X` 、`5.6.X` 是可以直接通过该方式升级到`5.7.X`。`5.0.X`未知，需要测试\n\n**注意：**\n如果原来数据二进制文件保存在**/usr/local/`mysql-5.6.27`-linux-glibc2.5-x86_64/data**目录下,在升级之前，要么将该目录的数据拷贝到新的你指定的data目录（比如**/usr/local/`mysql-5.7.9`-linux-glibc2.5-x86_64/data** ），要么修改`my.cnf`，将`datadir`指向`/usr/local/mysql-5.6.27-linux-glibc2.5-x86_64/data`，总之一定要确保`my.cnf`中的数据位置和你实际的数据位置是一致的，不管是默认的也好，还是你`datadir`指定的也好\n\n### 4.关于降级问题的说明\n通过覆盖`mysql`系统表的方式存在问题，会导致启动不起来。官方建议如下：\n>[官方MySQL5.7降级建议](http://dev.mysql.com/doc/refman/5.7/en/downgrading.html#downgrade-procedure-inplace)\n上述建议中使用的SQL语句可在`mysql5.7`的源码的`srcipts/mysql_system_tables_fix_for_downgrade.sql`中找到，或者直接运行这个sql脚本。\n\n姜老师测试后发现，有bug; 可以启动，但是原来的用户表，无法访问。\n当前记录时间是2015-11-17，等待下一步解决方案。\n\n\n\n\n------\n\n## 二.  MySQL的连接登录\n###1. **几种登录方式**\n* 方式一 `mysql -p`\n    - 该方法默认使用root用户, 可使用`select user();`查看当前用户\n\n* 方式二 `mysql -S /tmp/mysql.sock -u root -p`  *密码A*\n    - 该方法适用于在安装MySQL主机上进行本地登录\n    \n* 方式三 `mysql -h 127.0.0.1 -u root -p` *密码B*\n    - 使用`'root'@'127.0.0.1'`这个用户登录\n        \n* 方式四 `mysql -h localhost -u root -p` *密码A*\n    - 该方式等价与【方式二】，且和【方式三】属于两个不同的“用户”\n\n###2. 免密码登录\n* 方式一 `my.cnf`增加`[client]`标签   \n    ```bash   \n    [client]   \n    user=\"root\"  \n    password=\"你的密码\"  \n    ```\n    \n    ```bash\n    #单对定义不同的客户端\n    [mysql] # 这个是给/usr/loca/mysql/bin/mysql 使用的\n    user=root\n    password=\"你的密码\"\n    \n    [mysqladmin] # 这个是给/usr/local/mysql/bin/mysqladmin使用的\n    user=root\n    password=\"你的密码\"\n    ```\n\n    **每个不同的客户端需要定义不同的标签，使用`[client]`可以统一**\n    \n* 方式二  `login-path`\n    \n    ```bash\n    shell> mysql_config_editor set -G vm1 -S /tmp/mysql.sock -u root -p\n    Enter password [输入root的密码]\n    \n    shell> mysql_config_editor print --all\n    [vm1]\n    user=root\n    password=*****\n    socket=/tmp/mysql.sock\n    \n    #login\n    shell> mysql --login-path=vm1 # 这样登录就不需要密码，且文件二进制存储 ,位置是 ~/.mylogin.cnf\n    ```\n    **该方式相对安全。如果server被黑了，该二进制文件还是会被破解**\n        \n* 方式三 `~/.my.cnf`, 自己当前家目录\n    ```bash\n    #Filename: ~/.my.cnf\n    [client]\n    user=\"root\"\n    password=\"你的密码\"\n    ```\n\n-----\n    \n## 三. MySQL 参数介绍和设置\n###1. 参数的分类 \n* 全局参数：GLOBAL\n    - 可修改参数\n    - 不可修改参数\n* 会话参数：SESSION\n    - 可修改参数\n    - 不可修改参数\n        \n> 1: 用户可在线修改`非只读参数`，`只读参数`只能预先在配置文件中进行设置，通过重启数据库实例,方可生效。  \n\n> 2: 所有的在线修改过的参数(GLOBAL/SESSION)，在重启后，都会丢失，不会写如`my.cnf`，无法将修改进行持久化\n\n> 3: 有些参数，即存在于`GLOBAL`又存在于`SESSION`, 比如`autocommit` (PS：MySQL默认是提交的)\n\n###2. 查看参数\n \n```bash\nmysql> show variables; # 显示当前mysql的所有参数，且无隐藏参数\nmysql> show variables like \"max_%\"; #查以max_开头的变量\n```\n###3. 设置参数\n* 设置全局(GLOBAL)参数\n    ```bash\n    mysql> set global slow_query_log = off; #不加global，会提示错误\n                                            #slow_query_log是全局参数\n\n    mysql> set slow_query_log = off;  # 下面就报错了，默认是会话参数\n    ERROR 1229 (HY000): Variable 'slow_query_log' is a GLOBAL variable and should be set with SET GLOBAL\n    ```\n\n* 设置会话(SESSION)参数\n    \n    ```bash\n    mysql> set autocommit = 0;  # 当前会话生效\n    # 或者\n    mysql> set session autocommit = 0;  # 当前会话生效\n    ```\n    `autocommit`同样在`GLOBAL`中, 也有同样的参数\n    ```bash\n    mysql> set global autocommit = 1; #当前实例，全局生效\n    ```\n    **注意：如果这个时候/etc/init.d/mysqld restart, 则全局的autocommit的值会变成默认值，或者依赖于my.cnf的设置值。**\n    \n    执行的效果如下：\n    ```bash\n    mysql> show variables like \"slow%\"; # 原值为ON\n    +---------------------+----------+\n    | Variable_name       | Value    |\n    +---------------------+----------+\n    | slow_launch_time    | 2        |\n    | slow_query_log      | OFF      |\n    | slow_query_log_file | slow.log |\n    +---------------------+----------+\n    3 rows in set (0.00 sec)\n    \n    mysql> select @@session.autocommit; # 等价于 slect @@autocomit;\n    +----------------------+\n    | @@session.autocommit |\n    +----------------------+\n    |                    0 |\n    +----------------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select @@global.autocommit;       \n    +---------------------+\n    | @@global.autocommit |\n    +---------------------+\n    |                   1 |\n    +---------------------+\n    1 row in set (0.00 sec)\n    ```\n        \n-----\n\n## 四. 权限管理\n###1. “用户 + IP”的概念\nMySQL中同一个用户名，比如Bob,能否登录，以及用什么密码登录，可以访问什么库等等，都需要加上IP，才可以表示一个完整的用户标识\n\n>`bob@127.0.0.1` 和 `bob@loalhost` 以及 `bob@192.168.1.100` 这三个其实是`不同`的 **用户标识** \n\n###2. 用户权限管理\n\n* **系统表权限信息:**\n    - **a) 用户名和IP是否允许**\n    - **b) 查看mysql.user表**  `// 查看全局所有库的权限`\n    - **c) 查看mysql.db表**  `// 查看指定库的权限`\n    - **d) 查看mysql.table_priv表** `// 查看指定表的权限`\n    - **e) 查看mysql.column_priv表** `// 查看指定列的权限`\n\n    ***tips**: mysql> desc [tablename]; 可以查看表的结构信息；*\n    \n* **常用权限：**\n    - SQL语句：SELECT、INSERT、UPDATE、DELETE、INDEX\n    - 存储过程：CREATE ROUTINE、ALTER ROUTINE、EXECUTE、TRIGGER\n    - 管理权限：SUPER、RELOAD、SHOW DATABASE、SHUTDOWN、\n    \n    [所有权限猛戳这里](https://dev.mysql.com/doc/refman/5.x/en/privileges-provided.html)\n\n\n* **可选资源:**\n    - MAX_QUERIES_PER_HOUR *count*\n    - MAX_UPDATES_PER_HOUR *count*\n    - MAX_CONNECTIONS_PER_HOUR *count*\n    - MAX_USER_CONNECTIONS *count*\n\n    ***tips:**只能精确到小时，对于部分场景不适用，可以考虑中间件方式*\n\n\n* **显示当前用户的权限**\n    ```bash\n    #这三个是同一个意思\n    mysql> show grants;\n    mysql> show grants for current_user;\n    mysql> show grants for current_user();\n    ```\n\n###3. 基本操作\n```bash\nmysql> create user 'bob'@'127.0.0.1' identified by '123'; \n       #创建一个认证用户为'bob'@'127.0.0.1',密码是123\nmysql> grant all on NWDB.* to 'bob'@'127.0.0.1';\n       #授予他NWDB库下面所有表的所有访问权限; *.*表示所有库的所有表\n\nmysql> grant all on NWDB.* to 'alice'@'127.0.0.1' identified by '123';\n       #这个grant语句会搜索用户，如果用户不存在，则自动创建用户，\n       #如果不带identified by, 则该用户名密码为空\n\nmysql> grant all on *.* to 'tom'@'192.168.10.%' identified by '123' with grant option;\n       #表示这个用户'tom'@'127.0.0.1'可以访问所有库的所有表，\n       #同时，他还可以给其他用户授予权限(with grant option)，\n       #注意如果，*.*改成了某一个指定的非USER库，\n       #则tom没法去新建其他用户了，因为User库没有权限了\n       #192.168.10.% 表示属于192.168.10.0/24网段的用户可以访问\n```\n\n### 4. 撤销权限\n* `revoke` 关键字，该关键字只删除用户权限，不删除用户\n* `revoke` 语法同`grant`一致, 从`grant ... to` 变为`revoke ... from`\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"存储引擎二/多实例安装","url":"/2019-10-01/mysql/MySQL学习笔记（Day001-002：介绍和安装）/","content":"\nMySQL学习笔记（Day001-002：介绍和安装）\n=====================================\n@(MySQL学习)\n\n[TOC]\n\n##一.MySQL版本选择\n\n1. MySQL5.6以后的版本，推荐使用官方版本。\n2. Percona：在5.6版本以后，MySQL将Percon之前优化集成到官方版本中;\n3. MariaDB：无INNODB;且核心代码较老\n4. MySQL在5.6以后不断重构源码，安装包越来越大，功能和性能在持续改进\n\n-----\n\n## 二. MySQL官方网站介绍\n\n官方网站：http://www.mysql.com\n1. **Developer Zone**： MySQL开发工程师板块\n    * Articles： Oracle工程师自己的博客\n    * Plant MySQL： 和MySQL相关从业人员的博客\n    * Bugs：MySQL BugList\n    * Worklog：开发记录\n    * Labs：MySQL实验性项目\n\n2. **Downloads**：MySQL下载\n    * Enterprise：MySQL企业版本相关，略过\n    * Community：社区版，我们下载和使用社区版\n        - MySQL Community Server：MySQL Server\n        - MySQL Fabric : 和管理相关的工具\n        - MySQL Router：路由中间件\n        - MySQL Utilities：MySQL应用程序包\n        - MySQL Workbench：官方图型化管理界面\n        - MySQL Proxy：MySQL代理。Alpha版本，不推荐\n\n3. **Documentation**：MySQL文档\n    * 官方文档 版面更改，下载离线文档在左侧Menu的下面\n        - [PDF A4](http://downloads.mysql.com/docs/refman-5.7-en.a4.pdf)\n        - [EPUB](http://downloads.mysql.com/docs/refman-5.7-en.epub)\n        - [HTML](http://downloads.mysql.com/docs/refman-5.7-en.html-chapter.zip)\n        \n-----\n\n## 三. MySQL下载\n1. 推荐下载`Linux-Generic`版本\n2. `Source Code`版本主要作用是为了让开发人员研究源码使用，自己编译对性能提升不明显\n3. 不推荐`Version 5.5.X`，有部分bug\n4. 推荐使用`Version 5.6.X`和`Version 5.7.X`\n\n\n>*下载地址：*\n[MySQL Community Server 5.7.9 Linux Generic x86-64bit](http://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.9-linux-glibc2.5-x86_64.tar.gz)\n[MySQL Community Server 5.6.27 Linux Generic x86-64bit](http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.27-linux-glibc2.5-x86_64.tar.gz)\n\n-----\n\n## 四. MySQL安装\n1. 安装通用步骤：\n    * 解压缩`mysql-VERSION-linux-glibc2.5-x86_64.tar.gz`\n    * 打开`INSTALL_BINARY` 文件，按照`shell>`开头的步骤进行操作\n    * 将`export PATH=/安装路径/mysql/bin:$PATH`添加到`/etc/profile`\n    * `chkconfig mysqld on`或者`chkconfig mysqld.server on`视你的环境而定，详细步骤如下\n\n2. MySQL 5.6.X 安装：\n    ```bash\n    shell> yum install libaio # Debain系用户:apt-get install libaio1\n    shell> groupadd mysql\n    shell> useradd -r -g mysql mysql\n    shell> cd /usr/local\n    shell> tar zxvf /path/to/mysql-VERSION-OS.tar.gz\n    shell> ln -s full-path-to-mysql-VERSION-OS mysql\n    shell> cd mysql\n    shell> chown -R mysql .\n    shell> chgrp -R mysql .\n    shell> scripts/mysql_install_db --user=mysql\n    shell> chown -R root .\n    shell> chown -R mysql data\n    shell> bin/mysqld_safe --user=mysql &\n    # Next command is optional\n    shell> cp support-files/mysql.server /etc/init.d/mysql.server \n    ```\n\n3. MySQL 5.7.X 安装     \n    ```bash\n    shell> groupadd mysql\n    shell> useradd -r -g mysql mysql\n    shell> cd /usr/local\n    shell> tar zxvf /path/to/mysql-VERSION-OS.tar.gz\n    shell> ln -s full-path-to-mysql-VERSION-OS mysql\n    shell> cd mysql\n    shell> mkdir mysql-files\n    shell> chmod 770 mysql-files\n    shell> chown -R mysql .\n    shell> chgrp -R mysql .\n    shell> bin/mysqld --initialize --user=mysql #该步骤中会产生零时\n                                                #root@localhost密码\n                                                #需要自己记录下来\n    shell> bin/mysql_ssl_rsa_setup          \n    shell> chown -R root .\n    shell> chown -R mysql data mysql-files\n    shell> bin/mysqld_safe --user=mysql &\n    # Next command is optional\n    shell> cp support-files/mysql.server /etc/init.d/mysql.server\n    ```\n\n\n4. 验证安装\n    * `data`目录在安装之前是空目录，安装完成后应该有`ibXXX`等文件\n    * 安装过程中输出的信息中，不应该含有`ERROR`信息，错误信息`默认`会写入到`$HOSTNAME.err`的文件中\n    * 通过`bin/mysql`命令（*5.7.X含有零时密码*）可以正常登录\n\n5. MySQL启动\n    * `mysqld_safe --user=mysql &` 即可启动，`mysqld_safe`是一个守护`mysqld`进程的脚本程序，旨在`mysqld`意外停止时，可以重启`mysqld`进程\n    * 也可以通过`INSTALL_BINARRY`中的的步骤，使用`/etc/init.d/mysql.server  start`进行启动（启动脚本以你复制的实际名字为准，通常改名为`mysqld`,即`/etc/init.d/mysqld start`）\n\n-----\n\n## 五. 附录\n1. 姜老师的配置文件`my.cnf`   \n    \n    ```bash\n    [client]\n    user=david\n    password=88888888\n    \n    [mysqld]\n    ########basic settings########\n    server-id = 11 \n    port = 3306\n    user = mysql\n    bind_address = 10.166.224.32   #根据实际情况修改\n    autocommit = 0   #5.6.X安装时，需要注释掉，安装完成后再打开\n    character_set_server=utf8mb4\n    skip_name_resolve = 1\n    max_connections = 800\n    max_connect_errors = 1000\n    datadir = /data/mysql_data      #根据实际情况修改,建议和程序分离存放\n    transaction_isolation = READ-COMMITTED\n    explicit_defaults_for_timestamp = 1\n    join_buffer_size = 134217728\n    tmp_table_size = 67108864\n    tmpdir = /tmp\n    max_allowed_packet = 16777216\n    sql_mode = \"STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER\"\n    interactive_timeout = 1800\n    wait_timeout = 1800\n    read_buffer_size = 16777216\n    read_rnd_buffer_size = 33554432\n    sort_buffer_size = 33554432\n    ########log settings########\n    log_error = error.log\n    slow_query_log = 1\n    slow_query_log_file = slow.log\n    log_queries_not_using_indexes = 1\n    log_slow_admin_statements = 1\n    log_slow_slave_statements = 1\n    log_throttle_queries_not_using_indexes = 10\n    expire_logs_days = 90\n    long_query_time = 2\n    min_examined_row_limit = 100\n    ########replication settings########\n    master_info_repository = TABLE\n    relay_log_info_repository = TABLE\n    log_bin = bin.log\n    sync_binlog = 1\n    gtid_mode = on\n    enforce_gtid_consistency = 1\n    log_slave_updates\n    binlog_format = row \n    relay_log = relay.log\n    relay_log_recovery = 1\n    binlog_gtid_simple_recovery = 1\n    slave_skip_errors = ddl_exist_errors\n    ########innodb settings########\n    innodb_page_size = 8192\n    innodb_buffer_pool_size = 6G    #根据实际情况修改\n    innodb_buffer_pool_instances = 8\n    innodb_buffer_pool_load_at_startup = 1\n    innodb_buffer_pool_dump_at_shutdown = 1\n    innodb_lru_scan_depth = 2000\n    innodb_lock_wait_timeout = 5\n    innodb_io_capacity = 4000\n    innodb_io_capacity_max = 8000\n    innodb_flush_method = O_DIRECT\n    innodb_file_format = Barracuda\n    innodb_file_format_max = Barracuda\n    innodb_log_group_home_dir = /redolog/  #根据实际情况修改\n    innodb_undo_directory = /undolog/      #根据实际情况修改\n    innodb_undo_logs = 128\n    innodb_undo_tablespaces = 3\n    innodb_flush_neighbors = 1\n    innodb_log_file_size = 4G               #根据实际情况修改\n    innodb_log_buffer_size = 16777216\n    innodb_purge_threads = 4\n    innodb_large_prefix = 1\n    innodb_thread_concurrency = 64\n    innodb_print_all_deadlocks = 1\n    innodb_strict_mode = 1\n    innodb_sort_buffer_size = 67108864 \n    ########semi sync replication settings########\n    plugin_dir=/usr/local/mysql/lib/plugin      #根据实际情况修改\n    plugin_load = \"rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so\"\n    loose_rpl_semi_sync_master_enabled = 1\n    loose_rpl_semi_sync_slave_enabled = 1\n    loose_rpl_semi_sync_master_timeout = 5000\n    \n    [mysqld-5.7]\n    innodb_buffer_pool_dump_pct = 40\n    innodb_page_cleaners = 4\n    innodb_undo_log_truncate = 1\n    innodb_max_undo_log_size = 2G\n    innodb_purge_rseg_truncate_frequency = 128\n    binlog_gtid_simple_recovery=1\n    log_timestamps=system\n    transaction_write_set_extraction=MURMUR32\n    show_compatibility_56=on\n    ```\n\n\n2. 几个重要的参数配置和说明 \n    * `innodb_log_file_size = 4G ` :做实验可以更改的小点，线上环境推荐用4G，以前5.5和5.1等版本之所以官方给的值很小，是因为太大后有bug，现在bug已经修复\n    \n    * `innodb_undo_logs = 128`和`innodb_undo_tablespaces = 3`建议在安装之前就确定好该值，后续修改比较麻烦\n    * `[mysqld]`，`[mysqld-5.7]`这种tag表明了下面的配置在什么版本下才生效,`[mysqld]`下均生效\n    * `autocommit`,这个参数在5.5.X以后才有，安装5.6.X的时候要注意先把该参数注释掉，等安装完成后，再行打开, 5.7.X无需预先注释\n    * `datadir`, `innodb_log_group_home_dir`, `innodb_undo_directory`一定要注意他的权限是 `mysql:mysql`\n\n\n3. `my.cnf`问题\n    * 使用`mysqld --help -vv | grep my.cnf `查看mysql的配置文件读取顺序\n    * 后读取的`my.cnf`中的配置，如果有相同项，会覆盖之前的配置\n    * 使用`--defaults-files`可指定配置文件\n    ","tags":["mysql"],"categories":["mysql"]},{"title":"存储引擎二/多实例安装","url":"/2019-10-01/mysql/MySQL学习笔记(Day006)/","content":"\nMySQL学习笔记（Day006：存储引擎二/多实例安装）\n============================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. MyISAM存储引擎(下)\n### 1. MyISAM还在使用的原因\n- 历史原因，需要逐步替换\n- 部分如User，DB等系统表(MyISAM引擎)，可以直接拷贝，比较方便\n- 性能好，或者存储小`不是`MyISAM的优点，也不是存在的原因\n\n### 2. MyISAM文件组成\n- `frm` 表结构文件\n- `MYI` 索引文件\n- `MYD` 数据文件\n    - 数据文件是堆表数据结构，堆是无序数据的集合\n    - `MYI`中的叶子节点，指向`MYD`中的数据页\n    - 当数据移动到页外时，需要修改对应指针\n\n### 3. myisamchk\n**`myisamchk`通过扫描MYD文件来重建MYI文件；如果MYD文件中某条记录有问题，将跳过该记录**\n\n-----\n\n## 二. Memory存储引擎\n### 1. Memory介绍\n* 全内存存储的引擎\n* 数据库重启后数据丢失\n* 支持哈希索引\n* 不支持事物\n\n###2. Memory特性\n* **`千万不要用Memory存储引擎去做缓存(Cache)`, 性能上不及Redis和Memcahced**\n* Memory`不能禁用`，当涉及内部排序操作的临时表时，使用该存储引擎\n    - `max_heap_table_size`决定使用内存的大小，默认时`16M`\n        - 无论该表使用的什么引擎，只要使用到临时表，或者指定Memory，都受参数影响\n    - 当上面设置的内存放不下数据时，(>=5.6)转为MyISAM,(>=5.7)转为InnoDB\n        - 注意磁盘上临时路径空间的大小(`tmpdir`)\n    - 内存使用为会话(SESSION)级别，当心内核OOM\n* 支持哈希索引，且仅支持等值查询\n\n```sql\nmysql> show global status like \"%tmp%tables\";\n+-------------------------+-------+\n| Variable_name           | Value |\n+-------------------------+-------+\n| Created_tmp_disk_tables | 0     |   -- 内存放不下，转成磁盘存储的数量,如果过大，考虑增大内存参数\n| Created_tmp_tables      | 4     |   -- 创建临时表的数量\n+-------------------------+-------+\n2 rows in set (0.00 sec)\n\nmysql> show variables like 'tmpdir';\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| tmpdir        | /tmp  |  -- memory转成磁盘存储的路径\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show create table User\\G\n*************************** 1. row ***************************\nTable: User\nCreate Table: CREATE TABLE `User` (\n`id` int(11) NOT NULL,\n`name` varchar(128) DEFAULT NULL,\nPRIMARY KEY (`id`),\nKEY `name` (`name`) USING HASH  -- 对这个字段使用USING HASH,创建hash索引\n) ENGINE=MEMORY DEFAULT CHARSET=latin1\n1 row in set (0.00 sec)\n```\n\n### 3. Memory的物理特性\n* 内存不会一次性分配最大空间，而是随着使用逐步增到到最大值\n* 通过链表管理空闲空间\n* 使用固定长度存储数据\n* 不支持BLOB和TEXT类型\n* 可以创建自增主键\n\n----\n\n## 三. CSV存储引擎\n### 1. CSV介绍\n* CSV - Comma-Separated Values，使用逗号分隔\n* 不支持特殊字符\n* CSV是一种标准文件格式\n* 文件以纯文本形式存储表格数据\n* 使用广泛\n\n### 2. CSV文件组成\n* `frm` 表结构\n* `CSV` 数据文件\n* `CSM` 元数据信息\n    \n\n### 2. CSV特性\n* MySQL CSV存储引擎运行时，即创建CSV`文件`\n* 通过MySQL标准接口来查看和修改CSV文件\n* 无需将CSV文件导入到数据库，只需创建相同字段的表结构，拷贝CSV文件即可\n* CSV存储引擎表每个字段`必须是NOT NULL`属性\n\n----\n\n## 四.Federated存储引擎\n### 1. Federated介绍\n* 允许本地访问远程MySQL数据库中表的数据\n* 本地不存储任何数据文件\n* 类似Oracle中的DBLink\n* Federated存储引擎默认不开启, 需要在`my.cnf`的`[mysqld]`标签下添加 `federated`\n* MySQL的Federated不支持异构数据库访问，MariaDB中的`FederatedX`支持\n\n### 2. Federated 语法\n`scheme://user_name[:password]@host_name[:port_num]/db_name/tbl_name`\n\n`CONNECTION='mysql://username:password@hostname:port/database/tablename'`\n\n```sql\n--\n-- 例子\n--\nCREATE TABLE `T1` (\n`A` VARCHAR(100),\nUNIQUE KEY (`A` (30))\n) ENGINE=FEDERATED\nCONNECTION='MYSQL://david:123@127.0.0.1:3306/TEST/T1';\n```\n\n-----\n\n## 五. 多实例安装\n### 1.  多实例介绍\n* 一台服务器上安装多个MySQL数据库实例\n* 可以充分利用服务器的硬件资源\n* 通过mysqld_multi进行管理\n\n### 2. 安装要求\n* MySQL实例1 - `mysql1`\n    - port = 3306\n    - datadir = /data1\n    - socket = /tmp/mysql.sock1\n\n* MySQL实例2 - `mysql2`\n    - port = 3307\n    - datadir = /data2\n    - socket = /tmp/mysql.sock2\n\n* MySQL实例3 - `mysql3`\n    - port = 3308\n    - datadir = /data3\n    - socket = /tmp/mysql.sock3\n\n* MySQL实例4 - `mysql4`\n    - port = 3309\n    - datadir = /data4\n    - socket = /tmp/mysql.sock4\n\n>**`该三个参数必须定制，且必须不同 (port / datadir / socket)`**\n`server-id`和多数据库实例没有关系，和数据库复制有关系。\n\n### 3. 安装操作\n```bash\n#\n# 多实例配置文件，可以mysqld_multi --example 查看例子\n#\n[root@MyServer /]> cat /etc/my.cnf \n#[client]           # 这个标签如果配置了用户和密码，\n                    # 并且[mysqld_multi]下没有配置用户名密码，\n                    # 则mysqld_multi stop时, 会使用这个密码\n                    # 如果没有精确的匹配，则匹配[client]标签\n#user = root        \n#password = 123\n#-------------\n[mysqld_multi]\nmysqld = /usr/local/mysql/bin/mysqld_safe\nmysqladmin = /usr/local/mysql/bin/mysqladmin\nuser = multi_admin\npass = 123  # 官方文档中写的password，但是存在bug，需要改成pass(v5.7.9)\n            # 写成password，start时正常，stop时，报如下错误\n            # Access denied for user 'multi_admin'@'localhost' (using password: YES)\nlog = /var/log/mysqld_multi.log\n\n\n[mysqld1]  # mysqld后面的数字为GNR, 是该实例的标识\n           # mysqld_multi  start 1,  mysqld_multi start 2-4\nserver-id = 11\nsocket = /tmp/mysql.sock1\nport = 3306\nbind_address = 0.0.0.0\ndatadir = /data1\nuser = mysql\nperformance_schema = off\ninnodb_buffer_pool_size = 32M\nskip_name_resolve = 1\nlog_error = error.log\npid-file = /data1/mysql.pid1\n\n\n[mysqld2]\nserver-id = 12\nsocket = /tmp/mysql.sock2\nport = 3307\nbind_address = 0.0.0.0\ndatadir = /data2\nuser = mysql\nperformance_schema = off\ninnodb_buffer_pool_size = 32M\nskip_name_resolve = 1\nlog_error = error.log\npid-file = /data2/mysql.pid2\n\n\n[mysqld3]\nserver-id = 13\nsocket = /tmp/mysql.sock3\nport = 3308\nbind_address = 0.0.0.0\ndatadir = /data3\nuser = mysql\nperformance_schema = off\ninnodb_buffer_pool_size = 32M\nskip_name_resolve = 1\nlog_error = error.log\npid-file = /data3/mysql.pid3\n\n\n[mysqld4]\nserver-id = 14\nsocket = /tmp/mysql.sock4\nport = 3309\nbind_address = 0.0.0.0\ndatadir = /data4\nuser = mysql\nperformance_schema = off\ninnodb_buffer_pool_size = 32M\nskip_name_resolve = 1\nlog_error = error.log\npid-file = /data4/mysql.pid4\n```\n\n```bash\n#\n# 准备好数据目录，并初始化安装\n#\n[root@MyServer ~]> mkdir /data1\n[root@MyServer ~]> mkdir /data2\n[root@MyServer ~]> mkdir /data3\n[root@MyServer ~]> mkdir /data4\n[root@MyServer ~]> chown mysql.mysql /data{1..4}\n[root@MyServer ~]> mysqld --initialize --user=mysql --datadir=/data1\n#\n# 一些日志输出，并提示临时密码，下同\n#\n[root@MyServer ~]> mysqld --initialize --user=mysql --datadir=/data2\n[root@MyServer ~]> mysqld --initialize --user=mysql --datadir=/data3\n[root@MyServer ~]> mysqld --initialize --user=mysql --datadir=/data4\n# 安装后，需要检查error.log 确保没有错误出现\n[root@MyServer ~]> cp /usr/local/mysql/support-files/mysqld_multi.server  /etc/init.d/mysqld_multid \n# 拷贝启动脚本，方便自启\n[root@MyServer ~]> chkconfig mysqld_multid on\n```\n\n```bash\n[root@MyServer ~]> mysqld_multi  start\n[root@MyServer ~]> mysqld_multi  report\nReporting MySQL servers\nMySQL server from group: mysqld1 is running\nMySQL server from group: mysqld2 is running\nMySQL server from group: mysqld3 is running\nMySQL server from group: mysqld4 is running\n[root@MyServer ~]> netstat -tunlp | grep mysql\n[root@MyServer ~]> netstat -tunlp | grep mysql\ntcp        0      0 :::3307                     :::*                        LISTEN      6221/mysqld         \ntcp        0      0 :::3308                     :::*                        LISTEN      6232/mysqld         \ntcp        0      0 :::3309                     :::*                        LISTEN      6238/mysqld         \ntcp        0      0 :::3306                     :::*                        LISTEN      6201/mysqld         \n\n[root@MyServer ~]> mysql -u root -S /tmp/mysql.sock1 -p -P3306\n#\n# 使用-S /tmp/mysql.sock1 进行登录，并输入临时密码后，修改密码，下同\n#\n[root@MyServer ~]> mysql -u root -S /tmp/mysql.sock2 -p -P3307\n[root@MyServer ~]> mysql -u root -S /tmp/mysql.sock3 -p -P3308\n[root@MyServer ~]> mysql -u root -S /tmp/mysql.sock4 -p -P3309\n```\n\n```sql\n--\n-- mysql1\n--\nmysql> show variables like \"port\"; \n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3306  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"socket\";\n+---------------+------------------+\n| Variable_name | Value            |\n+---------------+------------------+\n| socket        | /tmp/mysql.sock1 |\n+---------------+------------------+\n1 row in set (0.01 sec)\n\nmysql> show variables like \"datadir\";\n+---------------+---------+\n| Variable_name | Value   |\n+---------------+---------+\n| datadir       | /data1/ |\n+---------------+---------+\n1 row in set (0.00 sec)\n\n--\n-- 这样才能进行关闭数据库的操作\n-- 和[mysqld_multi]中的user，pass(注意在5.7.9中不是password)对应起来 （类比[client]标签）\n-- 一会测试federated链接，需要增加federated参数，并重启mysql2\n--\nmysql> create user 'multi_admin'@'localhost' identified by '123';\nQuery OK, 0 rows affected (0.00 sec)\nmysql> grant shutdown on *.* to 'multi_admin'@'localhost';\n\n--\n-- mysql2, mysql3, mysql4 类似。可以看到与my.cnf中对应的port和socket\n--\n```\n\n## 六. Federated测试\n```sql\n--\n-- mysql1 准备数据\n--\nmysql> show variables like \"port\"; \n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3306  |   -- mysql1 实例端口\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> create database burn;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> use burn;\nDatabase changed\n\nmysql> create table book (\n    -> id int not null auto_increment,\n    -> name varchar(128) not null,\n    -> primary key(id)\n    -> );\nQuery OK, 0 rows affected (0.20 sec)\n\nmysql> insert into book values(1, \"book1\");\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from book;\n+----+-------+\n| id | name  |\n+----+-------+\n|  1 | book1 |\n+----+-------+\n1 row in set (0.00 sec)\n\nmysql> create user 'burn'@'127.0.0.1' identified by '123';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> grant select on burn.* to 'burn'@'127.0.0.1';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> show grants for 'burn'@'127.0.0.1';\n+------------------------------------------------+\n| Grants for burn@127.0.0.1                      |\n+------------------------------------------------+\n| GRANT USAGE ON *.* TO 'burn'@'127.0.0.1'       |\n| GRANT SELECT ON `burn`.* TO 'burn'@'127.0.0.1' |\n+------------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n```sql\n--\n-- mysql2 测试Federated\n--\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |  -- msyql2 实例端口\n+---------------+-------+\n1 row in set (0.01 sec)\n\nmysql> show engines;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |\n| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |\n| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |\n| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |\n| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |\n| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |\n| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables      | NO           | NO   | NO         |\n| FEDERATED          | NO      | Federated MySQL storage engine                                 | NULL         | NULL | NULL       |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n--\n-- federated 引擎没有打开\n--\n9 rows in set (0.00 sec)\n\n\n```\n\n```bash\n#\n# 在[mysqld2]标签下面增加federated\n#\n\n[root@MyServer ~]> cat /etc/my.cnf\n# ... 省略 ...\n[mysqld2]\nfederated # 新增的配置项，表示打开federated引擎\n# ... 省略 ...\n[root@MyServer ~]> mysqld_multi stop 2\n[root@MyServer ~]> mysqld_multi start 2   # 重启配置\n```\n\n```sql\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |  -- msyql2 实例端口\n+---------------+-------+\n1 row in set (0.01 sec)\n\nmysql> show engines;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |\n| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |\n| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |\n| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |\n| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |\n| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |\n| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables      | NO           | NO   | NO         |\n| FEDERATED          | YES     | Federated MySQL storage engine                                 | NO           | NO   | NO         |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n9 rows in set (0.00 sec)\n--\n-- 显示 federated 已经启用\n--\nmysql> create database federated_test;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> use federated_test;\nDatabase changed\n\nmysql> create table federated_table_1 (\n    -> id int not null auto_increment,\n    -> name varchar(128) not null,\n    -> primary key(id)\n    -> ) engine=federated\n    -> connection='mysql://burn:123@127.0.0.1:3306/burn/book';\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql> select * from federated_table_1;\n+----+-------+\n| id | name  |\n+----+-------+\n|  1 | book1 |  -- 和 mysqld1 上的内容一致。\n+----+-------+\n1 row in set (0.00 sec)\n--\n-- 由于只有select权限，无法对该表进行insert操作\n--\nmysql> insert into  federated_table_1 values(2, \"book2\");\nERROR 1296 (HY000): Got error 10000 'Error on remote system: 1142: INSERT command denied to user 'burn'@'127.0.0.1' for table 'book'' from FEDERATED\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL JSON格式","url":"/2019-10-01/mysql/MySQL学习笔记(Day009：JSON)/","content":"\nMySQL学习笔记（Day009：JSON）\n================================\n@(MySQL学习)\n\n\n[TOC]\n\n## 一. MySQL JSON类型\n### 1. JSON介绍\n* **JSON**（`J`ava`S`cript `O`bject `N`otation）是一种轻量级的数据交换语言，并且是独立于语言的文本格式。\n* 一些`NoSQL数据库`选择`JSON`作为其数据存储格式，比如：MongoDB、CouchDB等。\n* MySQL`5.7.x`开始支持**JSON**数据类型。\n\n>[官方文档(JSON类型)](http://dev.mysql.com/doc/refman/5.7/en/json.html)\n\n### 2. JSON格式示例\n```sql\n--\n-- 摘自 维基百科\n--\n\n{\n     \"firstName\": \"John\",    -- Key : Value 格式\n     \"lastName\": \"Smith\",\n     \"sex\": \"male\",\n     \"age\": 25,\n     \"address\":              -- Key : Value ; 其中 Value 也是一个 Key-Value 的结构\n     {\n         \"streetAddress\": \"21 2nd Street\",\n         \"city\": \"New York\",\n         \"state\": \"NY\",\n         \"postalCode\": \"10021\"\n     },\n     \"phoneNumber\": \n     [\n         {\n           \"type\": \"home\",\n           \"number\": \"212 555-1234\"\n         },\n         {\n           \"type\": \"fax\",\n           \"number\": \"646 555-4567\"\n         }\n     ]\n }\n```\n\n### 3. JSON VS BLOB\n* **JSON**\n    - JSON数据可以做有效性检查;\n    - JSON使得查询性能提升;\n    - JSON支持部分属性索引，通过虚拟列的功能可以对JSON中的部分数据进行索引;\n    \n* **BLOB**\n    - BLOB类型无法在数据库层做约束性检查;\n    - BLOB进行查询，需要遍历所有字符串;\n    - BLOB做只能做指定长度的索引;\n\n>5.7之前，只能把JSON当作BLOB进行存储。数据库层面无法对JSON数据做一些操作，只能由应用程序处理。\n\n### 4.结构化和非结构化\n* **结构化**\n    - 二维表结构（行和列）\n    - 使用SQL语句进行操作\n\n* **非结构化**\n    - 使用Key-Value格式定义数据，无结构定义\n    - Value可以嵌套Key-Value格式的数据\n    - 使用JSON进行实现\n\n\n```sql\n--\n-- SQL创建User表\n--\ncreate table user (\n    id bigint not null auto_increment,\n    user_name varchar(10),\n    age int,\n    primary key(id)\n);\n```\n\n\n```python\n#\n# JSON定义的User表\n#\n\ndb.user.insert({\n    user_name:\"tom\",\n    age:30\n})\n\ndb.createCollection(\"user\")\n```\n\n\n### 5. JSON操作示例\n#### 5.1 JSON入门\n```sql\n--\n-- 创建带json字段的表\n--\nmysql> create table user (\n    -> uid int auto_increment,\n    -> data json,\n    -> primary key(uid)\n    -> );\nQuery OK, 0 rows affected (0.11 sec)\n\n--\n-- 插入json数据\n--\nmysql> insert into user values (\n    -> null,  -- 自增长数据，可以插入null\n    -> '{\n    '> \"name\":\"tom\",\n    '> \"age\":18,\n    '> \"address\":\"SZ\"\n    '> }'\n    -> );\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into user values (\n    -> null,\n    -> '{\n    '> \"name\":\"jim\",\n    '> \"age\":28,\n    '> \"mail\":\"jim@163.com\"\n    '> }'\n    -> );\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into user values ( null, \"can you insert it?\");  -- 无法插入，因为是JSON类型\nERROR 3140 (22032): Invalid JSON text: \"Invalid value.\" at position 0 in value (or column) can you insert it?.  -- 这短话有单引号，但是渲染有问题，所以这里去掉了\n\nmysql> select * from user;\n+-----+---------------------------------------------------+\n| uid | data                                              |\n+-----+---------------------------------------------------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": \"SZ\"}       |  -- 这个json中有address字段\n|   2 | {\"age\": 28, \"mail\": \"jim@163.com\", \"name\": \"jim\"} |  -- 这个json中有mail字段\n+-----+---------------------------------------------------+\n2 rows in set (0.00 sec)\n\n```\n\n#### 5.2 JSON常用函数介绍\n```sql\n--\n-- 使用json_extract提取数据\n-- 原型 : JSON_EXTRACT(json_doc, path[, path] ...) \n--\nmysql> select json_extract('[10, 20, [30, 40]]', '$[1]');                  \n+--------------------------------------------+\n| json_extract('[10, 20, [30, 40]]', '$[1]') |\n+--------------------------------------------+\n| 20                                         |  -- 从list中抽取 下标 为1的元素（下标从0开始）\n+--------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> select\n    -> json_extract(data, '$.name'),    -- 提起name字段的数据\n    -> json_extract(data, '$.address')  -- 提取address字段的数据\n    -> from user;\n+------------------------------+---------------------------------+\n| json_extract(data, '$.name') | json_extract(data, '$.address') |\n+------------------------------+---------------------------------+\n| \"tom\"                        | \"SZ\"                            |\n| \"jim\"                        | NULL                            |  -- jim 没有address字段，填充了NULL\n+------------------------------+---------------------------------+\n2 rows in set (0.00 sec)\n\n--\n-- json_object 将list(K-V对)封装成json格式\n-- 原型 : JSON_OBJECT([key, val[, key, val] ...])\n--\nmysql> select json_object(\"name\", \"jery\", \"email\", \"jery@163.com\", \"age\",33);\n+----------------------------------------------------------------+\n| json_object(\"name\", \"jery\", \"email\", \"jery@163.com\", \"age\",33) |\n+----------------------------------------------------------------+\n| {\"age\": 33, \"name\": \"jery\", \"email\": \"jery@163.com\"}           |  -- 封装成了K-V对\n+----------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> insert into user values ( \n    -> null,\n    -> json_object(\"name\", \"jery\", \"email\", \"jery@163.com\", \"age\",33)  -- 进行封装\n    -> );\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from user;\n+-----+------------------------------------------------------+\n| uid | data                                                 |\n+-----+------------------------------------------------------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": \"SZ\"}          |\n|   2 | {\"age\": 28, \"mail\": \"jim@163.com\", \"name\": \"jim\"}    |\n|   4 | {\"age\": 33, \"name\": \"jery\", \"email\": \"jery@163.com\"} |\n+-----+------------------------------------------------------+\n3 rows in set (0.00 sec)\n\n\n--\n-- json_insert 插入数据\n-- 原型 : JSON_INSERT(json_doc, path, val[, path, val] ...)\n--\nmysql> set @j = '{ \"a\": 1, \"b\": [2, 3]}';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select json_insert(@j, '$.a', 10, '$.c', '[true, false]');                 \n+----------------------------------------------------+\n| json_insert(@j, '$.a', 10, '$.c', '[true, false]') |\n+----------------------------------------------------+\n| {\"a\": 1, \"b\": [2, 3], \"c\": \"[true, false]\"}        |  -- a还是=1，存在的被忽略，不受影响\n+----------------------------------------------------+  -- c之前不存在，则插入\n1 row in set (0.00 sec)\n\nmysql> update user set data = json_insert(data, \"$.address_2\", \"BJ\") where uid = 1;  -- 插入 addres_2\nQuery OK, 1 row affected (0.03 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from user;\n+-----+----------------------------------------------------------------+\n| uid | data                                                           |\n+-----+----------------------------------------------------------------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": \"SZ\", \"address_2\": \"BJ\"} |  -- 增加了addres_2 : \"BJ\"\n|   2 | {\"age\": 28, \"mail\": \"jim@163.com\", \"name\": \"jim\"}              |\n|   4 | {\"age\": 33, \"name\": \"jery\", \"email\": \"jery@163.com\"}           |\n+-----+----------------------------------------------------------------+\n3 rows in set (0.00 sec)\n\n--\n-- json_merge 合并数据并返回。注意：原数据不受影响\n-- 原型 : JSON_MERGE(json_doc, json_doc[, json_doc] ...)\n--\nmysql> select json_merge('{\"name\": \"x\"}', '{\"id\": 47}');    -- 原来有两个JSON             \n+-------------------------------------------+\n| json_merge('{\"name\": \"x\"}', '{\"id\": 47}') |\n+-------------------------------------------+\n| {\"id\": 47, \"name\": \"x\"}                   |  -- 合并多个JSON\n+-------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> select \n    -> json_merge(\n    -> json_extract(data, '$.address'),      -- json 1\n    -> json_extract(data, '$.address_2'))    -- jons 2\n    -> from user where uid = 1;\n+---------------------------------------------------------------------------------+\n| json_merge( json_extract(data, '$.address'), json_extract(data, '$.address_2')) |\n+---------------------------------------------------------------------------------+\n| [\"SZ\", \"BJ\"]                                                                    |  -- 合并成一个json\n+---------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\n--\n-- json_array_append 追加数据\n-- 原型 : JSON_ARRAY_APPEND(json_doc, path, val[, path, val] ...) \n-- json_append 在5.7.9 中重命名为 json_array_append\n--\nmysql> set @j = '[\"a\", [\"b\", \"c\"], \"d\"]';     -- 下标为1的元素中只有[\"b\", \"c\"]\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select json_array_append(@j, '$[1]', 1);                       \n+----------------------------------+\n| json_array_append(@j, '$[1]', 1) |\n+----------------------------------+\n| [\"a\", [\"b\", \"c\", 1], \"d\"]        |    --  现在插入了 数字 1\n+----------------------------------+\n1 row in set (0.00 sec)\nmysql> update user set data = json_array_append(\n    -> data,\n    -> '$.address',\n    -> json_extract(data, '$.address_2'))\n    -> where uid = 1;\nQuery OK, 1 row affected (0.02 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from user;\n+-----+------------------------------------------------------------------------+\n| uid | data                                                                   |\n+-----+------------------------------------------------------------------------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": [\"SZ\", \"BJ\"], \"address_2\": \"BJ\"} | --address_2追加到address\n|   2 | {\"age\": 28, \"mail\": \"jim@163.com\", \"name\": \"jim\"}                      |\n|   4 | {\"age\": 33, \"name\": \"jery\", \"email\": \"jery@163.com\"}                   |\n+-----+------------------------------------------------------------------------+\n3 rows in set (0.00 sec)\n\n\n--\n-- json_remove 从json记录中删除数据\n-- 原型 : JSON_REMOVE(json_doc, path[, path] ...)\n--\nmysql> set @j = '[\"a\", [\"b\", \"c\"], \"d\"]';   \nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select json_remove(@j, '$[1]');\n+-------------------------+\n| json_remove(@j, '$[1]') |\n+-------------------------+\n| [\"a\", \"d\"]              |  -- 删除了下标为1的元素[\"b\", \"c\"]\n+-------------------------+\n1 row in set (0.00 sec)\n\nmysql> update user set data = json_remove(data, \"$.address_2\") where uid = 1;\nQuery OK, 1 row affected (0.03 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from user;\n+-----+------------------------------------------------------+\n| uid | data                                                 |\n+-----+------------------------------------------------------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": [\"SZ\", \"BJ\"]}  |  -- address_2 的字段删除了\n|   2 | {\"age\": 28, \"mail\": \"jim@163.com\", \"name\": \"jim\"}    |\n|   4 | {\"age\": 33, \"name\": \"jery\", \"email\": \"jery@163.com\"} |\n+-----+------------------------------------------------------+\n3 rows in set (0.00 sec)\n```\n>[官方文档(JSON函数)](http://dev.mysql.com/doc/refman/5.7/en/json-function-reference.html)\n\n\n#### 5.3 JSON创建索引\n`JSON`类型数据本身`无法直接`创建索引，需要将需要索引的`JSON数据`重新`生成虚拟列(Virtual Columns)`之后，对`该列`进行`索引`\n>[官方文档--JSON创建索引](https://dev.mysql.com/doc/refman/5.7/en/create-table.html#create-table-secondary-indexes-virtual-columns)\n\n* **新建表时创建JSON索引**\n```sql\nmysql> create table test_inex_1(\n    -> data json,\n    -> gen_col varchar(10) generated always as (json_extract(data, '$.name')),  -- 抽取data中的name， 生成新的一列，名字为gen_col\n    -> index idx (gen_col)  -- 将gen_col 作为索引\n    -> );\nQuery OK, 0 rows affected (0.13 sec)\n\nmysql> show create table test_index_1；\n-- -----省略表格线-----\n| test_index_1 | CREATE TABLE `test_index_1` (\n  `data` json DEFAULT NULL,\n  `gen_col` varchar(10) GENERATED ALWAYS AS (json_extract(data, '$.name')) VIRTUAL,\n  KEY `idx` (`gen_col`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n-- -----省略表格线-----\n1 row in set (0.00 sec)\n\nmysql> insert into test_index_1(data) values ('{\"name\":\"tom\", \"age\":18, \"address\":\"SH\"}');\nQuery OK, 1 row affected (0.04 sec)\n\nmysql> insert into test_index_1(data) values ('{\"name\":\"jim\", \"age\":28, \"address\":\"SZ\"}');      \nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_index_1;\n+---------------------------------------------+---------+\n| data                                        | gen_col |\n+---------------------------------------------+---------+\n| {\"age\": 18, \"name\": \"tom\", \"address\": \"SH\"} | \"tom\"   |\n| {\"age\": 28, \"name\": \"jim\", \"address\": \"SZ\"} | \"jim\"   |\n+---------------------------------------------+---------+\n2 rows in set (0.00 sec)\n\nmysql> select json_extract(data,\"$.name\") as username from test_index_1 where gen_col=\"tom\";  -- 如果这样做，为空，原因如下\nEmpty set (0.00 sec)\n\nmysql> select hex('\"');\n+----------+\n| hex('\"') |\n+----------+\n| 22       |  -- 双引号的 16进制\n+----------+\n1 row in set (0.00 sec)\n\nmysql> select hex(gen_col) from test_index_1;\n+--------------+\n| hex(gen_col) |\n+--------------+\n| 226A696D22   |  -- 双引号本身也作为了存储内容\n| 22746F6D22   |\n+--------------+\n2 rows in set (0.00 sec)\n\nmysql> select json_extract(data,\"$.name\") as username from test_index_1 where gen_col='\"tom\"';  -- 使用'\"tome\"',用单引号括起来\n+----------+\n| username |\n+----------+\n| \"tom\"    |  -- 找到了对应的数据\n+----------+\n1 row in set (0.00 sec)\n\nmysql> explain select json_extract(data,\"$.name\") as username from test_index_1 where gen_col='\"tom\"'\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_1\n   partitions: NULL\n         type: ref \npossible_keys: idx    -- 使用了 key idx\n          key: idx\n      key_len: 43\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\n---\n--- 建立表的时候去掉双引用\n---\n\nmysql> create table test_index_2 (\n    -> data json,\n    -> gen_col varchar(10) generated always as (\n    ->    json_unquote(    -- 使用json_unquote函数进行去掉双引号\n    ->             json_extract(data, \"$.name\")\n    ->    )),\n    -> key idx(gen_col)\n    -> );\nQuery OK, 0 rows affected (0.13 sec)\n\nmysql> show create table test_index_2;\n-- -----省略表格线-----\n| test_index_2 | CREATE TABLE `test_index_2` (\n  `data` json DEFAULT NULL,\n  `gen_col` varchar(10) GENERATED ALWAYS AS (json_unquote(\n            json_extract(data, \"$.name\")\n   )) VIRTUAL,\n  KEY `idx` (`gen_col`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n-- -----省略表格线-----\n1 row in set (0.00 sec)\n\nmysql> insert into test_index_2(data) values ('{\"name\":\"tom\", \"age\":18, \"address\":\"SH\"}');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_index_2(data) values ('{\"name\":\"jim\", \"age\":28, \"address\":\"SZ\"}');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select json_extract(data,\"$.name\") as username from test_index_2 where gen_col=\"tom\";  -- 未加单引号\n+----------+\n| username |\n+----------+\n| \"tom\"    |  -- 可以找到数据\n+----------+\n1 row in set (0.00 sec)\n\nmysql> explain select json_extract(data,\"$.name\") as username from test_index_2 where gen_col=\"tom\"\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: test_index_2\n   partitions: NULL\n         type: ref\npossible_keys: idx   -- 使用了 key idx\n          key: idx\n      key_len: 43\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\n```\n* **修改已存在的表创建JSON索引**\n```sql\n--\n-- 使用之前的user表操作\n--\nmysql> show create table user;\n-- -----省略表格线-----\n| user  | CREATE TABLE `user` (\n  `uid` int(11) NOT NULL AUTO_INCREMENT,\n  `data` json DEFAULT NULL,\n  PRIMARY KEY (`uid`)\n) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4 |\n-- -----省略表格线-----\n1 row in set (0.00 sec)\n\nmysql> select * from user;\n+-----+------------------------------------------------------+\n| uid | data                                                 |\n+-----+------------------------------------------------------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": [\"SZ\", \"BJ\"]}  |\n|   2 | {\"age\": 28, \"mail\": \"jim@163.com\", \"name\": \"jim\"}    |\n|   4 | {\"age\": 33, \"name\": \"jery\", \"email\": \"jery@163.com\"} |\n+-----+------------------------------------------------------+\n\nmysql> alter table user \n    -> add user_name varchar(32)\n    -> generated always as (json_extract(data,\"$.name\")) virtual;\nQuery OK, 0 rows affected (0.05 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n-- virtual 关键字是不将该列的字段值存储，对应的是stored\n\nmysql> select user_name from user;  \n+-----------+\n| user_name |\n+-----------+\n| \"tom\"     |\n| \"jim\"     |\n| \"jery\"    |\n+-----------+\n3 rows in set (0.00 sec)\n\nmysql> alter table user add index idx(user_name);          \nQuery OK, 0 rows affected (0.13 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> select * from user where user_name='\"tom\"';  -- 加单引号\n+-----+-----------------------------------------------------+-----------+\n| uid | data                                                | user_name |\n+-----+-----------------------------------------------------+-----------+\n|   1 | {\"age\": 18, \"name\": \"tom\", \"address\": [\"SZ\", \"BJ\"]} | \"tom\"     |\n+-----+-----------------------------------------------------+-----------+\n1 row in set (0.00 sec)\n\nmysql> explain select * from user where user_name='\"tom\"'\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: user\n   partitions: NULL\n         type: ref\npossible_keys: idx   -- 使用了 key idx\n          key: idx\n      key_len: 131\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\nmysql> show create table user;\n-- -----省略表格线-----\n| user  | CREATE TABLE `user` (\n  `uid` int(11) NOT NULL AUTO_INCREMENT,\n  `data` json DEFAULT NULL,\n  `user_name` varchar(32) GENERATED ALWAYS AS (json_extract(data,\"$.name\")) VIRTUAL,\n  `user_name2` varchar(32) GENERATED ALWAYS AS (json_extract(data,\"$.name\")) VIRTUAL,\n  PRIMARY KEY (`uid`),\n  KEY `idx` (`user_name`)\n) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4 |\n-- -----省略表格线-----\n1 row in set (0.00 sec)\n```\n\n\n\n-----\n\n## 二. 附录\n```sql\n--\n-- 老师演示JSON的SQL\n--\ndrop table if exists User;\n\nCREATE TABLE User (\n    uid BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(32) NOT NULL,\n    email VARCHAR(256) NOT NULL,\n    address VARCHAR(512) NOT NULL,\n    UNIQUE KEY (name),\n    UNIQUE KEY (email)\n);\n\nINSERT INTO User VALUES (NULL,'David','david@gmail','Shanghai ...');\nINSERT INTO User VALUES (NULL,'Amy','amy@gmail','Beijing ...');\nINSERT INTO User VALUES (NULL,'Tom','tom@gmail','Guangzhou ...');\n\nSELECT * FROM User;\n\nALTER TABLE User ADD COLUMN address2 VARCHAR(512) NOT NULL;\nALTER TABLE User ADD COLUMN passport VARCHAR(64) NOT NULL;\n\nDROP TABLE IF EXISTS UserJson;\n\nCREATE TABLE UserJson(\n\tuid BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    data JSON\n);\n\ntruncate table UserJson;\n\ninsert into UserJson \nSELECT \n    uid,JSON_OBJECT('name',name,'email',email,'address',address) AS data\nFROM\n    User;\n    \nSELECT * FROM UserJson; \n\nSELECT uid,JSON_EXTRACT(data,'$.address2') from UserJson;\n    \nUPDATE UserJson\nset data = json_insert(data,\"$.address2\",\"HangZhou ...\")\nwhere uid = 1;\n\nSELECT JSON_EXTRACT(data,'$.address[1]') from UserJson;\n\nselect json_merge(JSON_EXTRACT(data,'$.address') ,JSON_EXTRACT(data,'$.address2')) \nfrom UserJson;\n\nbegin;\nUPDATE UserJson\nset data = json_array_append(data,\"$.address\",JSON_EXTRACT(data,'$.address2'))\nwhere JSON_EXTRACT(data,'$.address2') IS NOT NULL AND uid >0;\nselect JSON_EXTRACT(data,'$.address') from UserJson;\nUPDATE UserJson\nset data = JSON_REMOVE(data,'$.address2')\nwhere uid>0;\ncommit;\n```\n","tags":["mysql"],"categories":["mysql"]},{"title":"数据类型","url":"/2019-10-01/mysql/MySQL学习笔记(Day008)(含009前半部分)V2.0/","content":"\nMySQL学习笔记（Day008：数据类型）\n================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. INT类型\n### 1. INT类型的分类\n* **TINYINT**\n    - 存储空间 ： 1 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-128, 127]\n        - 无符号(unsigned) ：[0, 255]\n        \n* **SMALLINT**\n    - 存储空间 ： 2 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-32768, 32767]\n        - 无符号(unsigned) ：[0, 65535]\n        \n* **MEDIUMINT**\n    - 存储空间 ： 3 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-8388608, 8388607]\n        - 无符号(unsigned) ：[0, 16777215]\n        \n* **INT**\n    - 存储空间 ： 4 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-2147483648, 2147483647]\n        - 无符号(unsigned) ：[0, 4294967295]\n        \n* **BIGINT**\n    - 存储空间 ： 8 字节\n    - 取值范围 \n        - 有符号(signed) ：  [-9223372036854775808, 9223372036854775807]\n        - 无符号(unsigned) ：[0, 18446744073709551615]\n\n### 2. INT类型的使用\n* **自增长ID**\n    - `推荐`使用`BIGINT`，而不是INT；\n\n* **unsigned or signed**\n    - 根据实际情况使用，一般情况下推荐`默认`的`sigend`\n    - unsigned 的注意事项\n\n```sql\nmysql> create table test_unsigned(a int unsigned, b int unsigned);\nQuery OK, 0 rows affected (0.14 sec)\n\nmysql> insert into test_unsigned values(1, 2);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a - b from test_unsigned;\nERROR 1690 (22003): BIGINT UNSIGNED value is out of range in '(`burn_test`.`test_unsigned`.`a` - `burn_test`.`test_unsigned`.`b`)'\n\nmysql> select b - a from test_unsigned;   \n+-------+\n| b - a |\n+-------+\n|     1 |\n+-------+\n1 row in set (0.00 sec)\n\nmysql> set sql_mode = 'no_unsigned_subtraction'; -- 这样就可以得到负数\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select a - b from test_unsigned;\n+-------+\n| a - b |\n+-------+\n|    -1 |\n+-------+\n1 row in set (0.00 sec) \n```\n\n>一般情况下使用`int`时，推荐有符号数`(signed)`， 使用无符号数只是比原来多一倍的取值，数量级上没有改变。\n\n>如果需要取值范围很大，直接选择用`BIGINT`\n\n    \n### 3. INT(N) \n\n```sql\nmysql> show create table  test_unsigned;\n+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table         | Create Table                                                                                                                                    |\n+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n| test_unsigned | CREATE TABLE `test_unsigned` (\n  `a` int(10) unsigned DEFAULT NULL, \n  `b` int(10) unsigned DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n* **int(N) 和 zerofill**\n    - int(**N**)中的`N`是显示宽度，`不表示`存储的数字的`长度`的上限。\n    - `zerofill`表示当存储的数字`长度 < N`时，用`数字0`填充左边，直至补满长度`N`\n    - 当存储数字的长度`超过N时`，按照`实际存储`的数字显示\n    \n```sql\nmysql> create  table  test_int_n(a int(3) zerofill);  -- 显示宽度N=3\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> insert into test_int_n values(1);\nQuery OK, 1 row affected (0.04 sec)\n\nmysql> select * from test_int_n;\n+------+\n| a    |\n+------+\n|  001 |   -- 不满 N=3时，左边用0填充\n+------+\n1 row in set (0.00 sec)\n\nmysql> insert into test_int_n values(1111);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_int_n;\n+------+\n| a    |\n+------+\n|  001 |\n| 1111 |  -- 超过N=3的长度时，是什么数字，显示什么数字\n+------+\n2 rows in set (0.00 sec)\n\nmysql> select a, HEX(a) from test_int_n\\G\n*************************** 1. row ***************************\n     a: 001\nHEX(a): 1    -- 实际存储的还是1\n*************************** 2. row ***************************\n     a: 1111\nHEX(a): 457  -- 1111对应的16进制就是457\n2 rows in set (0.00 sec)\n```\n\n> int(N)中的`N`和`zerofill`配合才有意义，且仅仅是显示的时候才有意义，和实际存储没有关系，不会去截取数字的长度。\n\n### 4. AUTO_INCREMENT\n* 自增\n* 每张表一个\n* 必须是索引的一部分\n\n```sql\nmysql> create table test_auto_increment(a int auto_increment);\nERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key\n-- 没有指定为key，报错了\n\nmysql> create table test_auto_increment(a int auto_increment primary key);  -- 指定为key后有效\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> insert into test_auto_increment values(NULL);  -- 插入NULL值\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_auto_increment;\n+---+\n| a |\n+---+\n| 1 |  -- 插入NULL值，便可以让其自增，且默认从1开始\n+---+\n1 row in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(0);  -- 插入 0\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_auto_increment;\n+---+\n| a |\n+---+\n| 1 |\n| 2 |  -- 插入 0 ，自增长为2\n+---+\n2 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(-1);  -- 插入 -1\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_auto_increment;\n+----+\n| a  |\n+----+\n| -1 |   -- 刚刚插入的-1\n|  1 |\n|  2 |\n+----+\n3 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(NULL);  -- 继续插入NULL\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_auto_increment;\n+----+\n| a  |\n+----+\n| -1 |\n|  1 |\n|  2 |\n|  3 |  -- 刚刚插入NULL， 自增为3\n+----+\n4 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values('0'); -- 插入字符0\nQuery OK, 1 row affected (0.04 sec)\n\nmysql> select * from test_auto_increment;\n+----+\n| a  |\n+----+\n| -1 |\n|  1 |\n|  2 |\n|  3 |\n|  4 |  -- 插入字符'0' 后， 自增长为4\n+----+\n5 rows in set (0.00 sec)\n\nmysql> update test_auto_increment set a = 0 where a = -1;  -- 更新为0\nQuery OK, 1 row affected (0.03 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from test_auto_increment;\n+---+\n| a |\n+---+\n| 0 |  -- 原来的 -1 更新为0\n| 1 |\n| 2 |\n| 3 |\n| 4 |\n+---+\n5 rows in set (0.00 sec)\n\n--\n--  数字 0 这个值比较特殊， 插入0和插入NULL的效果是一样的，都是代表自增\n--\n\n-----\n\nmysql> insert into test_auto_increment values(NULL), (100), (NULL); \nQuery OK, 3 rows affected (0.02 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_auto_increment;\n+-----+\n| a   |\n+-----+\n|   0 |\n|   1 |\n|   2 |\n|   3 |\n|   4 |\n|   5 | -- 第一个NULL\n| 100 | -- 100\n| 101 | -- 第二个NULL, 按当前最大的值 +１来设置，之前是100，所以这里101\n+-----+\n8 rows in set (0.00 sec)\n\nmysql> insert into test_auto_increment values(99); -- 插入99\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_auto_increment;\n+-----+\n| a   |\n+-----+\n|   0 |\n|   1 |\n|   2 |\n|   3 |\n|   4 |\n|   5 |\n|  99 |  -- 刚刚插入的 99\n| 100 |\n| 101 |\n+-----+\n9 rows in set (0.00 sec)\n```\n> `AUTO_INCREMENT`是实例启动时，取当前表的最大值，然后 +1 即为下次自增的值。`（MAX + 1)`\n\n> **TIPS:**\n`insert into tablename select NULL;` 等价与 `insert into tablename values (NULL);`\n\n-----\n\n## 二. 数字类型\n### 1. 数字类型的分类\n* 单精度类型：FLOAT\n    - 存储空间：4 字节\n    - 精确性：低\n    \n* 双精度类型：DOUBLE\n    - 占用空间：8 字节\n    - 精确性：低，比FLOAT高\n    \n* 高精度类型：DECIMAL\n    - 占用空间：变长\n    - 精确性：非常高\n\n**注意：财务系统`必须使用DECIMAL`**\n    \n-----    \n    \n## 三. 字符串类型\n\n### 1. 字符串类型介绍\n\n| 类型 | 说明 | N的含义 |  是否有字符集 | 最大长度 |\n|:------:|:------:|:---------:|:------------:|:---:|\n| `CHAR(N)` | 定长字符 | 字符 | 是 | 255 | \n| `VARCHAR(N)` | 变长字符 | 字符 | 是 | 16384 | \n| BINARY(N) | 定长二进制字节 | 字节 | 否 | 255 | \n| VARBINARY(N) | 变长二进制字节 | 字节 | 否 | 16384 | \n| TINYBLOB(N) | 二进制大对象 | 字节 | 否 | 256 | \n| BLOB(N) | 二进制大对象 | 字节 | 否 | 16K | \n| MEDIUMBLOB(N) | 二进制大对象 | 字节 | 否 | 16M | \n| LONGBLOB(N) | 二进制大对象 | 字节 | 否 | 4G | \n| TINYTEXT(N) | 大对象 | 字节 | 是 | 256 | \n| TEXT(N) | 大对象 | 字节 | 是 | 16K | \n| MEDIUMTEXT(N) | 大对象 | 字节 | 是 | 16M | \n| LONGTEXT(N) | 大对象 | 字节 | 是 | 4G | \n\n    \n### 2. N和字符集\n* **char(N)**\n    - 假设当前table的字符集的`最大长度`为`W`, 则`char(N)`的最大存储空间为 `(N X W)Byte`;假设使用`UTF-8`，则char(10)可以最小存储10个字节的字符，最大存储30个字节的字符，其实是另一种意义上的`varchar`\n    - 当存储的字符数`小于N`时，尾部使用`空格`填充，并且填充最小字节的空格\n    \n```sql\nmysql> create table test_char(a char(10));\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql> show create table test_char;\n+-----------+------------------------------------------------------------------------------------------------+\n| Table     | Create Table                                                                                   |\n+-----------+------------------------------------------------------------------------------------------------+\n| test_char | CREATE TABLE `test_char` (\n  `a` char(10) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n+-----------+------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> insert into test_char values('abc');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_char values('你好吗');\nQuery OK, 1 row affected (0.05 sec)\n\nmysql> insert into test_char values('大家好ab');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_char values('大家ab好');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_char values('大家ab好吗');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a, length(a) from test_char;\n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n+----------------+-----------+\n5 rows in set (0.00 sec)\n\nmysql> select a, hex(a) from test_char;\n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |    -- 注意这里，以及下面的16进制值，一会可以对比\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n+----------------+------------------------------+\n5 rows in set (0.00 sec)\n\nmysql> select hex(' ');\n+----------+\n| hex(' ') |\n+----------+\n| 20       |   -- 注意 空格 空格对应的16进制数字是 20\n+----------+\n1 row in set (0.00 sec)\n```\n\n**`test_char`表实际二进制存储文件**\n\n```sql\n-- \n-- shell> hexdump -C test_char.idb\n--\n\n-- 1:abc\n-- 2:你好吗\n-- 3:大家好ab\n-- 4:大家ab好\n-- 5:大家ab好吗\n\n-- ---省略---\n00006070  73 75 70 72 65 6d 75 6d  0a 00 00 00 10 00 24 00  |supremum......$.|\n00006080  00 00 00 02 03 00 00 00  00 1f 33 a8 00 00 00 26  |..........3....&|\n00006090  01 10 61 62 63 20 20 20  20 20 20 20 0a 00 00 00  |..abc       ....| -- 1:后面补了7个空格\n000060a0  18 00 24 00 00 00 00 02  04 00 00 00 00 1f 34 a9  |..$...........4.|\n000060b0  00 00 00 25 01 10 e4 bd  a0 e5 a5 bd e5 90 97 20  |...%........... | -- 2:补充了1个空格\n000060c0  0b 00 00 00 20 00 25 00  00 00 00 02 05 00 00 00  |.... .%.........|\n000060d0  00 1f 39 ac 00 00 00 26  01 10 e5 a4 a7 e5 ae b6  |..9....&........| -- 3:没有补充空格\n000060e0  e5 a5 bd 61 62 0b 00 00  00 28 00 25 00 00 00 00  |...ab....(.%....|  -- \n000060f0  02 06 00 00 00 00 1f 3a  ad 00 00 00 28 01 10 e5  |.......:....(...| --\n00006100  a4 a7 e5 ae b6 61 62 e5  a5 bd 0e 00 00 00 30 ff  |.....ab.......0.|  -- 4：没有补充空格\n00006110  5f 00 00 00 00 02 07 00  00 00 00 1f 3f b0 00 00  |_...........?...|\n00006120  00 29 01 10 e5 a4 a7 e5  ae b6 61 62 e5 a5 bd e5  |.)........ab....|--\n00006130  90 97 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................| -- 5：没有补充空格\n00006140  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n-- ---省略---\n```\n\n* **varchar(N)**\n```sql\nmysql> create table test_varchar(a varchar(10));\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql> show create table test_varchar;\n+--------------+------------------------------------------------------------------------------------------------------+\n| Table        | Create Table                                                                                         |\n+--------------+------------------------------------------------------------------------------------------------------+\n| test_varchar | CREATE TABLE `test_varchar` (\n  `a` varchar(10) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |\n+--------------+------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> insert into test_varchar values('abc');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_varchar values('你好吗');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_varchar values('大家好ab');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_varchar values('大家ab好');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_varchar values('大家ab好吗');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a, hex(a) from test_varchar;\n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n+----------------+------------------------------+\n5 rows in set (0.00 sec)\n\nmysql> select a, length(a) from test_varchar;\n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n+----------------+-----------+\n5 rows in set (0.00 sec)\n```\n\n\n**`test_varchar`表实际二进制存储文件**\n\n```sql\n-- \n-- shell> hexdump -C test_char.idb\n--\n\n-- 1:abc\n-- 2:你好吗\n-- 3:大家好ab\n-- 4:大家ab好\n-- 5:大家ab好吗\n\n-- 和char一样观察，都没有进行空格的填充\n\n00006070  73 75 70 72 65 6d 75 6d  03 00 00 00 10 00 1d 00  |supremum........|\n00006080  00 00 00 02 08 00 00 00  00 1f 44 b5 00 00 00 29  |..........D....)|\n00006090  01 10 61 62 63 09 00 00  00 18 00 23 00 00 00 00  |..abc......#....| \n000060a0  02 09 00 00 00 00 1f 45  b6 00 00 00 2b 01 10 e4  |.......E....+...|\n000060b0  bd a0 e5 a5 bd e5 90 97  0b 00 00 00 20 00 25 00  |............ .%.|\n000060c0  00 00 00 02 0a 00 00 00  00 1f 4a b9 00 00 00 2c  |..........J....,|\n000060d0  01 10 e5 a4 a7 e5 ae b6  e5 a5 bd 61 62 0b 00 00  |...........ab...|\n000060e0  00 28 00 25 00 00 00 00  02 0b 00 00 00 00 1f 4b  |.(.%...........K|\n000060f0  ba 00 00 00 2c 01 10 e5  a4 a7 e5 ae b6 61 62 e5  |....,........ab.|\n00006100  a5 bd 0e 00 00 00 30 ff  67 00 00 00 00 02 0c 00  |......0.g.......|\n00006110  00 00 00 1f 50 bd 00 00  00 2d 01 10 e5 a4 a7 e5  |....P....-......|\n00006120  ae b6 61 62 e5 a5 bd e5  90 97 00 00 00 00 00 00  |..ab............|\n00006130  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n```\n\n* **插入数据尾部带空格**\n\n```sql\nmysql> insert into test_char values('好好好   ');  -- 后面有3个空格\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into test_varchar values('好好好   '); -- 后面有3个空格\nQuery OK, 1 row affected (0.02 sec)\n\n-- \n-- test_char 表\n--\nmysql> select a, length(a) from test_char;   \n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n| 好好好         |         9 |  -- 只有9个字节\n+----------------+-----------+\n6 rows in set (0.00 sec)\n\nmysql> select a, hex(a) from test_char;\n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n| 好好好         | E5A5BDE5A5BDE5A5BD           | -- 无填充空格\n+----------------+------------------------------+\n6 rows in set (0.00 sec)\n\n\n--\n-- test_varchar表\n--\nmysql> select a, length(a) from test_varchar;\n+----------------+-----------+\n| a              | length(a) |\n+----------------+-----------+\n| abc            |         3 |\n| 你好吗         |         9 |\n| 大家好ab       |        11 |\n| 大家ab好       |        11 |\n| 大家ab好吗     |        14 |\n| 好好好         |        12 |  -- (好好好)9个字节 +  3个字节的空格\n+----------------+-----------+\n7 rows in set (0.00 sec)\n\nmysql> select a, hex(a) from test_varchar;      \n+----------------+------------------------------+\n| a              | hex(a)                       |\n+----------------+------------------------------+\n| abc            | 616263                       |\n| 你好吗         | E4BDA0E5A5BDE59097           |\n| 大家好ab       | E5A4A7E5AEB6E5A5BD6162       |\n| 大家ab好       | E5A4A7E5AEB66162E5A5BD       |\n| 大家ab好吗     | E5A4A7E5AEB66162E5A5BDE59097 |\n| 好好好         | E5A5BDE5A5BDE5A5BD202020     |  -- 后面有20 20 20 ，表示3个自己的空格\n+----------------+------------------------------+\n7 rows in set (0.00 sec)\n```\n\n>上面的现象无法用统一的规则进行表述，但是[官方文档](http://dev.mysql.com/doc/refman/5.7/en/innodb-physical-record.html)给出的解释是，这样的安排是为了避免索引页的碎片 \n\n### 3.BLOB和TEXT\n* 在BLOB和TEXT上创建索引时，必须指定索引前缀的长度\n```sql\nmysql> create table test_text(a int primary key, b text, key(b));\nERROR 1170 (42000): BLOB/TEXT column 'b' used in key specification without a key length\n\nmysql> create table test_text(a int primary key, b text, key(b(64)));\nQuery OK, 0 rows affected (0.13 sec)\n```\n\n* BLOB和TEXT列不能有默认值\n* BLOB和TEXT列排序时只使用该列的前max_sort_length个字节\n```sql\nmysql> select @@max_sort_length;\n+-------------------+\n| @@max_sort_length |\n+-------------------+\n|              1024 |\n+-------------------+\n1 row in set (0.00 sec)\n```\n>不建议在MySQL中存储大型的二进制数据，比如歌曲，视频\n\n-----\n\n## 四. 字符集\n### 1. 常见的字符集\n* utf8\n* utf8mb4\n* gbk\n* gb18030\n\n```sql\nmysql> show character set;\n+----------+---------------------------------+---------------------+--------+\n| Charset  | Description                     | Default collation   | Maxlen |\n+----------+---------------------------------+---------------------+--------+\n| big5     | Big5 Traditional Chinese        | big5_chinese_ci     |      2 |\n| dec8     | DEC West European               | dec8_swedish_ci     |      1 |\n| cp850    | DOS West European               | cp850_general_ci    |      1 |\n| hp8      | HP West European                | hp8_english_ci      |      1 |\n| koi8r    | KOI8-R Relcom Russian           | koi8r_general_ci    |      1 |\n| latin1   | cp1252 West European            | latin1_swedish_ci   |      1 |\n| latin2   | ISO 8859-2 Central European     | latin2_general_ci   |      1 |\n| swe7     | 7bit Swedish                    | swe7_swedish_ci     |      1 |\n| ascii    | US ASCII                        | ascii_general_ci    |      1 |\n| ujis     | EUC-JP Japanese                 | ujis_japanese_ci    |      3 |\n| sjis     | Shift-JIS Japanese              | sjis_japanese_ci    |      2 |\n| hebrew   | ISO 8859-8 Hebrew               | hebrew_general_ci   |      1 |\n| tis620   | TIS620 Thai                     | tis620_thai_ci      |      1 |\n| euckr    | EUC-KR Korean                   | euckr_korean_ci     |      2 |\n| koi8u    | KOI8-U Ukrainian                | koi8u_general_ci    |      1 |\n| gb2312   | GB2312 Simplified Chinese       | gb2312_chinese_ci   |      2 |\n| greek    | ISO 8859-7 Greek                | greek_general_ci    |      1 |\n| cp1250   | Windows Central European        | cp1250_general_ci   |      1 |\n| gbk      | GBK Simplified Chinese          | gbk_chinese_ci      |      2 | -- gbk，表示的字符有限\n| latin5   | ISO 8859-9 Turkish              | latin5_turkish_ci   |      1 |\n| armscii8 | ARMSCII-8 Armenian              | armscii8_general_ci |      1 |\n| utf8     | UTF-8 Unicode                   | utf8_general_ci     |      3 | -- utf8，最长3字节\n| ucs2     | UCS-2 Unicode                   | ucs2_general_ci     |      2 |\n| cp866    | DOS Russian                     | cp866_general_ci    |      1 |\n| keybcs2  | DOS Kamenicky Czech-Slovak      | keybcs2_general_ci  |      1 |\n| macce    | Mac Central European            | macce_general_ci    |      1 |\n| macroman | Mac West European               | macroman_general_ci |      1 |\n| cp852    | DOS Central European            | cp852_general_ci    |      1 |\n| latin7   | ISO 8859-13 Baltic              | latin7_general_ci   |      1 |\n| utf8mb4  | UTF-8 Unicode                   | utf8mb4_general_ci  |      4 | -- utf8 + mobile端字符\n| cp1251   | Windows Cyrillic                | cp1251_general_ci   |      1 |\n| utf16    | UTF-16 Unicode                  | utf16_general_ci    |      4 |\n| utf16le  | UTF-16LE Unicode                | utf16le_general_ci  |      4 |\n| cp1256   | Windows Arabic                  | cp1256_general_ci   |      1 |\n| cp1257   | Windows Baltic                  | cp1257_general_ci   |      1 |\n| utf32    | UTF-32 Unicode                  | utf32_general_ci    |      4 |\n| binary   | Binary pseudo charset           | binary              |      1 |\n| geostd8  | GEOSTD8 Georgian                | geostd8_general_ci  |      1 |\n| cp932    | SJIS for Windows Japanese       | cp932_japanese_ci   |      2 |\n| eucjpms  | UJIS for Windows Japanese       | eucjpms_japanese_ci |      3 |\n| gb18030  | China National Standard GB18030 | gb18030_chinese_ci  |      4 | -- gb18030,最长4个字节\n+----------+---------------------------------+---------------------+--------+\n41 rows in set (0.00 sec)\n```\n\n### 2. collation\ncollation的含义是指排序规则，`ci（case insensitive）`结尾的排序集是不区分大小写的\n```sql\nmysql> select 'a' = 'A';\n+-----------+\n| 'a' = 'A' |\n+-----------+\n|         1 |  -- 因为大小写无关，所以返回1\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> create table test_ci (a varchar(10), key(a));\nQuery OK, 0 rows affected (0.13 sec)\n\nmysql> insert into test_ci values('a');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_ci values('A');\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_ci where a = 'a';\n+------+\n| a    |\n+------+\n| a    |  \n| A    |  -- A也被我们查到了\n+------+\n2 rows in set (0.00 sec)\n```\n> 上面的情况如果从业务的角度上看，可以很好理解，比如创建一个用户叫做Tom，你是不希望再创建一个叫做tom的用户的\n\n* **修改默认的collation**\n```sql\nmysql> set names utf8mb4 collate utf8mb4_bin;  -- 当前会话有效\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select 'a' = 'A';\n+-----------+\n| 'a' = 'A' |\n+-----------+\n|         0 |\n+-----------+\n1 row in set (0.00 sec)\n```\n\n>字符集的指定，可以在创建数据库的时候指定，也可以在创建表的时候单独指定，也可以创建列的时候进行指定\n\n-----\n\n## 五. 集合类型\n* 集合类型ENUM 和 SET\n* ENUM类型最多允许65536个值\n* SET类型最多允许64个值\n* 通过sql_mode参数可以用户约束检查\n\n### 1. 集合类型的排序\n```sql\nmysql> create table test_col (\n    -> user varchar(10),\n    -> sex enum('male', 'female')  -- 虽然写的是字符串，单其实存储的整型，效率还是可以的\n    -> );\n    \nmysql> insert into test_col values (\"tom\", \"male\");\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into test_col values (\"tom\", \"xmale\");  -- 不是male 和 female\nQuery OK, 1 row affected, 1 warning (0.03 sec)  -- 有warning\n\nmysql> set sql_mode='strict_trans_tables';  -- 设置为严格模式\nQuery OK, 0 rows affected, 2 warnings (0.00 sec)\n\nmysql> insert into test_col values (\"tom\", \"xmale\");\nERROR 1265 (01000): Data truncated for column 'sex' at row 1\n```\n>强烈建议新业务上都设置成严格模式\n\n### 2. 集合类型的排序\n```sql\nmysql> create table test_col_sort(\n    -> user char(10),\n    -> type enum('aaa','zzz','bbb','yyy','fff')  -- aaa=0, zzz=1, bbb=2, yyy=3, fff=4\n    -> );\nQuery OK, 0 rows affected (0.20 sec)\n\nmysql> select * from test_col_sort order by type asc; -- 以type作为key，进行升序排序\n+-------+------+\n| user  | type |\n+-------+------+\n| user1 | aaa  |  -- 0\n| user4 | zzz  |  -- 1\n| user2 | bbb  |  -- 2\n| user3 | yyy  |  -- 3\n+-------+------+  -- 枚举类型实际是整型数据，按照插入顺序进行排列\n4 rows in set (0.00 sec)\n\n--\n-- 使用ascii排序\n--\nmysql> select * from test_col_sort order by cast(type as char) asc;  -- 使用cast()函数转换成某种型\n+-------+------+                                                     -- 这里我们转成char型\n| user  | type |                                                     -- 然后进行排序(ascii)\n+-------+------+\n| user1 | aaa  | -- 0\n| user2 | bbb  | -- 2\n| user3 | yyy  | -- 3\n| user4 | zzz  | -- 1\n+-------+------+\n4 rows in set (0.00 sec)\n\n-- 或者使用concat\n\nmysql> select * from test_col_sort order by concat(type) asc;   -- concat()是连接字符串函数            \n+-------+------+\n| user  | type |\n+-------+------+\n| user1 | aaa  |  -- 0\n| user2 | bbb  |  -- 2\n| user3 | yyy  |  -- 3\n| user4 | zzz  |  -- 1\n+-------+------+\n4 rows in set (0.00 sec)\n\nmysql> select concat(\"abc\", \"大家好\");\n+----------------------------+\n| concat(\"abc\", \"大家好\")    |\n+----------------------------+\n| abc大家好                  |\n+----------------------------+\n1 row in set (0.00 sec)\n```\n\n----\n\n## 六. 日期类型\n\n| 日期类型 | 占用空间(byte)（<5.6）| 占用空间(byte)(>=5.6) |表示范围 |\n|----------|----------|----------|---------|\n| `DATETIME` | 8 | 5 + 微秒存储空间 |1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 |\n| DATE| 3 | 3 | 1000-01-01 ~ 9999-12-31 |\n| `TIMESTAMP` | 4 | 4 + 微秒存储空间 | 1970-01-01 00:00:00UTC ~ 2038-01-19 03:14:07UTC |\n| YEAR | 1 | 1 | YEAR(2):1970-2070, YEAR(4):1901-2155 |\n| TIME | 3 | 3 + 微秒存储空间 | -838:59:59 ~ 838:59:59 |\n\n| 微秒位数 | 所需存储空间 |\n| ---------|--------------|\n| 0\t       | 0            |\n| 1, 2     | 1 byte       |\n| 3, 4\t   | 2 bytes      |\n| 5, 6\t   | 3 bytes      |\n\n>TIMESTAMP 带时区功能\n\n### 1. TIMESTAMP和DATETIME\n\n```sql\nmysql> create table test_time(a timestamp, b datetime);\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql> insert into test_time values (now(), now());\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_time;\n+---------------------+---------------------+\n| a                   | b                   |\n+---------------------+---------------------+\n| 2015-11-28 10:00:39 | 2015-11-28 10:00:39 |\n+---------------------+---------------------+\n1 row in set (0.00 sec)\n\nmysql> select @@time_zone; \n+-------------+\n| @@time_zone |\n+-------------+\n| SYSTEM      |\n+-------------+\n1 row in set (0.00 sec)\n\nmysql> set time_zone='+00:00';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @@time_zone;\n+-------------+\n| @@time_zone |\n+-------------+\n| +00:00      |\n+-------------+\n1 row in set (0.00 sec)\n\nmysql> select * from test_time;\n+---------------------+---------------------+\n| a                   | b                   |\n+---------------------+---------------------+\n| 2015-11-28 2:00:39 | 2015-11-28 10:00:39  |  -- 时区的差别体现出来了\n+---------------------+---------------------+\n1 row in set (0.00 sec)\n```\n\n### 2. 微秒\n**从`MySQL5.6.X`开始，支持`微秒`，最大显示`6位`**\n```sql\nmysql> select now(6);\n+----------------------------+\n| now(6)                     |\n+----------------------------+\n| 2015-11-30 21:15:36.415358 |  -- 6位 微秒显示\n+----------------------------+\n1 row in set (0.00 sec)\n\nmysql> select now(7);\nERROR 1426 (42000): Too-big precision 7 specified for 'now'. Maximum is 6.  -- 不支持，最大到6\n\nmysql> create table test_time_fac (t datetime(6));\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> insert into test_time_fac values(now(6)); \nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_time_fac;\n+----------------------------+\n| t                          |\n+----------------------------+\n| 2015-11-30 21:19:27.900393 |  -- 由于是用了6位微秒位数，根据表格显示，实际存储的空间是 5 + 3 = 8 byte\n+----------------------------+\n1 row in set (0.00 sec)\n```\n\n\n### 3. 时间函数\n\n* 常用函数\n| 函数名 | 函数说明 | 备注 | \n|--------|----------|------|\n| NOW    | 返回`SQL执行时`的时间| 如果不考虑其他因素，可以理解为写完SQL，敲下回车瞬间的时间 |\n| CURRENT_TIMESTAMP| 与NOW()函数同义||\n| SYSDATE | 返回`函数执行时`的时间|MySQL处理你的函数时的时间，统一SQL语句中，大于NOW|\n| DATA_ADD(date, interval expr uint) | 增加时间||\n| DATA_SUB(date, interval expr uint) | 减少时间|可用ADD，然后unit给负数|\n| DATE FORMAT| 格式化时间| |\n\n>[所有时间函数--官方文档](http://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html)\n\n```sql\n\n--\n-- NOW和SYSDATE的区别\n--\nmysql> select now(6),sysdate(6),sleep(5),now(6),sysdate(6);\n+----------------------------+----------------------------+----------+----------------------------+----------------------------+\n| now(6)                     | sysdate(6)                 | sleep(5) | now(6)                     | sysdate(6)                 |\n+----------------------------+----------------------------+----------+----------------------------+----------------------------+\n| 2015-11-30 21:40:58.572383 | 2015-11-30 21:40:58.572542 |        0 | 2015-11-30 21:40:58.572383 | 2015-11-30 21:41:03.572720 |\n+----------------------------+----------------------------+----------+----------------------------+----------------------------+\n1 row in set (5.00 sec)\n--\n-- 两个now(6)都相等，因为是SQL执行时的时间(可以简单立理解为按回车的时间)\n-- 两个sysdate(6)差了5秒，刚好是sleep(5)的时间\n--\n\n-----\n\n--\n-- date_add\n--\nmysql> select date_add(now(), interval 5 day);   -- 增加5天\n+---------------------------------+\n| date_add(now(), interval 5 day) |\n+---------------------------------+\n| 2015-12-05 21:42:39             |\n+---------------------------------+\n1 row in set (0.00 sec)\n\nmysql> select date_add(now(), interval -5 month);  -- 减少 5个月\n+------------------------------------+\n| date_add(now(), interval -5 month) |\n+------------------------------------+\n| 2015-06-30 21:43:49                |\n+------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> select date_sub(now(), interval 5 month);  -- 与add + 负数一致\n+-----------------------------------+\n| date_sub(now(), interval 5 month) |\n+-----------------------------------+\n| 2015-06-30 21:44:21               |\n+-----------------------------------+\n1 row in set (0.00 sec)\n\n\n--\n-- date_format\n--\nmysql> SELECT DATE_FORMAT((select now(6)), '%H:%i:%s');\n+------------------------------------------+\n| DATE_FORMAT((select now(6)), '%H:%i:%s') |\n+------------------------------------------+\n| 21:48:30                                 |\n+------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### 4. 字段更新时间\n```sql\nmysql> create  table test_field_update(\n    -> a int(10),\n    -> b timestamp not null default current_timestamp on update current_timestamp\n    -> );\n\nmysql> insert into test_field_update values(1, now(6));\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from test_field_update;\n+------+---------------------+\n| a    | b                   |\n+------+---------------------+\n|    1 | 2015-11-30 21:55:18 |  -- 上面使用了now(6)，但是这里没有微秒，是因为定义的时候就是timestamp\n+------+---------------------+  -- 如果写成timestamp(6),就可以显示微秒\n1 row in set (0.00 sec)\n\nmysql> update test_field_update set a=100 where a=1;  -- 只更新a字段\nQuery OK, 1 row affected (0.03 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from test_field_update;    \n+------+---------------------+\n| a    | b                   |\n+------+---------------------+\n|  100 | 2015-11-30 22:01:03 |  -- 发现b字段跟着改变了\n+------+---------------------+\n1 row in set (0.00 sec)\n\n--\n-- 测试timestamp(6)\n--\nmysql> create table test_time_disp(\n    -> a int(10),\n    -> b timestamp(6) not null default current_timestamp(6) on update current_timestamp(6)  -- 定义了(6)\n    -> );\n\nmysql> insert into test_time_disp values(1, now(6));      \nQuery OK, 1 row affected (0.02 sec)\n\nmysql> select * from test_time_disp;\n+------+----------------------------+\n| a    | b                          |\n+------+----------------------------+\n|    1 | 2015-11-30 22:03:23.545406 |  -- 插入了now(6), 这里就显示了6位微秒\n+------+----------------------------+\n1 row in set (0.00 sec)\n\n```\n\n\n\n","tags":["mysql"],"categories":["mysql"]},{"title":"作业讲解一/Rank/视图/UNION/触发器","url":"/2019-10-01/mysql/MySQL学习笔记(Day013)/","content":"\nMySQL学习笔记（Day013：作业讲解一/Rank/视图/UNION/触发器）\n=========================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 作业讲解\n* **查询employees表中非基层用户的最近详细信息**\n> 老师的讲解的版本中存在问题，重新作为作业\n\n* **统计dbt3库下orders每周每个客户的订单数量**\n1. 思路\n    - 找到订单中最小周(week)之前的一周的周一，这里进行了简化，使用了`1970-01-05`作为周一标记，作为起始(`start`)\n    ```bash\n    [root@MyServer ~]> cal 1 1970\n        January 1970    \n    Su Mo Tu We Th Fr Sa\n                 1  2  3\n     4  5  6  7  8  9 10  # 1970-01-05 刚好是周一，用1月12号，19号，26号等也是可以的\n    11 12 13 14 15 16 17\n    18 19 20 21 22 23 24\n    25 26 27 28 29 30 31\n    ```\n    - 在起始条件周一（`start`)的基础上 `增加6天`时间，就是周日，即为一周的结束标记(`end`)\n    - 通过对`start`（周一）、`end`（周日）以及`o_custkey`进行`分组`，使用`count(o_orderkey)`得到对应的数量\n\n3. SQL语句\n    ```sql\n    -- 最终结果\n    select o_custkey,\n    ADDDATE('1970-01-05', INTERVAL FLOOR(DATEDIFF(o_orderdate, '1970-01-05')/7)*7 Day) as start,\n    ADDDATE('1970-01-05', INTERVAL FLOOR(DATEDIFF(o_orderdate, '1970-01-05')/7)*7 + 6 Day) as end,\n    count(o_orderkey) as total \n    from dbt3.orders group by o_custkey, start, end;\n\n    -- DATEDIFF(o_orderdate, '1970-01-05')\n    mysql> select datediff('1971-01-01', '1970-01-05');  -- 随意取一个1971-01-01，作为演示\n    +--------------------------------------+\n    | datediff('1971-01-01', '1970-01-05') |\n    +--------------------------------------+\n    |                                  361 |  -- 1971-01-01 减去 1970-01-5 为361天\n    +--------------------------------------+\n    1 row in set (0.00 sec)\n        \n    mysql> select datediff('1971-01-01', '1970-01-05') / 7;  -- 求周数\n    +------------------------------------------+\n    | datediff('1971-01-01', '1970-01-05') / 7 |\n    +------------------------------------------+\n    |                                  51.5714 |  -- 相隔51.5714周\n    +------------------------------------------+\n    1 row in set (0.00 sec)\n    \n    -- FLOOR 和 ROUND函数\n    -- FLOOR(arg)是返回一个不大于arg的最大整数，其实就是取整\n    mysql> select floor(5.4);\n    +------------+\n    | floor(5.4) |\n    +------------+\n    |          5 |\n    +------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select floor(5.5);\n    +------------+\n    | floor(5.5) |\n    +------------+\n    |          5 |\n    +------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select floor(5.6);\n    +------------+\n    | floor(5.6) |\n    +------------+\n    |          5 |\n    +------------+\n    1 row in set (0.00 sec)\n    \n    -- ROUND(X, D) 返回值是对数字X保留到小數点后D位，D默认为0，结果符合四舍五入原则；如果D为负数，则保留小数点左边（整数）的位数\n    mysql> select round(5.4);  -- 默认D为0，四舍五入\n    +------------+\n    | round(5.4) |\n    +------------+\n    |          5 |\n    +------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select round(5.5);  -- 默认D为0，四舍五入\n    +------------+\n    | round(5.5) |\n    +------------+\n    |          6 |\n    +------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select round(5.123, 1); -- 设D为1，保留小数点右边1为，四舍五入，不进位\n    +-----------------+\n    | round(5.123, 1) |\n    +-----------------+\n    |             5.1 |\n    +-----------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select round(5.163, 1); -- 设D为1，保留小数点右边1为，四舍五入，进位\n    +-----------------+\n    | round(5.163, 1) |\n    +-----------------+\n    |             5.2 |\n    +-----------------+\n    1 row in set (0.00 sec)\n            \n    mysql> select round(524.163, -1);  -- 保留小数点左边1位，不进位\n    +--------------------+\n    | round(524.163, -1) |\n    +--------------------+\n    |                520 |\n    +--------------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select round(524.163, -2);  -- 保留小数点左边2位, 不进位\n    +--------------------+\n    | round(524.163, -2) |\n    +--------------------+\n    |                500 |\n    +--------------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select round(554.163, -2);  -- 保留小数点左边2位, 进位\n    +--------------------+\n    | round(554.163, -2) |\n    +--------------------+\n    |                600 |\n    +--------------------+\n    1 row in set (0.00 sec)\n    \n    \n    -- 所以这里使用floor函数取整， 超过一周且不满一周的则落在start 和 end 中间\n    \n    mysql> select floor(datediff('1971-01-01', '1970-01-05') / 7);\n    +-------------------------------------------------+\n    | floor(datediff('1971-01-01', '1970-01-05') / 7) |\n    +-------------------------------------------------+\n    |                                              51 |  -- 取整为51周\n    +-------------------------------------------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select floor(datediff('1971-01-01', '1970-01-05') / 7) * 7;\n    +-----------------------------------------------------+\n    | floor(datediff('1971-01-01', '1970-01-05') / 7) * 7 |\n    +-----------------------------------------------------+\n    |                                                 357 |  -- 乘以7，得出357天差值\n    +-----------------------------------------------------+\n    1 row in set (0.00 sec)\n    \n    mysql> select adddate('1970-01-05', interval floor(datediff('1971-01-01', '1970-01-05')/7)*7 day) as Monday_start;    \n    -- 使用adddate函数，在1970-01-05(周一)的基础上，增加357天（51周），得到的值1970-12-28也是周一，标记为start\n    +--------------+\n    | Monday_start |\n    +--------------+\n    | 1970-12-28   |  -- 1970-12-28（周一），而传入的值1971-01-01是该周的周五\n    +--------------+\n    1 row in set (0.00 sec)\n    \n    -- 以同样的方法得到周日\n    mysql> select adddate('1970-01-05', interval floor(datediff('1971-01-01', '1970-01-05')/7)*7 + 6 day) as Sunday_end;\n    +------------+\n    | Sunday_end |\n    +------------+\n    | 1971-01-03 |\n    +------------+\n    1 row in set (0.00 sec)\n    \n    -- 通过以上的周一和周日的计算，即可求出1971-01-01这天所在的一周，得出start（周一）和end（周日）\n    -- 然后对start和end以及o_custkey做分组操作，并通过count(o_orderkey)得到分组的定单量\n    ```\n\n* **使用子查询实现RowNumber**\n1. 思路\n    - 假设当前在第N行记录，通过主键emp_no遍历有多少行的记录`小于等于`当前行,即为当前行的行数\n\n2. SQL语句\n    ```sql\n    SELECT \n    (SELECT COUNT(1) FROM employees b WHERE b.emp_no <= a.emp_no ) AS row_number,\n    emp_no,CONCAT(last_name,\" \",first_name) name,gender,hire_date\n    FROM employees a ORDER BY emp_no LIMIT 10;\n\n    -- 假设当前在第5行\n    mysql> select  b.emp_no  from employees.employees as b order by b.emp_no limit 5;\n    +--------+\n    | emp_no |\n    +--------+\n    |  10001 |\n    |  10002 |\n    |  10003 |\n    |  10004 |\n    |  10005 |  -- 第5行的emp_no是10005\n    +--------+\n    5 rows in set (0.00 sec)\n    \n    mysql> select  count(*)  from employees.employees as b where b.emp_no <= 10005 order by b.emp_no;\n    查找小于等于5的行数有几行\n    +----------+\n    | count(*) |\n    +----------+\n    |        5 |  -- 小于等于10005的记录有5行，则5就是10005该行记录的行号\n    +----------+\n    1 row in set (0.00 sec)\n    \n    -- 将该子查询的结果即可作为RowNumber\n    -- 但是该子查询的效率较低。不推荐使用。\n    \n    -- 推荐Day012中提及的自定义变量方式\n    SELECT @a:=@a+1 AS row_number,\n        emp_no,CONCAT(last_name,\" \",first_name) name,gender,hire_date\n        FROM employees,(SELECT @a:=0) AS a LIMIT 10;\n    ```\n\n----\n\n## 二. Rank\n>给出不同的用户的分数，然后根据分数计算排名\n```sql\nmysql> create table test_rank(id int, score int);\nQuery OK, 0 rows affected (0.16 sec)\n\nmysql> insert into test_rank values(1, 10), (2, 20), (3, 30), (4, 30), (5, 40), (6, 40);\nQuery OK, 6 rows affected (0.05 sec)\nRecords: 6  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_rank;\n+------+-------+\n| id   | score |\n+------+-------+\n|    1 |    10 |\n|    2 |    20 |\n|    3 |    30 |\n|    4 |    30 |\n|    5 |    40 |\n|    6 |    40 |\n+------+-------+\n6 rows in set (0.00 sec)\n\nmysql> set @prev_value := NULL;  -- 假设比较到第N行，设置一个变量prev_value用于存放第N-1行score的分数\n                                 -- 用于比较第N行的score和第N-1行的score\n                                 -- prev_value可以理解为 是临时保存第N-1行的score的变量\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> set @rank_count := 0;     -- 用于存放当前的排名\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select  id, score, \n    -> case\n    ->     when @prev_value = score then @rank_count  \n           -- 相等则prev_value不变， 并返回rank_count（第一次为NULL，不会相等，所以跳转到下一个when语句）\n    ->     when @prev_value := score then @rank_count := @rank_count + 1 \n           -- 不等，则第N行的score赋值(:=)给prev_value。且rank_count增加1\n    -> end as rank_column   -- case 开始的，end结尾\n    -> from test_rank\n    -> order by score desc;\n+------+-------+-------------+\n| id   | score | rank_column |\n+------+-------+-------------+\n|    5 |    40 |           1 |\n|    6 |    40 |           1 |\n|    3 |    30 |           2 |\n|    4 |    30 |           2 |\n|    2 |    20 |           3 |\n|    1 |    10 |           4 |\n+------+-------+-------------+\n6 rows in set (0.00 sec)\n\n-- case  \n--   when [condition_1] then [do_something_1] \n--   when [condition_2] then [do_something_2] \n--   end\n-- 语法：如果 condition_1条件满足，则执行 do_something_1 然后就跳出,不会执行condition_2;\n--       如果 condition_1条件不满足，则继续执行到 condition_2。以此类推。\n\n--\n-- 作业：使用一条语句写出Rank\n--\n```\n\n-----\n\n## 三. 视图\n>[官方文档](http://dev.mysql.com/doc/refman/5.7/en/create-view.html)\n\n* **创建视图**\n\n```sql\n--\n-- 创建视图\n--\nmysql> create view view_rank as select * from test_rank;  -- 针对上面的test_rank创建一个视图\nQuery OK, 0 rows affected (0.03 sec)\n\n-- 也可以对select结果增加条件进行过滤后，再创建视图\n\n\nmysql> show create table test_rank\\G\n*************************** 1. row ***************************\n       Table: test_rank\nCreate Table: CREATE TABLE `test_rank` (    -- 得到的是表结构\n  `id` int(11) DEFAULT NULL,\n  `score` int(11) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n1 row in set (0.00 sec)\n\n\nmysql> show create table view_rank\\G  -- 他是以一张表的形式存在的，可通过show tables看到\n*************************** 1. row ***************************\n                View: view_rank\n         Create View: CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`localhost` SQL SECURITY DEFINER VIEW `view_rank` AS select `test_rank`.`id` AS `id`,`test_rank`.`score` AS `score` from `test_rank`\n                      -- 和真正的表不同的是，这里show出来的是视图的定义\ncharacter_set_client: utf8\ncollation_connection: utf8_general_ci\n1 row in set (0.00 sec)\n\nmysql> select * from view_rank;  -- 可以直接查询该视图得结果\n+------+-------+\n| id   | score |\n+------+-------+\n|    1 |    10 |\n|    2 |    20 |\n|    3 |    30 |\n|    4 |    30 |\n|    5 |    40 |\n|    6 |    40 |\n+------+-------+\n6 rows in set (0.00 sec)\n\n-- 视图的作用是，可以对开发人员是透明的，可以隐藏部分关键的列\n-- 视图在mysql是虚拟表。根据视图的定义，还是取执行定义中的select语句。\n\n\n-- 只开放部分列\nmysql> create view view_rank_1 as select id from test_rank; -- 只开放id列\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql> select * from view_rank_1;  -- 即使 select * ，也只能看到id列，具有隐藏原来表中部分列的功能                        \n+------+\n| id   |\n+------+\n|    1 |\n|    2 |\n|    3 |\n|    4 |\n|    5 |\n|    6 |\n+------+\n6 rows in set (0.00 sec)\n\n-- 不要取用select * from 去创建视图，因为mysql会把*逐个解析成列。\n-- 当原来的表结构发生变化时，视图的表结构是不会发生变化的，视图在创建的瞬间，便确定了结构。\n-- 比如，当你alter原来的表 增加列(add columns)时,再去查询该视图,新增加的列是不存在的。\n\nmysql> alter table test_rank add column  c int default 0; -- 增加一列名字为c，默认值为0\nQuery OK, 0 rows affected (0.30 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_rank;  -- 查询原表，新的列c出现了\n+------+-------+------+\n| id   | score | c    |\n+------+-------+------+\n|    1 |    10 |    0 |\n|    2 |    20 |    0 |\n|    3 |    30 |    0 |\n|    4 |    30 |    0 |\n|    5 |    40 |    0 |\n|    6 |    40 |    0 |\n+------+-------+------+\n6 rows in set (0.00 sec)\n\nmysql> select * from view_rank; -- 尽管view_rank用select * 创建，但当时没有列c，所以无法得到c列的值\n+------+-------+\n| id   | score |\n+------+-------+\n|    1 |    10 |\n|    2 |    20 |\n|    3 |    30 |\n|    4 |    30 |\n|    5 |    40 |\n|    6 |    40 |\n+------+-------+\n6 rows in set (0.00 sec)\n\n-- 注意：mysql中的视图都是虚拟表。不像Oracle可以物化成真实存在的表。\n--      每次查询视图，实际上还是去查询的原来的表，只是查询的规则是在视图创建时经过定义的。\n\n```\n\n* **视图的算法**\n视图的算法(`ALGORITHM`)有三种方式：\n    - `UNDEFINED` 默认方式，由MySQL来判断使用下面的哪种算法\n    - `MERGE` ： `每次`通过`物理表`查询得到结果，把结果merge(合并)起来返回\n    - `TEMPTABLE` ： 产生一张`临时表`，把数据放入临时表后，客户端再去临时表取数据（`不会缓存`）\n    \n    >`TEMPTABLE 特点`：即使访问条件一样，第二次查询还是会去读取物理表中的内容，并重新生成一张临时表,并不会取缓存之前的表。*（临时表是Memory存储引擎，默认放内存，超过配置大小放磁盘）*\n    \n    >当查询有一个较大的结果集时，使用`TEMPTABLE`可以快速的结束对该物理表的访问，从而可以快速释放这张物理表上占用的资源。然后客户端可以对临时表上的数据做一些耗时的操作，而不影响原来的物理表。\n\n\n    **所以一般我们使用`UNDEFINED`，由MySQL自己去判断**\n\n----\n\n## 四. UNION\n\n1. `UNION` 的作用是将两个查询的结果集进行合并。\n2. UNION必须由`两条或两条以上`的SELECT语句组成，语句之间用关键字`UNION`分隔。\n3. UNION中的每个查询必须包含相同的列（`类型相同或可以隐式转换`）、表达式或聚集函数。\n\n```sql\nmysql> create table test_union_1(a int, b int);\nQuery OK, 0 rows affected (0.18 sec)\n\nmysql> create table test_union_2(a int, c int);  \nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> insert into test_union_1 values(1, 2), (3, 4), (5, 6), (10, 20);\nQuery OK, 4 rows affected (0.06 sec)\nRecords: 4  Duplicates: 0  Warnings: 0\n\nmysql> insert into test_union_2 values(10, 20), (30, 40), (50, 60);  \nQuery OK, 3 rows affected (0.03 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> select * from test_union_1;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    2 |\n|    3 |    4 |\n|    5 |    6 |\n|   10 |   20 |  -- test_union_1 中的10, 20\n+------+------+\n4 rows in set (0.00 sec)\n\nmysql> select * from test_union_2;\n+------+------+\n| a    | c    |\n+------+------+\n|   10 |   20 | -- test_union_2 中的10, 20\n|   30 |   40 |\n|   50 |   60 |  \n+------+------+\n3 rows in set (0.00 sec)\n\nmysql> select a, b as t from test_union_1\n    -> union\n    -> select * from test_union_2;\n+------+------+\n| a    | t    |\n+------+------+\n|    1 |    2 |\n|    3 |    4 |\n|    5 |    6 |\n|   10 |   20 | -- 只出现了一次 10, 20，union会去重\n|   30 |   40 |\n|   50 |   60 |\n+------+------+\n6 rows in set (0.00 sec)\n\nmysql> select a, b as t from test_union_1\n    -> union all   -- 使用 union all 可以不去重\n    -> select * from test_union_2;\n+------+------+\n| a    | t    |\n+------+------+\n|    1 |    2 |\n|    3 |    4 |\n|    5 |    6 |\n|   10 |   20 | -- test_union_1 中的10, 20\n|   10 |   20 | -- test_union_2 中的10, 20\n|   30 |   40 |\n|   50 |   60 |\n+------+------+\n7 rows in set (0.00 sec)\n\nmysql> select a, b as t from test_union_1 where a > 2 \n    -> union\n    -> select * from test_union_2 where c > 50;  -- 使用where过滤也可以 \n+------+------+\n| a    | t    |\n+------+------+\n|    3 |    4 |\n|    5 |    6 |\n|   10 |   20 |\n|   50 |   60 |\n+------+------+\n4 rows in set (0.00 sec)\n```\n\n> 如果知道数据本身具有唯一性，没有重复，则建议使用`union all`，因为`union`会做`去重操作`，性能会比`union all`要低\n\n----\n\n\n## 五. 触发器\n>[官方文档](http://dev.mysql.com/doc/refman/5.7/en/create-trigger.html)\n\n* **定义**\n    - 触发器的对象是`表`，当表上出现`特定的事件`时`触发`该程序的执行\n\n* **触发器的类型**\n    - **`UPDATE`**\n        - update 操作\n\n    - **`DELETE`**\n        - delete 操作\n        - replace 操作\n            - **注意：**drop，truncate等DDL操作`不会触发`DELETE\n    - **`INSERT`**\n        - insert 操作\n        - load data 操作\n        - replace 操作\n\n    > 注意，`replace`操作会**`触发两次`**，一次是`UPDATE`类型的触发器，一次是`INSERT`类型的触发器\n    >MySQL 5.6版本同一个类型的触发器只能有一个*(针对一个表)*\n    >MySQL 5.7允许多个同一类型的触发器\n\n    >**注意：触发器只触发DML(Data Manipulation Language)操作，不会触发DDL(Data Definition Language)操作*（create,drop等操作）* **\n        \n        \n* **创建触发器**\n\n    ```sql\n    CREATE\n        [DEFINER = { user | CURRENT_USER }]\n        TRIGGER trigger_name  -- 触发器名字\n        trigger_time trigger_event  -- 触发时间和事件\n        ON tbl_name FOR EACH ROW    \n        [trigger_order]\n        trigger_body\n    \n    trigger_time: { BEFORE | AFTER }   -- 事件之前还是之后触发\n    \n    trigger_event: { INSERT | UPDATE | DELETE }  -- 三个类型\n    \n    trigger_order: { FOLLOWS | PRECEDES } other_trigger_name\n    ```\n    \n    ```sql\n    mysql> create table test_trigger_1 (\n        ->     name varchar(10),\n        ->     score int(10),\n        -> primary key (name));\n    Query OK, 0 rows affected (0.14 sec)\n        \n        mysql> delimiter //  -- 将语句分隔符定义成 // （原来是';'）\n        mysql> create trigger  trg_upd_score  -- 定义触发器名字\n            -> before update on test_trigger_1  -- 作用在test_trigger_1 更新(update)之前(before)\n            -> for each row  -- 每行\n            -> begin  -- 开始定义\n            -> if new.score < 0 then   -- 如果新值小于0\n            ->     set new.score=0;        -- 则设置成0\n            -> elseif new.score > 100 then  -- 如果新值大于100\n            ->     set new.score = 100;  -- 则设置成100\n            -> end if;  -- begin对应的 结束 \n            -> end;//   -- 结束，使用新定义的 '//' 结尾\n        Query OK, 0 rows affected (0.03 sec)\n    \n    mysql> delimiter ;  -- 恢复 ';' 结束符\n    \n    -- new.col : 表示更新以后的值\n    -- old.col : 表示更新以前的值(只读)\n    \n    mysql> insert into test_trigger_1 values (\"tom\", 200);  -- 插入新值\n    Query OK, 1 row affected (0.04 sec)\n    \n    mysql> select * from test_trigger_1;\n    +------+-------+\n    | name | score |\n    +------+-------+\n    | tom  |   200 |  -- 没改成100，因为定义的是update，而执行的是insert\n    +------+-------+\n    1 row in set (0.00 sec)\n    \n    mysql> update test_trigger_1 \n        -> set score=300 where name='tom'; -- 改成300\n    Query OK, 0 rows affected (0.00 sec)\n    Rows matched: 1  Changed: 0  Warnings: 0\n    \n    mysql> select * from test_trigger_1;\n    +------+-------+\n    | name | score |\n    +------+-------+\n    | tom  |   100 | -- 通过触发器的设置，大于100的值被修改成100\n    +------+-------+\n    1 row in set (0.00 sec)\n    ```\n        \n        \n        \n        \n        ","tags":["mysql"],"categories":["mysql"]},{"title":"MySQL多实例下/SSL","url":"/2019-10-01/mysql/MySQL学习笔记(Day007)/","content":"\nMySQL学习笔记（Day007：多实例下/SSL）\n============================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 多实例安装 -- 多版本\n\n### 1. [mysqld_multi]标签\n* **`[mysqld_multi]` 是否需要配置**\n从操作演示来看，在`my.cnf`(*老师给的模板配置*)上直接配置`[mysqld1]`、`[mysqld2]`等实例标签，而`不配置[mysqld_multi]`,使用`mysqld_multi start 1`也是`可以启动`数据库实例的，但是没有`mysqld_safe`的守护进程。所以该标签`需要配置`\n\n### 2. 停止mysql实例\n* **`multi_admin用户`的作用**\n通过[官方文档](http://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html)中我们看到，`'multi_admin'@'localhost'`这个用户主要的作用是用来`关闭`数据库实例，因为文档中只授权了`SHUTDOWN`权限。所以在`[mysqld_multi]`标签下，我们需要配置`user`和`password`**(注意`5.7.9`中是`pass`)**来进行关闭数据库实例。\n\n* **`[client]`标签**\n从操作演示来看，老师并没有在`[mysqld_multi]`下配置`user`和`password`，但是仍然可以关闭数据库，原因是因为`/root/.my.cnf`中存在了`[client]`标签。该标签下的用户`user = root`有关闭数据库实例的权限，因此可以关闭数据库。\n\n>如果在`[client]`和`[mysqld_multi]`标签中同时存在`user`和`password`, 则在关闭数据库实例中会使用`[mysqld_multi]`中的`user`去关闭。\n**(`存在精确匹配的标签，则优先使用精确匹配标签下的配置项`)**\n\n### 3. 多实例安装 -- 多版本\n* **环境说明**\n    - `mysqld1` -- MySQL 5.7.9\n    - `mysqld2` -- MySQL 5.7.9\n    - `mysqld3` -- MySQL 5.6.27\n    - `mysqld4` -- MySQL 5.6.27\n\n* **配置文件**\n```bash\n[client]\nuser = root\npassword = 123\n\n[mysqld_multi]  # 这里使用了client标签中的user，故这里不再定义user\nmysqld = /usr/local/mysql/bin/mysqld_safe\nlog = /var/log/mysqld_multi.log\n\n[mysqld1]\nserver-id = 11\ndatadir = /data1\nbasedir = /usr/local/mysql   # basedir定义使用了5.7的mysql版本\nport = 3307\nsocket = /tmp/mysql.sock1\n\n[mysqld2]\nserver-id = 22\ndatadir = /data2\nbasedir = /usr/local/mysql\nport = 3308\nsocket = /tmp/mysql.sock2\n\n[mysqld3]\nserver-id = 33\ndatadir = /data3\nbasedir = /usr/local/mysql56  # basedir定义了使用5.6的mysql版本\nport = 3309\nsocket = /tmp/mysql.sock3\nplugin_dir=/usr/local/mysql56/lib/plugin  # plugin 目录也变了\n\n#这里无需特别配置mysqld, 可以继承使用[mysqld_multi]中的配置，然后根据basedir找到对应的mysqld\n\n[mysqld4]\nserver-id = 44\ndatadir = /data4\nbasedir = /usr/local/mysql56\nport = 3310\nsocket = /tmp/mysql.sock4\nplugin_dir=/usr/local/mysql56/lib/plugin\n\n#--------------以下参数是老师的模板，只是将个别size调小-----------\n[mysqld]\n########basic settings########\nserver-id = 100\nport = 3306\nuser = mysql\nbind_address = 0.0.0.0\n#autocommit = 0\ncharacter_set_server=utf8mb4\nskip_name_resolve = 1\nmax_connections = 800\nmax_connect_errors = 1000\ndatadir = /data/mysql_data\ntransaction_isolation = READ-COMMITTED\nexplicit_defaults_for_timestamp = 1\njoin_buffer_size = 134217728\ntmp_table_size = 67108864\ntmpdir = /tmp\nmax_allowed_packet = 16777216\nsql_mode = \"STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER\"\ninteractive_timeout = 1800\nwait_timeout = 1800\nread_buffer_size = 16777216\nread_rnd_buffer_size = 33554432\nsort_buffer_size = 33554432\n########log settings########\nlog_error = error.log\nslow_query_log = 1\nslow_query_log_file = slow.log\nlog_queries_not_using_indexes = 1\nlog_slow_admin_statements = 1\nlog_slow_slave_statements = 1\nlog_throttle_queries_not_using_indexes = 10\nexpire_logs_days = 90\nlong_query_time = 2\nmin_examined_row_limit = 100\n########replication settings########\nmaster_info_repository = TABLE\nrelay_log_info_repository = TABLE\nlog_bin = bin.log\nsync_binlog = 1\ngtid_mode = on\nenforce_gtid_consistency = 1\nlog_slave_updates\nbinlog_format = row \nrelay_log = relay.log\nrelay_log_recovery = 1\nbinlog_gtid_simple_recovery = 1\nslave_skip_errors = ddl_exist_errors\n########innodb settings########\ninnodb_page_size = 8192\ninnodb_buffer_pool_size = 1G    # 该参数减小到1G\ninnodb_buffer_pool_instances = 8\ninnodb_buffer_pool_load_at_startup = 1\ninnodb_buffer_pool_dump_at_shutdown = 1\ninnodb_lru_scan_depth = 2000\ninnodb_lock_wait_timeout = 5\ninnodb_io_capacity = 4000\ninnodb_io_capacity_max = 8000\ninnodb_flush_method = O_DIRECT\ninnodb_file_format = Barracuda\ninnodb_file_format_max = Barracuda\n#innodb_log_group_home_dir = /redolog/\n#innodb_undo_directory = /undolog/\ninnodb_undo_logs = 128\ninnodb_undo_tablespaces = 3\ninnodb_flush_neighbors = 1\ninnodb_log_file_size = 128M  # 该参数减小到 128M\ninnodb_log_buffer_size = 16777216\ninnodb_purge_threads = 4\ninnodb_large_prefix = 1\ninnodb_thread_concurrency = 64\ninnodb_print_all_deadlocks = 1\ninnodb_strict_mode = 1\ninnodb_sort_buffer_size = 67108864 \n########semi sync replication settings########\nplugin_dir=/usr/local/mysql/lib/plugin\n#plugin_load = \"rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so\"\nloose_rpl_semi_sync_master_enabled = 1\nloose_rpl_semi_sync_slave_enabled = 1\nloose_rpl_semi_sync_master_timeout = 5000\n\n[mysqld-5.7]\ninnodb_buffer_pool_dump_pct = 40\ninnodb_page_cleaners = 4\ninnodb_undo_log_truncate = 1\ninnodb_max_undo_log_size = 1G   # 该参数减小到1G\ninnodb_purge_rseg_truncate_frequency = 128\nbinlog_gtid_simple_recovery=1\nlog_timestamps=system\ntransaction_write_set_extraction=MURMUR32\nshow_compatibility_56=on\n```\n>**注意MySQL5.6.27的`plugin_dir`的路径**\n\n>配置说明：\n1：配置的标签顺序没有关系，不会影响最终配置的有效性。\n2：同类型标签中的配置项会合并，形成一个大的配置项\n2：`匹配度高`的标签中的配置项的`值`，会`覆盖`掉`匹配度低`的标签中的配置项的`值`\n\n>**[mysqld`N`]中的配置项会和[mysqld]中的配置项进行合并，并且[mysqld`N`]中已有的配置项的值，会覆盖掉[mysqld]中的配置项的值,如`datadir`, `port`等**\n\n\n* **安装操作**\n```bash\n#\n# 准备好数据目录，并初始化安装\n#\n[root@MyServer ~]> mkdir /data1\n[root@MyServer ~]> mkdir /data2\n[root@MyServer ~]> mkdir /data3\n[root@MyServer ~]> mkdir /data4\n[root@MyServer ~]> chown mysql.mysql /data{1..4}\n[root@MyServer ~]> mysqld --initialize --user=mysql --datadir=/data1\n#\n# 这里无输出，临时密码见 /data1/error.log\n#\n[root@MyServer ~]> mysqld --initialize --user=mysql --datadir=/data2\n#\n# 这里无输出，临时密码见 /data1/error.log\n#\n[root@MyServer mysql56]> pwd\n/usr/local/mysql56\n[root@MyServer mysql56]> scripts/mysql_install_db  --user=mysql --datadir=/data3\n#\n# 这里有部分信息输出\n# 安装后，需要检查error.log 确保没有错误出现\n# 注意使用空密码登录后，修改密码\n#\n[root@MyServer mysql56]> scripts/mysql_install_db  --user=mysql --datadir=/data4\n#\n# 这里有部分信息输出\n# 安装后，需要检查error.log 确保没有错误出现\n# 注意使用空密码登录后，修改密码\n#\n[root@MyServer ~]> cp /usr/local/mysql/support-files/mysqld_multi.server  /etc/init.d/mysqld_multid \n# 拷贝启动脚本，方便自启\n[root@MyServer ~]> chkconfig mysqld_multid on\n\n[root@MyServer ~]> mysqld_multi report                       \nReporting MySQL servers\nMySQL server from group: mysqld1 is not running\nMySQL server from group: mysqld2 is not running\nMySQL server from group: mysqld3 is not running\nMySQL server from group: mysqld4 is not running\n\n[root@MyServer ~]> mysqld_multi  report\nReporting MySQL servers\nMySQL server from group: mysqld1 is running\nMySQL server from group: mysqld2 is running\nMySQL server from group: mysqld3 is running\nMySQL server from group: mysqld4 is running\n\n[root@MyServer ~]> ps -ef | grep mysqld\nroot     13859     1  0 22:35 pts/1    00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --server-id=11 --datadir=/data1 --basedir=/usr/local/mysql --port=3307 --socket=/tmp/mysql.sock1\nroot     13865     1  0 22:35 pts/1    00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --server-id=22 --datadir=/data2 --basedir=/usr/local/mysql --port=3308 --socket=/tmp/mysql.sock2\nroot     13872     1  0 22:35 pts/1    00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --server-id=33 --datadir=/data3 --basedir=/usr/local/mysql56 --port=3309 --socket=/tmp/mysql.sock3 --plugin_dir=/usr/local/mysql56/lib/plugin\nroot     13886     1  0 22:35 pts/1    00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --server-id=44 --datadir=/data4 --basedir=/usr/local/mysql56 --port=3310 --socket=/tmp/mysql.sock4 --plugin_dir=/usr/local/mysql56/lib/plugin\n#\n# 上面是mysqld_safe的守护进程\n# 下面是实际的mysqld的进程，观察mysqld的路径\n# 因为指定了basedir，所以会自动识别mysqld的路径\n#\nmysql    17783 13859  0 22:35 pts/1    00:00:00 /usr/local/mysql-5.7.9-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql --datadir=/data1 --plugin-dir=/usr/local/mysql/lib/plugin --user=mysql --server-id=11 --log-error=/data1/error.log --pid-file=/data1/MyServer.pid --socket=/tmp/mysql.sock1 --port=3307\nmysql    17784 13865  0 22:35 pts/1    00:00:00 /usr/local/mysql-5.7.9-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql --datadir=/data2 --plugin-dir=/usr/local/mysql/lib/plugin --user=mysql --server-id=22 --log-error=/data2/error.log --pid-file=/data2/MyServer.pid --socket=/tmp/mysql.sock2 --port=3308\nmysql    17819 13872  0 22:35 pts/1    00:00:00 /usr/local/mysql-5.6.27-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql56 --datadir=/data3 --plugin-dir=/usr/local/mysql56/lib/plugin --user=mysql --server-id=33 --log-error=/data3/error.log --pid-file=/data3/MyServer.pid --socket=/tmp/mysql.sock3 --port=3309\nmysql    17824 13886  0 22:35 pts/1    00:00:00 /usr/local/mysql-5.6.27-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql56 --datadir=/data4 --plugin-dir=/usr/local/mysql56/lib/plugin --user=mysql --server-id=44 --log-error=/data4/error.log --pid-file=/data4/MyServer.pid --socket=/tmp/mysql.sock4 --port=3310\nroot     17988  2657  0 22:44 pts/1    00:00:00 grep mysqld\n\n\n[root@MyServer ~]> ps -ef | grep mysqld |  grep -v mysqld_safe | grep -v grep | awk '{print $8\" \"$9}'  \n/usr/local/mysql-5.7.9-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql\n/usr/local/mysql-5.7.9-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql\n/usr/local/mysql-5.6.27-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql56\n/usr/local/mysql-5.6.27-linux-glibc2.5-x86_64/bin/mysqld --basedir=/usr/local/mysql56\n```\n>`mysql3` 和 `mysql4`初始状态没有密码，以前可以直接使用`mysql -S  mysql.sock`登录，而现在登录的时候特别注意，因为我们使用了`[client]`标签,登录的时候如果不加`-p`参数会默认使用标签下的`user`和`password`, 然后导致登录不进去，所以需要使用如下登录方式：\n\n```bash\nshell> mysql -u root -P3309 -S /tmp/mysql.sock3 -p\nEnter password: [直接回车]\nelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 3\nServer version: 5.6.27-log MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> set password = password(\"123\"); #进行修改密码\n```\n\n* **设置login-path**\n设置`login-path`主要为了能够简化登录，同时还可以让每个数据库的密码都不同，避免使用[client]下的统一用户名密码\n```bash\n[root@MyServer ~]> mysql_config_editor  set -G mysql1 -u root -p -S /tmp/mysql.sock1\n[root@MyServer ~]> mysql_config_editor  set -G mysql2 -u root -p -S /tmp/mysql.sock2\n[root@MyServer ~]> mysql_config_editor  set -G mysql3 -u root -p -S /tmp/mysql.sock3\n[root@MyServer ~]> mysql_config_editor  set -G mysql4 -u root -p -S /tmp/mysql.sock4\n\n# 然后可以使用mysql --login-path=mysql1 这种方式登录\n```\n\n```sql\n--\n-- mysql1 \n--\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.01 sec)\n\nmysql> show variables like \"port\"; \n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\n--\n-- mysql2\n--\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3308  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\n--\n-- mysql3\n--\nmysql> select version();\n+------------+\n| version()  |\n+------------+\n| 5.6.27-log |  -- mysql 5.6.27 \n+------------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3309  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\n--\n-- mysql4\n--\nmysql> select version();\n+------------+\n| version()  |\n+------------+\n| 5.6.27-log |  -- mysql 5.6.27\n+------------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3310  |\n+---------------+-------+\n1 row in set (0.00 sec)\n```\n\n## 二. SSL安装\nSSL（Secure Socket Layer）是维护Client - Server之间加密通讯的一套安全协议；\n\n```sql\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"%ssl%\";\n+---------------+----------+\n| Variable_name | Value    |\n+---------------+----------+\n| have_openssl  | DISABLED |  --  SSL被禁止了\n| have_ssl      | DISABLED |\n| ssl_ca        |          |\n| ssl_capath    |          |\n| ssl_cert      |          |\n| ssl_cipher    |          |\n| ssl_crl       |          |\n| ssl_crlpath   |          |\n| ssl_key       |          |\n+---------------+----------+\n9 rows in set (0.00 sec)\n```\n>经过之前的多实例安装，是没有开启SSL配置的\n\n\n### 1. 开启SSL (5.7.9)\n* 环境说明\n    - 虚拟机1：MyServer； IP：172.18.14.68, MySQL实例1 - mysql1\n    - 虚拟机2：MyServer； IP：172.18.14.41, MySQL客户端\n    >操作过程中看到的192.168.115.223 是宿主机IP，因为使用KVM虚拟机的NAT功能，所以会被转换\n```bash\n#\n# 当前虚拟机1 MyServer\n#\n[root@MyServer mysql]> pwd\n/usr/local/mysql\n[root@MyServer mysql]> bin/mysql_ssl_rsa_setup  --datadir=/data1 --user=mysql --uid=mysql \n# 使用--uid后，就不需要chown mysql.mysql *.pem\n\n[root@MyServer data1]# pwd\n/data1\n[root@MyServer data1]# ll | grep pem\n-rw-------. 1 mysql mysql      1675 Nov 25 23:55 ca-key.pem\n-rw-r--r--. 1 mysql mysql      1070 Nov 25 23:55 ca.pem\n-rw-r--r--. 1 mysql mysql      1078 Nov 25 23:55 client-cert.pem #客户端证书文件\n-rw-------. 1 mysql mysql      1679 Nov 25 23:55 client-key.pem  #客户端私钥文件\n-rw-------. 1 mysql mysql      1675 Nov 25 23:55 private_key.pem #用于密钥交换的公钥\n-rw-r--r--. 1 mysql mysql       451 Nov 25 23:55 public_key.pem #用户密钥交换的私钥\n-rw-r--r--. 1 mysql mysql      1078 Nov 25 23:55 server-cert.pem #服务器端证书文件\n-rw-------. 1 mysql mysql      1679 Nov 25 23:55 server-key.pem #服务器端私钥文件\n[root@MyServer data1]> mysqld_multi  stop 1\n[root@MyServer data1]> mysqld_multi  start 1\n```\n>关于几个pem文件的用途说面，见[官方文档](http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-5.html)，并搜索关键字`private/public key-pair`\n\n```sql\n--\n-- 当前虚拟机1 MyServer ,当前实例为 mysql1\n--\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"%ssl%\";\n+---------------+-----------------+\n| Variable_name | Value           |\n+---------------+-----------------+\n| have_openssl  | YES             |  -- 已经支持SSL\n| have_ssl      | YES             |\n| ssl_ca        | ca.pem          |\n| ssl_capath    |                 |\n| ssl_cert      | server-cert.pem |  -- 公钥文件\n| ssl_cipher    |                 |\n| ssl_crl       |                 |\n| ssl_crlpath   |                 |\n| ssl_key       | server-key.pem  |  -- 私钥文件\n+---------------+-----------------+\n9 rows in set (0.00 sec)\n\nmysql> \\s  -- status\n--------------\nmysql  Ver 14.14 Distrib 5.7.9, for linux-glibc2.5 (x86_64) using  EditLine wrapper\n\nConnection id:          2\nCurrent database:\nCurrent user:           root@localhost\nSSL:                    Not in use  -- 此时本地socket登录，不用SSL\nCurrent pager:          stdout\nUsing outfile:          ''\nUsing delimiter:        ;\nServer version:         5.7.9-log MySQL Community Server (GPL)\nProtocol version:       10\nConnection:             Localhost via UNIX socket\nServer characterset:    utf8mb4\nDb     characterset:    utf8mb4\nClient characterset:    utf8\nConn.  characterset:    utf8\nUNIX socket:            /tmp/mysql.sock1\nUptime:                 6 min 16 sec\n\nThreads: 1  Questions: 7  Slow queries: 0  Opens: 108  Flush tables: 1  Open tables: 101  Queries per second avg: 0.018\n--------------\n\nmysql> create user 'burn'@'%' identified by '123'; -- 创建一个burn@%用户，先不require ssl\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql> grant all on *.* to 'burn'@'%';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> select * from mysql.user where user='burn'\\G\n*************************** 1. row ***************************\n                  Host: %\n                  User: burn\n           Select_priv: Y\n           Insert_priv: Y\n           Update_priv: Y\n           Delete_priv: Y\n           Create_priv: Y\n             Drop_priv: Y\n           Reload_priv: Y\n         Shutdown_priv: Y\n          Process_priv: Y\n             File_priv: Y\n            Grant_priv: N\n       References_priv: Y\n            Index_priv: Y\n            Alter_priv: Y\n          Show_db_priv: Y\n            Super_priv: Y\n Create_tmp_table_priv: Y\n      Lock_tables_priv: Y\n          Execute_priv: Y\n       Repl_slave_priv: Y\n      Repl_client_priv: Y\n      Create_view_priv: Y\n        Show_view_priv: Y\n   Create_routine_priv: Y\n    Alter_routine_priv: Y\n      Create_user_priv: Y\n            Event_priv: Y\n          Trigger_priv: Y\nCreate_tablespace_priv: Y\n              ssl_type:         --  此处为空\n            ssl_cipher: \n           x509_issuer: \n          x509_subject: \n         max_questions: 0\n           max_updates: 0\n       max_connections: 0\n  max_user_connections: 0\n                plugin: mysql_native_password\n authentication_string: *23AE809DDACAF96AF0FD78ED04B6A265E05AA257\n      password_expired: N\n password_last_changed: 2015-11-26 09:55:31\n     password_lifetime: NULL\n        account_locked: N\n1 row in set (0.00 sec)\n```\n\n```bash\n#\n# 当前虚拟机2  MyServer2\n#\n[root@MyServer2 bin]> ./mysql -u burn -h 172.18.14.68 -P3307 -p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 6\nServer version: 5.7.9-log MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> \\s\n--------------\n./mysql  Ver 14.14 Distrib 5.7.9, for linux-glibc2.5 (x86_64) using  EditLine wrapper\n\nConnection id:          6\nCurrent database:\nCurrent user:           burn@192.168.115.223\nSSL:                    Cipher in use is DHE-RSA-AES256-SHA  #已经使用了ssl登录了\nCurrent pager:          stdout\nUsing outfile:          ''\nUsing delimiter:        ;\nServer version:         5.7.9-log MySQL Community Server (GPL)\nProtocol version:       10\nConnection:             172.18.14.68 via TCP/IP\nServer characterset:    utf8mb4\nDb     characterset:    utf8mb4\nClient characterset:    utf8\nConn.  characterset:    utf8\nTCP port:               3307\nUptime:                 3 min 6 sec\n\nThreads: 2  Questions: 19  Slow queries: 0  Opens: 109  Flush tables: 1  Open tables: 102  Queries per second avg: 0.102\n--------\n```\n\n```bash\n#\n# 当前虚拟机2 MyServer2\n# 上面测试中我们没有使用--ssl参数，也是用了ssl登录的，原因如下\n#\n[root@MyServer2 bin]> ./mysql --help | grep ssl                \n  --ssl               If set to ON, this option enforces that SSL is\n                      server. To disable client SSL capabilities use --ssl=OFF.\n                      (Defaults to on; use --skip-ssl to disable.)\n                      # 这里说，默认是开启的，可以用--skip-ssl 禁用\n```\n\n```bash\n#\n# 当前虚拟机2 MyServer2 \n# 禁用ssl登录测试\n#\n[root@MyServer2 bin]> ./mysql -u burn -h 172.18.14.68 -P3307 -p --skip-ssl  #这里跳过了ssl\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 7\nServer version: 5.7.9-log MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> \\s\n--------------\n./mysql  Ver 14.14 Distrib 5.7.9, for linux-glibc2.5 (x86_64) using  EditLine wrapper\n\nConnection id:          7\nCurrent database:\nCurrent user:           burn@192.168.115.223\nSSL:                    Not in use    # 果然就禁用了ssl\nCurrent pager:          stdout\nUsing outfile:          ''\nUsing delimiter:        ;\nServer version:         5.7.9-log MySQL Community Server (GPL)\nProtocol version:       10\nConnection:             172.18.14.68 via TCP/IP\nServer characterset:    utf8mb4\nDb     characterset:    utf8mb4\nClient characterset:    utf8\nConn.  characterset:    utf8\nTCP port:               3307\nUptime:                 5 min 50 sec\n\nThreads: 2  Questions: 24  Slow queries: 0  Opens: 109  Flush tables: 1  Open tables: 102  Queries per second avg: 0.068\n--------------\n```\n\n```sql\n--\n-- 当前虚拟机1 MyServer, 当前实例mysql1\n-- 让用户必须使用ssl\n--\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> alter user 'burn'@'%' require ssl;\nQuery OK, 0 rows affected (0.02 sec)\n```\n\n```bash\n#\n# 当前虚拟机2 MyServer2\n#\n[root@MyServer2 bin]> ./mysql -u burn -h 172.18.14.68 -P3307 -p --skip-ssl\nEnter password: \nERROR 1045 (28000): Access denied for user 'burn'@'192.168.115.223' (using password: YES) ## 禁用了SSL就无法登录了\n##\n[root@MyServer2 bin]> ./mysql -u burn -h 172.18.14.68 -P3307 -p # 默认就启用ssl\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 9\nServer version: 5.7.9-log MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> \\s\n--------------\n./mysql  Ver 14.14 Distrib 5.7.9, for linux-glibc2.5 (x86_64) using  EditLine wrapper\n\nConnection id:          9\nCurrent database:\nCurrent user:           burn@192.168.115.223\nSSL:                    Cipher in use is DHE-RSA-AES256-SHA  # 确实启用了\nCurrent pager:          stdout\nUsing outfile:          ''\nUsing delimiter:        ;\nServer version:         5.7.9-log MySQL Community Server (GPL)\nProtocol version:       10\nConnection:             172.18.14.68 via TCP/IP\nServer characterset:    utf8mb4\nDb     characterset:    utf8mb4\nClient characterset:    utf8\nConn.  characterset:    utf8\nTCP port:               3307\nUptime:                 14 min 25 sec\n\nThreads: 2  Questions: 32  Slow queries: 0  Opens: 109  Flush tables: 1  Open tables: 102  Queries per second avg: 0.036\n--------------\n```\n\n### 2. 开启证书认证(5.7.9)\n```sql\n--\n-- 当前虚拟机1 MyServer， 当前实例 msyql1\n--\nmysql> show variables like \"port\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| port          | 3307  |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> create user 'burn_x509'@'%' identified by '123' require x509; -- 启用证书认证\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql> grant all on *.* to 'burn'@'%';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> select * from mysql.user where user='burn_x509'\\G\n*************************** 1. row ***************************\n                  Host: %\n                  User: burn_x509\n           Select_priv: N\n           Insert_priv: N\n           Update_priv: N\n           Delete_priv: N\n           Create_priv: N\n             Drop_priv: N\n           Reload_priv: N\n         Shutdown_priv: N\n          Process_priv: N\n             File_priv: N\n            Grant_priv: N\n       References_priv: N\n            Index_priv: N\n            Alter_priv: N\n          Show_db_priv: N\n            Super_priv: N\n Create_tmp_table_priv: N\n      Lock_tables_priv: N\n          Execute_priv: N\n       Repl_slave_priv: N\n      Repl_client_priv: N\n      Create_view_priv: N\n        Show_view_priv: N\n   Create_routine_priv: N\n    Alter_routine_priv: N\n      Create_user_priv: N\n            Event_priv: N\n          Trigger_priv: N\nCreate_tablespace_priv: N\n              ssl_type: X509    -- 使用X509登录\n            ssl_cipher: \n           x509_issuer: \n          x509_subject: \n         max_questions: 0\n           max_updates: 0\n       max_connections: 0\n  max_user_connections: 0\n                plugin: mysql_native_password\n authentication_string: *23AE809DDACAF96AF0FD78ED04B6A265E05AA257\n      password_expired: N\n password_last_changed: 2015-11-26 10:14:43\n     password_lifetime: NULL\n        account_locked: N\n1 row in set (0.00 sec)\n```\n\n```bash\n#\n# 当前虚拟机2 MyServer2 \n#\n[root@MyServer2 bin]> ./mysql -u burn_x509 -h 172.18.14.68 -P3307 -p\nEnter password: \nERROR 1045 (28000): Access denied for user 'burn_x509'@'192.168.115.223' (using password: YES)  # 即使默认开启了ssl，也是无法登录的\n```\n\n```bash\n#\n# 当前虚拟机1 MyServer\n#\n[root@MyServer data1]> pwd\n/data1\n[root@MyServer data1]> ll | grep pem\n-rw-------. 1 mysql mysql      1675 Nov 25 23:55 ca-key.pem\n-rw-r--r--. 1 mysql mysql      1070 Nov 25 23:55 ca.pem\n-rw-r--r--. 1 mysql mysql      1078 Nov 25 23:55 client-cert.pem\n-rw-------. 1 mysql mysql      1679 Nov 25 23:55 client-key.pem\n-rw-------. 1 mysql mysql      1675 Nov 25 23:55 private_key.pem\n-rw-r--r--. 1 mysql mysql       451 Nov 25 23:55 public_key.pem\n-rw-r--r--. 1 mysql mysql      1078 Nov 25 23:55 server-cert.pem\n-rw-------. 1 mysql mysql      1679 Nov 25 23:55 server-key.pem\n[root@MyServer data1]> scp client-cert.pem  client-key.pem  root@172.18.14.41:~/\nThe authenticity of host '172.18.14.41 (172.18.14.41)' can't be established.\nRSA key fingerprint is 5f:f5:3c:b0:57:79:8d:50:c6:c8:69:b0:90:6e:98:3b.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '172.18.14.41' (RSA) to the list of known hosts.\nroot@172.18.14.41's password: \nclient-cert.pem                                                                                           100% 1078     1.1KB/s   00:00    \nclient-key.pem                                                                                            100% 1679     1.6KB/s   00:00    \n```\n\n```bash\n#\n# 当前虚拟机2 MyServer2\n#\n[root@MyServer2 ~]> ll | grep pem\n-rw-r--r--.  1 root root       1078 Nov 26 10:22 client-cert.pem\n-rw-------.  1 root root       1679 Nov 26 10:22 client-key.pem\n\n[root@MyServer2 ~]> mysql -u burn_x509 -h 172.18.14.68 -P 3307 -p --ssl-cert=./client-cert.pem  --ssl-key=./client-key.pem \nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 12\nServer version: 5.7.9-log MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql> \\s\n--------------\nmysql  Ver 14.14 Distrib 5.6.27, for linux-glibc2.5 (x86_64) using  EditLine wrapper\n\nConnection id:          12\nCurrent database:\nCurrent user:           burn_x509@192.168.115.223\nSSL:                    Cipher in use is DHE-RSA-AES256-SHA # 使用加密方式登录，且通过证书，因为这个用户 require X509\nCurrent pager:          stdout\nUsing outfile:          ''\nUsing delimiter:        ;\nServer version:         5.7.9-log MySQL Community Server (GPL)\nProtocol version:       10\nConnection:             172.18.14.68 via TCP/IP\nServer characterset:    utf8mb4\nDb     characterset:    utf8mb4\nClient characterset:    utf8\nConn.  characterset:    utf8\nTCP port:               3307\nUptime:                 32 min 15 sec\n\nThreads: 2  Questions: 41  Slow queries: 0  Opens: 114  Flush tables: 1  Open tables: 107  Queries per second avg: 0.021\n--------------\n```","tags":["mysql"],"categories":["mysql"]},{"title":"触发器下/存储过程/自定义函数","url":"/2019-10-01/mysql/MySQL学习笔记(Day014)/","content":"\nMySQL学习笔记（Day014：触发器下/存储过程/自定义函数）\n=========================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 作业讲解\n* **查询employees表中非基层用户的最近详细信息**\n\n    >**关于`Group By`在《SQL必知必会》中提及的部分规定：**\n    1. `GROUP BY`子句中列出的每一列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在SELECT中使用表达式，则必须在`GROUP BY`子句中指定相同的表达式。不能使用别名。\n    2. 除聚集计算语句外，`SELECT语句中的每一列都必须在GROUP BY子句中给出`。\n    \n```sql\nSELECT \n    e.emp_no,\n    CONCAT(last_name, ' ', first_name) AS name,\n    t.title,\n    dp.dept_name,\n    s.salary\nFROM\n    employees e\n        LEFT JOIN\n    dept_manager d ON e.emp_no = d.emp_no\n        LEFT JOIN\n    (SELECT \n        emp_no, title, from_date, to_date\n    FROM\n        titles\n    WHERE\n        (emp_no , from_date, to_date) IN (SELECT \n                emp_no, MAX(from_date), MAX(to_date)\n            FROM\n                titles AS b\n            GROUP BY b.emp_no)) t ON t.emp_no = e.emp_no\n        LEFT JOIN\n    (SELECT \n        dept_no, emp_no, from_date, to_date\n    FROM\n        dept_emp\n    WHERE\n        (emp_no , from_date, to_date) IN (SELECT \n                emp_no, MAX(from_date), MAX(to_date)\n            FROM\n                dept_emp AS b\n            GROUP BY b.emp_no)) de ON de.emp_no = e.emp_no\n        LEFT JOIN\n    (SELECT \n        emp_no, salary, from_date, to_date\n    FROM\n        salaries\n    WHERE\n        (emp_no , from_date, to_date) IN (SELECT \n                emp_no, MAX(from_date), MAX(to_date)\n            FROM\n                salaries AS b\n            GROUP BY b.emp_no)) s ON s.emp_no = e.emp_no\n        LEFT JOIN\n    departments dp ON dp.dept_no = de.dept_no\nWHERE\n    d.emp_no IS NULL;\n```\n\n```sql\n--\n-- 改进的子查询语句 - 1\n--\nSELECT \n    emp_no, title, from_date, to_date\nFROM\n    titles\n        WHERE\n        (emp_no , from_date, to_date) IN \n            (\n                SELECT \n                    emp_no, MAX(from_date), MAX(to_date)  -- 因为数据本身的问题，这里from_date和to_date都要\n                FROM\n                    titles AS b\n                    GROUP BY b.emp_no\n            ) -- 这个子查询表示以emp_no分类，找到最大（最近）的from_date和to_date\n              -- 而where条件在这个最大的基础上，过滤出我们要的title。(salary同理)\n\n\n--\n-- 改进的子查询语句 - 2\n--\nSELECT \n    emp_no, title, from_date, to_date\nFROM\n    titles AS a\nWHERE\n    (from_date, to_date) = (SELECT \n            MAX(from_date), MAX(to_date)  -- 同样使用from_date和to_date\n        FROM\n            titles AS b\n        WHERE\n            a.emp_no = b.emp_no  -- 这个是一个关联子查询\n        GROUP BY b.emp_no);\n```\n\n* **Rank排名一条SQL语句**\n\n```sql\nmysql> select * from test_rank_2;\n+------+-------+\n| id   | score |\n+------+-------+\n|    1 |    10 |\n|    2 |    20 |\n|    3 |    30 |\n|    4 |    30 |\n|    5 |    40 |\n|    6 |    40 |\n+------+-------+\n6 rows in set (0.00 sec)\n\nmysql> select id, score,\n    -> case\n    ->     when @prev_value = score then @rank_count\n    ->     when @prev_value := score then @rank_count := @rank_count + 1\n    -> end as rank_column\n    -> from test_rank_2, (select @prev_value:=NULL, @rank_count:=0) as t  -- 和RowNumber思路一样，增加一个表\n    -> order by score desc;\n+------+-------+-------------+\n| id   | score | rank_column |\n+------+-------+-------------+\n|    5 |    40 | 1           |\n|    6 |    40 | 1           |\n|    3 |    30 | 2           |\n|    4 |    30 | 2           |\n|    2 |    20 | 3           |\n|    1 |    10 | 4           |\n+------+-------+-------------+\n6 rows in set (0.00 sec)\n```\n\n----\n\n## 二. 触发器 ・ 下\n### 1. 触发器总结\n* 触发器对性能有损耗，应当非常慎重使用；\n* 对于事物表，`触发器执行失败则整个语句回滚`；\n* Row格式主从复制，`触发器不会在从库上执行`； \n    - 因为从库复制的肯定是主库已经提交的数据，既然已经提交了说明触发器已经被触发过了，所以从库不会执行。\n* 使用触发器时应防止递归执行；\n\n    ```sql\n    delimiter //\n    create trigger trg_test\n        before update on 'test_trigger'\n        for each row\n    begin\n        update test_trigger set score=20 where name = old.name;  -- 又触发了update操作，循环触发了\n    end;//\n    ```\n\n### 2. 触发器模拟物化视图\n* **物化视图的概念**\n    - 不是基于基表的虚表\n    - 根据基表实际存在的实表\n    - 预先计算并保存耗时较多的SQL操作结果（如多表链接(join)或者group by等）\n\n* **模拟物化视图**\n\n```sql\nmysql> create table Orders  \n    -> (order_id int unsigned not null auto_increment,\n    -> product_name varchar(30) not null,\n    -> price decimal(8,2) not null,\n    -> amount smallint not null,\n    -> primary key(order_id));\nQuery OK, 0 rows affected (0.13 sec)  -- 创建Orders表\n\nmysql> insert into Orders values \n    -> (null, 'cpu', 135.5 ,1),\n    -> (null, 'memory', 48.2, 3),\n    -> (null, 'cpu', 125.6, 3),\n    -> (null, 'cpu', 105.3, 4);\nQuery OK, 4 rows affected (0.06 sec)  -- 插入测试数据\nRecords: 4  Duplicates: 0  Warnings: 0\n\nmysql> select * from  Orders;\n+----------+--------------+--------+--------+\n| order_id | product_name | price  | amount |\n+----------+--------------+--------+--------+\n|        1 | cpu          | 135.50 |      1 |\n|        2 | memory       |  48.20 |      3 |\n|        3 | cpu          | 125.60 |      3 |\n|        4 | cpu          | 105.30 |      4 |\n+----------+--------------+--------+--------+\n4 rows in set (0.00 sec)\n\n-- 建立一个模拟物化视图的表（即用这张表来模拟物化视图）\nmysql> create table Orders_MV \n    -> ( product_name varchar(30) not null,\n    ->  price_sum decimal(8,2) not null,\n    ->  amount_sum int not null,\n    ->  price_avg float not null,\n    ->  orders_cnt int not null,\n    ->  unique index (product_name));\nQuery OK, 0 rows affected (0.14 sec)\n\n-- 通过Orders表的数据，将测试数据初始化到Orders_MV表中\nmysql> insert into Orders_MV \n    ->     select product_name, sum(price), \n    ->            sum(amount), avg(price), count(*)\n    ->     from Orders\n    ->     group by product_name;\nQuery OK, 2 rows affected (0.07 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\nmysql> select * from Orders_MV;\n+--------------+-----------+------------+-----------+------------+\n| product_name | price_sum | amount_sum | price_avg | orders_cnt |\n+--------------+-----------+------------+-----------+------------+\n| cpu          |    366.40 |          8 |   122.133 |          3 |\n| memory       |     48.20 |          3 |      48.2 |          1 |\n+--------------+-----------+------------+-----------+------------+\n2 rows in set (0.00 sec)\n\n-- 在MySQL workbench中输入，比较方便\ndelimiter //\n\nCREATE TRIGGER tgr_Orders_insert -- 创建触发器为tgr_Orders_insert\n\tAFTER INSERT ON Orders  -- 触发器是INSERT类型的，且作用于Orders表\n\tFOR EACH ROW\nBEGIN\n\tSET @old_price_sum := 0;  -- 设置临时存放Orders_MV表(模拟物化视图)的字段的变量\n\tSET @old_amount_sum := 0;\n\tSET @old_price_avg := 0;\n\tSET @old_orders_cnt := 0;\n\tSELECT   -- select ... into ... 在更新Orders_MV之前，将Orders_MV中对应某个产品的信息写入临时变量 \n\t\tIFNULL(price_sum, 0),\n\t\tIFNULL(amount_sum, 0),\n\t\tIFNULL(price_avg, 0),\n\t\tIFNULL(orders_cnt, 0)\n\tFROM\n\t\tOrders_MV\n\tWHERE\n\t\tproduct_name = NEW.product_name INTO @old_price_sum , @old_amount_sum , @old_price_avg , @old_orders_cnt;\n\n\tSET @new_price_sum = @old_price_sum + NEW.price; -- 累加新的值\n\tSET @new_amount_sum = @old_amount_sum + NEW.amount;\n\tSET @new_orders_cnt = @old_orders_cnt + 1;\n\tSET @new_price_avg = @new_price_sum / @new_orders_cnt ;\n\t\n    REPLACE INTO Orders_MV   \n\t\t\tVALUES(NEW.product_name, @new_price_sum,\n\t\t\t\t   @new_amount_sum, @new_price_avg, @new_orders_cnt );\n   -- REPLACE 将对应的物品（唯一索引）的字段值替换new_xxx的值\nEND;//\n\ndelimiter ;\n\n\nmysql> insert into Orders values (null, 'ssd', 299, 3);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into Orders values (null, 'memory', 47.9, 5);\nQuery OK, 1 row affected (0.05 sec)\n\nmysql> select * from Orders_MV;\n+--------------+-----------+------------+-----------+------------+\n| product_name | price_sum | amount_sum | price_avg | orders_cnt |\n+--------------+-----------+------------+-----------+------------+\n| cpu          |    366.40 |          8 |   122.133 |          3 |\n| memory       |     96.10 |          8 |     48.05 |          2 | -- 数量自动增加了1，价格也发生了变化\n| ssd          |    299.00 |          3 |       299 |          1 | -- 新增加的ssd产品\n+--------------+-----------+------------+-----------+------------+\n3 rows in set (0.00 sec)\n\n\n--\n-- IFNULL MySQL内建函数的演示\n--\nmysql> select @test;\n+-------+\n| @test |\n+-------+\n| NULL  |  -- 当前会话中没有test变量\n+-------+\n1 row in set (0.00 sec)\n\nmysql> select ifnull(@test, 100);   -- 如果test为NULL，则ifnull返回100\n+--------------------+\n| ifnull(@test, 100) |\n+--------------------+\n| 100                |  -- ifnull函数return的值是100\n+--------------------+\n1 row in set (0.00 sec)\n\nmysql> select @test;\n+-------+\n| @test |\n+-------+\n| NULL  |  -- 但是test还是NULL\n+-------+\n1 row in set (0.00 sec)\n\nmysql> set @test:=200;  -- 给test变量赋值为200\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select ifnull(@test, 100);  -- 再次ifnull判断，此时test不为null，则返回test变量的值\n+--------------------+\n| ifnull(@test, 100) |\n+--------------------+\n|                200 |  -- test不为null。返回test的值200\n+--------------------+\n1 row in set (0.00 sec)\n\n--\n-- select into 用法\n--\nmysql> select @id_1;\n+-------+\n| @id_1 |\n+-------+\n|  NULL | -- 当前变量id_1为null \n+-------+\n1 row in set (0.00 sec)\n\nmysql> select @score_1;\n+----------+\n| @score_1 |\n+----------+\n|     NULL |  -- 当前变量score_1为null\n+----------+\n1 row in set (0.00 sec)\n\nmysql> select * from test_rank_2;\n+------+-------+\n| id   | score |\n+------+-------+\n|    1 |    10 |\n|    2 |    20 |\n|    3 |    30 |\n|    4 |    30 |\n|    5 |    40 |\n|    6 |    40 |\n+------+-------+\n6 rows in set (0.00 sec)\n\nmysql> select * from test_rank_2 \n    -> where id=1 into @id_1, @score_1;\n-- 选择id=1的记录，将对应的id和score赋值给变量 id_1 和 score_1\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> select @id_1;\n+-------+\n| @id_1 |\n+-------+\n|     1 |\n+-------+\n1 row in set (0.00 sec)\n\nmysql> select @score_1;\n+----------+\n| @score_1 |\n+----------+\n|       10 |\n+----------+\n1 row in set (0.00 sec)\n\n-- 触发器对性能会有影响，相当于在一个事物中插入了其他的事物\n```\n\n-----\n\n## 三. 存储过程\n### 1. 存储过程介绍\n* 存储在数据库端的一组SQL语句集；\n* 用户可以通过存储过程名和传参多次调用的程序模块；\n* 存储过程的特点：\n    - 使用灵活，可以使用流控语句、自定义变量等完成复杂的业务逻辑；\n    - 提高数据安全性，屏蔽应用程序直接对表的操作，易于进行审计；\n    - 减少网络传输；\n    - 提高代码维护的复杂度，实际使用需要结合业务评估；\n\n```sql\nCREATE\n    [DEFINER = { user | CURRENT_USER }]\n    PROCEDURE sp_name ([proc_parameter[,...]])\n    [characteristic ...] routine_body\n\n\nproc_parameter:  -- 注意，只有procedure才有in(传入),out(传出),inout(传入传出)参数，自定义函数（只有）默认就是 in。\n    [ IN | OUT | INOUT ] param_name type \n\n\ncharacteristic:\n    COMMENT 'string'\n  | LANGUAGE SQL\n  | [NOT] DETERMINISTIC\n  | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }\n  | SQL SECURITY { DEFINER | INVOKER }\n\nroutine_body:\n    Valid SQL routine statement\n\n\n-- 删除\nDROP PROCEDURE  procedure_name;\n```\n\n### 2. 存储过程举例与流程控制语句\n> [流程控制语句 官方文档](http://dev.mysql.com/doc/refman/5.7/en/flow-control-statements.html)\n\n \n```sql\n--\n-- IF\n--\n-- 语法\nIF search_condition THEN statement_list\n    [ELSEIF search_condition THEN statement_list] ...\n    [ELSE statement_list]\nEND IF\n\n-- 例子\nmysql> delimiter //\nmysql> create procedure pcd_test_1 (in param_a int) -- 创建一个\n    -> begin\n    -> declare a int;  -- delcare声明了该变量的作用域在该procedure中\n    ->     if param_a > 10 then set a:=11;        \n    ->         elseif param_a = 10 then set a:=10;\n    ->         else set a:=9;\n    ->     end if;\n    -> end;//\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> select @a;  -- 查看当前会话中变量a的值\n+------+\n| @a   |\n+------+\n| NULL |  -- 当前会话中a为NULL\n+------+\n1 row in set (0.00 sec)\n\nmysql> call pcd_test_1(1);\n+------+\n| a    |\n+------+\n|    9 |\n+------+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> call pcd_test_1(10);\n+------+\n| a    |\n+------+\n|   10 |\n+------+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> call pcd_test_1(20);\n+------+\n| a    |\n+------+\n|   11 |\n+------+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @a;\n+------+\n| @a   |\n+------+\n| NULL | -- 使用了declare，使得procedure中a的作用域限制在了procedure内\n+------+\n1 row in set (0.00 sec)\n\n--\n-- CASE WHEN\n--\n-- CASE WHEN 语法\nCASE case_value\n    WHEN when_value THEN statement_list\n    [WHEN when_value THEN statement_list] ...\n    [ELSE statement_list]\nEND CASE\n-- 或者是\nCASE\n    WHEN search_condition THEN statement_list\n    [WHEN search_condition THEN statement_list] ...\n    [ELSE statement_list]\nEND CASE\n\n--\n-- CASE WHEN 例子\n--\nmysql> delimiter //\nmysql> \nmysql> create procedure pcd_test_2(in param_1 int)\n    ->   begin\n    ->     case param_1  \n    -- 当case后面有value时，该value会和when中的when_value进行\"=\"判断\n    -- 相等则执行then后面的语句，然后跳出；否则就进行下一次when的匹配\n    ->       when 2 then select 200;\n    ->       when 3 then select 300;\n    ->       else\n    ->         begin  \n                    -- 当没有匹配时，且else中没有要执行的语句\n                    -- 则给一个begin/end的空语句；\n                    -- 或者不写else语句；\n    ->         end;\n    ->     end case;\n    -> end;//\nQuery OK, 0 rows affected (0.03 sec)\n\nmysql> delimiter ;\nmysql> call pcd_test_2(1);\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> call pcd_test_2(2);\n+-----+\n| 200 |\n+-----+\n| 200 |\n+-----+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> call pcd_test_2(3);\n+-----+\n| 300 |\n+-----+\n| 300 |\n+-----+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\n-- 另外一种SQL语法请参考rank排名作业；注意when后跟的是condition\n\n--\n-- WHILE  循环\n--\n-- WHILE 语法\n[begin_label:] WHILE search_condition DO\n    statement_list\nEND WHILE [end_label]\n\n-- WHILE举例\nmysql> delimiter //\nmysql> \nmysql> create procedure pcd_test_3(in param_1 int)\n    -> begin\n    ->     declare a int default 1;\n    ->     while param_1 > 10 do\n    ->         set param_1 = param_1 - 1;\n    ->         set a = a + 1;\n    ->     end while;\n    ->     select a;\n    -> end;//\n\nQuery OK, 0 rows affected (0.01 sec)\nmysql> delimiter ;\n\nmysql> call pcd_test_3(15); -- 15 - 10 = 5；需要5次循环\n+------+\n| a    |\n+------+\n|    6 |  -- a + 5 = 6\n+------+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\n\n--\n-- REPEAT   循环\n--\n-- REPEAT 语法\n[begin_label:] REPEAT\n    statement_list\nUNTIL search_condition\nEND REPEAT [end_label]\n\nmysql> delimiter //\nmysql> create procedure pcd_test_4(in param_1 int)\n    -> begin\n    ->     SET @x = 0;  -- 没有使用declare，所以x是会话级别的\n    ->     REPEAT\n    ->         SET @x = @x + 1;\n    ->     UNTIL @x > param_1 END REPEAT;\n    -> end;//\nQuery OK, 0 rows affected (0.01 sec)\nmysql> delimiter ;\n\nmysql> call pcd_test_4(10);\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @x; -- x是会话级别的\n+------+\n| @x   |\n+------+\n|   11 |  -- 一共循环11次(10>10 为False，11 > 10为True，才跳出)\n+------+\n1 row in set (0.00 sec)\n\n--\n-- loop 循环\n--\n-- loop语法\n[begin_label:] LOOP\n    statement_list\nEND LOOP [end_label]\n\n-- ITERATE 和label相结合，表示继续从label处执行\n-- LEAVE   和label相结合，表示从label 标记的代码段离开\n\n-- loop 例子\nmysql> delimiter //\nmysql> create procedure pcd_test_5(in param_1 int)\n    -> begin\n    ->     test_label: loop\n    ->         set param_1 := param_1 + 1; -- 参数累加\n    ->         if param_1 < 10 then        -- 如果累加的值小于10\n    ->             iterate test_label;     -- 继续执行 标签 test_label\n    ->         end if;\n    ->         leave test_label;  -- 如果>=10则离开这个test_label(loop)\n    ->     end loop test_label; \n    ->     set @x = param_1; -- 设置会话级别的变量 \n    -> end;//\n\nQuery OK, 0 rows affected (0.02 sec)\nmysql> delimiter ;\n\nmysql> call pcd_test_5(5); -- 5<10 ，累加5次后>=10为true，离开循环\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @x;\n+------+\n| @x   |\n+------+\n|   10 |  -- 累加到10的 param_1 赋值给 x, 即为10\n+------+\n1 row in set (0.00 sec)\n\n\n-- 老师给出的例子， 阶乘\nmysql> create table test_proc_1(a int, b int); -- 给一个存放数据的表\nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> delimiter //\nmysql> create procedure proc_test1(in total int, out res int)\n    -> begin\n    ->     declare i int;\n    ->     set i := 1;\n    ->     set res := 1;\n    ->     if total <= 0 then\n    ->         set total := 1;\n    ->     end if;\n    ->     while i <= total do\n    ->         set res := res * i;\n    ->         insert into test_proc_1 values(i, res);\n    ->         set i := i + 1;\n    ->     end while;\n    -> end;//\nQuery OK, 0 rows affected (0.01 sec)\nmysql> delimiter ;\n\nmysql> set @res_value := 0;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> call proc_test1(5, @res_value); -- 因为res是out变量，要预先有这个变量，这里上面设置了res_value(实参和形参不必同名)\nQuery OK, 1 row affected (0.15 sec)\n\nmysql> select @res_value;\n+------------+\n| @res_value |\n+------------+\n|        120 | -- 5的阶乘的结果是120\n+------------+\n1 row in set (0.00 sec)\n\nmysql> select * from test_proc_1;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    1 |\n|    2 |    2 |\n|    3 |    6 |\n|    4 |   24 |\n|    5 |  120 |  -- 每次insert的结果\n+------+------+\n5 rows in set (0.00 sec)\n```\n\n-----\n\n## 三. 自定义函数\n* 自定义函数和存储过程很类似，但是必须要有返回值；\n* 与内置的函数(sum(), max()等)使用方法类似\n    - select fun(val);\n    - select * from t where  col= fun(val);\n* 自定义函数可能在遍历每条记录中使用；\n\n```sql\nCREATE\n    [DEFINER = { user | CURRENT_USER }]\n    FUNCTION sp_name ([func_parameter[,...]])\n    RETURNS type  -- 必须有返回值\n    [characteristic ...] routine_body\n    \nfunc_parameter:\n    param_name type\n\ntype:\n    Any valid MySQL data type\n\ncharacteristic:\n    COMMENT 'string'\n  | LANGUAGE SQL\n  | [NOT] DETERMINISTIC\n  | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA }\n  | SQL SECURITY { DEFINER | INVOKER }\n\nroutine_body:\n    Valid SQL routine statement\n    \n-- 删除\nDROP FUNCTION fun_name;\n```\n\n```sql\n-- 老师给的例子，还是阶乘，用自定义函数的方式\n\nmysql> delimiter //\nmysql> \nmysql> create function fun_test_1(total int)\n    -> returns int\n    -> begin\n    ->     declare i int;\n    ->     declare res int;\n    ->     set i := 1;\n    ->     set res := 1;\n    ->     if total <= 0 then\n    ->         set total := 1;\n    -> end if;\n    ->     while i <= total do\n    ->         set res := res * i;\n    ->         set i := i + 1;\n    ->     end while;\n    ->     return res;\n    -> end;//\nERROR 1418 (HY000): This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators variable)\n-- 报错，提示因为函的声明中没有\"DETERMINISTIC, NO SQL, or READS SQL DATA\"等关键字 ，需要使用打开参数 log_bin_trust_function_creators\n\n-- 解决方法，set global log_bin_trust_function_creators=1; 开启该选项可能会引起主从服务器不一致\n--           或者 增加 上述相应功能的关键字\n\n\n-- 使用 deterministic 关键字\n-- 当你声明一个函数的返回是确定性的，则必须显示的使用deterministic关键字，默认是 no deterministic的\nmysql> delimiter //\nmysql> create function fun_test_1(total int)\n    -> returns int deterministic -- 这个只是告诉MySQL我这个函数是否会改变数据\n                                 -- 即使我下面使用了insert，update等DML语句，MySQL不会检查\n                                 -- 函数是否会改变数据，完全依赖创建函数的用户去指定的关键字\n                                 -- 而非真的是否有修改数据\n                                 -- 只是声明，而非约束\n    -> begin\n    ->     declare i int;\n    ->     declare res int;\n    ->     set i := 1;\n    ->     set res := 1;\n    ->     if total <= 0 then\n    ->         set total := 1;\n    ->     end if;\n    ->     while i <= total do\n    ->         set res := res * i;\n    ->         insert into test_proc_1 values(i, res);  -- 在自定义函数中，同样可以使用sql\n                                                        -- 并且该SQL是insert，其实和deterministic违背。\n    ->         set i := i + 1;\n    ->     end while;\n    ->     return res;\n    -> end;//\nQuery OK, 0 rows affected (0.01 sec)\nmysql> delimiter ;\n\nmysql> truncate table test_proc_1;\nQuery OK, 0 rows affected (0.11 sec)\n\nmysql> select fun_test_1(6);  -- return了6的阶乘，720\n+---------------+\n| fun_test_1(6) |\n+---------------+\n|           720 |\n+---------------+\n1 row in set (0.02 sec)\n\nmysql> select * from test_proc_1; \n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    1 |\n|    2 |    2 |\n|    3 |    6 |\n|    4 |   24 |\n|    5 |  120 |\n|    6 |  720 |  -- 使用了insert语句进行插入阶乘的历史记录\n+------+------+\n6 rows in set (0.00 sec)\n\n-- 关键字简单说明\n-- DETERMINISTIC ： 当给定相同的输入，产生确定的结果\n-- NOT DETERMINISTIC ： 默认值，认为产生的结果是不确定的\n\n-- READS SQL DATA  ： 只是读取SQL数据\n-- MODIFIES SQL DATA ： 会修改数据\n-- NO SQL ： 没有SQL遇见\n-- CONTAINS SQL ： 包含SQL语句，但是没有读写语句，理论有select now()等\n\n```\n[原文链接](https://dev.mysql.com/doc/refman/5.7/en/stored-programs-logging.html)\n**部分原文：**\n*By default, for a CREATE FUNCTION statement to be accepted, at least one of DETERMINISTIC, NO SQL, or READS SQL DATA must be specified explicitly. Otherwise an error occurs:*\n默认情况下，在创建自定义函数时，必须显示的指定关键字`DETERMINISTIC, NO SQL, 或者是 READS SQL DATA`中的至少一个(可以多个)，否则就会有如下错误\n\n>ERROR 1418 (HY000): This function has none of DETERMINISTIC, NO SQL,or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators\nvariable)\n\n\n[网上参考资料1](http://stackoverflow.com/questions/26015160/deterministic-no-sql-or-reads-sql-data-in-its-declaration-and-binary-logging-i)\n[网上参考资料2](http://blog.csdn.net/melody_mr/article/details/43529719)","tags":["mysql"],"categories":["mysql"]},{"title":"磁盘测试","url":"/2019-10-01/mysql/MySQL学习笔记(Day019)/","content":"\nMySQL学习笔记（Day019：磁盘测试）\n=============================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 磁盘调度算法介绍\n### 1. CFQ\n>CFQ把I/O请求`按照进程`分别放入进程对应的队列中，所以A进程和B进程发出的I/O请求会在两个队列中。而各个队列内部仍然采用`合并和排序`的方法，区别仅在于，每一个提交I/O请求的进程都有自己的I/O队列。\n>CFQ的“公平”是针对进程而言的，它以时间片算法为前提，轮转调度队列，默认从当前队列中取4个请求处理，然后处理下一个队列的4个请求。这样就可以确保每个进程享有的I/O资源是均衡的。\n>CFQ的缺点是先来的IO请求不一定能被及时满足，可能出现`饥饿`的情况。\n[CFQ Wiki](https://en.wikipedia.org/wiki/CFQ)\n\n### 2. Deadline\n>同CFQ一样，除了维护一个拥有合并和排序功能的请求队列以外，还额外维护了两个队列，分别是`读请求队列`和`写请求队列`，它们都是`带有超时的FIFO队列`。当新来一个I/O请求时，会被同时插入普通队列和读/写队列，然后处理普通队列中的请求。当调度器发现读/写请求队列中的请求超时的时候，会优先处理这些请求，保证尽可能不产生请求饥饿\n>在DeadLine算法中，每个I/O请求都有一个超时时间，默认`读请求是500ms`，`写请求是5s`。\n[Deadline Wiki](https://en.wikipedia.org/wiki/Deadline_scheduler)\n\n### 3. Noop\n>Noop做的事情非常简单，它不会对I/O请求排序也不会进行任何其它优化（除了合并）。Noop除了对请求合并以外，不再进行任何处理，直接以类似FIFO的顺序提交I/O请求。\n>Noop面向的不是普通的块设备，而是随机访问设备（例如SSD），对于这种设备，不存在传统的寻道时间，那么就没有必要去做那些多余的为了减少寻道时间而采取的事情了。\n[Noop Wiki](https://en.wikipedia.org/wiki/Noop_scheduler)\n\n-----\n\n## 二. iostat（下）\n* **rrqm/s 和 wrqm/s**\n    - `Merge`将若干个连续地址的IO请求进行合并。来提高IO的效率\n    - `rrqm/s`是每秒读（read）请求合并的次数\n    - `wrqm/s`是每秒写（write）请求合并的次数\n\n* **r/s和w/s**\n    - 在`合并之后（after merge）` IO请求的次数\n    - `r/s`合并之后每秒读IO的次数\n    - `w/s`合并之后每秒写IO的次数\n    - **`r/s + w/s = IOPS`**\n \n* **rsec/s（rKB/s、rMB/s）和 wsec/s（wKB/s、wMB/s）**\n    - `sec`是`Sector（扇区）`，为`512Byte`\n    - `KB` 和 `MB` 是通过扇区的`512Byte`进行的换算\n\n* **avgrq-sz**\n    - 一块磁盘可能存储数据的同时还存储日志，所以请求的IO大小是不一样的\n    - 该参数就是平均的请求数，注意，该值需要 * 512Byte 才是最终的结果，因为该值是以扇区为单位的\n\n* **avgqu-sz**\n    - 请求的IO队列的平均长度`（比较重要）`\n    - HDD可能在4左右，SSD可以达到30左右\n\n* **await、r_await、w_await**\n    - IO请求平均等待的时间，单位是ms\n    - `r_await`和`w_await`分别对应`读IO请求的等待`和`写IO请求的等待`\n    \n* **svctm**\n    - 服务于IO请求的平均时间\n    - man文档中提示不要相信该值，以后会被移除\n\n* **%util**\n    - 磁盘是否空闲；不能简单的等同于IO的使用率；该值可以解释为磁盘是否繁忙\n    - 如果该值100% 不能简单的等同于磁盘的负载满了，达到了瓶颈\n    - 需要综合`avgqu-sz`、`await`等其他指标进行综合判断磁盘是否达到瓶颈\n\n-----\n\n## 三. MySQL的IO使用情况\n### 1. iotop\n```bash\nshell> iotop -u mysql  # -u 表示监控哪个user的进程，所以前提是你的mysql服务是用mysql用户启动的 \n```\n>**注意：**\n>上述命令只能看到MySQL的线程ID（Thread ID）\n\n\n### 2. performance_schema.threads\n\n```sql\nmysql> use performance_schema;\nDatabase changed\n\nmysql> desc threads;\n+---------------------+---------------------+------+-----+---------+-------+\n| Field               | Type                | Null | Key | Default | Extra |\n+---------------------+---------------------+------+-----+---------+-------+\n| THREAD_ID           | bigint(20) unsigned | NO   |     | NULL    |       | -- MySQL内部线程ID\n| NAME                | varchar(128)        | NO   |     | NULL    |       |\n| TYPE                | varchar(10)         | NO   |     | NULL    |       |\n| PROCESSLIST_ID      | bigint(20) unsigned | YES  |     | NULL    |       |\n| PROCESSLIST_USER    | varchar(32)         | YES  |     | NULL    |       |\n| PROCESSLIST_HOST    | varchar(60)         | YES  |     | NULL    |       |\n| PROCESSLIST_DB      | varchar(64)         | YES  |     | NULL    |       |\n| PROCESSLIST_COMMAND | varchar(16)         | YES  |     | NULL    |       |\n| PROCESSLIST_TIME    | bigint(20)          | YES  |     | NULL    |       |\n| PROCESSLIST_STATE   | varchar(64)         | YES  |     | NULL    |       |\n| PROCESSLIST_INFO    | longtext            | YES  |     | NULL    |       |\n| PARENT_THREAD_ID    | bigint(20) unsigned | YES  |     | NULL    |       |\n| ROLE                | varchar(64)         | YES  |     | NULL    |       |\n| INSTRUMENTED        | enum('YES','NO')    | NO   |     | NULL    |       |\n| HISTORY             | enum('YES','NO')    | NO   |     | NULL    |       |\n| CONNECTION_TYPE     | varchar(16)         | YES  |     | NULL    |       |\n| THREAD_OS_ID        | bigint(20) unsigned | YES  |     | NULL    |       | -- 操作系统的线程ID\n+---------------------+---------------------+------+-----+---------+-------+\n17 rows in set (0.00 sec)\n\nmysql> select name,type,thread_id,thread_os_id from threads;\n+----------------------------------------+------------+-----------+--------------+\n| name                                   | type       | thread_id | thread_os_id |\n+----------------------------------------+------------+-----------+--------------+\n| thread/sql/main                        | BACKGROUND |         1 |         2481 |\n| thread/sql/thread_timer_notifier       | BACKGROUND |         2 |         2482 |\n| thread/innodb/io_read_thread           | BACKGROUND |         3 |         2486 |\n| thread/innodb/io_read_thread           | BACKGROUND |         4 |         2487 |\n| thread/innodb/io_read_thread           | BACKGROUND |         5 |         2488 |\n| thread/innodb/io_write_thread          | BACKGROUND |         6 |         2489 |\n| thread/innodb/io_write_thread          | BACKGROUND |         7 |         2490 |\n| thread/innodb/io_write_thread          | BACKGROUND |         8 |         2491 |\n| thread/innodb/io_write_thread          | BACKGROUND |         9 |         2492 |\n| thread/innodb/page_cleaner_thread      | BACKGROUND |        10 |         2493 |\n| thread/innodb/io_read_thread           | BACKGROUND |        11 |         2485 |\n| thread/innodb/io_log_thread            | BACKGROUND |        12 |         2484 |\n| thread/innodb/io_ibuf_thread           | BACKGROUND |        13 |         2483 |\n| thread/innodb/srv_master_thread        | BACKGROUND |        15 |         2501 |  -- 主线程\n| thread/sql/background                  | BACKGROUND |        16 |         2502 |\n| thread/innodb/srv_purge_thread         | BACKGROUND |        17 |         2502 |\n| thread/sql/background                  | BACKGROUND |        18 |         2503 |\n| thread/innodb/srv_monitor_thread       | BACKGROUND |        19 |         2500 |\n| thread/innodb/srv_error_monitor_thread | BACKGROUND |        20 |         2499 |\n| thread/sql/background                  | BACKGROUND |        21 |         2504 |\n| thread/sql/background                  | BACKGROUND |        22 |         2505 |\n| thread/innodb/srv_lock_timeout_thread  | BACKGROUND |        23 |         2498 |\n| thread/innodb/dict_stats_thread        | BACKGROUND |        24 |         2507 |\n| thread/innodb/buf_dump_thread          | BACKGROUND |        25 |         2506 |\n| thread/sql/signal_handler              | BACKGROUND |        26 |         2510 |\n| thread/sql/compress_gtid_table         | FOREGROUND |        27 |         2511 |\n| thread/sql/one_connection              | FOREGROUND |        28 |         2514 |  -- FOREGROUND前台线程\n+----------------------------------------+------------+-----------+--------------+\n27 rows in set (0.00 sec)\n\n-- thread/sql/one_connection 就是我连接的线程\n\nmysql> select name,thread_id,thread_os_id,processlist_id from threads;  -- 查看processlist_id\n+----------------------------------------+-----------+--------------+----------------+\n| name                                   | thread_id | thread_os_id | processlist_id |\n+----------------------------------------+-----------+--------------+----------------+\n| thread/sql/main                        |         1 |         2481 |           NULL |\n| thread/sql/thread_timer_notifier       |         2 |         2482 |           NULL |\n| thread/innodb/io_read_thread           |         3 |         2486 |           NULL |\n| thread/innodb/io_read_thread           |         4 |         2487 |           NULL |\n| thread/innodb/io_read_thread           |         5 |         2488 |           NULL |\n| thread/innodb/io_write_thread          |         6 |         2489 |           NULL |\n| thread/innodb/io_write_thread          |         7 |         2490 |           NULL |\n| thread/innodb/io_write_thread          |         8 |         2491 |           NULL |\n| thread/innodb/io_write_thread          |         9 |         2492 |           NULL |\n| thread/innodb/page_cleaner_thread      |        10 |         2493 |           NULL |\n| thread/innodb/io_read_thread           |        11 |         2485 |           NULL |\n| thread/innodb/io_log_thread            |        12 |         2484 |           NULL |\n| thread/innodb/io_ibuf_thread           |        13 |         2483 |           NULL |\n| thread/innodb/srv_master_thread        |        15 |         2501 |           NULL |\n| thread/sql/background                  |        16 |         2502 |           NULL |\n| thread/innodb/srv_purge_thread         |        17 |         2502 |           NULL |\n| thread/sql/background                  |        18 |         2503 |           NULL |\n| thread/innodb/srv_monitor_thread       |        19 |         2500 |           NULL |\n| thread/innodb/srv_error_monitor_thread |        20 |         2499 |           NULL |\n| thread/sql/background                  |        21 |         2504 |           NULL |\n| thread/sql/background                  |        22 |         2505 |           NULL |\n| thread/innodb/srv_lock_timeout_thread  |        23 |         2498 |           NULL |\n| thread/innodb/dict_stats_thread        |        24 |         2507 |           NULL |\n| thread/innodb/buf_dump_thread          |        25 |         2506 |           NULL |\n| thread/sql/signal_handler              |        26 |         2510 |           NULL |\n| thread/sql/compress_gtid_table         |        27 |         2511 |              1 |\n| thread/sql/one_connection              |        28 |         2514 |              2 |\n+----------------------------------------+-----------+--------------+----------------+\n27 rows in set (0.00 sec)\n\n-- processlist_id 对应的就是 show processlist中的id\nmysql> show processlist; \n+----+------+-----------+--------------------+---------+------+----------+------------------+\n| Id | User | Host      | db                 | Command | Time | State    | Info             |\n+----+------+-----------+--------------------+---------+------+----------+------------------+\n|  2 | root | localhost | performance_schema | Query   |    0 | starting | show processlist |\n+----+------+-----------+--------------------+---------+------+----------+------------------+\n1 row in set (0.00 sec)\n\nmysql> select connection_id();   -- 查看当前connection的id\n+-----------------+\n| connection_id() |\n+-----------------+\n|               2 |\n+-----------------+\n1 row in set (0.00 sec)\n```\n\n>**通过`threads表`中的信息，结合`iotop -u mysql`的输出，就可以知道某个线程的io使用情况**\n\n>MySQL 5.6 版本中没有`thread_os_id`这个列。\n>作业一：如何将iotop中的Thread ID和MySQL5.6中的threads表中的信息对应起来。\n\n\n### 3. 存储结构对应关系\n\n```bash\n                  +-------------+-------------+-------------+\n      Database    |     16K     |     16K     |     16K     |\n                  +------+------+-------------+-------------+\n                         |\n+------------------------------------------------------------------------+\n                         |\n                         +------+\n                                |\n                  +------+------v------+------+\n     Filesystem   |  4K  |  4K  |  4K  |  4K  |\n                  +---+--+------+------+------+\n                      |\n+------------------------------------------------------------------------+\n                      |\n                      +--+\n                         |\n                         v\n                  +------+------+          +------+\n        Disk      | 512B | 512B | ...  ... | 512B |\n                  +------+------+          +------+\n```\n>**SSD扇区的大小一般为4K或者8K。但是为了兼容HDD，SSD通过Flash Translation Layer (FTL)的方式转换成512B**\n\n\n### 4. O_DIRECT\n* **fwrite / fsync**\n    - `fwrite`是把数据写入文件系统层（Filesystem）（可能有cache），并不能保证写入Disk\n    - `fsync`可以保证把数据写入到Disk（数据落盘）\n\n    > 只通过`fwrite`写入数据特别快（因为有缓存），但随后调用`fsync`就会很慢，这个速度取决于磁盘的`IOPS`\n    > 如果不手工执行`fysnc`，当Filesystem的`cache`小于`10%`时，操作系统才会将数据刷入磁盘。所以可能存在数据丢失的风险，比如掉电\n\n* **O_DIRECT**\n\n    ```bash\n    +-------------------+               +-------------------+              +-------------------+\n    |                   |    fwrite     |                   |    fsync     |                   |\n    |    Buffer Pool    +---------------> Filesystem Cache  +-------------->       Disk        |\n    |                   |               |                   |              |                   |\n    +--------+----------+               +-------------------+              +---------+---------+\n             |                                                                       ^\n             |                                                                       |\n             |                   innodb_flush_method = O_DIRECT                      |\n             +-----------------------------------------------------------------------+\n    ```\n    \n    > **`O_DIRECT`的设置参数是告诉系统`直接将数据写入磁盘`，跳过文件系统的缓存。等同于使用`裸设备`的效果**\n\n\n-----\n\n\n## 四. sysbench\n### 1. 安装\n建议安装`sysbenh-0.5`的版本\n```bash\nshell> https://github.com/akopytov/sysbench.git # 通过git clone得到源码\nshell> cd sysbench\nshell> ./autogen.sh\nshell> ./configure  --with-mysql-includes=/usr/local/mysql56/include/  --with-mysql-libs=/usr/local/mysql56/lib/  # 关联mysql的头文件和库\n##\n## 注意，如果我这里使用mysql5.7.9 的include和lib ，提示我 /usr/bin/ld: cannot find -lmysqlclient_r\n##\n\nshell> make -j 2 # -j 2 表示用几个cpu核心进行编译\nshell> make install # 默认安装到 /usr/local/bin , 如果有自定义目录，configure增加参数 --prefix=自定义目录\nshell> echo \"export LD_LIBRARY_PATH=/usr/local/mysql56/lib/:$LD_LIBRARY_PATH\" >> ~/.bashrc # 添加LD_LIBRARY_PATH\nshell> source ~/.bashrc\nshell> sysbench --version\nsysbench 0.5\n```\n\n### 3. 测试\n```bash\n#\n# 生成测试文件\n#\nshell> sysbench --test=fileio \\               # File IO测试\n                --file-num=4 \\                # 测试文件数是4个\n                --file-block-size=8K \\        # block size是8K\n                --file-total-size=1G \\        # 4个文件的总大小是1G\n                --file-test-mode=rndrd \\       # 测试方法是随机读\n                --file-extra-flags=direct \\   # direct io，跳过缓存\n                --max-requests=0 \\            # 一共发起多少请求，0表示任意\n                --max-time=3600 \\             # 测试3600s \n                --num-threads=4 \\             # 使用4个线程\n                prepare #  run or cleanup     # prepare：生成文件\n                                              # run：开始测试\n                                              # cleanup：删除测试文件\n\n## 其他说明  sysbench --test=fileio help\n# --file-num=N                  创建文件数\n  # --file-block-size=N           block size大小\n  # --file-total-size=SIZE        文件数的大小总和\n  # --file-test-mode=STRING       测试模式 {seqwr, seqrewr, seqrd, rndrd, rndwr, rndrw} （顺序写，顺序读写，顺序读，随机读，随机写，随机读写）\n  # --file-io-mode=STRING         文件操作方式 {sync,async,mmap} \n  # --file-extra-flags=STRING     打开文件的额外标志 {sync,dsync,direct} []\n  # --file-fsync-freq=N           多少请求后执行fsync。默认是0，不执行\n  # --file-fsync-all=[on|off]     是否每次操作后都执行fsync\n  # --file-fsync-end=[on|off]     测完成后执行fsync，默认是on\n  # --file-fsync-mode=STRING      同步的方法 {fsync, fdatasync}默认是 [fsync]\n  # --file-merged-requests=N      最多多少IO请求被合并，默认为0，不合并\n  # --file-rw-ratio=N             读写比例默认是 [1.5]，即 3:2\n                                            \n```\n\n```bash\n#\n# 开始测试\n#\nshell> sysbench --test=fileio \\               \n                --file-num=4 \\               \n                --file-block-size=8K \\       \n                --file-total-size=1G \\        \n                --file-test-mode=rndrd \\    \n                --file-extra-flags=direct \\   \n                --max-requests=0 \\            \n                --max-time=30 \\             # 简单测试，测试30秒\n                --num-threads=4 \\             \n                --report-interval=3 \\       # 每3秒产生报告\n                run\nsysbench 0.5:  multi-threaded system evaluation benchmark\n\nRunning the test with following options:\nNumber of threads: 4\nReport intermediate results every 3 second(s)\nRandom number generator seed is 0 and will be ignored\n\n\nExtra file open flags: 3\n4 files, 256Mb each\n1Gb total file size\nBlock size 8Kb\nNumber of IO requests: 0\nRead/Write ratio for combined random IO test: 1.50\nPeriodic FSYNC enabled, calling fsync() each 100 requests.\nCalling fsync() at the end of test, Enabled.\nUsing synchronous I/O mode\nDoing random read test\nThreads started!\n\n[   3s] reads: 1.70 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 54.416ms (95%)\n[   6s] reads: 1.78 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 55.469ms (95%)\n[   9s] reads: 1.75 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 55.253ms (95%)\n[  12s] reads: 1.66 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 52.120ms (95%)\n[  15s] reads: 1.76 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 51.840ms (95%)\n[  18s] reads: 1.79 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 50.933ms (95%)\n[  21s] reads: 1.78 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 54.858ms (95%)\n[  24s] reads: 1.88 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 50.857ms (95%)\n[  27s] reads: 1.75 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 56.238ms (95%)\n[  30s] reads: 1.61 MB/s writes: 0.00 MB/s fsyncs: 0.00/s response time: 64.097ms (95%)\nOperations performed:  6709 reads, 0 writes, 0 Other = 6709 Total\nRead 52.414Mb  Written 0b  Total transferred 52.414Mb  (1.7462Mb/sec)\n  223.51 Requests/sec executed  # 这个就是IOPS\n\nGeneral statistics:\n    total time:                          30.0160s\n    total number of events:              6709\n    total time taken by event execution: 120.0223s\n    response time:\n         min:                                  0.13ms\n         avg:                                 17.89ms\n         max:                                254.62ms\n         approx.  95 percentile:              54.97ms\n\nThreads fairness:\n    events (avg/stddev):           1677.2500/28.16\n    execution time (avg/stddev):   30.0056/0.01\n\n##\n## 上述测试随机读的速度在1.7MB/s左右， \n## （1.7MB/s * 1024 / 8KB =217）换算后得到的值就是IOPS，约等于上面的223。\n##\n\n```\n> 测试完成后执行`cleanup`\n> **如果是真实的测试 `max-time`设置成一周的时间**\n> `run`期间可以使用`iotop`或者`iostat`进行观察","tags":["mysql"],"categories":["mysql"]},{"title":"子查询/INSERT/UPDATE/DELETE/REPLACE","url":"/2019-10-01/mysql/MySQL学习笔记(Day012)/","content":"\nMySQL学习笔记（Day012：子查询/INSERT/UPDATE/DELETE/REPLACE）\n=====================================================\n@(MySQL学习)\n\n[TOC]\n\n\n## 一. 子查询\n>子查询就是指在一个select语句中嵌套另一个select语句。同时，子查询必须包含括号。\n`MySQL 5.6.x` 版本之前，MySQL的子查询性能较差，但是从5.6开始，不存在性能差的问题。\n\n```sql\nselect a from t1 where a > any(select a from t2);\n```\n1. `select a from t1` 是外部查询(outer query)\n2. `(select a from t2)` 是子查询\n\n一般说来，子查询嵌套于外部查询中，可以将两个或两个以上的子查询进行嵌套\n\n\n\n### 1. 子查询的使用\n#### 1.1.  ANY / SOME\n如果外部查询的列的结果和子查询的列的结果比较得到为True的话，则返回比较值为True的（外查询）的记录\n\n```sql\nmysql> create table  t1 (a int);\nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> create table  t2 (a int); \nQuery OK, 0 rows affected (0.14 sec)\n\nmysql> insert into t1 values(10),(4);\nQuery OK, 2 rows affected (0.02 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\nmysql> insert into t2 values(12),(13),(5);\nQuery OK, 3 rows affected (0.03 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> select a from t1;\n+------+\n| a    |\n+------+\n|   10 |\n|    4 |\n+------+\n2 rows in set (0.00 sec)\n\nmysql> select * from t2;\n+------+\n| a    |\n+------+\n|   12 |   -- t1中10，4比12小\n|   13 |   -- t1中10，4比13小\n|    5 |   -- t1中10比5大，4比5小\n+------+\n3 rows in set (0.00 sec)\n\nmysql> select a from t1  \n    -> where a > any\n    -> (select a from t2); -- 返回(12，13，4)\n                           -- t1中a列的值，只要大于(12,13,4)中任意一值\n                           -- 即t1.a > t2.a为True，则返回对应的t1.a\n+------+\n| a    |\n+------+\n|   10 |  -- 10 比 5 大为True，则返回该值，4比t2中所有的a值小，为False\n+------+\n1 row in set (0.00 sec)\n\n-- 这个查询可以解释为，t1表内a列的值 大于 t2表中a列的任意(any)一个值（t1.a > any(t2.a) == true）,则返回t1.a的记录\n```\n\n>`ANY`关键词必须与一个`比较操作符`一起使用： `=`, `>`, `<`, `>=`, `<=`, `<>` *(这个是!=的意思)*\n\n>**子查询中`SOME`和`ANY`是同一个意思**\n\n#### 1.2. IN\n`in`是`ANY`的一种特殊情况：**`\"in\"`**  `equals`  **`\"= any\"`**\n\n```sql\nmysql> insert into t1 values(5);  -- 向t1中插入一个t2中存在的值 5\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select a from t1 where a = any(select a from t2); -- t1.a==t2.a 的只有5\n+------+\n| a    |\n+------+\n|    5 |\n+------+\n1 row in set (0.00 sec)\n\nmysql> select a from t1 where a in (select a from t2); -- in的结果等同于 =any 的结果\n+------+\n| a    |\n+------+\n|    5 |\n+------+\n1 row in set (0.00 sec)\n```\n\n>`select a from s1 where  a in (select a in t2);`是用的比较多的一种语法\n\n#### 1.3. ALL\n如果外部查询的列的结果和子查询的列的`所有结果`比较得到为True的话，则返回比较值为True的（外查询）的记录\n```sql\nmysql> truncate t1;   -- 清空t1\nQuery OK, 0 rows affected (0.07 sec)\n\nmysql> truncate t2;   -- 清空t2\nQuery OK, 0 rows affected (0.10 sec)\n\nmysql> insert into t1 values(10),(4);\nQuery OK, 2 rows affected (0.02 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\nmysql> insert into t2 values(5),(4),(3);  \nQuery OK, 3 rows affected (0.03 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> select a from t1 where a > all(select a from t2);\n+------+\n| a    |\n+------+\n|   10 |  -- (10 > 5, 4, 3 为 True) 而 (4 >5, 4, 3 为 False)\n+------+\n1 row in set (0.00 sec)\n```\n>`ALL`关键词必须与一个`比较操作符`一起使用\n>`NOT IN` 是 `<> ALL`的别名 \n\n### 2. 子查询的分类\n* **独立子查询**\n    - 不依赖外部查询而运行的子查询\n    ```sql\n    mysql> select a from t1 where a in (1,2,3,4,5);\n    +------+\n    | a    |\n    +------+\n    |    4 |  \n    +------+\n    1 row in set (0.00 sec)\n    ```\n\n* **相关子查询**\n    - 引用了外部查询列的子查询\n    ```sql\n    -- 在这个例子中，子查询中使用到了外部的列t2.a \n    mysql> select a from t1 where a in (select * from t2 where t1.a = t2.a);\n    +------+\n    | a    |\n    +------+\n    |    4 |\n    +------+\n    1 row in set (0.00 sec)\n    ```\n\n### 3. 子查询的优化\n* **MySQL5.6之前**\n在`MySQL5.6`之前，优化器会把子查询重写成`exists`的形式\n```sql\nselect a from t1 where a in (select a from t2); -- 这个是一条独立的子查询，时间复杂度 O(M+N)\n--\n-- 经过优化器重写后\n--\nselect a from t1 where exists (select 1 from t2 where t1.a = t2.a); -- 这是相关子查询，复杂度O(M*N + M)\n```\n所以在`MySQL 5.6`之前，部分的子查询需要重写成join的形式 *（注意表的大小）*\n```sql\nmysql> select t1.a from t1 join t2 on t1.a = t2.a;\n+------+\n| a    |\n+------+\n|    4 |\n+------+\n1 row in set (0.00 sec)\n```\n\n* **MySQL 5.6之后**\n在`MySQL 5.6`之后，优化器`不会`将子查询`重写`成`exists`的形式，而是自动优化，性能有了大幅提升\n\n>可通过`explain extended`来查看子查询优化的结果。由于`explain`还未讲到，该部分暂时跳过\n\n### 4. 包含NULL值的NOT IN\n```sql\nmysql> select null in ('a', 'b', null);\n+--------------------------+\n| null in ('a', 'b', null) |\n+--------------------------+\n|                     NULL |\n+--------------------------+\n1 row in set (0.00 sec)\n```\n>MySQL数据库的`比较操作`，除了返回`1(True)`, `0(False)`之外，还会返回`NULL`\n>`NULL`和`NULL`的比较，返回的还是`NULL`\n\n```sql\nmysql> select null not in ('a', 'b', null);  \n+------------------------------+\n| null not in ('a', 'b', null) |\n+------------------------------+\n|                         NULL |  -- null不在('a', 'b', null)中，返回的还是null，因为有null和null的比较\n+------------------------------+\n1 row in set (0.00 sec)\n\nmysql> select 'a' not in ('a', 'b', null);  \n+-----------------------------+\n| 'a' not in ('a', 'b', null) |\n+-----------------------------+\n|                           0 |  -- a 不在 ('a', 'b', null)中，返回0,即False\n+-----------------------------+\n1 row in set (0.00 sec)\n\nmysql> select 'c' not in ('a', 'b');\n+-----------------------+\n| 'c' not in ('a', 'b') |\n+-----------------------+\n|                     1 |  -- 这个返回值可以理解 'c'不在('a', 'b')中，返回1，即为True\n+-----------------------+\n1 row in set (0.00 sec)\n\nmysql> select 'c' not in ('a', 'b', null); \n+-----------------------------+\n| 'c' not in ('a', 'b', null) |\n+-----------------------------+\n|                        NULL | -- 理论上应该是返回1，即True的。但是包含了null值。则返回null\n+-----------------------------+\n1 row in set (0.00 sec)\n```\n>**对于包含了`NULL`值的`IN`操作，总是返回`True`或者`NULL`**\n>**`NOT IN`返回`NOT True (False)`或者`NOT NULL (NULL)`**\n\n\n```sql\n--\n-- SQL语句一 使用 EXISTS\n--\nselect customerid, companyname \n    from customers as A\n    where country = 'Spain' \n        and not exists\n            ( select * from orders as B\n              where A.customerid = B.customerid );\n              \n--\n-- SQL语句二 使用 IN\n--\nselect customerid, companyname \n    from customers as A\n    where country = 'Spain' \n        and customerid not in (select customerid from orders);\n              \n-----\n-- 当结果集合中没有NULL值时，上述两条SQL语句查询的结果是一致的 \n-----\n\n--\n-- 插入一个NULL值\n--\ninsert into orders(orderid) values (null);\n\n-----\n-- SQL语句1 : 返回和之前一致\n-- SQL语句2 : 返回为空表，因为子查询返回的结果集中存在NULL值。not in null 永远返回False或者NULL\n--            此时 where (country = 'Spain' and (False or NULL)) 为 False OR NULL，条件永远不匹配\n-----\n\n--\n-- SQL语句2 改写后\n--\nselect customerid, companyname \n    from customers as A\n    where country = 'Spain' \n        and customerid not in (select customerid from orders \n                                where customerid is not null);  -- 增加这个过滤条件，使用is not，而不是<>\n\n\n--\n-- 和 null比较，使用is和is not， 而不是 = 和 <>\n--\nmysql> select null = null; \n+-------------+\n| null = null |\n+-------------+\n|        NULL |\n+-------------+\n1 row in set (0.00 sec)\n\nmysql> select null <> null;\n+--------------+\n| null <> null |\n+--------------+\n|         NULL |\n+--------------+\n1 row in set (0.00 sec)\n\nmysql> select null is null; \n+--------------+\n| null is null |\n+--------------+\n|            1 |  -- 返回 True\n+--------------+\n1 row in set (0.00 sec)\n\nmysql> select null is not  null;\n+-------------------+\n| null is not  null |\n+-------------------+\n|                 0 |  -- 返回 False\n+-------------------+\n1 row in set (0.00 sec)\n```\n\n>`EXISTS`不管返回值是什么，而是看是否有`行`返回，所以`EXISTS`中子查询都是`select *`、`select 1`等，因为只关心返回是否有行（结果集）\n\n----\n\n## 二. INSERT\n\n>[官方文档](http://dev.mysql.com/doc/refman/5.7/en/insert.html)\n\n```sql\nmysql> insert into t1 values(1);  -- 插入一个值\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into t1 values(2),(3),(-1);  -- 插入多个值，MySQL独有\nQuery OK, 3 rows affected (0.03 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> insert into t1 select 8;   -- insert XXX select XXX 语法，MySQ独有\nQuery OK, 1 row affected (0.02 sec)\nRecords: 1  Duplicates: 0  Warnings: 0\n\nmysql> create table t3 (a int, b int); -- 有多个列\nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> insert into t3 select 8;  -- 没有指定列，报错\nERROR 1136 (21S01): Column count doesn't match value count at row 1\n\nmysql> insert into t3(a) select 8;  -- 指定列a\nQuery OK, 1 row affected (0.04 sec)\nRecords: 1  Duplicates: 0  Warnings: 0\n\nmysql> insert into t3 select 8, 9;  -- 不指定列，但是插入值匹配列的个数和类型\nQuery OK, 1 row affected (0.03 sec)\nRecords: 1  Duplicates: 0  Warnings: 0\n\nmysql> select * from t3;\n+------+------+\n| a    | b    |\n+------+------+\n|    8 | NULL |\n|    8 |    9 |\n+------+------+\n2 rows in set (0.00 sec)\n\nmysql> insert into t3(b) select a from t2;  -- 从t2表中查询数据并插入到t3(a)中，注意指定列\nQuery OK, 3 rows affected (0.03 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n\nmysql> select * from t3;\n+------+------+\n| a    | b    |\n+------+------+\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n+------+------+\n5 rows in set (0.00 sec)\n\n--\n-- 如果想快速增长表格中的数据，可以使用如下方法，使得数据成倍增长\n--\nmysql> insert into t3 select * from t3;\nQuery OK, 5 rows affected (0.03 sec)  -- 插入了5列\nRecords: 5  Duplicates: 0  Warnings: 0\n\nmysql> select * from t3;               \n+------+------+\n| a    | b    |\n+------+------+\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n+------+------+\n10 rows in set (0.00 sec)\n\nmysql> insert into t3 select * from t3;\nQuery OK, 10 rows affected (0.03 sec)  -- 插入了10列，成倍增长\nRecords: 10  Duplicates: 0  Warnings: 0\n\nmysql> select * from t3;               \n+------+------+\n| a    | b    |\n+------+------+\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n|    8 | NULL |\n|    8 |    9 |\n| NULL |    5 |\n| NULL |    4 |\n| NULL |    3 |\n+------+------+\n20 rows in set (0.00 sec)\n```\n\n-----\n\n## 三. DELETE\n\n>[官方文档](http://dev.mysql.com/doc/refman/5.7/en/delete.html)\n\n```sql\nmysql> delete from t3 where a is null;  -- 根据过滤条件删除\nQuery OK, 12 rows affected (0.03 sec)\n\nmysql> select * from t3;               \n+------+------+\n| a    | b    |\n+------+------+\n|    8 | NULL |\n|    8 |    9 |\n|    8 | NULL |\n|    8 |    9 |\n|    8 | NULL |\n|    8 |    9 |\n|    8 | NULL |\n|    8 |    9 |\n+------+------+\n8 rows in set (0.00 sec)\n\nmysql> delete from t3;   -- 删除整个表\nQuery OK, 8 rows affected (0.03 sec)\n\nmysql> select * from t3;\nEmpty set (0.00 sec)\n```\n\n-----\n\n## 四. UPDATE\n\n>[官方文档](http://dev.mysql.com/doc/refman/5.7/en/update.html)\n\n```sql\nmysql> insert into t3 select 1,2;\nQuery OK, 1 row affected (0.03 sec)\nRecords: 1  Duplicates: 0  Warnings: 0\n\nmysql> select * from t3;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    2 |\n+------+------+\n1 row in set (0.00 sec)\n\nmysql> update t3 set a=10 where a=1;\nQuery OK, 1 row affected (0.03 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from t3;\n+------+------+\n| a    | b    |\n+------+------+\n|   10 |    2 |\n+------+------+\n1 row in set (0.00 sec)\n\n--\n-- 关联后更新\n--\nmysql> select * from t1;\n+------+\n| a    |\n+------+\n|   10 |\n|    4 |  -- 和t2中的4相等\n|    1 |\n|    2 |\n|    3 |  -- 和t2中的3相等\n|   -1 |\n|    8 |\n+------+\n7 rows in set (0.00 sec)\n\nmysql> select * from t2;\n+------+\n| a    |\n+------+\n|    5 |\n|    4 |  -- 和t1中的4相等\n|    3 |  -- 和t1中的3相等\n+------+\n3 rows in set (0.00 sec)\n\nmysql> update t1 join t2 on t1.a = t2.a set t1.a=100;  -- 先得到t1.a=t2.a的结果集\n                                                       -- 然后将结果集中的t1.a设置为100\nQuery OK, 2 rows affected (0.03 sec)\nRows matched: 2  Changed: 2  Warnings: 0\n\nmysql> select * from t1;\n+------+\n| a    |\n+------+\n|   10 |\n|  100 |  -- 该行被更新成100\n|    1 |\n|    2 |\n|  100 |  -- 该行被更新成100\n|   -1 |\n|    8 |\n+------+\n7 rows in set (0.00 sec)\n```\n\n-----\n\n## 五. REPLACE\n\n>[官方文档](http://dev.mysql.com/doc/refman/5.7/en/replace.html)\n\n```sql\nmysql> create table t4(a int primary key auto_increment, b int);\nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> insert into t4 values(NULL, 10);\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> insert into t4 values(NULL, 11);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> insert into t4 values(NULL, 12);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from t4;\n+---+------+\n| a | b    |\n+---+------+\n| 1 |   10 |\n| 2 |   11 |\n| 3 |   12 |\n+---+------+\n3 rows in set (0.00 sec)\n\nmysql> insert into t4 values(1, 100);  -- 报错，说存在重复的主键记录 \"1\"\nERROR 1062 (23000): Duplicate entry '1' for key 'PRIMARY'\n\nmysql> replace into t4 values(1, 100); -- 替换该主键对应的值  \nQuery OK, 2 rows affected (0.03 sec)   -- 两行记录受到影响\n\nmysql> select * from t4;\n+---+------+\n| a | b    |\n+---+------+\n| 1 |  100 |  -- 已经被替换\n| 2 |   11 |\n| 3 |   12 |\n+---+------+\n3 rows in set (0.00 sec)\n-----\n-- replace的原理是：先delete，在insert\n-----\n\nmysql> replace into t4 values(5, 50);  -- 没有替换对象时，类似插入效果\nQuery OK, 1 row affected (0.03 sec)    -- 只影响1行\n\nmysql> select * from t4;\n+---+------+\n| a | b    |\n+---+------+\n| 1 |  100 |\n| 2 |   11 |\n| 3 |   12 |\n| 5 |   50 |  -- 插入了1行\n+---+------+\n4 rows in set (0.00 sec)\n\n--\n-- replace原理更明显的例子 \n--\n\nmysql> create table t6 \n    -> (a int primary key, \n    -> b int auto_increment,   -- b是auto_increment的int型数据\n    -> c int, key(b));\nQuery OK, 0 rows affected (0.15 sec)\n\nmysql> insert into t6 values(10, NULL, 100),(20,NULL,200);  -- b自增长\nQuery OK, 2 rows affected (0.02 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\nmysql> select * from t6;\n+----+---+------+\n| a  | b | c    |\n+----+---+------+\n| 10 | 1 |  100 |  -- b为1\n| 20 | 2 |  200 |  -- b为2\n+----+---+------+\n2 rows in set (0.00 sec)\n\nmysql> replace into t6 values(10,NULL,150);  -- 将a=10的替换掉\nQuery OK, 2 rows affected (0.03 sec)\n\nmysql> select * from t6;\n+----+---+------+\n| a  | b | c    |\n+----+---+------+\n| 10 | 3 |  150 |  -- 替换后b从1变成了3，说明是先删除，再插入\n| 20 | 2 |  200 |\n+----+---+------+\n2 rows in set (0.00 sec)\n\n-----\n\n--\n-- insert on duplicate 效果和 replace类似\n--\nmysql> insert into t4 values(1,1);  -- 插入报错，存在key为1的记录\nERROR 1062 (23000): Duplicate entry '1' for key 'PRIMARY'\n\nmysql> insert into t4 values(1,1) on duplicate key update b=1;  -- 带上on duplicate参数\n                                                                -- 非SQL标准，不推荐\nQuery OK, 2 rows affected (0.03 sec)\n\nmysql> select * from t4;\n+---+------+\n| a | b    |\n+---+------+\n| 1 |    1 |  -- 该行的b列从100被替换成1\n| 2 |   11 |\n| 3 |   12 |\n| 5 |   50 |\n+---+------+\n\n--\n-- insert ignore\n--\nmysql> insert ignore into t4 values(1,1);   -- 忽略重复的错误\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql> show warnings;\n+---------+------+---------------------------------------+\n| Level   | Code | Message                               |\n+---------+------+---------------------------------------+\n| Warning | 1062 | Duplicate entry '1' for key 'PRIMARY' |\n+---------+------+---------------------------------------+\n1 row in set (0.00 sec)\n```\n\n-----\n\n## 六. 其他知识点\n\n* **更新有关系的值**\n\n```sql\nmysql> create table t5 (a int, b int);\nQuery OK, 0 rows affected (0.14 sec)\n\nmysql> insert into t5 values(1,1);\nQuery OK, 1 row affected (0.03 sec)\n\nmysql> select * from t5;\n+------+------+\n| a    | b    |\n+------+------+\n|    1 |    1 |\n+------+------+\n1 row in set (0.00 sec)\n\nmysql> update t5 set a=a+1, b=a where a=1;\nQuery OK, 1 row affected (0.02 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\nmysql> select * from t5;\n+------+------+\n| a    | b    |\n+------+------+\n|    2 |    2 |  -- SQL Server和Oracle中得到的值是2, 1\n+------+------+\n1 row in set (0.00 se\n```\n\n* **显示行号(RowNumber)**\n\n```sql\n--\n-- 方法一\n--\nmysql> use employees ;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql> set @rn:=0;  -- 产生 SESSION(会话)级别的变量\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @rn:=@rn+1 as rownumber, emp_no, gender from employees limit 10;  -- ：= 是赋值的意思\n+-----------+--------+--------+\n| rownumber | emp_no | gender |\n+-----------+--------+--------+\n|        11 |  10001 | M      |\n|        12 |  10002 | F      |\n|        13 |  10003 | M      |\n|        14 |  10004 | M      |\n|        15 |  10005 | M      |\n|        16 |  10006 | F      |\n|        17 |  10007 | F      |\n|        18 |  10008 | M      |\n|        19 |  10009 | F      |\n|        20 |  10010 | F      |\n+-----------+--------+--------+\n10 rows in set (0.00 sec)\n\n--\n-- 方法二 （推荐）\n--\nmysql> select @rn1:=@rn1+1 as rownumber, emp_no, gender from employees, (select @rn1:=0) as a limit 10;\n+-----------+--------+--------+\n| rownumber | emp_no | gender |\n+-----------+--------+--------+\n|         1 |  10001 | M      |\n|         2 |  10002 | F      |\n|         3 |  10003 | M      |\n|         4 |  10004 | M      |\n|         5 |  10005 | M      |\n|         6 |  10006 | F      |\n|         7 |  10007 | F      |\n|         8 |  10008 | M      |\n|         9 |  10009 | F      |\n|        10 |  10010 | F      |\n+-----------+--------+--------+\n10 rows in set (0.00 sec)\n\n-- MySQL 自定义变量，根据每一记录进行变化的\n\nmysql> select @rn1:=0;\n+---------+\n| @rn1:=0 |\n+---------+\n|       0 |  -- 只有一行记录\n+---------+\n1 row in set (0.00 sec)\n\n-- 相当于 把 employees 和 (select @rn1:=0)做了笛卡尔积，然后使用@rn1:=@rn + 1，根据每行进行累加\n\n--\n-- \":=\" 和 \"=\"\n--\nmysql> set @a:=1;  -- 赋值为1\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @a;\n+------+\n| @a   |\n+------+\n|    1 |\n+------+\n1 row in set (0.00 sec)\n\nmysql> set @a:=10;  -- 赋值为10\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> select @a;\n+------+\n| @a   |\n+------+\n|   10 |\n+------+\n1 row in set (0.00 sec)\n\nmysql> select @a=9;  -- 进行比较\n+------+\n| @a=9 |\n+------+\n|    0 |  -- 返回为False\n+------+\n1 row in set (0.00 sec)\n\n--\n-- 作业：通过子查询或者其他方式，计算出employees的行号\n--\n```\n","tags":["mysql"],"categories":["mysql"]},{"title":"slow_log/generic_log/audit/存储引擎一","url":"/2019-10-01/mysql/MySQL学习笔记(Day005)/","content":"\nMySQL学习笔记（Day005：slow_log/generic_log/audit/存储引擎一）\n============================================================\n@(MySQL学习)\n\n[TOC]\n\n## 一. 慢查询日志进阶\n\n### 1. 相关参数：\n* **`slow_query_log`**\n    - 是否开启慢查询日志                \n    \n* **`slow_query_log_file`**\n    - 慢查询日志文件名, 在`my.cnf`我们已经定义为slow.log，默认是 *机器名-slow.log*          \n     \n* **`long_query_time`**\n    - 制定慢查询阈值, 单位是秒，且当版本 `>=5.5.X`，支持毫秒。例如`0.5`即为`500ms`\n    - `大于`该值，不包括值本身。例如该值为2，则执行时间正好`等于`2的SQL语句`不会记录`\n\n* **`log_queries_not_using_indexes`**\n    - 将没有使用索引的SQL记录到慢查询日志 \n        - 如果一开始因为数据少，查表快，耗时的SQL语句没被记录，当数据量大时，该SQL可能会执行很长时间\n        - 需要测试阶段就要发现问题，减小上线后出现问题的概率\n\n* **`log_throttle_queries_not_using_indexes`**\n    - 限制每分钟内，在慢查询日志中，去记录没有使用索引的SQL语句的次数；版本需要`>=5.6.X`\n        - 因为没有使用索引的SQL可能会短时间重复执行，为了避免日志快速增大，限制每分钟的记录次数\n        \n* **`min_examined_row_limit`**\n    - 扫描记录少于改值的SQL不记录到慢查询日志 \n        - 结合去记录没有使用索引的SQL语句的例子，有可能存在某一个表，数据量维持在百行左右，且没有建立索引。这种表即使不建立索引，查询也很快，扫描记录很小，如果确定有这种表，则可以通过此参数设置，将这个SQL不记录到慢查询日志。\n    \n* **`log_slow_admin_statements`**\n    - 记录超时的管理操作SQL到慢查询日志，比如ALTER/ANALYZE TABLE\n\n* **`log_output`**\n    - 慢查询日志的格式，[FILE | TABLE | NONE]，默认是FILE；版本`>=5.5`\n    - 如果设置为TABLE，则记录的到`mysql.slow_log`\n\n* **`log_slow_slave_statements`**\n    - 在从服务器上开启慢查询日志\n\n* **`log_timestamps`**\n    - 写入时区信息。可根据需求记录UTC时间或者服务器本地系统时间\n\n\n### 2. 慢查询日志实践\n\n* 设置慢查询记录的相关参数\n```sql\n--\n-- 终端A\n--\n-- 注意做实验以前，先把my.cnf中的 slow_query_log = 0, 同时将min_examined_row_limit = 100 进行注释\n--\nmysql> select version();\n+-----------+\n| version() |\n+-----------+\n| 5.7.9-log |\n+-----------+\n1 row in set (0.01 sec)\n\nmysql> show variables like \"slow_query_log\"； -- 为了测试，特地在my.cnf中关闭了该选项\n+----------------+-------+\n| Variable_name  | Value |\n+----------------+-------+\n| slow_query_log | OFF   |\n+----------------+-------+\n1 row in set (0.00 sec)\n\nmysql> set global slow_query_log = 1;         -- slow_query_log可以在线打开\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> show variables like \"slow_query_log\";  -- 已经打开\n+----------------+-------+\n| Variable_name  | Value |\n+----------------+-------+\n| slow_query_log | ON    |\n+----------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"long_query_time\";\n+-----------------+----------+\n| Variable_name   | Value    |\n+-----------------+----------+\n| long_query_time | 2.000000 |   -- my.cnf 中该值设置为2秒\n+-----------------+----------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"min_ex%\";  -- my.cnf 中已经关闭注释，所以这里为0\n+------------------------+-------+\n| Variable_name          | Value |\n+------------------------+-------+\n| min_examined_row_limit | 0     |\n+------------------------+-------+\n1 row in set (0.00 sec)\n```\n\n* 查看慢查询日志\n```bash\n#\n#终端B\n#\n[root@localhost mysql_data]# tail -f slow.log \n/usr/local/mysql/bin/mysqld, Version: 5.7.9-log (MySQL Community Server (GPL)). started with:\nTcp port: 3306  Unix socket: (null)\nTime                 Id Command    Argument  #测试没有任何慢查询日志信息\n```\n\n* 进行模拟耗时操作\n```sql\n--\n-- 终端A\n--\nmysql> select sleep(4);\n+----------+\n| sleep(4) |\n+----------+\n|        0 |\n+----------+\n1 row in set (4.00 sec)\n```\n\n* 最终产生慢查询日志\n```bash\n#\n#终端B\n#\n[root@localhost mysql_data]# tail -f slow.log \n/usr/local/mysql/bin/mysqld, Version: 5.7.9-log (MySQL Community Server (GPL)). started with:\nTcp port: 3306  Unix socket: (null)\nTime                 Id Command    Argument  #测试没有任何慢查询日志信息\n# Time: 2015-11-21T07:18:10.741663+08:00\n# User@Host: root[root] @ localhost []  Id:     2\n# Query_time: 4.000333  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0 \n                                                          #这个就是min_examined_row_limit\n                                                          #设置的意义。如my.cnf中设置该值为100\n                                                          #则这条语句因为Rows_examined < 100,而不会被记录\nSET timestamp=1448061490;\nselect sleep(4);\n```\n\n> **注意**\n如果在终端A中`set global min_examined_row_limit = 100;`, 然后执行`select sleep(5);`，会发现该记录仍然被记录到慢查询日志中。原因是因为`set global min_examined_row_limit`设置的是全局变量，此次会话不生效。\n\n>**但是我们上面`set global slow_query_log = 1；`却是在线生效的，这点有所不通**\n\n\n* mysqldumpslow\n```bash\n[root@localhost mysql_data]# mysqldumpslow  slow.log\n\nReading mysql slow query log from slow.log\nCount: 2  Time=0.00s (0s)  Lock=0.00s (0s)  Rows=0.0 (0), 0users@0hosts\n  Time: N-N-21T07:N:N.N+N:N\n  # User@Host: root[root] @ localhost []  Id:     N\n  # Query_time: N.N  Lock_time: N.N Rows_sent: N  Rows_examined: N\n  SET timestamp=N;\n  select sleep(N)\n\nCount: 1  Time=0.00s (0s)  Lock=0.00s (0s)  Rows=0.0 (0), 0users@0hosts\n  # Time: N-N-21T07:N:N.N+N:N\n  # User@Host: root[root] @ localhost []  Id:     N\n  # Query_time: N.N  Lock_time: N.N Rows_sent: N  Rows_examined: N\n  SET timestamp=N;\n  select sleep(N)\n  \n#######################################################################\n\n[root@localhost mysql_data]# mysqldumpslow  --help\nUsage: mysqldumpslow [ OPTS... ] [ LOGS... ]\n\nParse and summarize the MySQL slow query log. Options are\n\n  --verbose    verbose\n  --debug      debug\n  --help       write this text to standard output\n\n  -v           verbose\n  -d           debug\n  -s ORDER     what to sort by (al, at, ar, c, l, r, t), 'at' is default #根据以下某个信息来排序\n                al: average lock time\n                ar: average rows sent\n                at: average query time\n                 c: count\n                 l: lock time\n                 r: rows sent\n                 t: query time  \n  -r           reverse the sort order (largest last instead of first)  # 逆序输出\n  -t NUM       just show the top n queries      # TOP(n)参数\n  -a           don't abstract all numbers to N and strings to 'S'\n  -n NUM       abstract numbers with at least n digits within names\n  -g PATTERN   grep: only consider stmts that include this string\n  -h HOSTNAME  hostname of db server for *-slow.log filename (can be wildcard),\n               default is '*', i.e. match all\n  -i NAME      name of server instance (if using mysql.server startup script)\n  -l           don't subtract lock time from total time\n```\n\n> 如果在线上操作，不需要`mysqldumpslow`去扫整个`slow.log`， 可以去` tail -n 10000 slow.log > last_10000_slow.log`(*10000这个数字根据实际情况进行调整*),然后进行`mysqldumpslow last_10000_slow.log`\n\n* 慢查询日志存入表\n```sql\n--\n-- 在my.cnf 中增加 log_output = TABLE，打开slow_query_log选项，然后重启数据库实例\n--\nmysql> show variables like \"log_output%\";\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| log_output    | TABLE |\n+---------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show variables like \"slow_query_log\";\n+----------------+-------+\n| Variable_name  | Value |\n+----------------+-------+\n| slow_query_log | ON    |\n+----------------+-------+\n1 row in set (0.00 sec)\n\nmysql> select * from mysql.slow_log;\n+----------------------------+---------------------------+-----------------+-----------------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+\n| start_time                 | user_host                 | query_time      | lock_time       | rows_sent | rows_examined | db | last_insert_id | insert_id | server_id | sql_text        | thread_id |\n+----------------------------+---------------------------+-----------------+-----------------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+\n| 2015-11-20 19:50:28.574677 | root[root] @ localhost [] | 00:00:04.000306 | 00:00:00.000000 |         1 |             0 |    |              0 |         0 |        11 | select sleep(4) |         3 |\n+----------------------------+---------------------------+-----------------+-----------------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+\n1 row in set (0.00 sec)\n\nmysql> show create table mysql.slow_log;\n--\n-- 表结构输出省略\n-- 关键一句如下：\n--\nENGINE=CSV DEFAULT CHARSET=utf8 COMMENT='Slow log'  -- ENGINE=CSV 这里使用的是CSV的引擎,性能较差\n\n-- 建议将slow_log表的存储引擎改成MyISAM\nmysql> alter table mysql.slow_log engine = myisam;\nERROR 1580 (HY000): You cannot 'ALTER' a log table if logging is enabled  '-- 提示我正在记录日志中，不能转换\n\nmysql> set global slow_query_log = 0;    -- 先停止记录日志\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> alter table mysql.slow_log engine = myisam;   -- 然后转换表的引擎\nQuery OK, 2 rows affected (5.05 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\nmysql> set global slow_query_log = 1;     -- 再开启记录日志\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> show create table mysql.slow_log;\n--\n-- 表结构输出省略\n-- 关键一句如下：\n--\nENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT='Slow log'  -- ENGINE 变成了MyISAM\n```\n> 使用`TABLE`的优势在于方便查询，但是记住当在备份的时候，不要备份慢查询日志的表，避免备份过大。\n使用`FILE`也可以，需要定时清除该文件，避免单文件过大。\n\n-----\n\n## 二. 通用日志(generic_log)与审计\n###1. 通用日志作用\n* 当需要查找某条特定SQL语句，且该SQL语句执行较快，无法记录到slow_log中时，可以开启通用日志`generic_log`,进行全面记录， 可用于审计`Audit`\n\n* 通用日志会记录所有操作，性能下降明显。所以如果需要审计，需要`Audit Plugin`\n\n\n###2. 审计插件\n* MariaDB Audit 插件\n    - MySQL社区版本目前没有提供Audit的功能，企业版本提供了该功能。MariaDB 提供了开源的Audit插件，且MySQL也能使用。\n\n* 插件下载\n    - [server_audit-1.2.0.tar.gz](https://downloads.mariadb.com/enterprise/6ahb-eete/mariadb-audit-plugin/server_audit-1.2.0.tar.gz) 上述链接如果失效，可以进入官方页面注册，然后下载\n    - [官方注册下载插件](https://mariadb.com/my_portal/download/audit_plugin)\n\n###3.  Audit Plugin安装\n* **`MySQL5.7.9` 审计插件安装失败，提示如下：**\n>`ERROR 1126 (HY000): Can't open shared library '/usr/lib64/mysql/plugin/server_audit.so' (errno: 13 /usr/lib64/mysql/plugin/server_audit.so: undefined symbol: _my_thread_var)`\n\n* **MySQL5.6.27 审计插件安装成功，步骤如下：**\n```bash\n# 找到plugin位置\n[root@localhost ~]> cat /etc/my.cnf  | grep plugin_dir\nplugin_dir=/usr/local/mysql/lib/plugin\n\n# 解压plugin\n[root@localhost ~]> tar zxvf server_audit-1.2.0.tar.gz\nserver_audit-1.2.0/\nserver_audit-1.2.0/linux-32_debug/\nserver_audit-1.2.0/linux-32_debug/server_audit.so\nserver_audit-1.2.0/linux-32/\nserver_audit-1.2.0/linux-32/server_audit.so\nserver_audit-1.2.0/linux-64_debug/\nserver_audit-1.2.0/linux-64_debug/server_audit.so\nserver_audit-1.2.0/windows-32/\nserver_audit-1.2.0/windows-32/server_audit.dll\nserver_audit-1.2.0/windows-64_debug/\nserver_audit-1.2.0/windows-64_debug/server_audit.dll\nserver_audit-1.2.0/linux-64/\nserver_audit-1.2.0/linux-64/server_audit.so\nserver_audit-1.2.0/windows-64/\nserver_audit-1.2.0/windows-64/server_audit.dll\nserver_audit-1.2.0/windows-32_debug/\nserver_audit-1.2.0/windows-32_debug/server_audit.dll\n\n# 移动插件到对应的插件目录\n[root@localhost ~]> mv server_audit-1.2.0/linux-64/server_audit.so /usr/local/mysql/lib/plugin\n[root@localhost ~]> cd /usr/local/mysql/lib/plugin\n```\n\n\n```sql\n--\n-- 相关安装步骤\n--\nmysql> select version();\n+------------+\n| version()  |\n+------------+\n| 5.6.27-log |\n+------------+\n1 row in set (0.00 sec)\n\nmysql> INSTALL PLUGIN server_audit SONAME 'server_audit.so';  -- 安装插件，该步骤在5.7.9中失败\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> show variables like \"%server_audit%\";    -- 查看和server_audit相关的参数\n+-------------------------------+-----------------------+\n| Variable_name                 | Value                 |\n+-------------------------------+-----------------------+\n| server_audit_events           |                       |\n| server_audit_excl_users       |                       |\n| server_audit_file_path        | server_audit.log      |\n| server_audit_file_rotate_now  | OFF                   |\n| server_audit_file_rotate_size | 1000000               |\n| server_audit_file_rotations   | 9                     |\n| server_audit_incl_users       |                       |\n| server_audit_logging          | OFF                   |\n| server_audit_mode             | 1                     |\n| server_audit_output_type      | file                  |\n| server_audit_syslog_facility  | LOG_USER              |\n| server_audit_syslog_ident     | mysql-server_auditing |\n| server_audit_syslog_info      |                       |\n| server_audit_syslog_priority  | LOG_INFO              |\n+-------------------------------+-----------------------+\n14 rows in set (0.00 sec)\n\nmysql> set global server_audit_logging = 1;   -- 打开审计功能\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> show variables like \"server_audit_logging\"; \n+----------------------+-------+\n| Variable_name        | Value |\n+----------------------+-------+\n| server_audit_logging | ON    |\n+----------------------+-------+\n1 row in set (0.00 sec)\n\nmysql> show status like '%audit%';\n+----------------------------+------------------+\n| Variable_name              | Value            |\n+----------------------------+------------------+\n| server_audit_active        | ON               |\n| server_audit_current_log   | server_audit.log |\n| server_audit_last_error    |                  |\n| server_audit_writes_failed | 0                |\n+----------------------------+------------------+\n4 rows in set (0.00 sec)\n```\n\n```bash\n#\n#查看审计日志\n#\n[root@MyServer mysql_data]> tail -f server_audit.log \n20151120 22:40:54,MyServer,root,localhost,2,9,QUERY,,'set global server_audit_logging = 1',0\n20151120 22:41:16,MyServer,root,localhost,2,10,QUERY,,'show variables like \"server_audit_logging\"',0\n20151120 22:41:53,MyServer,root,localhost,1,5,QUERY,,'show status like \\'%audit%\\'',0\n```\n\n>以上仅为基本功能操作，详细的细粒度控制请参考[官方文档](https://mariadb.com/kb/en/mariadb/about-the-mariadb-audit-plugin/)\n\n\n## 三. 存储引擎(一)\n\n### 1.Mysql上支持的存储引擎\n```sql\nmysql> show engines;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |\n| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |\n| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |\n| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |\n| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |\n| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |\n| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables      | NO           | NO   | NO         |\n| FEDERATED          | NO      | Federated MySQL storage engine                                 | NULL         | NULL | NULL       |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n9 rows in set (0.00 sec)\n```\n### 2. 存储引擎的概念\n>用来处理数据库的相关CRUD操作\n\n每个数据库都有存储引擎，只是MySQL比较强调存储引擎的概念。\n\n### 3. MySQL存储引擎\n* 官方存储引擎\n    - MyISAM\n    - `InnoDB`  -- 推荐；其他引擎已经体停止维护和开发\n    - Memory\n    - Federated\n    - CSV\n    - Archive\n* 第三方存储引擎\n    - TokuDB  -- 开源，适合插入密集型\n    - InfoBright  -- 商业，开源版本有数据量限制。属于列存储，面向OLAP场景\n    - Spider\n>第三方存储引擎在特定场合下比较适合，除此之外，都应该使用InnoDB\n\n### 3. 存储引擎之MyISAM\n* MySQL5.1版本之前的默认存储引擎\n* 堆表数据结构\n* 表锁设计\n* 支持数据静态压缩\n* 不支持事物\n* 数据容易丢失\n* 索引容易损坏\n* 唯一优点\n    - 数据文件可以直接拷贝到另一台服务器使用\n    \n> 现在MySQL中还有用MyISAM的表，主要是历史原因。数据库文件以`MY`开头的基本都是MyISAM的表\n","tags":["mysql"],"categories":["mysql"]},{"title":"hexo博客next主题添加google adsense","url":"/2019-09-19/hexo_google_adsense/","content":"\n## 流程\n\n### 说明\n\n1.  google adsense对网站内容有要求。\n2.  网站必须提交几篇(博主写了五篇)博客之后再去申请google adsense，否则会被拒。\n\n### google adsense账号\n\n申请google adsense账号，见[网址](https://www.google.com/adsense)\n\n### adsense代码\n\n1.  登录google adsense，首页申请google adsense，google会生成一段代码。\n2.  复制代码到themes/next/layout/_partials/head.swig其中一script块下面。\n3.  hexo deploy网站，google adsense首页上确认已添加代码到网站中，等待审核。\n\n```\n<script async src=\"//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script>\n<script>\n     (adsbygoogle = window.adsbygoogle || []).push({\n          google_ad_client: \"ca-pub-7126877795421398\",\n          enable_page_level_ads: true\n     });\n</script>\n复制代码\n```\n\n### 广告单元\n\n审核通过之后我们可以以自己的方式放置google adsense广告，这时候广告单元就派上用场了。\n\n1.  点击google adsense首页下方广告的子模块广告单元，右侧点击新建广告单元。\n2.  广告类型有三种，选择文章内嵌广告即可。\n\n\n## 结语\n\n*   [关于制作高质量网站的合作规范提示（第 1 部分）](http://adsense.blogspot.com/2012/04/tips-for-creating-high-quality-sites.html)\n*   [关于制作高质量网站的提示（第 2 部分）](http://adsense.blogspot.com/2012/09/tips-for-creating-high-quality-sites.html)\n*   [网站站长质量指南](https://support.google.com/adsense/answer/1348737)\n*   [AdSense 合作规范](https://support.google.com/adsense/answer/48182?utm_source=crs&utm_medium=email&utm_campaign=notification)\n\n","tags":["睡后收入","google adsense"],"categories":["运维"]},{"title":"Android学习笔记","url":"/2019-08-27/reference/Android/android-learn-note/","content":"\n## Android 开发者文档\n\n> [Android 官方 开发者文档](https://developer.android.com/)\n>\n> [中文翻译项目](hukai.me/android-training-course-in-chinese/index.html)\n\n## Progressive Web App\n\n> [简单介绍一下Progressive Web App(PWA)](https://juejin.im/post/5a6c86e451882573505174e7)\n>\n> [下一代 Web 应用模型 —— Progressive Web App](https://huangxuan.me/2017/02/09/nextgen-web-pwa/)\n>\n> [LAVAS 基于 Vue.js 的 PWA 解决方案](https://lavas.baidu.com/)\n>\n> [PWA超简单入门](https://juejin.im/post/5abba6a7f265da239706ec60)\n>\n> [PWA，现代前端必会的黑科技](https://zhuanlan.zhihu.com/p/40236256)\n\n## Java 知识汇总\n\n> [JAVA的abstract修饰符 && 接口interface用法 && 抽象类和interface的差别](https://blog.csdn.net/zhandoushi1982/article/details/8458081)\n\n","tags":["Android"],"categories":["Android"]},{"title":"IntelliJ IDEA 常用快捷键","url":"/2019-08-12/reference/MacOS/mac-clion-md/","content":"\n> **温馨提示**：在 IntelliJ IDEA 中有两个 Mac 版本的快捷键，分别为 Mac OS X 和 Mac OS X 10.5+， 其中 Mac OS X 10.5+ 为 IntelliJ IDEA 默认的快捷键版本。此外，建议将 Mac 系统中与 IntelliJ IDEA 冲突的快捷键取消或更改，不建议改 IntelliJ IDEA 的默认快捷键。\n\n## Mac 键盘符号和修饰键说明\n\n- `⌘` ——> Command\n- `⇧` ——> Shift\n- `⌥` ——> Option\n- `⌃` ——> Control\n- `↩︎` ——> Return/Enter\n- `⌫` ——> Delete\n- `⌦` ——> 向前删除键(Fn + Delete)\n- `↑` ——> 上箭头\n- `↓` ——> 下箭头\n- `←` ——> 左箭头\n- `→` ——> 右箭头\n- `⇞` ——> Page Up(`Fn + ↑`)\n- `⇟` ——> Page Down(`Fn + ↓`)\n- `⇥` ——> 右制表符(Tab键)\n- `⇤` ——> 左制表符(Shift + Tab)\n- `⎋` ——> Escape(Esc)\n- `End` ——> `Fn + →`\n- `Home` ——> `Fn + ←`\n\n## Editing（编辑）\n\n| 快捷键                                      | 作用                                                         |\n| ------------------------------------------- | ------------------------------------------------------------ |\n| `Control + Space`                           | 基本的代码补全（补全任何类、方法、变量）                     |\n| `Control + Shift + Space`                   | 智能代码补全（过滤器方法列表和变量的预期类型）               |\n| `Command + Shift + Enter`                   | 自动结束代码，行末自动添加分号                               |\n| `Command + P`                               | 显示方法的参数信息                                           |\n| `Control + J`                               | 快速查看文档                                                 |\n| `Shift + F1`                                | 查看外部文档（在某些代码上会触发打开浏览器显示相关文档）     |\n| `Command + 鼠标放在代码上`                  | 显示代码简要信息                                             |\n| `Command + F1`                              | 在错误或警告处显示具体描述信息                               |\n| `Command + N, Control + Enter, Control + N` | 生成代码（getter、setter、hashCode、equals、toString、构造函数等） |\n| `Control + O`                               | 覆盖方法（重写父类方法）                                     |\n| `Control + I`                               | 实现方法（实现接口中的方法）                                 |\n| `Command + Option + T`                      | 包围代码（使用if...else、try...catch、for、synchronized等包围选中的代码） |\n| `Command + /`                               | 注释 / 取消注释与行注释                                      |\n| `Command + Option + /`                      | 注释 / 取消注释与块注释                                      |\n| `Option + 方向键上`                         | 连续选中代码块                                               |\n| `Option + 方向键下`                         | 减少当前选中的代码块                                         |\n| `Control + Shift + Q`                       | 显示上下文信息                                               |\n| `Option + Enter`                            | 显示意向动作和快速修复代码                                   |\n| `Command + Option + L`                      | 格式化代码                                                   |\n| `Control + Option + O`                      | 优化 import                                                  |\n| `Control + Option + I`                      | 自动缩进线                                                   |\n| `Tab / Shift + Tab`                         | 缩进代码 / 反缩进代码                                        |\n| `Command + X`                               | 剪切当前行或选定的块到剪贴板                                 |\n| `Command + C`                               | 复制当前行或选定的块到剪贴板                                 |\n| `Command + V`                               | 从剪贴板粘贴                                                 |\n| `Command + Shift + V`                       | 从最近的缓冲区粘贴                                           |\n| `Command + D`                               | 复制当前行或选定的块                                         |\n| `Command + Delete`                          | 删除当前行或选定的块的行                                     |\n| `Control + Shift + J`                       | 智能的将代码拼接成一行                                       |\n| `Command + Enter`                           | 智能的拆分拼接的行                                           |\n| `Shift + Enter`                             | 开始新的一行                                                 |\n| `Command + Shift + U`                       | 大小写切换                                                   |\n| `Command + Shift + ] / Command + Shift + [` | 选择直到代码块结束 / 开始                                    |\n| `Option + Fn + Delete`                      | 删除到单词的末尾                                             |\n| `Option + Delete`                           | 删除到单词的开头                                             |\n| `Command + 加号 / Command + 减号`           | 展开 / 折叠代码块                                            |\n| `Command + Shift + 加号`                    | 展开所以代码块                                               |\n| `Command + Shift + 减号`                    | 折叠所有代码块                                               |\n| `Command + W`                               | 关闭活动的编辑器选项卡                                       |\n\n## Search / Replace（查询/替换）\n\n| 快捷键                | 作用                                                      |\n| --------------------- | --------------------------------------------------------- |\n| `Double Shift`        | 查询任何东西                                              |\n| `Command + F`         | 文件内查找                                                |\n| `Command + G`         | 查找模式下，向下查找                                      |\n| `Command + Shift + G` | 查找模式下，向上查找                                      |\n| `Command + R`         | 文件内替换                                                |\n| `Command + Shift + F` | 全局查找（根据路径）                                      |\n| `Command + Shift + R` | 全局替换（根据路径）                                      |\n| `Command + Shift + S` | 查询结构（Ultimate Edition 版专用，需要在 Keymap 中设置） |\n| `Command + Shift + M` | 替换结构（Ultimate Edition 版专用，需要在 Keymap 中设置） |\n\n## Usage Search（使用查询）\n\n| 快捷键                       | 作用                              |\n| ---------------------------- | --------------------------------- |\n| `Option + F7 / Command + F7` | 在文件中查找用法 / 在类中查找用法 |\n| `Command + Shift + F7`       | 在文件中突出显示的用法            |\n| `Command + Option + F7`      | 显示用法                          |\n\n## Compile and Run（编译和运行）\n\n| 快捷键                                     | 作用                       |\n| ------------------------------------------ | -------------------------- |\n| `Command + F9`                             | 编译 Project               |\n| `Command + Shift + F9`                     | 编译选择的文件、包或模块   |\n| `Control + Option + R`                     | 弹出 Run 的可选择菜单      |\n| `Control + Option + D`                     | 弹出 Debug 的可选择菜单    |\n| `Control + R`                              | 运行                       |\n| `Control + D`                              | 调试                       |\n| `Control + Shift + R, Control + Shift + D` | 从编辑器运行上下文环境配置 |\n\n## Debugging（调试）\n\n| 快捷键                 | 作用                                                         |\n| ---------------------- | ------------------------------------------------------------ |\n| `F8`                   | 进入下一步，如果当前行断点是一个方法，则不进入当前方法体内   |\n| `F7`                   | 进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中 |\n| `Shift + F7`           | 智能步入，断点所在行上有多个方法调用，会弹出进入哪个方法     |\n| `Shift + F8`           | 跳出                                                         |\n| `Option + F9`          | 运行到光标处，如果光标前有其他断点会进入到该断点             |\n| `Option + F8`          | 计算表达式（可以更改变量值使其生效）                         |\n| `Command + Option + R` | 恢复程序运行，如果该断点下面代码还有断点则停在下一个断点上   |\n| `Command + F8`         | 切换断点（若光标当前行有断点则取消断点，没有则加上断点）     |\n| `Command + Shift + F8` | 查看断点信息                                                 |\n\n## Navigation（导航）\n\n| 快捷键                                                      | 作用                                                         |\n| ----------------------------------------------------------- | ------------------------------------------------------------ |\n| `Command + O`                                               | 查找类文件                                                   |\n| `Command + Shift + O`                                       | 查找所有类型文件、打开文件、打开目录，打开目录需要在输入的内容前面或后面加一个反斜杠 |\n| `Command + Option + O`                                      | 前往指定的变量 / 方法                                        |\n| `Control + 方向键左 / Control + 方向键右`                   | 左右切换打开的编辑 tab 页                                    |\n| `F12`                                                       | 返回到前一个工具窗口                                         |\n| `Esc`                                                       | 从工具窗口进入代码文件窗口                                   |\n| `Shift + Esc`                                               | 隐藏当前或最后一个活动的窗口，且光标进入代码文件窗口         |\n| `Command + Shift + F4`                                      | 关闭活动 run/messages/find/... tab                           |\n| `Command + L`                                               | 在当前文件跳转到某一行的指定处                               |\n| `Command + E`                                               | 显示最近打开的文件记录列表                                   |\n| `Option + 方向键左 / Option + 方向键右`                     | 光标跳转到当前单词 / 中文句的左 / 右侧开头位置               |\n| `Command + Option + 方向键左 / Command + Option + 方向键右` | 退回 / 前进到上一个操作的地方                                |\n| `Command + Shift + Delete`                                  | 跳转到最后一个编辑的地方                                     |\n| `Option + F1`                                               | 显示当前文件选择目标弹出层，弹出层中有很多目标可以进行选择(如在代码编辑窗口可以选择显示该文件的 Finder) |\n| `Command + B / Command + 鼠标点击`                          | 进入光标所在的方法/变量的接口或是定义处                      |\n| `Command + Option + B`                                      | 跳转到实现处，在某个调用的方法名上使用会跳到具体的实现处，可以跳过接口 |\n| `Option + Space, Command + Y`                               | 快速打开光标所在方法、类的定义                               |\n| `Control + Shift + B`                                       | 跳转到类型声明处                                             |\n| `Command + U`                                               | 前往当前光标所在方法的父类的方法 / 接口定义                  |\n| `Control + 方向键下 / Control + 方向键上`                   | 当前光标跳转到当前文件的前一个 / 后一个方法名位置            |\n| `Command + ] / Command + [`                                 | 移动光标到当前所在代码的花括号开始 / 结束位置                |\n| `Command + F12`                                             | 弹出当前文件结构层，可以在弹出的层上直接输入进行筛选（可用于搜索类中的方法） |\n| `Control + H`                                               | 显示当前类的层次结构                                         |\n| `Command + Shift + H`                                       | 显示方法层次结构                                             |\n| `Control + Option + H`                                      | 显示调用层次结构                                             |\n| `F2 / Shift + F2`                                           | 跳转到下一个 / 上一个突出错误或警告的位置                    |\n| `F4 / Command + 方向键下`                                   | 编辑 / 查看代码源                                            |\n| `Option + Home`                                             | 显示到当前文件的导航条                                       |\n| `F3`                                                        | 选中文件 / 文件夹 / 代码行，添加 / 取消书签                  |\n| `Option + F3`                                               | 选中文件 / 文件夹/代码行，使用助记符添加 / 取消书签          |\n| `Control + 0…Control + 9`                                   | 定位到对应数值的书签位置                                     |\n| `Command + F3`                                              | 显示所有书签                                                 |\n\n## Refactoring（重构）\n\n| 快捷键                 | 作用                               |\n| ---------------------- | ---------------------------------- |\n| `F5`                   | 复制文件到指定目录                 |\n| `F6`                   | 移动文件到指定目录                 |\n| `Command + Delete`     | 在文件上为安全删除文件，弹出确认框 |\n| `Shift + F6`           | 重命名文件                         |\n| `Command + F6`         | 更改签名                           |\n| `Command + Option + N` | 一致性                             |\n| `Command + Option + M` | 将选中的代码提取为方法             |\n| `Command + Option + V` | 提取变量                           |\n| `Command + Option + F` | 提取字段                           |\n| `Command + Option + C` | 提取常量                           |\n| `Command + Option + P` | 提取参数                           |\n\n## VCS / Local History（版本控制 / 本地历史记录）\n\n| 快捷键               | 作用                       |\n| -------------------- | -------------------------- |\n| `Command + K`        | 提交代码到版本控制器       |\n| `Command + T`        | 从版本控制器更新代码       |\n| `Option + Shift + C` | 查看最近的变更记录         |\n| `Control + C`        | 快速弹出版本控制器操作面板 |\n\n## Live Templates（动态代码模板）\n\n| 快捷键                 | 作用                                           |\n| ---------------------- | ---------------------------------------------- |\n| `Command + Option + J` | 弹出模板选择窗口，将选定的代码使用动态模板包住 |\n| `Command + J`          | 插入自定义动态代码模板                         |\n\n## General（通用）\n\n| 快捷键                    | 作用                                                         |\n| ------------------------- | ------------------------------------------------------------ |\n| `Command + 1…Command + 9` | 打开相应编号的工具窗口                                       |\n| `Command + S`             | 保存所有                                                     |\n| `Command + Option + Y`    | 同步、刷新                                                   |\n| `Control + Command + F`   | 切换全屏模式                                                 |\n| `Command + Shift + F12`   | 切换最大化编辑器                                             |\n| `Option + Shift + F`      | 添加到收藏夹                                                 |\n| `Option + Shift + I`      | 检查当前文件与当前的配置文件                                 |\n| `Control + ``             | 快速切换当前的 scheme（切换主题、代码样式等）                |\n| `Command + ,\t`         | 打开 IDEA 系统设置                                           |\n| `Command + ;`             | 打开项目结构对话框                                           |\n| `Shift + Command + A`     | 查找动作（可设置相关选项）                                   |\n| `Control + Shift + Tab`   | 编辑窗口标签和工具窗口之间切换（如果在切换的过程加按上 delete，则是关闭对应选中的窗口） |\n\n\n\n\t\n\n​\t\n\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n​\t\n\n\n\n\n\n\n\n​\t\n\n\n\t\n\t\n\t\n\n​\t\n\n\n\n\n\n\t\n\t\n\t\n\t\n\t\n\n​\t\n\n","tags":["MacOS"],"categories":["MacOS"]},{"title":"Mac 高效shell终端设置","url":"/2019-07-10/reference/MacOS/mac-shell/","content":"\n## iterm2 快捷键\n\n### 光标控制\n\n| 快捷键       | 描述                                                         |\n| ------------ | ------------------------------------------------------------ |\n| `ctrl + a`   | 到行首                                                       |\n| `ctrl + e`   | 行末                                                         |\n| `ctrl + u`   | 清除当前行                                                   |\n| `ctrl + f/b` | 前进后退，相当于左右方向键，但是显然比移开手按方向键更快     |\n| `ctrl + p`   | 上一条命令，相当于方向键上                                   |\n| `ctrl + r`   | 搜索命令历史，这个大家都应该很熟悉了                         |\n| `ctrl + d`   | 删除当前字符                                                 |\n| `ctrl + h`   | 删除之前的字符                                               |\n| `ctrl + w`   | 删除光标前的单词                                             |\n| `ctrl + k`   | 删除到文本末尾                                               |\n| `ctrl + t`   | 交换光标处文本                                               |\n| `⌘ + —/+/0`  | 调整字体大小                                        |\n|  `⌘ + r` | 清屏，其实是滚到新的一屏，并没有清空。`ctrl + l` 也可以做到           |\n\n### 标签\n\n| 快捷键                                | 描述     |\n| ------------------------------------- | -------- |\n| `command + t`                         | 新建标签 |\n| `command + w`                         | 关闭标签 |\n| `command + 数字 command + 左右方向键` | 切换标签 |\n| `command + enter`                     | 切换全屏 |\n| `command + f`                         | 查找     |\n\n### 分屏\n\n| 快捷键                                         | 描述           |\n| ---------------------------------------------- | -------------- |\n| `command + d`                                  | 垂直分屏       |\n| `command + shift + d`                          | 水平分屏       |\n| `command + option + 方向键 或者 command + [ ]` | 切换屏幕       |\n| `command + ;`                                  | 查看历史命令   |\n| `command + shift + h`                          | 查看剪贴板历史 |\n\n### 自带有哪些很实用的功能/快捷键\n\n| 快捷键                      | 描述                      |\n| --------------------------- | ------------------------- |\n| `⌘ + 数字`                  | 在各 tab 标签直接来回切换 |\n| `选择即复制 + 鼠标中键粘贴` | 这个很实用                |\n| `⌘ + f`                     | 所查找的内容会被自动复制  |\n| `输入开头命令后 按 ⌘ + ;`   | 会自动列出输入过的命令    |\n\n\n\n---\n\n\n## Reference\n\n\n> [mac：高效shell终端设置](<https://my.oschina.net/wii01/blog/1486254>)\n>\n> [zsh+on-my-zsh配置教程指南（程序员必备）](https://segmentfault.com/a/1190000013612471)\n\n","tags":["MacOS"],"categories":["MacOS"]},{"title":"CCKS-2017 行业知识图谱构建与应用-下篇","url":"/2019-07-03/行业知识图谱构建与应用-下篇/","content":"\n\n## **行业知识图谱关键技术**\n\n上篇我们讲行业知识图谱生命周期划分为6个阶段，分为知识建模，知识获取，知识融合，知识存储，知识计算和知识应用。接下来我们详细描述每个阶段具体是如何完成的，有没有现成可用的工具、工具的优缺点如何、没有现成工具的话如何实现；同时，我们会以金融证券领域的创投知识图谱为例来进行实战描述。\n\n## **知识建模**\n\n即为行业的知识和数据进行抽象建模。上篇描述了行业数据的特点和企业面临的数据利用挑战，也提到了使用行业知识图谱来克服这些难题。具体而言，首先使用知识图谱相关技术对行业知识和数据进行建模：\n\n*   以**实体**为主体目标，实现对不同来源的数据进行映射与合并。（实体抽取与合并）\n*   利用**属性**来表示不同数据源中针对实体的描述，形成对实体的全方位描述。（属性映射与归并）\n*   利用**关系**来描述各类抽象建模成实体的数据之间的关联关系，从而支持关联分析。（关系抽取）\n*   通过**实体链接**技术，实现围绕实体的多种类型数据的关联存储。（实体链接）\n*   使用**事件**机制描述客观世界中动态发展，体现事件与实体间的关联；并利用**时序**描述事件的发展状况。（动态事件描述）\n\n了解语义网或知识工程的朋友应该都使用过或者听过Protégé，它是由斯坦福大学开发的一个本体编辑器软件，基于RDF(S)，OWL等语义网规范，并且还提供图形化界面以及在线版本（WebProtégé）。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-ddc7b9658b96ff67.jpg)\n\n但是Protégé在使用过程中存在以下不足之处：\n\n1.  基本只提供单人编辑，在线版本的并发功能支持也不完善；并发编辑时需要通过文件共享来实现；\n2.  完全依靠人工，难以实现与知识图谱构建（半）自动化过程的交互。\n3.  不支持复杂事件及时态的建模；\n4.  因为基于单机构建，因此对大数据量支持不够，会出现内存溢出； 因此，可以说Protégé适用于原型快速构建的场景，在生产环境下使用会遇到各种问题。\n\n通过我们在行业中的经验积累，结合以上几个问题，总结得出构建一个良好的知识建模工具应该具备如下一些特性：\n\n**一）在线并发协作编辑**\n\n支持在线并发协作编辑，能够将编辑的知识实时保存，当其它用户对当前用户正在编辑的内容有更新时，系统自动提示加载最新版本，因此能够有效地解决并发知识编辑冲突。\n\n这是我们所构建的知识建模的在线编辑界面\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-7b21755c8f2646c8.jpg)\n\n这是上下位关系定义界面：\n\n这是属性定义编辑界面：\n\n**二）自动导入、集成**\n\n同时它提供导入、集成功能，能够把现有的知识通过导入功能进行集成，以存储为桥梁，可以对自动算法的结果进行编辑。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-1b8a6285775f5fa9.jpg)\n\n**三）支持复杂动态事件建模**\n\n在对动态事件数据的建模时，使用时态信息存储实现事件时间的描述。\n\n该图为融资事件的建模示例\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-4b7137410d977541.jpg)\n\n**四）大数据量支持**\n\n支持大数据量的知识图谱编辑：编辑是基于底层的知识图谱存储的，每次编辑时加载到前端的仅为当前相关的数据，因此不会造成内存溢出等问题。\n\n## **知识获取**\n\n针对不同种类的数据，我们利用不同的技术进行获取。利用D2R从结构化数据库中获取知识，利用包装器Wrapper从半结构化数据中获取知识，利用文本信息抽取技术从非结构化文本中获取知识。\n\n**一）D2R**\n\n我们先看下目前主流的D2R工具—D2RQ，D2RQ是一个将关系数据库转换为虚拟的RDF数据库的平台，主要包括三个模块\n\n1\\. D2R Server，HTTP Server，提供对 RDF数据的查询访问接口，以供上层的 RDF 浏览器、SPARQL查询客户端以及传统的 HTML 浏览器调用；\n\n2\\. D2RQ Engine：利用一个可定制的 D2RQ Mapping 文件将关系型数据库中的数据换成 RDF 格式；\n\n3\\. D2RQ Mapping Language：定义将关系型数据转换成 RDF 格式的 Mapping 规则。\n\nD2RQ的主体架构如下：\n\n从其功能描述我们可知D2RQ是直接将关系型数据库转换成RDF的，因此难以与知识建模结果对应，同样也难以同其他知识进行融合，在新数据的增量映射以及海量数据映射时会出现问题。\n\n因此我们的解决方案是将D2R映射与知识建模结合，在数据模式的基础上进行映射；例如从数据库中的“企业信息表”中把记录映射成概念“企业”下的实体。\n\n同时通过设置合并条件，把D2R的结果与知识图中的已有知识进行融合；例如对于企业，设置“如果企业名称相同则进行合并”的规则。\n\n在实现数据的增量映射功能时，通过特定的关键词及规则来设置数据更新的标记；例如，对于企业，设置“若成立时间为上次更新时间之后的企业为新的企业”。\n\n最终经过D2R映射的数据直接存储成为知识图谱中的知识，因而其数据量仅取决于存储的支撑量，也就不存在海量数据映射会导致的性能问题。\n\n下面是D2R操作过程演示的视频，从创投新闻数据库中进行实体、实体属性和关系知识的抽取。\n\n [![视频封面](https://upload-images.jianshu.io/upload_images/18390058-400b2f537e635ebf.jpg)\n\nPlantData平台D2R演示-公众号版_腾讯视频\n\nv.qq.com视频](https://v.qq.com/x/cover/u05458xapar/u05458xapar.html) \n\n**二）包装器**\n\n半结构化行业数据源解析一般采用**包装器**的方式，由于这些行业的数据源网站大多通过模板生成。包装器可以自动进行学习，但为了保证准确度，我们通常会使用**人机结合**的方法。\n\n在行业数据源解析实践中，由于网站的高度可变性，因此目前尚没有统一的包装器工具，所以在实际应用中，**通常针对不同结构的数据配置相应的包装器**，完成数据的解析。\n\n下图为我们研发使用的包装器配置工具，一般分为5个步骤，前两步较为直观，首先设置输入源，可以是网页的URL、网页的源码文件等，然后进行预处理，主要是过滤CSS、JavaScript等与信息抽取无关的噪声数据；第三步，配置抽取的目标，指定抽取的是知识图谱中的何种元素；第四步中我们为抽取的目标设置了一系列的抽取规则，包括网页标签规则、前置规则、后置规则，正则表达式等；最后是对抽取结果的过滤等后处理操作\n\n下面我们来看几个从半结构化数据中抽取专利知识的包装器的示例\n\n示例1：抽取专利，设置抽取的目标为专利实体名称及专利的属性，然后依据网页的特征设置相应的规则。\n\n示例2：另一个专利信息抽取的例子\n\n**三）文本信息抽取**\n\n行业知识图谱构建过程中文本信息抽取的任务非常多，这里主要讨论实体识别、概念抽取、关系抽取以及事件抽取。\n\n信息抽取的方法主要有CloseIE和OpenIE两类，两者的对比状况如下：\n\nOpenIE 的典型代表工具有 ReVerb、TextRunner。由于OpenIE 工具准确率比较低，会增加知识融合的难度，因此在行业知识图谱构建中实用性不高。通常被用于做第一轮的信息抽取探索，从它的结果中发现新的关系，然后在此基础上应用其它的信息抽取方法。\n\nCloseIE的典型工具为DeepDive。\n\nDeepdive是由斯坦福大学InfoLab实验室开发的一个开源知识抽取系统。它通过弱监督学习，从非结构化的文本中抽取结构化的关系数据，核心关键点是能够在更短的时间内提供更高质量的数据。\n\n其基于联合推理的算法，让用户只需要关心**特征**本身，要求用户**思考特征而不是算法**，而其他机器学习系统则要求开发者思考聚类算法、分类算法的使用等；同时DeepDive允许**用户使用简单的规则来影响学习过程以提升结果的质量**，也会**根据用户反馈来提高预测的准确度**；DeepDive使用机器学习算法训练系统来减少各种形式的噪音和不确定性，并**为每一个决断进行复杂的可能性计算**。\n\n下图为Deep Dive进行关系抽取的基本过程，首先指定抽取任务；其次进行中文分词、命名实体识别；再进行实体定位；生成候选实体关系对；接着使用DDlib为候选词和候选关系自动生成特征；加载标注的数据源；然后基于规则的远程监督自动标注；模型训练；最终输出结果。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-02446ff982bc689a.jpg)\n\n总结一下：DeepDive主要针对**关系抽取**，在指定的关系抽取中效果比较理想，在实体确定后可以很好地进行关系抽取。同时也支持中文关系抽取，仅需要引入中文相关的基础处理工具即可。不足之处在于未提供专门的针对概念、实体和事件抽取的支持，同时需要大量的标注语料支持，并通过人工设置标注规则。\n\n对于文本信息抽取的方法，目前还没有统一的实现各类信息抽取的现成工具。 我们采用的是把现有的工具进行集成，依据抽取任务使用不同的工具，包括\n\n*   NLP分词、命名实体识别工具：NLPIR、LTP、FudanNLP、Stanford NLP……\n*   关系抽取工具：DeepDive\n\n而对于行业抽取任务，需要针对性的方法来完成，通常采用的做法是基于已有的结构化知识进行远程监督学习。\n\n接下来介绍事件抽取\n\n事件抽取可以分为预定义事件抽取和开放域事件抽取，行业知识图谱中主要为**预定义事件抽取**。 我们通常会采用采用模式匹配方法，包括三个步骤：\n\n1.  准备事件触发词表\n2.  候选事件抽取：寻找含有触发词的句子\n3.  事件元素识别：根据事件模版抽取相应的元素\n\n下图为创投知识图谱融资事件抽取的示例，事件元素识别的过程与包装器模板配置过程基本相似\n\n知识抽取实践过程中，我们采用的是一种称为“多策略学习”的方法；“多策略”体现在多数据源、多目标类型、多抽取方法，总体原则是：利用不同数据源之间的冗余信息，使用较易抽取的信息（结构化数据库）来辅助抽取那些不易抽取的信息。\n\n下图是多策略学习方法的整体过程示意图：\n\n以下为我们的多策略学习方法示例\n\n## **知识融合**\n\n知识图谱中的知识融合是一件非常复杂的工作，包括数据模式层（概念、概念的上下位关系、概念的属性）的融合与数据层的融合。行业知识图谱的数据模式通常采用自顶向下和自底向上结合的方式，因此基本都经过人工的校验，保证了可靠性；所以知识融合的关键任务在数据层的融合。对于数据层的融合，为保证数据的质量，通常**在知识抽取环节中进行控制**，减少知识融合过程的难度。\n\n下面直接介绍我们的实践方案\n\n**数据模式层融合方法**：\n\n行业知识图谱的数据模式层通常是由专家人工构建或从可靠的结构化数据中映射得到的，通常在映射时会通过**设置融合的规则**来确保数据的统一。\n\n**数据层的融合**：\n\n实体合并，在构建行业知识图谱时，实体优先从结构化的数据中获取；\n\n对于结构化的数据，通常有对实体进行唯一标识的主键，因此在进行知识抽取时即可设定实体合并的依据；\n\n从非结构化数据中抽取的实体，同样使用设置合并条件的规则来完成实体的合并；例如：企业合并可以通过企业名称直接合并；企业高管合并可以采用人名相同+同一企业进行合并，因为同一家企业高管中同名的概率是极低的。\n\n实体属性与关系的合并，具有时态特性的属性，我们可以使用新的数据覆盖老的数据；其次可依据数据源的可靠性进行选取，通常结构化数据源中的质量较高。\n\n以人物实体合并为例来看下数据层融合的方法过程：\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-cbcd610ce2a3c4cb.jpg)\n\n## **知识存储**\n\n针对知识存储，我们微信服务号之前《[大规模知识图谱数据存储实战解析](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI5NjY0NDEyNg%3D%3D%26mid%3D2247483806%26idx%3D1%26sn%3D952518d2f15cbf8ddf494a6c1684666b%26chksm%3Dec407aefdb37f3f9303cde2e994f44f04e2b7b4cabdb6ca305f828e9ac792085d1aad38703ad%26scene%3D21%23wechat_redirect)》里详细介绍过，主要理念就是“**使用不同的存储来实现不同类型数据的存储和使用**”。\n\n知识图谱是基于图的数据结构，其存储方式主要有两种方式：**RDF存储** 和 **图数据库(Graph Database)**。Wikipedia对两者的解释分别如下：\n\n> A triplestore or RDF store is a purpose-built database for the storage and retrieval of triples through semantic queries. A triple is a data entity composed of subject-predicate-object. [Wikipedia]\n\n> A graph database has a more generalized structure than a triplestore, using graph structures with nodes, edges, and properties to represent and store data. [Wikipedia]\n\n下图为db-engine中常见图数据存储的排名，里面的多个Graph DBMS我们都有介绍过。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-9e101efb95fbf8db.jpg)\n\n如何选用合适的图数据存储呢？下列指标是我们重点考量的：\n\n*   数据存储支持\n*   数据操作和管理方式\n*   支持的图结构\n*   实体和关系表示\n*   查询机制\n\n针对主流图数据存储我们整理了如下表格，从数据存储支持、数据操作和管理方式、支持的图结构、实体和关系表示以及查询机制角度来进行全面的比较（数据可能不全是最新的）。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-e5f546d6054c0e44.jpg)\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-fadb219f196087cb.jpg)\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-849e98c1fa8210c0.jpg)\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-c04eefbb212d2864.jpg)\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-b37d704b19dac365.jpg)\n\n介绍一下Neo4j，它是当前图数据存储排名第一的图数据库，它具有支持原生图存储和处理、支持ACID事务处理的特点，同时neo4j不使用schema，因此在开始添加数据之前，你是不需要定义概念、属性和关系的，但这样带来的不足之处在于，在企业数据管理场景下如不使用 Schema则难以从整体把握数据；同时neo4j也不支持时态信息的存储，非企业版本会受到数据量、查询速度等方面的限制。\n\n我们的大规模知识图谱存储解决方案如下：\n\n1.  **基础存储**\n\n1.  可按数据场景选择使用关系数据库、NoSQL数据库及内存数据库。\n2.  基础存储保证可扩展、高可用\n\n3.  **数据分割**\n\n1.  属性表：依据数据类型划分\n\n1.  基本类型：整数表、浮点数表、日期类型表、…\n2.  集合类型：List型表、Range型表、Map型表、…\n\n3.  大属性单独列表：例如数量超过10M的属性单独列表\n\n5.  **缓存与索引**\n\n1.  使用分布式 Redis 作为缓存，按需对数据进行缓存。\n2.  对三元组表按需进行索引，最多情况下可建立九重索引。\n\n7.  **善于使用现在成熟存储**\n\n1.  使用 ElasticSearch 实现数据的全文检索\n2.  结构固定型的数据可使用关系数据库或NoSQL\n\n9.  **对于非关系型的数据尽量不入图存储，避免形成大节点**\n\n1.  非关系型的数据，使用适合的数据存储机器进行存储，通过实体链接的方式实现与图谱数据的关联。\n\n11.  **不直接在图存储中进行统计分析计算**\n\n1.  对于需要进行统计分析计算的数据，需要导出到合适的存储中进行。\n\n前面还提到知识图谱中的时态信息，时态信息的需求与技术一直是伴随着数据库技术的发展而产生和发展的，下图为时态数据库与其他类型数据库相比的发展趋势：\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-2b34754173473c63.jpg)\n\n可以发现时态数据库的发展趋势远远高于与其他数据存储方式。\n\n知识图谱中的时态信息包含事实的生成时间和某事实的有效时间段，比如融资事件的事件。\n\n因此在知识图谱时态信息存储的实践应用中，我们使用了**历史数据库**，用于记录事实的有效时间，用有限的数据冗余实现数据时态信息的应用。\n\n具体实践原则如下：\n\n*   在基础知识图谱的基础上，构建针对时态数据处理的中间件；\n*   对于特定类型的时序型数据，采用其它的存储机制进行存储。\n\n关于知识图谱数据存储的更详细的内容，请参看之前公众号发布的详细专题文章。\n\n## **知识计算**\n\n知识计算的范畴同样非常广，这里仅以图挖掘计算、基于本体的推理以及基于规则的推理三个最常用的技术进行介绍。\n\n**一）图挖掘计算**\n\n前面讲到知识图谱是一种基于图的数据结构，因此它自然会集成实现基本图算法，这些算法在我们之前那篇《[图谱在手 天下我有](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI5NjY0NDEyNg%3D%3D%26mid%3D2247484181%26idx%3D1%26sn%3D4a9b9ee30168538b4a71f12b36497ba4%26chksm%3Dec407864db37f1728259f17671687c7ff40813f7645904ac15ef442f0cd71e912e25f9e7cffd%26scene%3D21%23wechat_redirect)》文章中详细介绍过，可以参照对比阅读。\n\n列举一些我们常用的图算法：\n\n1.  图遍历：广度优先遍历、深度优先遍历\n2.  最短路径查询： Dijkstra（迪杰斯特拉算法）、Floyd（弗洛伊德算法）\n3.  路径探寻：给定两个或多个节点，发现它们之间的关联关系\n4.  权威节点分析：PageRank算法\n5.  族群发现：最大流算法\n6.  相似节点发现：基于节点属性、关系的相似度算法\n\n其中权威节点分析做过社交网络分析的人应该都知道，可以用来做社交网络里的权威人物分析，我们在创投知识图谱中用来做权威投资机构的发现。\n\n族群发现算法一般用来在社交网络中主题社区的发现，在这里我们同样可以用来识别企业知识图谱中的派系（阿里系、腾讯系）。\n\n相似节点发现应用就更加广泛了，在企业知识图谱中可以做相似企业的发现，这里有个很重要的实际应用场景，可以利用相似企业进行精准的获客营销。\n\n**二）基于本体的推理**\n\n本体推理基本方法包括：\n\n*   基于表运算及改进的方法：FaCT++、Racer、 Pellet Hermit等\n*   基于一阶查询重写的方法（Ontology based data access，基于本体的数据访问）\n*   基于产生式规则的算法（如rete）：Jena 、Sesame、OWLIM等\n*   基于Datalog转换的方法如KAON、RDFox等\n*   回答集程序 Answer set programming\n\n这里我们介绍一个本体知识推理工具：RDFox，它的特点如下：\n\n*   支持共享内存并行OWL 2 RL推理\n*   三元组数据可以导出为Turtle文件，规则文件可以导出为RDF数据记录文件；全部数据内容可以导出为二进制文件，可完全恢复数据存储状态\n*   支持Java、Python多语言APIs访问，并且 RDFox 还支持一种简单的脚本语言与系统的命令行交互\n\n由于RDFox是完全基于内存的，所以对硬件的要求较高。\n\n当然基于本体的知识推理应用也非常的多，比如我们在实际场景中的冲突检测。因为不管是手动构建，还是自动构建知识图谱，都会碰到这样一个问题：或者数据来源不同，或者构建的人员不同、方法不同，这就会不可避免的导致一些冲突，这些冲突自身很难直观的去发现，但是可以利用知识图谱里面的冲突检测去发现存在的有矛盾的、有冲突的知识。\n\n下图为我们从不同渠道获取花椒直播融资金额的冲突检测示例\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-6ef3770f0dfec142.jpg)\n\n**三）基于规则的推理**\n\n基于规则的推理是在知识图谱基础知识的基础上，专家依据行业应用的业务特征进行规则的定义，这在业务应用中是非常常见的。\n\n介绍一下我们常用的Drools（因被JBOSS收购，现已更名为JBoss Rules），它是为Java量身定制的基于Charles Forgy的RETE算法的规则引擎的实现，使用了OO接口的RETE,使得商业规则有了更自然的表达，其推理的效率也比较高。\n\n结合规则引擎工具，基于基础知识与所定义的规则，执行推理过程给出推理结果。\n\n以下代码为我们使用drools定义的一个高风险企业规则\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-9e33df3ff7b6d276.jpg)\n\n## **知识应用**\n\n关于知识应用我们主要介绍以下三个方向的关键技术：语义搜索、智能问答和可视化辅助决策。\n\n**一）语义搜索**\n\n知识图谱提出的初衷即为解决搜索的准确率问题，改进搜索质量；由于传统基于关键词的检索完全不考虑语义信息，因此传统的搜索主要面临两个难题：\n\n1.  自然语言表达的多样性\n2.  自然语言的歧义\n\n对此我们的解决方案为利用实体链接技术，进行基于知识图谱的语义搜索。\n\n先看下目前常见的两款实体链接工具\n\nWikipedia Miner，在应用中集成维基百科的数据。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-d485f45eaf09c563.jpg)\n\n该工具用来分析有歧义的实体的上下文和发现出现在维基百科里的概念。\n\n另一个广泛使用的基于维基百科的语义标注系统是DBpedia Spotlight，这是一个免费的可定制的web系统，它通过DBpeida的URIs标注文本，其目标是DBpedia本体。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-927a511606f4e34f.jpg)\n\n市面上目前现有的实体链接工具大部分都是针对百科类的知识库工作的，基本上不支持中文的处理。\n\n在我们具体实践过程中采用了如下三种实体链接方法：\n\n1.  基于向量模型相似度计算的实体链接方法\n2.  基于知识图谱语义扩展的实体链接方法\n3.  基于propagation计算相似度的实体链接方法\n\n实体链接的基本方法过程如下图：\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-8c0760657359ce7f.jpg)\n\n该视频为基于实体连接的创投数据语义标注演示：\n\n [![视频封面](https://upload-images.jianshu.io/upload_images/18390058-2dc9be02efff1217.jpg)\n\nPlanData平台创投新闻标注--公众号_腾讯视频\n\nv.qq.com视频](https://v.qq.com/x/cover/m0545qe2i6r/m0545qe2i6r.html) \n\n实体链接完成后，我们可以把它引入到基于知识图谱的语义搜索中。语义搜索是对传统搜索的一种改变，其发展的最终形态为智能问答。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-df09efb8193dedda.jpg)\n\n最后看一个创投知识图谱中的语义搜索示例，在PlantData平台中搜索“北京小桔科技”实体，会识别出这是一个企业实体，系统会自动返回企业相关的融资状况，高管信息以及链接的新闻数据信息。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-88773c81122e8943.jpg)\n\n**二）智能问答**\n\n智能问答是指用户以自然语言提问的形式提出信息查询需求，系统对用户查询意图分析与理解，从各种数据资源中自动查询检索出最符合用户意图的答案。\n\n以下为基于知识图谱的自动问答系统的基本过程流程图\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-e089207bc7f7f916.jpg)\n\n智能问答现在的方法主要有这么四类\n\n**1\\. 基于信息检索的方法**\n\n基于信息检索的方法首先利用中文分词，命名实体识别等自然语言处理工具找到问句中所涉及到的实体和关键词，然后去知识资源库中去进行检索。它的优点在于实现简单，应用面广，在大部分场景下均可得到结果。缺点在于要求答案中必须至少包含问句中的一个字或词，所以不如语义解析方法精确。改进的方法可以利用基于知识图谱的知识进行**语义扩充**，提高匹配率，同时基于知识图谱进行检索时的**语义消岐**。\n\n**2\\. 基于语义分析的方法**\n\n基于语义分析的方法非常符合人们的直觉，它将一个自然语言形式的问句，按照特定语言的语法规则，解析成语义表达式，在得到语义表达式之后，我们可以非常容易地将其转化为某种数据库的查询语言。最常用的方法是利用组合范畴语法（CCG），CCG的核心是词汇，首先自然语言问句的词汇被映射到语义表达式中的词汇，然后按照特定的语法规则将汇组合起来，进而得到了最终的语义表达式。我们的做法是在特定的领域里边，**基于知识图谱的实体、属性、概念等进行词法解析与映射**，然后基于图结构进行语法规则匹配，这就相当于是图里面的子图查询匹配问题。\n\n**3\\. 基于规则的专家系统方法**\n\n专家系统是一个具有大量的专门知识与经验的程序系统，它应用人工智能技术和计算机技术，根据某领域一个或多个专家提供的知识和经验，进行推理和判断，模拟人类专家的决策过程，以便解决那些需要人类专家处理的复杂问题。\n\n在智能问答系统中，不是所有的问题都可以利用现存的知识库直接进行回答，有很多隐含知识我们需要通过已经抽取到的知识进行推理回答。因此基于知识推理的方法通常不单独使用，而是与其它的方法进行结合，增强对复杂问题回答的支持。\n\n**4\\. 基于深度学习的方法**\n\n近几年卷积神经网络（CNN）和循环神经网络（RNN）在NLP领域任务中表现出来的语言表示能力，越来越多的研究人员尝试深度学习的方法完成问答领域的关键任务，包括问题分类（question classification），语义匹配与答案选择（answer selection），答案自动生成（answer generation）。此外，互联网用户为了交流信息而产生的大规模诸如微博回复、社区问答对的自然标注数据，给训练深度神经网络模型提供了可靠的数据资源，并很大程度上解决自动问答研究领域的数据匮乏问题。\n\n该方法优点是实现“**端到端**”的问答：把问题与答案均使用复杂的特征向量表示，使用深度学习来计算问题与答案的相似度。不足之处在于不支持复杂的查询；需要比较长的训练过程，不适用于现实应用场景中的知识更新后的实时查询。\n\n知识图谱应该算是自动问答里面的大脑，我们在实践过程中选用的最佳方法是**基于语义解析的方法**加上**基于信息检索的方法**。\n\n这样做的好处在于**基于语义解析的方法可解释性强**，并且能够方便地转换成知识图谱的查询，给出明确的答案；因此对于用户输入，首先使用基于语义解析的方法进行回答；\n\n而**基于信息检索的方法应用面广**，因此当语义解析方法无法给出结果时，则使用信息检索的方法进行回答。\n\n下面两张图是我们在创投行业应用里实践智能问答的示例，首先我们会人工配置语义解析的模板，其次对用户的输入进行分词匹配，与知识图谱里面的元素进行映射，即知识图谱通用的子图匹配模板。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-907c03f3f9a3641e.jpg)\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-d9b110260ffc9b29.jpg)\n\n**三）可视化辅助决策**\n\n首先介绍两个比较常见的可视化工具D3.js和ECharts。\n\nD3.js全称Data-Driven Documents，是一个用动态图形显示数据的JavaScript库，一个数据可视化工具，它提供了各种简单易用的函数，大大方便了数据可视化的工作。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-04e13e8afaf4f972.jpg)\n\nECharts是一款由百度前端技术部开发的，同样基于Javascript的数据可视化图标库。（个人觉得这是目前百度最有价值的产品）它提供大量常用的数据可视化图表，底层基于ZRender（一个全新的轻量级canvas类库），创建了坐标系，图例，提示，工具箱等基础组件，并在此上构建出折线图（区域图）、柱状图（条状图）、散点图（气泡图）、饼图（环形图）、K线图、地图、力导向布局图以及和弦图，同时支持任意维度的堆积和多图表混合展现。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-62e7ca74052ed0c1.jpg)\n\n这两款都是非常优秀的可视化工具，但是目前还没有一款面向知识图谱的可视化工具，在选择知识图谱可视化的时候，需要思考以下三点问题：\n\n1.  依托的设备及环境是什么？\n2.  需要展现数据的什么特点？\n3.  数据量过大（小）时我该怎么做？\n\n我们最终选用集成现有的可视化工具，实现知识图谱的可视化。以下是PlantData的图谱可视化基本组件，包括图谱展示、统计分析等。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-18e4df1c5865511f.jpg)\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-6f917ad5dd9906e2.jpg)\n\n这是PlantData平台在创投图谱中进行可视化应用的演示视频\n\n [![视频封面](https://upload-images.jianshu.io/upload_images/18390058-570d35e4eea1bfe8.jpg)\n\nPlantData平台图谱可视化应用--公众号_腾讯视频\n\nv.qq.com视频](https://v.qq.com/x/cover/m05454e3gr0/m05454e3gr0.html) \n\n## **PlantData**\n\n基于知识图谱各过程的最佳实践，我们构建了一个**基于知识图谱大数据的统一决策支持分析平台**——PlantData，下图描述了平台中从数据源获取到到最终的知识应用的所有过程。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-918cc8ab78cfa23d.jpg)\n\n底层首先对多源异构的数据源进行数据的采集，采集即运用信息抽取相关的技术对数据尤其是非结构化数据进行处理，然后形成结构化的知识，再往上进行数据的融合与合并，形成知识图谱形式的知识，存储在右边大规模知识图谱数据存储引擎中，最上面是一个统一的消费平台，通过数据可视化，知识计算，语义检索智能问答，以及与数据分析挖掘相关的一些方法去提供给用户。\n\n**未来规划**\n\n最后介绍下我们PlantData平台未来的规划\n\n*   在更多的行业中应用知识图谱技术\n*   研发更丰富的组件，使用户可以组合不同的组件实现应用场景\n*   面向非商业用户的开放知识图谱平台，为开放知识图谱社区作贡献\n\n*   基础平台开放：让用户可以快速地在线体验知识图谱行业应用\n*   开放平台中的数据集\n*   提供面向用户的开发接口、集成接口\n","tags":["Knowledge Graph"],"categories":["知识图谱"]},{"title":"CCKS-2017 行业知识图谱构建与应用-上篇","url":"/2019-07-02/行业知识图谱构建与应用-上篇/","content":"\n\n本次Tutorial主要包括以下三方面内容：\n\n1.  行业知识图谱概述，包括行业图谱**简介**，行业知识图谱的**应用及挑战**，以及行业知识图谱**生命周期**管理。\n\n2.  行业知识图谱关键技术，包括行业知识图谱生命周期中各过程的**相关技术**、现有**可用的工具**，以及各过程中的**最佳实践**及相关组件。\n\n3.  行业知识图谱应用实战，以**金融证券**行业应用为例，演示知识图谱从知识建模、知识抽取到行业应用的全过程。\n\n目标听众与我们公众号的粉丝群体一致：\n\n*   知识图谱学习者，对知识图谱在行业应用感兴趣的**技术人员**。\n\n*   各行业应用中想引入知识图谱相关技术的**知识及数据管理人员**，尤其是有行业知识库构建及上层问答搜索等需求的。\n\n*   希望了解知识图谱如何在行业中应用的**管理决策者**。\n\n这篇文章通读完大致需要60分钟时间，不过这些时间都是值得的，因为你将可以：\n\n1.  了解行业知识图谱相关概念及其在**行业中的现有应用**，理解其给行业应用带来的价值。\n\n2.  理解知识图谱在行业中应用的相关**挑战**与**生命周期**，理解生命周期各过程的基本目标及相关组件。\n\n3.  对行业知识图谱应用相关的技术进行熟悉，了解有哪些**现有的工具**可以使用和相关注意事项、以及一些**行业应用的最佳实践**。\n\n我们假定阅读本Tutorial的听众具备如下基础知识：\n\n> **RD****F**：资源描述框架\n> \n> **OWL**： RDF Schema 的扩展\n> \n> **SPARQL**：RDF查询语言\n\n **第一节 行业知识图谱简介**\n\n**“Things not strings”**\n\n众所周知，知识图谱是Google于2012年提出，用来优化搜索结果。\n\n经过多年的发展，知识图谱在人工智能的许多行业都拥有了成熟落地的应用。\n\n![行业知识图谱](https://upload-images.jianshu.io/upload_images/18390058-a2e7426c1a7688a3.jpg) \n\n按照知识图谱的覆盖面来看，主要分为通用知识图谱与行业知识图谱。\n\n**1.1 通用知识图谱**\n\nGoogle所提出的知识图谱即为通用知识图谱，他是面向**全领域**的。通用知识图谱主要应用于面向互联网的**搜索、推荐、问答**等业务场景。由于它强调的是**广度**，因而更多的是强调**实体**，很难生成完整的全局性本体层的统一管理。\n\n通用知识图谱一些常见的项目如下：\n\n![通用知识图谱](https://upload-images.jianshu.io/upload_images/18390058-403d9b372f8ec537.jpg) \n\n**1.2 行业知识图谱**\n\n号称“硅谷最神秘科技公司”的Palantir是行业知识图谱领域的典型代表，其软件允许客户对大量的敏感数据进行语义关联分析，以防止欺诈，确保数据安全等。\n\n行业知识图谱相对通用知识图谱拥有如下特性：\n\n*   面向**特定领域**的知识图谱。\n\n*   用户目标对象需要考虑行业中各种级别的人员，不同人员对应的操作和业务场景不同，因而需要一定的**深度**与**完备性**。\n\n*   行业知识图谱对准确度要求非常高，通常用于辅助各种**复杂的分析应用**或**决策支持**。\n\n*   有**严格与丰富的数据模式**，行业知识图谱中的实体通常属性比较多且具有行业意义。\n\n行业数据的特点包括：\n\n*   **数据来源多**：内部数据、互联网数据、第三方数据。\n\n*   **数据类型多**：包含结构化、半结构化、非结构化数据，且后两者越来越多。\n\n*   **数据模式无法预先确定**：模式在数据出现之后才能确定；数据模式随数据增长不断演变。\n\n*   **数据量大**：在大数据背景下，行业应用的数据的数量通常都以亿级别计算，存在通常在TB、PB级别甚至更多。\n\n行业知识图谱已经在以下很多领域有了很好的应用，在后面我们会以金融证券领域为例，详细展开介绍。\n\n![行业知识图谱](https://upload-images.jianshu.io/upload_images/18390058-a307410fb155d258.jpg)\n\n下面这张图是部分知名的行业知识图谱项目\n\n![行业知识图谱项目](https://upload-images.jianshu.io/upload_images/18390058-7978b46cd6d49e35.jpg) \n\n通过上面的介绍我们已经大致了解通用知识图谱和行业知识图谱的区别，这里简单总结一下：\n\n![知识图谱和行业知识图谱的区别](https://upload-images.jianshu.io/upload_images/18390058-4e58c70db2a18b64.jpg) \n\n当然通用知识图谱与行业知识图谱并不是相互对立，而是**相互补充**的一个关系，利用通用知识图谱的广度结合行业知识图谱的深度，可以形成更加完善的知识图谱。\n\n通用知识图谱中的知识，可以作为行业知识图谱构建的基础；而构建的行业知识图谱，再融合到通用知识图谱中。\n\n因此两者是相辅相成，结合使用的。\n\n![通用知识图谱](https://upload-images.jianshu.io/upload_images/18390058-671530d678a02982.jpg)\n\n## **第二节 行业知识图谱应用**\n\n## 介绍完行业知识图谱的基本知识后，我们来看下行业知识图谱都有哪些应用。\n\n## 首先看金融领域，目前金融证券领域应用主要侧重于两个方面，一个是企业知识图谱，另一个是金融交易知识图谱。\n\n## **2.1 企业知识图谱**\n\n**![企业知识图谱](https://upload-images.jianshu.io/upload_images/18390058-49cf9330cdd1da9e.jpg)** \n\n企业数据包括：企业基础数据、投资关系、任职关系、企业专利数据、企业招投标数据、企业招聘数据、企业诉讼数据、企业失信数据、企业新闻数据。\n\n利用知识图谱融合以上企业数据，我们做了企业知识图谱，并在企业知识图谱之上利用图谱的特性，针对金融业务场景研发了一系列的图谱应用。\n\n以下6大功能为我们当前已投入使用的企业知识图谱应用。\n\n**2.1.1 企业风险评估**\n\n基于企业的基础信息、投资关系、诉讼、失信等多维度关联数据，利用图计算等方法构建科学、严谨的企业风险评估体系，有效规避潜在的经营风险与资金风险。\n\n![企业风险评估](https://upload-images.jianshu.io/upload_images/18390058-be4b4a7a64f20766.jpg) \n\n**2.1.2 企业社交图谱查询**\n\n基于投资、任职、专利、招投标、涉诉关系以目标企业为核心向外层层扩散，形成一个网络关系图，直观立体展现企业关联。\n\n![网络关系图](https://upload-images.jianshu.io/upload_images/18390058-f39bc935b348cd19.jpg)\n\n**2.1.3 企业最终控制人查询**\n\n基于股权投资关系寻找持股比例最大的股东，最终追溯至自然人或国有资产管理部门。\n\n![企业最终控制人查询](https://upload-images.jianshu.io/upload_images/18390058-80f272ba2d53e7be.jpg) \n\n**2.1.4 企业之间路径发现**\n\n在基于股权、任职、专利、招投标、涉诉等关系形成的网络关系中，查询企业之间的最短关系路径，衡量企业之间的联系密切度。\n\n![企业之间路径发](https://upload-images.jianshu.io/upload_images/18390058-84edeadc6e50a4b7.jpg) \n\n**2.1.5 初创企业融资发展历程**\n\n基于企业知识图谱中的投融资事件发生的时间顺序，记录企业的融资发展历程。\n\n![融资发展历程](https://upload-images.jianshu.io/upload_images/18390058-e4b2bc10cb25f078.png)\n\n![融资发展历程](https://upload-images.jianshu.io/upload_images/18390058-20a9efa1ceb1e27a.jpg)\n\n**2.1.6 上市企业智能问答**\n\n用户可以通过输入自然语言问题，系统直接给出用户想要的答案。\n\n![上市企业智能问答](https://upload-images.jianshu.io/upload_images/18390058-16610f15f136818b.jpg) \n\n**2.2 金融交易知识图谱**\n\n金融交易知识图谱在企业知识图谱之上，增加交易客户数据、客户之间的关系数据以及交易行为数据等，利用图挖掘技术，包括很多业务相关的规则，来分析实体与实体之间的关联关系，最终形成金融领域的交易知识图谱。\n\n![金融交易知识图谱](https://upload-images.jianshu.io/upload_images/18390058-f76f7d18e49db257.jpg)\n\n有了这样一个交易知识图谱之后，可以支持如下的应用（这块普惠金融的李文哲之前有篇文章有过相似介绍）。\n\n**2.2.1 辅助信贷审核**\n\n基于知识图谱数据的统一查询，全面掌握客户信息；避免由于系统、数据等孤立造成的信息不一致造成信用重复使用、信息不完整等问题。\n\n![辅助信贷审核](https://upload-images.jianshu.io/upload_images/18390058-1e4d2d5ac159421b.jpg)\n\n**2.2.2 反欺诈（1）**\n\n不一致性验证可以用来判断一个借款人的欺诈风险，类似交叉验证。比如借款人A和借款人B填写的是同一个公司电话，但借款人A填写的公司和借款人B填写的公司完全不一样，这就成了一个风险点，需要审核人员格外的注意。\n\n![反欺诈（1）](https://upload-images.jianshu.io/upload_images/18390058-e62105af0fc806a2.jpg)\n\n**2.2.3 反欺诈（2）**\n\n组团进行欺诈的成员会用虚假的身份去申请贷款，但部分信息是共享的。如下图可以看出贷款人A、B和C之间没有直接的关系，但通过知识图谱可以很容易的看出这三者之间都共享着某一部分信息，存在一定的组团骗贷风险。\n\n![反欺诈（2）](https://upload-images.jianshu.io/upload_images/18390058-4dd0ea480dcff903.jpg) \n\n**2.2.4 其它应用场景**\n\n异常分析（异常交易、异常客户）\n\n失联客户管理\n\n精准营销\n\n智能投研\n\n智能公告\n\n……\n\n**2.3 医疗知识图谱**\n\n![医疗知识图谱](https://upload-images.jianshu.io/upload_images/18390058-29348df096ba443c.jpg)\n\n知识图谱与医疗数据的结合形成医疗知识图谱，医疗数据包括：医疗专业知识、医疗文献、医疗常识、电子病历大数据、医案、现有医疗资源、疾病库、指南与规范。\n\n行业内比较知名的应用和项目如下\n\n**2.3.1 中医药知识平台**\n\n中医药知识平台（http://www.tcmkb.cn）是一个针对中医药知识体系系统梳理、建模和展示的平台，它以图形可视化方式展示核心概念之间的关系，辅助中医专家厘清学术发展脉络，浏览中医知识，发现知识点之间的联系。\n\n其优势在于与阅读文献等手段相比，可大幅度节约知识检索获取时间。\n\n![中医药知识平台](https://upload-images.jianshu.io/upload_images/18390058-ac9fc6306ec1ade1.jpg)\n\n**2.3.2 Watson辅助诊断与治疗**\n\nIBM 的 Watson 机器人已经在医院里辅助医生对病人进行医疗诊断。安德森癌症中心联合IBM Watson开展终结癌症的任务，其底层核心就是用了知识图谱相关技术。\n\n![Watson](https://upload-images.jianshu.io/upload_images/18390058-26a88befe1b54310.jpg) \n\n![Watson](https://upload-images.jianshu.io/upload_images/18390058-79979342e869d48f.jpg)\n\n**2.3.3 Open PHACTS 新药物发现**\n\n在欧盟，Open PHACTS重大联合攻关项目，这一面向药物研发的开放数据访问平台开发，其核心技术就是采用语义技术为有关研究人员提供高效的数据访问技术环境的支持。\n\n![新药物发现](https://upload-images.jianshu.io/upload_images/18390058-9519b3929e387005.jpg)\n\n**2.4 图情资源知识图谱**\n\n通过行业知识图谱结合图书情报资源，包括图书馆分类学体系、特定方向的知识体系、图书、期刊、论文、专利、报刊、百科数据、行业网站等数据，构建图情资源知识图谱，可以帮助图情领域在文献信息检索和文献信息资源管理推荐等方面，提供新的思路。\n\n以下为我们在图情资源知识图谱之上的一些应用案例。\n\n**2.4.1 知识导航与资源展示**\n\n使用知识图谱中的知识体系进行知识导航，引导用户学习知识体系，以及通过实体链接所关联的资源。\n\n![知识导航与资源展示](https://upload-images.jianshu.io/upload_images/18390058-dec67526e334d54a.jpg) \n\n**2.4.2 知识点推荐与搜索**\n\n![知识点推荐与搜索](https://upload-images.jianshu.io/upload_images/18390058-7a752d871d9eefba.jpg)\n\n**2.4.3 图情资源统计**\n\n![图情资源统计](https://upload-images.jianshu.io/upload_images/18390058-5ce28264b35ead93.jpg)\n\n**2.5 其他行业应用**\n\n知识图谱在许多其他行业也有应用，这里篇幅有限，我们仅列出方向及应用点。\n\n*   农业\n\n*   识别作物危害\n\n*   政府行业\n\n*   政府大数据管理\n\n*   客服系统\n\n*   基于知识图谱的智能客服系统\n\n*   ……\n\n## **第三节 知识图谱应用挑战**\n\n从数据库时代发展到大数据时代，企业希望融合使用全量数据，在融合使用的过程中会遇到非常多的挑战，首先我们看下从DB（DataBase）到BD（BigData）到底会有哪些不一样的地方？\n\n![知识图谱应用挑战](https://upload-images.jianshu.io/upload_images/18390058-cef2795206b888e9.jpg) \n\n这个图在我们公众号[前几期分享](http://mp.weixin.qq.com/s?__biz=MzI5NjY0NDEyNg==&mid=2247484181&idx=1&sn=4a9b9ee30168538b4a71f12b36497ba4&chksm=ec407864db37f1728259f17671687c7ff40813f7645904ac15ef442f0cd71e912e25f9e7cffd&scene=21#wechat_redirect)中有介绍过，无论是从数据规模、数据类型、数据模式还是处理方法，数据库时代与大数据时代均存在非常大的差异性，大数据时代，**没有一种通用的处理方法可以解决所有问题**。因此我们结合PlantData平台在许多行业应用中的实战经验，总结了以下5点企业全量数据应用中会面临到的挑战。\n\n**一） 多源异构数据难以融合**\n\n![多源异构数据难以融合](https://upload-images.jianshu.io/upload_images/18390058-1a60a296b7e2ca06.jpg)\n\n企业包括不仅内部的数据，还有从第三方接入的数据以及互联网公开数据，甚至是采购的数据，这些数据很多，可能描述同一个事物有很多的数据源，它们分散在不同的地方，怎么去把它们融合起来？这是我们面临的第一个挑战。\n\n**二）数据模式动态变迁困难**\n\n![数据模式动态变迁困难](https://upload-images.jianshu.io/upload_images/18390058-14d4f3feeb7446b1.jpg)\n\n大数据时代，数据模式是在不断变化的场景下，因此迫切需要一种可自由扩展的数据模式，而传统的数据库定义好数据库表结构和业务逻辑之后，修改起来成本非常高。第二个挑战在于如何简便的进行数据模式的动态变迁。\n\n**三）非结构化数据计算机难以理解**\n\n![非结构化数据计算机难以理解](https://upload-images.jianshu.io/upload_images/18390058-e3eaa984f72fae69.jpg)\n\n非结构化的数据最重要的部分就是文本数据，对于文本数据如何处理，传统的方式主要是基于字符串的检索，对文本中丰富信息的使用率非常低，因此第三个挑战在于怎么对非结构化的数据去进行有效的应用？\n\n**四）数据使用专业程度过高**\n\n![数据使用专业程度过高](https://upload-images.jianshu.io/upload_images/18390058-7357a29b1957321e.jpg)\n\n传统数据在使用的时候需要专业的程序员去编写程序去进行查询使用，普通的应用分析人员很难对数据去进行探索，第四个挑战在于需要有一种快速的方式去对数据进行探索和使用。\n\n**五）分散的数据难以统一消费利用**\n\n**![分散的数据难以统一消费利用](https://upload-images.jianshu.io/upload_images/18390058-9c03d19e78c41e9e.jpg)** \n\n在传统的数据库时代，构建的不同系统使用方式不一样，数据集成的时候会非常混乱，我们更多的期待把这些分散的数据融合起来，形成一个统一的消费利用的入口，工作人员可以从统一的入口进行数据的消费。\n\n针对以上五个挑战，我们的解决方案是：**基于行业知识图谱进行数据融合使用**。\n\n首先从最底层开始，针对挑战1，使用知识图谱（本体）对各种类型的数据进行抽象建模，**基于可动态变化的“概念—实体—属性—关系”数据模型**，实现各类数据的统一建模。\n\n针对挑战2，使用**可支持数据模式动态变化的知识图谱的数据存储**，实现对大数据及数据模式动态变化的支持。\n\n针对挑战3：利用信息抽取、实体链接相关的技术，**对非结构化及半结构化数据进行抽取和转换**，形成知识图谱形式的知识，以及和知识图谱里面的结构化的知识进行链接。\n\n最后针对挑战4、5：在**知识融合**的基础上，基于**语义检索、智能问答、图计算、推理、可视化**等技术，提供统一的数据检索、分析和利用平台。\n\n![语义检索](https://upload-images.jianshu.io/upload_images/18390058-0d83c9d21739b4da.jpg)\n\n**第四节 行业知识图谱生命周期**\n\n从行业知识图谱的全生命周期来看，可以分为知识建模，知识获取，知识融合，知识融合，知识存储，知识计算和知识应用6个部分。\n\n![行业知识图谱生命周期](https://upload-images.jianshu.io/upload_images/18390058-331706243e6d4342.jpg)\n\n**4.1 知识建模**\n\n知识建模即建立知识图谱的数据模式，行业知识图谱的数据模式对整个知识图谱的结构进行定义，因此需要保证可靠性。\n\n通常采用两种方法：一种是自顶向下的方法，专家手工编辑形成数据模式；另一种是自底向上的方法，基于行业现有的标准进行转换或者从现有的高质量行业数据源（如业务系统数据库表）中进行映射。\n\n其中的关键技术与难点包括：\n\n*   如何保证多人在线协同编辑，并且实时更新；\n\n*   能够支持导入集成使用现有的（结构化）知识；\n\n*   支持大数据量；\n\n*   能够支撑时间、时序等复杂知识表达；\n\n*   可以与自动算法进行结合，避免全人工操作\n\n建模完成后，需要往里面填充相应的知识，这就需要用到知识获取。\n\n**4.2 知识获取**\n\n![知识获取](https://upload-images.jianshu.io/upload_images/18390058-b19aa09dbc8b4bc7.jpg)\n\n从不同来源、不同结构的数据中进行知识提取，形成知识存入到知识图谱，这一过程我们称为知识获取。\n\n上图中的三类数据基本涵盖了我们目前所需要处理的所有数据类型，针对不同种类的数据，我们利用不同的技术进行提取。\n\n*   从结构化数据库中获取知识：D2R\n\n    *   难点：复杂表数据的处理\n\n*   从链接数据中获取知识：图映射\n\n    *   难点：数据对齐\n\n*   从半结构化（网站）数据中获取知识：使用包装器\n\n    *   难点：方便的包装器定义方法，包装器自动生成、更新与维护\n\n*   从文本中获取知识：信息抽取\n\n    *   难点：结果的准确率与覆盖率\n\n**4.3 知识融合**\n\n已经从不同的数据源把不同结构的数据提取知识之后，接下来要做的是把它们融合成一个统一的知识图谱，这时候需要用到知识融合的技术。\n\n知识融合主要分为数据模式层融合和数据层融合，分别用的技术如下：\n\na) 数据模式层融合\n\n*   概念合并\n\n*   概念上下位关系合并\n\n*   概念的属性定义合并\n\nb) 数据层融合\n\n*   实体合并\n\n*   实体属性融合\n\n*   冲突检测与解决\n\n由于行业知识图谱的数据模式通常采用自顶向下和自底向上结合的方式，在模式层基本都经过人工的校验，保证了可靠性，因此，知识融合的关键任务在数据层的融合。\n\n举几个知识融合工具的例子：一体化医学语言系统（Unified Medical Language System，UMLS），它提供了一种位于生物医学领域词表之间的映射结构，方便不同术语系统之间能够彼此转换。\n\n![知识融合](https://upload-images.jianshu.io/upload_images/18390058-57b9833082ad7226.jpg) \n\n另一个例子是Dbpedia知识图谱，依托于维基百科，通过实体链接实现不同语言实体间的映射。\n\n![Dbpedia知识图谱](https://upload-images.jianshu.io/upload_images/18390058-91d05a3dd14e4c44.jpg)\n\n还有zhishi.me，它主要利用识别sameAs关系，将分散在中文三大百科网站（中文维基百科、互动百科和百度百科）中的知识进行融合。\n\n![sameAs关系](https://upload-images.jianshu.io/upload_images/18390058-b4d3e879bcc2f478.jpg) \n\nGoogle在收购了大型知识库Freebase后，对于其众包形式的信息扩展速度仍不满意，因此创建了名为Knowledge Vault的全球最大知识库，通过特定算法**自动搜集整编****互联网信息**，再将其融入整体数据库中。\n\n截至2014年，Knowledge Vault的入库信息已达16亿条，其中2.71亿条内容为“事实”（真实性在90%以上）。\n\nKnowledge Vault能够建立**历史和社会**的模型。\n\n![大型知识库Freebase后](https://upload-images.jianshu.io/upload_images/18390058-0829f29e84c0dc39.jpg) \n\n以上简单介绍了目前国内外在知识融合方面的一些项目的基本情况，总结一下知识融合中的关键技术与难点，包括四点：\n\n1.  实现不同来源、不同形态数据的融合\n\n2.  海量数据的高效融合\n\n3.  新增知识的实时融合\n\n4.  多语言的融合\n\n**4.4 知识存储**\n\n接下来要介绍的是知识存储，图谱的数据存储既需要完成基本的数据存储，同时也要能支持上层的知识推理、知识快速查询、图实时计算等应用，因此需要存储以下信息：\n\n*   三元组知识的存储\n\n*   事件信息的存储\n\n*   时态信息的存储\n\n*   使用知识图谱组织的数据的存储\n\n其关键技术和难点就在于：\n\n*   大规模三元组数据的存储\n\n*   知识图谱组织的大数据的存储\n\n*   事件与时态信息的存储\n\n*   快速推理与图计算的支持\n\n**4.5 知识计算**\n\n知识计算主要是在知识图谱中知识和数据的基础上，通过各种算法，发现其中显式的或隐含的知识、模式或规则等，知识计算的范畴非常大，这里主要讲三个方面：\n\n*   图挖掘计算：基于图论的相关算法，实现对图谱的探索和挖掘。\n\n*   本体推理：使用本体推理进行新知识发现或冲突检测。\n\n*   基于规则的推理：使用规则引擎，编写相应的业务规则，通过推理辅助业务决策。\n\n知识计算涉及到的技术非常多，每一项都需要专门去研究，而且已经有很多研究成果，此处我们先列出关键技术和难点，在下一篇文章中详细介绍：\n\n*   图挖掘计算\n\n    *   大规模图算法的效率\n\n*   本体推理与规则推理\n\n    *   大数据量下的快速推理\n\n    *   对于增量知识和规则的快速加载\n\n**4.6 知识应用**\n\n基于知识图谱融合的海量知识和数据，结合上一步的知识计算相关技术，知识图谱可以产生大量的智能应用，比如我们之前提到的企业画像，反欺诈不一致性检测，用户通过自然语言进行搜索等。知识图谱目前的应用很多，这里主要讲三类常见的应用：\n\n*   语义搜索：基于知识图谱中的知识，解决传统搜索中遇到的关键字语义多样性及语义消歧的难题；通过实体链接实现知识与文档的混合检索。\n\n*   智能问答：针对用户输入的自然语言进行理解，从知识图谱中或目标数据中给出用户问题的答案。\n\n*   可视化决策支持：通过提供统一的图形接口，结合可视化、推理、检索等，为用户提供信息获取的入口。\n\n这三类的关键技术与难点分别为：\n\n语义检索：\n\n*   自然语言的表达多样性问题\n\n*   自然语言的的歧义问题\n\n智能问答：\n\n*   准确的语义解析\n\n*   正确理解用户的真实意图\n\n*   答案确定与排序\n\n可视化决策支持\n\n*   通过可视化方式辅助用户模式快速发现\n\n*   高效地缩放和导航\n\n*   大图环境下底层算法（图挖掘算法）的效率\n\n通过上述分析，我们列举一下行业知识图谱全生命周期中相关的9大关键技术，这些技术保证了行业知识图谱更加规范的数据表示，更强的数据关联，以及能够体现更深邃的数据价值。\n\n![下行业知识图谱全生命周期](https://upload-images.jianshu.io/upload_images/18390058-a921631a5fc6e68f.jpg)\n\n**4.7 LOD2**\n\n当然在行业内，知识图谱的全生命周期管理方面已经有部分现成的套装工具，比如欧盟的LOD2项目，它主要目标是构建结构化链接数据的企业级管理工具和方法学，提供一个搜索、浏览和生成链接数据的平台。\n\n不过LOD2 侧重于链接数据的生命周期管理，其它类型的数据需要首先转换成链接数据，并且LOD2 没有对中文处理的支持。\n\n![LOD2](https://upload-images.jianshu.io/upload_images/18390058-885f407fc4ed5d00.jpg) \n\n**4.8 Stardog**\n\n另一个代表性工具是Stardog，它是一个企业级知识图谱平台，通过把数据转换成知识，使用知识图谱进行组织，对外提供查询、检索、分析服务。其主要特点为：\n\n*   把关系数据库映射成虚拟图\n\n*   支持OWL2的推理\n\n*   支持Gremlin\n\n但 Stardog 的不足之处在于仅包含对结构化数据（RDBMS、Excel等）的处理，没有针对非结构化数据的知识抽取，也没有包含知识融合功能。\n\n![Stardog](https://upload-images.jianshu.io/upload_images/18390058-811a30e62e80b815.jpg) \n\n总结一下，我们首先从根源介绍了知识图谱的前身及其基础技术规范，其次对生命周期中每个环节进行了简单的概述，以及关键技术和难点总结，最后介绍了两个目前较为主流的知识图谱平台。\n\n根据难易程度，对于如何在行业应用中使用知识图谱，大致有如下几种方式：\n\n*   使用现有的套装工具（如 LOD2、Stardog）\n\n*   在现有套装工具的基础上进行扩充：\n\n*   使用各生命周期过程的相应工具并进行组合使用\n\n*   针对性开发或扩展生命周期中特定工具\n\n*   完全从零开始构建\n\n那么究竟使用哪种方式呢？这需要根据不同的数据，以及不同的业务需求进行相应的取舍。\n\n在下一篇文章中，我们会结合自身在PlantData平台里对图谱全生命周期管理的一些探索，深度介绍行业知识图谱全生命周期的关键技术。\n","tags":["Knowledge Graph"],"categories":["知识图谱"]},{"title":"架构师之路","url":"/2019-07-02/reference/Architect/架构师之路/","content":"\n## 架构师入门\n\n> [我的职业是架构师：12年经验带你入门](<https://www.jianshu.com/p/a79dc35f4597>)\n>\n> [软件架构入门 -- 阮一峰](<http://www.ruanyifeng.com/blog/2016/09/software-architecture.html>)\n>\n> [成为1个架构师的入门到进阶之路（学习路线图）](<https://zhuanlan.zhihu.com/p/37078580>)\n\n> [架构师之路](<https://mp.weixin.qq.com/s/RahJtAlTF9vkrsj4ZlqPig>)\n>\n> [W3Cschool架构师之路](<https://www.w3cschool.cn/architectroad/architectroad-awk.html>)\n\n","tags":["架构师"],"categories":["架构师"]},{"title":"知识图谱研究进展","url":"/2019-07-01/知识图谱研究进展/","content":"\n**1 知识图谱构建技术**\n\n本节首先给出知识图谱的技术地图，然后介绍知识图谱构建的关键技术，包括关系抽取技术、知识融合技术、实体链接技术和知识推理技术。\n\n**1.1 知识图谱技术地图**\n\n构建知识图谱的主要目的是获取大量的、让计算机可读的知识。在互联网飞速发展的今天，知识大量存在于非结构化的文本数据、大量半结构化的表格和网页以及生产系统的结构化数据中。为了阐述如何构建知识图谱，本文给出了构建知识图谱的技术地图，该技术地图如图1所示。整个技术图主要分为三个部分，第一个部分是知识获取，主要阐述如何从非结构化、半结构化、以及结构化数据中获取知识。第二部是数据融合，主要阐述如何将不同数据源获取的知识进行融合构建数据之间的关联。第三部分是知识计算及应用，这一部分关注的是基于知识图谱计算功能以及基于知识图谱的应用。\n\n**1.1.1 知识获取**\n\n在处理非结构化数据方面，首先要对用户的非结构化数据提取正文。目前的互联网数据存在着大量的广告，正文提取技术希望有效的过滤广告而只保留用户关注的文本内容。当得到正文文本后，需要通过自然语言技术识别文章中的实体，实体识别通常有两种方法，一种是用户本身有一个知识库则可以使用实体链接将文章中可能的候选实体链接到用户的知识库上。另一种是当用户没有知识库则需要使用命名实体识别技术识别文章中的实体。若文章中存在实体的别名或者简称还需要构建实体间的同义词表，这样可以使不同实体具有相同的描述。在识别实体的过程中可能会用到分词、词性标注，以及深度学习模型中需要用到分布式表达如词向量。同时为了得到不同粒度的知识还可能需要提取文中的关键词，获取文章的潜在主题等。当用户获得实体后，则需要关注实体间的关系，我们称为实体关系识别，有些实体关系识别的方法会利用句法结构来帮助确定两个实体间的关系，因此在有些算法中会利用依存分析或者语义解析。如果用户不仅仅想获取实体间的关系，还想获取一个事件的详细内容，那么则需要确定事件的触发词并获取事件相应描述的句子，同时识别事件描述句子中实体对应事件的角色。\n\n在处理半结构化数据方面，主要的工作是通过包装器学习半结构化数据的抽取规则。由于半结构化数据具有大量的重复性的结构，因此对数据进行少量的标注，可以让机器学出一定的规则进而在整个站点下使用规则对同类型或者符合某种关系的数据进行抽取。最后当用户的数据存储在生产系统的数据库中时，需要通过 ETL 工具对用户生产系统下的数据进行重新组织、清洗、检测最后得到符合用户使用目的数据。\n\n**1.1.2 知识融合**\n\n当知识从各个数据源下获取时需要提供统一的术语将各个数据源获取的知识融合成一个庞大的知识库。提供统一术语的结构或者数据被称为本体，本体不仅提供了统一的术语字典，还构建了各个术语间的关系以及限制。本体可以让用户非常方便和灵活的根据自己的业务建立或者修改数据模型。通过数据映射技术建立本体中术语和不同数据源抽取知识中词汇的映射关系，进而将不同数据源的数据融合在一起。同时不同源的实体可能会指向现实世界的同一个客体，这时需要使用实体匹配将不同数据源相同客体的数据进行融合。不同本体间也会存在某些术语描述同一类数据，那么对这些本体间则需要本体融合技术把不同的本体融合。最后融合而成的知识库需要一个存储、管理的解决方案。知识存储和管理的解决方案会根据用户查询场景的不同采用不同的存储架构如 NoSQL 或者关系数据库。同时大规模的知识库也符合大数据的特征，因此需要传统的大数据平台如 Spark 或者 Hadoop 提供高性能计算能力，支持快速运算。\n\n**1.1.2 知识计算及应用**\n\n知识计算主要是根据图谱提供的信息得到更多隐含的知识，如通过本体或者规则推理技术可以获取数据中存在的隐含知识；而链接预测则可预测实体间隐含的关系；同时使用社会计算的不同算法在知识网络上计算获取知识图谱上存在的社区，提供知识间关联的路径；通过不一致检测技术发现数据中的噪声和缺陷。通过知识计算知识图谱可以产生大量的智能应用如可以提供精确的用户画像为精准营销系统提供潜在的客户；提供领域知识给专家系统提供决策数据，给律师、医生、公司 CEO 等提供辅助决策的意见；提供更智能的检索方式，使用户可以通过自然语言进行搜索；当然知识图谱也是问答必不可少的重要组建。\n\n![tu1](https://upload-images.jianshu.io/upload_images/18390058-cad70b500d300e96.jpeg)\n\n*图1*  \n\n从上图可以看出，知识图谱涉及到的技术非常多，每一项技术都需要专门去研究，而且已经有很多研究成果。由于篇幅的限制，本文重点介绍知识图谱构建和知识计算的几个核心技术。\n\n## **1.2　实体关系识别技术**\n\n最初实体关系识别任务在 1998 年 MUC（Message Understanding Conference）中以 MUC-7 任务被引入，目的是通过填充关系模板槽的方式抽去文本中特定的关系。1998 后，在 ACE（Automatic Content Extraction）中被定义为关系检测和识别的任务；2009 年 ACE 并入 TAC (Text Analysis Conference)，关系抽取被并入到 KBP（knowledgeBase Population）领域的槽填充任务。从关系任务定义上，分为限定领域（Close Domain）和开放领域（Open IE）；从方法上看，实体关系识别了从流水线识别方法逐渐过渡到端到端的识别方法。\n\n基于统计学的方法将从文本中识别实体间关系的问题转化为分类问题。基于统计学的方法在实体关系识别时需要加入实体关系上下文信息确定实体间的关系，然而基于监督的方法依赖大量的标注数据，因此半监督或者无监督的方法受到了更多关注。\n\n（1）监督学习：Zhou[13] 在 Kambhatla 的基础上加入了基本词组块信息和 WordNet，使用 SVM 作为分类器，在实体关系识别的准确率达到了 55.5%，实验表明实体类别信息的特征有助于提高关系抽取性能； Zelenko[14] 等人使用浅层句法分析树上最小公共子树来表达关系实例，计算两颗子树之间的核函数，通过训练例如 SVM 模型的分类器来对实例进行分。但基于核函数的方法的问题是召回率普遍较低，这是由于相似度计算过程匹配约束比较严格，因此在后续研究对基于核函数改进中，大部分是围绕改进召回率。但随着时间的推移，语料的增多、深度学习在图像和语音领域获得成功，信息抽取逐渐转向了基于神经模型的研究，相关的语料被提出作为测试标准，如 SemEval-2010 task 8[15]。基于神经网络方法的研究有，Hashimoto[16] 等人利用 Word Embedding 方法从标注语料中学习特定的名词对的上下文特征，然后将该特征加入到神经网络分类器中，在 SemEval-2010 task 8 上取得了 F1 值 82.8% 的效果。基于神经网络模型显著的特点是不需要加入太多的特征，一般可用的特征有词向量、位置等，因此有人提出利用基于联合抽取模型，这种模型可以同时抽取实体和其之间的关系。联合抽取模型的优点是可以避免流水线模型存在的错误累积[17-22]。其中比较有代表性的工作是[20]，该方法通过提出全新的全局特征作为算法的软约束，进而同时提高关系抽取和实体抽取的准确率，该方法在 ACE 语料上比传统的流水线方法 F1 提高了 1.5%，；另一项工作是 [22]，利用双层的 LSTM-RNN 模型训练分类模型，第一层 LSTM 输入的是词向量、位置特征和词性来识别实体的类型。训练得到的 LSTM 中隐藏层的分布式表达和实体的分类标签信息作为第二层 RNN 模型的输入，第二层的输入实体之间的依存路径，第二层训练对关系的分类，通过神经网络同时优化 LSTM 和 RNN 的模型参数，实验与另一个采用神经网络的联合抽取模型[21]相比在关系分类上有一定的提升。但无论是流水线方法还是联合抽取方法，都属于有监督学习，因此需要大量的训练语料，尤其是对基于神经网络的方法，需要大量的语料进行模型训练，因此这些方法都不适用于构建大规模的 Knowledge Base。\n\n（2）半（弱）监督学习：半监督学习主要是利用少量的标注信息进行学习，这方面的工作主要是基于 Bootstrap 的方法。基于 Bootstrap 的方法主要是利用少量的实例作为初始种子的集合，然后利用 pattern 学习方法进行学习，通过不断的迭代，从非结构化数据中抽取实例，然后从新学到的实例中学习新的 pattern 并扩种 pattern 集合。Brin[23]等人通过少量的实例学习种子模板，从网络上大量非结构化文本中抽取新的实例，同时学习新的抽取模板，其主要贡献是构建了 DIPRE 系统；Agichtein[24]在 Brin 的基础上对新抽取的实例进行可信度的评分和完善关系描述的模式，设计实现了 Snowball 抽取系统；此后的一些系统都沿着 Bootstrap 的方法，但会加入更合理的对 pattern 描述、更加合理的限制条件和评分策略，或者基于先前系统抽取结果上构建大规模 pattern；如 NELL（Never-EndingLanguage Learner）系统[25-26]，NELL 初始化一个本体和种子 pattern，从大规模的 Web 文本中学习，通过对学习到的内容进行打分来提高准确率，目前已经获得了 280 万个事实。\n\n（3）无监督学习： Bollegala[27]从搜索引擎摘要中获取和聚合抽取模板，将模板聚类后发现由实体对代表的隐含语义关系; Bollegala[28]使用联合聚类(Co-clustering)算法，利用关系实例和关系模板的对偶性，提高了关系模板聚类效果，同时使用 L1 正则化 Logistics 回归模型，在关系模板聚类结果中筛选出代表性的抽取模板，使得关系抽取在准确率和召回率上都有所提高。\n\n无监督学习一般利用语料中存在的大量冗余信息做聚类，在聚类结果的基础上给定关系，但由于聚类方法本身就存在难以描述关系和低频实例召回率低的问题，因此无监督学习一般难以得很好的抽取效果。\n\n## **1.3　知识融合技术**\n\n知识融合（knowledge fusion）指的是将多个数据源抽取的知识进行融合。与传统数据融合（datafusion）[29]任务的主要不同是，知识融合可能使用多个知识抽取工具为每个数据项从每个数据源中抽取相应的值，而数据融合未考虑多个抽取工具[30]。由此，知识融合除了应对抽取出来的事实本身可能存在的噪音外，还比数据融合多引入了一个噪音，就是不同抽取工具通过实体链接和本体匹配可能产生不同的结果。另外，知识融合还需要考虑本体的融合和实例的融合。\n\n文献[30]首先从已有的数据融合方法中挑选出易于产生有意义概率的、便于使用基于 MapReduce 框架的、有前途的最新方法，然后对这些挑选出的方法做出以下改进以用于知识融合：将每个抽取工具同每个信息源配对，每对作为数据融合任务中的一个数据源，这样就变成了传统的数据融合任务；改进已有数据融合方法使其输出概率，代替原来的真假二值；根据知识融合中的数据特征修改基于 MapReduce 的框架。文献[31]提出一个将通过不同搜索引擎得到的知识卡片（即结构化的总结）融合起来的方法。针对一个实体查询，不同搜索引擎可能返回不同的知识卡片，即便同一个搜索引擎也可能返回多个知识卡片。将这些知识卡片融合起来时，同文献[30]中提出的方法类似，将知识融合中的三维问题将为二维问题，再应用传统的数据融合技术。不过，文献[31]提出了一个新的概率打分算法，用于挑选一个知识卡片最有可能指向的实体，并设计了一个基于学习的方法来做属性匹配。\n\n在知识融合技术中，本体匹配扮演着非常重要的角色，提供了概念或者实体之间的对应关系。截止目前，人们已经提出了各种各样的本体匹配算法，一般可以分为模式匹配（schema matching）和实例匹配（instance matching），也有少量的同时考虑模式和实例的匹配[32-34]。从技术层面来讲，本体匹配可分为启发式方法、概率方法、基于图的方法、基于学习的方法和基于推理的方法。下面围绕模式匹配和实例匹配，具体介绍各自分类中几个具有代表性的匹配方法。 \n\n模式匹配主要寻找本体中属性和概念之间的对应关系，文献[35]和[36]给出比较详尽的综述。文献[37]提出一个自动的语义匹配方法，该方法首先利用像 WordNet 之类的词典以及本体的结构等信息进行模式匹配，然后将结果根据加权平均的方法整合起来，再利用一些模式（patterns）进行一致性检查，去除那些导致不一致的对应关系。该过程可循环的，直到不再找到新的对应关系为止。文献[38]也是考虑多种匹配算法的结合，利用基于术语的一些相似度计算算法，例如 n-gram 和编辑距离，这里算法计算的结果根据加权求和进行合并，还考虑了概念的层次关系和一些背景知识，最后通过用户定义的权重进行合并。为了应对大规模的本体，文献[39]提出一个使用锚（anchor）的系统，该系统以一对来自两个本体的相似概念为起点，根据这些概念的父概念和子概念等邻居信息逐渐地构建小片段，从中找出匹配的概念。新找出的匹配的概念对又可作为新的锚，然后再根据邻居信息构建新的片段。该过程不断地重复，直到未找到新的匹配概念对时停止。文献[40]则以分而治之的思想处理大规模本体，该方法先根据本体的结构对其进行划分获得组块，然后从不同本体获得的组块进行基于锚的匹配，这里的锚是指事先匹配好的实体对，最后再从匹配的组块中找出对应的概念和属性。现有的匹配方法通常是将多个匹配算法相结合，采用加权平均或加权求和的方式进行合并。但是，由于本体结构的不对称性等特征，这种固定的加权方法显出不足。文献[41]基于贝叶斯决策的风险最小化提出一个动态的合并方法，该方法可以根据本体的特征，在计算每个实体对的相似度时动态地选择使用哪几个匹配算法，如何合并这些算法，其灵活性带来了很好的匹配结果。\n\n实例匹配是评估异构知识源之间实例对的相似度，用来判断这些实例是否指向给定领域的相同实体。最近几年，随着 Web 2.0 和语义 Web 技术的不断发展，越来越多的语义数据往往具有丰富实例和薄弱模式的特点，促使本体匹配的研究工作慢慢的从模式层转移到实例层[42]。文献[43]提出一个自训练的方法进行实例匹配，该方法首先根据 owl:sameAs、函数型属性（functional properties）和基数（cardinalities）构建一个核（kernel），再根据区别比较明显的属性值对递归的对该核进行扩展。文献[44]利用现有的局部敏感哈希（locality-sensitivehashing）技术来大幅提高实例匹配的可扩展性，该方法首先需要定义用于实例相似性分析的粒度，然后使用分割好的字符串技术实例相似度。文献[45]首先使用向量空间模型表示实例的描述性信息，再基于规则采用倒排索引（inverted indexes）获取最初的匹配候选，在使用用户定义的属性值对候选进行过滤，最后计算出的匹配候选相似度用来作为整合的向量距离，由此抽取出匹配结果。虽然已有方法中已有不少用于处理大规模本体的实例匹配问题，但是同时保证高效和高精度仍然是个很大的挑战。文献[46]提出了一个迭代的框架，充分利用特征明显的已有匹配方法来提高效率，同时基于相似度传播的方法利用一个加权指数函数来确保实例匹配的高精度。\n\n## **1.4　实体链接技术**\n\n歧义性和多样性是自然语言的固有属性，也是实体链接的根本难点。如何挖掘更多、更加有效的消歧证据，设计更高性能的消歧算法依然是实体链接系统的核心研究问题，值得进一步研究。下面按照不同的实体消歧方法进行分类。\n\n基于概率生成模型方法：韩先培和孙乐[47]提出了一种生成概率模型，将候选实体 e 出现在某页面中的概率、特定实体 e 被表示为实体指称项的概率以及实体 e 出现在特定上下文中的概率三者相乘，得到候选实体同实体指称项之间的相似度评分值。Blanco 和 Ottaviano 等人[48]提出了用于搜索查询实体链接的概率模型，该方法采用了散列技术与上下文知识，有效地提高了实体链接的效率。\n\n基于主题模型的方法：Zhang 等人[49]通过模型自动对文本中的实体指称进行标注，生成训练数据集用于训练 LDA 主题模型，然后计算实体指称和候选实体的上下文语义相似度从而消歧得到目标实体。王建勇等人[50]提出了对用户的兴趣主题建模的方法，首先构建关系图，图中包含了不同命名实体间的相互依赖关系，然后利用局部信息对关系图中每个命名实体赋予初始兴趣值，最后利用传播算法对不同命名实体的兴趣值进行传播得到最终兴趣值，选择具有最高兴趣值的候选实体。\n\n基于图的方法：Han 等人[51]构造了一种基于图的模型，其中图节点为所有实体指称和所有候选实体；图的边分为两类，一类是实体指称和其对应的候选实体之间的边，权重为实体指称和候选实体之间的局部文本相似度，采用词袋模型和余弦距离计算得出。另一类是候选实体之间的边，权重为候选实体之间的语义相关度，采用谷歌距离计算。算法首先采集不同实体的初始置信度，然后通过图中的边对置信度进行传播和增强。Gentile 和 Zhang[52]等人提出了基于图和语义关系的命名实体消歧方法，该方法在维基百科上建立基于图的模型，然后在该模型上计算各个命名实体的得分从而确定了目标实体，该方法在新闻数据上取得了较高的准确率。Alhelbawy 等人[53]也采用基于图的方法，图中的节点为所有的候选实体，边采用两种方式构建，一种是实体之间的维基百科链接，另一种是使用实体在维基百科文章中句子的共现。图中的候选实体节点通过和实体指称的相似度值被赋予初始值，采用 PageRank 选择目标实体。Hoffart 等人[54]使用实体的先验概率，实体指称和候选实体的上下文相似度，以及候选实体之间的内聚性构成一个加权图，从中选择出一个候选实体的密集子图作为最可能的目标实体分配给实体指称。\n\n基于深度神经网络的方法：周明和王厚峰等人[55]提出了一种用于实体消歧的实体表示训练方法。该方法对文章内容进行自编码，利用深度神经网络模型以有监督的方式训练实体表示，依据语义表示相似度对候选实体进行排序，但该方法是一种局部性方法，没有考虑同一文本中共同出现的实体间相关性。黄洪钊和季姮等人[56]基于深度神经网络和语义知识图谱，提出了一种基于图的半监督实体消歧义方法，将深度神经网络模型得到的实体间语义关联度作为图中的边权值。从实验结果得出：基于语义知识图谱的 NGD 和 VSM[57]方法比起 Wikipedia anchor links 无论在关联性测试上还是在消歧性能上都具有更好的测试结果。相比 NGD 和 VSM，基于 DNN[58]的深度语义关联方法在关联性测试上还是在消歧性能上都具有更好的关联性和更高的准确性。但该方法存在两点不足，一方面在构建深度语义关联模型时采用词袋子方法，没有考虑上下文词之间位置关系，另外一方面在消歧的过程中，构建的图模型没有充分利用已消歧实体，边权值和顶点得分随着未消歧实体增加保持不变，并没有为后续的歧义实体增加信息量。\n\n## **1.5　知识推理技术**\n\n知识库推理可以粗略地分为基于符号的推理和基于统计的推理。在人工智能的研究中，基于符号的推理一般是基于经典逻辑（一阶谓词逻辑或者命题逻辑）或者经典逻辑的变异（比如说缺省逻辑）。基于符号的推理可以从一个已有的知识图谱，利用规则，推理出新的实体间关系，还可以对知识图谱进行逻辑的冲突检测。基于统计的方法一般指关系机器学习方法，通过统计规律从知识图谱中学习到新的实体间关系。\n\n**1.5.1 基于符号逻辑的推理方法**\n\n为了使得语义网络同时具备形式化语义和高效推理，一些研究人员提出了易处理（tractable）概念语言，并且开发了一些商用化的语义网络系统。这些系统的提出，使得针对概念描述的一系列逻辑语言，统称描述逻辑（description logic），得到了学术界和业界广泛关注。但是这些系统的推理效率难以满足日益增长的数据的需求，最终没能得到广泛应用。这一困局被利物浦大学的 Ian Horrocks 教授打破，他开发的 FaCT 系统可以处理一个比较大的医疗术语本体 GALEN，而且性能比其他类似的推理机要好得多。描述逻辑最终成为了 W3C 推荐的 Web 本体语言 OWL 的逻辑基础。\n\n虽然描述逻辑推理机的优化取得了很大的进展，但是还是跟不上数据增长的速度，特别是当数据规模大到目前的基于内存的服务器无法处理的情况下。为了应对这一挑战，最近几年，研究人员开始考虑将描述逻辑和 RDFS 的推理并行来提升推理的效率和可扩展性，并且取得了很多成果。并行推理工作所借助的并行技术分为以下两类：1）单机环境下的多核、多处理器技术，比如多线程，GPU 技术等；2）多机环境下基于网络通信的分布式技术，比如 MapReduce 计算框架、Peer-To-Peer 网络框架等。很多工作尝试利用这些技术实现高效的并行推理。\n\n单机环境下的并行技术以共享内存模型为特点，侧重于提升本体推理的时间效率。对于实时性要求较高的应用场景，这种方法成为首选。对于表达能力较低的语言，比如 RDFS、OWL EL，单机环境下的并行技术将显著地提升本体推理效率。Goodman 等人在[59]中利用高性能计算平台 Cray XMT 实现了大规模的 RDFS 本体推理，利用平台计算资源的优势限制所有推理任务在内存完成。然而对于计算资源有限的平台，内存使用率的优化成为了不可避免的问题。Motik 等人在[60]工作中将 RDFS，以及表达能力更高的 OWL RL 等价地转换为 Datalog 程序，然后利用 Datalog 中的并行优化技术来解决内存的使用率问题。在[61]中，作者尝试利用并行与串行的混合方法来提升OWL RL的推理效率。Kazakov 等人在 [62]中提出了利用多线程技术实现 OWL EL 分类(classification)的方法，并实现推理机 ELK。\n\n尽管单机环境的推理技术可以满足高推理性能的需求，但是由于计算资源有限（比如内存，存储容量），推理方法的可伸缩性（scalability）受到不同程度的限制。因此，很多工作利用分布式技术突破大规模数据的处理界限。这种方法利用多机搭建集群来实现本体推理。\n\nMavin[63]是首个尝试利用 Peer-To-Peer 的分布式框架实现 RDF 数据推理的工作。实验结果表明，利用分布式技术可以完成很多在单机环境下无法完成的大数据量推理任务。很多工作基于 MapReduce 的开源实现（如 Hadoop，Spark 等）设计提出了大规模本体的推理方法。其中较为成功的一个尝试是 Urbani 等人在 2010 年公布的推理系统 WebPIE [64]。实验结果证实其在大集群上可以完成上百亿的 RDF 三元组的推理。他们又在这个基础上研究提出了基于 MapReduce 的 OWL RL 查询算法[65]。利用 MapReduce 来实现 OWL EL 本体的推理算法在 [66]中提出，实验证明 MapReduce 技术同样可以解决大规模的 OWL EL 本体推理。在[67]的工作中，进一步扩展 OWL EL 的推理技术，使得推理可以在多个并行计算平台完成。\n\n**1.5.2 基于统计的推理方法**\n\n知识图谱中基于统计的推理方法一般指关系机器学习方法。下面介绍一些典型的方法。\n\n**实体关系学习方法**\n\n实体关系学习的目的是学习知识图谱中实例和实例之间的关系。这方面的工作非常多，也是最近几年知识图谱的一个比较热的研究方向。按照文献[68]的分类，可以分为潜在特征模型和图特征模型两种。潜在特征模型通过实例的潜在特征来解释三元组。比如说，莫言获得诺贝尔文学奖的一个可能解释是他是一个有名的作家。Nickel等人在[69]中给出了一个关系潜在特征模型，称为双线性（bilinear）模型，该模型考虑了潜在特征的两两交互来学习潜在的实体关系。Drumond 等人在[70]中应用两两交互的张量分解模型来学习知识图谱中的潜在关系。\n\n翻译（translation）模型[71]将实体与关系统一映射至低维向量空间中，且认为关系向量中承载了头实体翻译至尾实体的潜在特征。因此，通过发掘、对比向量空间中存在类似潜在特征的实体向量对，我们可以得到知识图谱中潜在的三元组关系。全息嵌入（Holographic Embedding，HolE）模型[72]分别利用圆周相关计算三元组的组合表示及利用圆周卷积从组合表示中恢复出实体及关系的表示。与张量分解模型类似，HolE 可以获得大量的实体交互来学习潜在关系，而且有效减少了训练参数，提高了训练效率。\n\n基于图特征模型的方法从知识图谱中观察到的三元组的边的特征来预测一条可能的边的存在。典型的方法有基于基于归纳逻辑程序（ILP）的方法[73]，基于关联规则挖掘（ARM）的方法[74]和路径排序（path ranking）的方法[75]。基于 ILP 的方法和基于 ARM 的方法的共同之处在于通过挖掘的方法从知识图谱中抽取一些规则，然后把这些规则应用到知识图谱上，推出新的关系。而路径排序方法则是根据两个实体间连通路径作为特征来判断两个实体是否属于某个关系。\n\n**类型推理（typeinference）方法**\n\n知识图谱上的类型推理目的是学习知识图谱中的实例和概念之间的属于关系。SDType[76]利用三元组主语或谓语所连接属性的统计分布以预测实例的类型。该方法可以用在任意单数据源的知识图谱，但是无法做到跨数据集的类型推理。Tipalo[77]与LHD[78]均使用 DBpedia 中特有的 abstract 数据，利用特定模式进行实例类型的抽取。此类方法依赖于特定结构的文本数据，无法扩展到其他知识库。\n\n**模式归纳（schemainduction）方法**\n\n模式归纳方法学习概念之间的关系，主要有基于 ILP 的方法和基于 ARM 的方法。ILP 结合了机器学习和逻辑编程技术，使得人们可以从实例和背景知识中获得逻辑结论。Lehmann 等在[79]中提出用向下精化算子学习描述逻辑的概念定义公理的方法，即从最一般的概念（即顶概念）开始，采用启发式搜索方法使该概念不断特殊化，最终得到概念的定义。为了处理像 DBpedia 这样大规模的语义数据，该方法在[80]中得到进一步的扩展。这些方法都在 DL-Learner[81]中得以实现。Völker 等人在[82]中介绍了从知识图谱中生成概念关系的统计方法，该方法通过 SPARQL 查询来获取信息，用以构建事务表。然后使用 ARM 技术从事务表中挖掘出一些相关联的概念关系。在他们的后续工作中，使用负关联规则挖掘技术学习不交概念关系[83]，并在文献[84]中给出了丰富的试验结果。\n\n**2 开放知识图谱**\n\n本节首先介绍当前世界范围内知名的高质量大规模开放知识图谱，包括 DBpedia[85][86]、Yago[87][88]、Wikidata[89]、BabelNet[90][91]、ConceptNet[92][93]以及Microsoft Concept Graph[94][95]等。然后介绍中文开放知识图谱平台 OpenKG。\n\n**2.1 开放知识图谱**\n\nDBpedia 是一个大规模的多语言百科知识图谱，可视为是维基百科的结构化版本。DBpedia 使用固定的模式对维基百科中的实体信息进行抽取，包括 abstract、infobox、category 和 page link 等信息。图 2 示例了如何将维基百科中的实体“Busan”的 infobox 信息转换成 RDF 三元组。DBpedia 目前拥有 127 种语言的超过两千八百万个实体与数亿个 RDF 三元组，并且作为链接数据的核心，与许多其他数据集均存在实体映射关系。而根据抽样评测[96]，DBpedia 中 RDF 三元组的正确率达 88%。DBpedia 支持数据集的完全下载。\n\nYago 是一个整合了维基百科与 WordNet[97]的大规模本体，它首先制定一些固定的规则对维基百科中每个实体的 infobox 进行抽取，然后利用维基百科的category进行实体类别推断（Type Inference）获得了大量的实体与概念之间的 IsA 关系（如：“Elvis Presley” IsA “American Rock Singers”），最后将维基百科的 category 与 WordNet 中的 Synset（一个 Synset 表示一个概念）进行映射，从而利用了 WordNet 严格定义的 Taxonomy 完成大规模本体的构建。随着时间的推移，Yago 的开发人员为该本体中的 RDF 三元组增加了时间与空间信息，从而完成了 Yago2[98]的构建，又利用相同的方法对不同语言维基百科的进行抽取，完成了 Yago3[99]的构建。目前，Yago 拥有 10 种语言约 459 万个实体，2400 万个 Facts，Yago 中 Facts的正确率约为 95%。Yago 支持数据集的完全下载。\n\n![tu2](https://upload-images.jianshu.io/upload_images/18390058-2578d4c0735b9f5f.jpeg)\n\n*图2*\n\nWikidata 是一个可以自由协作编辑的多语言百科知识库，它由维基媒体基金会发起，期望将维基百科、维基文库、维基导游等项目中结构化知识进行抽取、存储、关联。Wikidata 中的每个实体存在多个不同语言的标签，别名，描述，以及声明（statement），比如 Wikidata 会给出实体“London”的中文标签“伦敦”，中文描述“英国首都”以及图 3 给出了一个关于“London”的声明的具体例子。“London”的一个声明由一个 claim 与一个 reference 组成，claim 包括property:“Population”、value:“8173900”以及一些 qualifiers（备注说明）组成，而 reference 则表示一个 claim 的出处，可以为空值。目前 Wikidata 目前支持超过 350 种语言，拥有近 2500 万个实体及超过 7000 万的声明[100]，并且目前 Freebase 正在往 Wikidata 上进行迁移以进一步支持 Google 的语义搜索。Wikidata 支持数据集的完全下载。\n\n![tu3](https://upload-images.jianshu.io/upload_images/18390058-e43a15d1f3d91bbc.jpeg)\n\n*图3 *\n\nBabelNet 是目前世界范围内最大的多语言百科同义词典，它本身可被视为一个由概念、实体、关系构成的语义网络（Semantic Network）。BabelNet 目前有超过 1400 万个词目，每个词目对应一个 synset。每个 synset 包含所有表达相同含义的不同语言的同义词。比如：“中国”、“中华人民共和国”、“China”以及“people’srepublic of China”均存在于一个 synset 中。BabelNet 由 WordNet 中的英文 synsets 与维基百科页面进行映射，再利用维基百科中的跨语言页面链接以及翻译系统，从而得到 BabelNet 的初始版本。目前 BabelNet 又整合了 Wikidata、GeoNames、OmegaWiki 等多种资源，共拥有 271 个语言版本。由于 BabelNet 中的错误来源主要在于维基百科与 WordNet 之间的映射，而映射目前的正确率大约在 91%。关于数据集的使用，BabelNet 目前支持 HTTP API 调用，而数据集的完全下载需要经过非商用的认证后才能完成。\n\nConceptNet 是一个大规模的多语言常识知识库，其本质为一个以自然语言的方式描述人类常识的大型语义网络。ConceptNet 起源于一个众包项目 Open Mind Common Sense，自 1999 年开始通过文本抽取、众包、融合现有知识库中的常识知识以及设计一些游戏从而不断获取常识知识。ConceptNet 中共拥有 36 种固定的关系，如 IsA、UsedFor、CapableOf 等，图 4 给出了一个具体的例子，从中可以更加清晰地了解 ConceptNet 的结构。ConceptNet 目前拥有 304 个语言的版本，共有超过 390 万个概念，2800 万个声明（statements，即语义网络中边的数量），正确率约为 81%。另外，ConceptNet 目前支持数据集的完全下载。\n\n![tu4](https://upload-images.jianshu.io/upload_images/18390058-d20d35d4b02c2dd5.jpeg)\n\n*图4*\n\nMicrosoft Concept Graph 是一个大规模的英文 Taxonomy，其中主要包含的是概念间以及实例（等同于上文中的实体）概念间的 IsA 关系，其中并不区分 instanceOf 与 subclassOf 关系。Microsoft Concept Graph 的前身是 Probase，它过自动化地抽取自数十亿网页与搜索引擎查询记录，其中每一个 IsA 关系均附带一个概率值，即该知识库中的每个 IsA 关系不是绝对的，而是存在一个成立的概率值以支持各种应用，如短文本理解、基于 taxonomy 的关键词搜索和万维网表格理解等。目前，Microsoft Concept Graph 拥有约 530 万个概念，1250 万个实例以及 8500 万个 IsA 关系（正确率约为 92.8%）。关于数据集的使用，MicrosoftConcept Graph 目前支持 HTTP API 调用，而数据集的完全下载需要经过非商用的认证后才能完成。\n\n除了上述知识图谱外，中文目前可用的大规模开放知识图谱有 Zhishi.me[101]、Zhishi.schema[102]与XLore[103]等。Zhishi.me 是第一份构建中文链接数据的工作，与 DBpedia 类似，Zhishi.me 首先指定固定的抽取规则对百度百科、互动百科和中文维基百科中的实体信息进行抽取，包括 abstract、infobox、category 等信息；然后对源自不同百科的实体进行对齐，从而完成数据集的链接。目前 Zhishi.me 中拥有约 1000 万个实体与一亿两千万个 RDF 三元组，所有数据可以通过在线 SPARQL Endpoint 查询得到。Zhishi.schema 是一个大规模的中文模式（Schema）知识库，其本质是一个语义网络，其中包含三种概念间的关系，即equal、related与subClassOf关系。Zhishi.schema抽取自社交站点的分类目录(Category Taxonomy)及标签云（Tag Cloud），目前拥有约40万的中文概念与150万RDF三元组，正确率约为84%，并支持数据集的完全下载。XLore 是一个大型的中英文知识图谱，它旨在从各种不同的中英文在线百科中抽取 RDF 三元组，并建立中英文实体间的跨语言链接。目前，XLore 大约有 66 万个概念，5 万个属性，1000 万的实体，所有数据可以通过在线 SPARQL Endpoint 查询得到。\n\n**2.2 中文开放知识图谱联盟介绍**\n\n中文开放知识图谱联盟（OpenKG）旨在推动中文知识图谱的开放与互联，推动知识图谱技术在中国的普及与应用，为中国人工智能的发展以及创新创业做出贡献。联盟已经搭建有 OpenKG.CN 技术平台，如图 5 所示，目前已有 35 家机构入驻。吸引了国内最著名知识图谱资源的加入，如 Zhishi.me， CN-DBPedia, PKUBase。并已经包含了来自于常识、医疗、金融、城市、出行等 15 个类目的开放知识图谱。\n\n![tu5](https://upload-images.jianshu.io/upload_images/18390058-d36848ec17043f86.jpeg)\n\n*图5 中文开放知识图谱联盟*\n\n**3 知识图谱在情报分析的案例**\n\n## **3.1 股票投研情报分析**\n\n通过知识图谱相关技术从招股书、年报、公司公告、券商研究报告、新闻等半结构化表格和非结构化文本数据中批量自动抽取公司的股东、子公司、供应商、客户、合作伙伴、竞争对手等信息，构建出公司的知识图谱。在某个宏观经济事件或者企业相关事件发生的时候，券商分析师、交易员、基金公司基金经理等投资研究人员可以通过此图谱做更深层次的分析和更好的投资决策，比如在美国限制向中兴通讯出口的消息发布之后，如果我们有中兴通讯的客户供应商、合作伙伴以及竞争对手的关系图谱，就能在中兴通讯停牌的情况下快速地筛选出受影响的国际国内上市公司从而挖掘投资机会或者进行投资组合风险控制（图6）。 \n\n![股票投研情报分析](https://upload-images.jianshu.io/upload_images/18390058-e60b36c0a58081a6.jpeg)\n\n*图6 股票投研情报分析*\n\n## **2.2 公安情报分析**\n\n通过融合企业和个人银行资金交易明细、通话、出行、住宿、工商、税务等信息构建初步的“资金账户-人-公司”关联知识图谱。同时从案件描述、笔录等非结构化文本中抽取人(受害人、嫌疑人、报案人)、事、物、组织、卡号、时间、地点等信息，链接并补充到原有的知识图谱中形成一个完整的证据链。辅助公安刑侦、经侦、银行进行案件线索侦查和挖掘同伙。比如银行和公安经侦监控资金账户，当有一段时间内有大量资金流动并集中到某个账户的时候很可能是非法集资，系统触发预警（图7）。 \n\n![公安情报分析](https://upload-images.jianshu.io/upload_images/18390058-dc9ee3814802757d.jpeg)\n\n*图7 公安情报分析*\n\n## **3.3 反欺诈情报分析**\n\n通过融合来自不同数据源的信息构成知识图谱，同时引入领域专家建立业务专家规则。我们通过数据不一致性检测，利用绘制出的知识图谱可以识别潜在的欺诈风险。比如借款人张xx和借款人吴x填写信息为同事，但是两个人填写的公司名却不一样, 以及同一个电话号码属于两个借款人，这些不一致性很可能有欺诈行为 （图8）。\n\n*图8 ![反欺诈情报分析](https://upload-images.jianshu.io/upload_images/18390058-5e1bc2e36d3f5e23.jpeg)* \n\n**4 总结**\n\n知识图谱是知识工程的一个分支，以知识工程中语义网络作为理论基础，并且结合了机器学习，自然语言处理和知识表示和推理的最新成果，在大数据的推动下受到了业界和学术界的广泛关注。知识图谱对于解决大数据中文本分析和图像理解问题发挥重要作用。目前，知识图谱研究已经取得了很多成果，形成了一些开放的知识图谱。但是，知识图谱的发展还存在以下障碍。首先，虽然大数据时代已经产生了海量的数据，但是数据发布缺乏规范，而且数据质量不高，从这些数据中挖掘高质量的知识需要处理数据噪音问题。其次，垂直领域的知识图谱构建缺乏自然语言处理方面的资源，特别是词典的匮乏使得垂直领域知识图谱构建代价很大。最后，知识图谱构建缺乏开源的工具，目前很多研究工作都不具备实用性，而且很少有工具发布。通用的知识图谱构建平台还很难实现。\n\n**参考文献和完整原文阅读链接：**\n\nhttp://tie.istic.ac.cn/ch/reader/view_abstract.aspx?file_no=201701002&flag=1&from=timeline&isappinstalled=0\n","tags":["Knowledge Graph"],"categories":["知识图谱"]},{"title":"十大编程算法","url":"/2019-06-26/十大编程算法/","content":"\n## ****算法一：快速排序算法****\n\n快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要Ο(n log n)次比较。在最坏状况下则需要Ο(n2)次比较，但这种状况并不常见。\n\n事实上，快速排序通常明显比其他Ο(n log n) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。\n\n快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。\n\n**算法步骤：**\n\n*   1 从数列中挑出一个元素，称为 “基准”（pivot），\n\n*   2 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。\n\n*   3 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。\n\n递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会退出，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。\n\n![1](https://upload-images.jianshu.io/upload_images/18390058-8f09fc4b158535da.gif)\n\n## **算法二：堆排序算法**\n\n堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。\n\n堆排序的平均时间复杂度为Ο(nlogn) 。\n\n**算法步骤：**\n\n*   1\\. 创建一个堆H[0..n-1]\n\n*   2\\. 把堆首（最大值）和堆尾互换\n\n*   3\\. 把堆的尺寸缩小1，并调用shift_down(0),目的是把新的数组顶端数据调整到相应位置\n\n*   4\\. 重复步骤2，直到堆的尺寸为1\n\n## **算法三：归并排序**\n\n归并排序（Merge sort，台湾译作：合并排序）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。\n\n**算法步骤：**\n\n*   1\\. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列\n\n*   2\\. 设定两个指针，最初位置分别为两个已经排序序列的起始位置\n\n*   3\\. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置\n\n*   4\\. 重复步骤3直到某一指针达到序列尾\n\n*   5\\. 将另一序列剩下的所有元素直接复制到合并序列尾\n\n![11](https://upload-images.jianshu.io/upload_images/18390058-1018aa3c04df3c42.gif)\n\n## **算法四：二分查找算法**\n\n二分查找算法是一种在有序数组中查找某一特定元素的搜索算法。搜素过程从数组的中间元素开始：\n\n*   如果中间元素正好是要查找的元素，则搜素过程结束；\n\n*   如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。\n\n*   如果在某一步骤数组为空，则代表找不到。\n\n这种搜索算法每一次比较都使搜索范围缩小一半。折半搜索每次把搜索区域减少一半，时间复杂度为Ο(logn) 。\n\n## **算法五：BFPRT(线性查找算法)**\n\nBFPRT算法解决的问题十分经典，即从某n个元素的序列中选出第k大（第k小）的元素，通过巧妙的分析，BFPRT可以保证在最坏情况下仍为线性时间复杂度。\n\n该算法的思想与快速排序思想相似，当然，为使得算法在最坏情况下，依然能达到o(n)的时间复杂度，五位算法作者做了精妙的处理。\n\n**算法步骤：**\n\n*   1\\. 将n个元素每5个一组，分成n/5(上界)组。\n\n*   2\\. 取出每一组的中位数，任意排序方法，比如插入排序。\n\n*   3\\. 递归的调用selection算法查找上一步中所有中位数的中位数，设为x，偶数个中位数的情况下设定为选取中间小的一个。\n\n*   4\\. 用x来分割数组，设小于等于x的个数为k，大于x的个数即为n-k。\n\n*   5\\. 若i==k，返回x；若ik，在大于x的元素中递归查找第i-k小的元素。\n\n终止条件：n=1时，返回的即是i小元素。\n\n## **算法六：DFS（深度优先搜索）**\n\n深度优先搜索算法（Depth-First-Search），是搜索算法的一种。它沿着树的深度遍历树的节点，尽可能深的搜索树的分支。\n\n当节点v的所有边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。\n\n如果还存在未被发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止。DFS属于盲目搜索。\n\n深度优先搜索是图论中的经典算法，利用深度优先搜索算法可以产生目标图的相应拓扑排序表，利用拓扑排序表可以方便的解决很多相关的图论问题，如最大路径问题等等。一般用堆数据结构来辅助实现DFS算法。\n\n**深度优先遍历图算法步骤：**\n\n*   1\\. 访问顶点v；\n\n*   2\\. 依次从v的未被访问的邻接点出发，对图进行深度优先遍历；直至图中和v有路径相通的顶点都被访问；\n\n*   3\\. 若此时图中尚有顶点未被访问，则从一个未被访问的顶点出发，重新进行深度优先遍历，直到图中所有顶点均被访问过为止。\n\n上述描述可能比较抽象，举个实例：\n\nDFS 在访问图中某一起始顶点 v 后，由 v 出发，访问它的任一邻接顶点 w1；再从 w1 出发，访问与 w1邻 接但还没有访问过的顶点 w2；然后再从 w2 出发，进行类似的访问，… 如此进行下去，直至到达所有的邻接顶点都被访问过的顶点 u 为止。\n\n接着，退回一步，退到前一次刚访问过的顶点，看是否还有其它没有被访问的邻接顶点。如果有，则访问此顶点，之后再从此顶点出发，进行与前述类似的访问；如果没有，就再退回一步进行搜索。重复上述过程，直到连通图中所有顶点都被访问过为止。\n\n## **算法七：BFS(广度优先搜索)**\n\n广度优先搜索算法（Breadth-First-Search），是一种图形搜索算法。简单的说，BFS是从根节点开始，沿着树(图)的宽度遍历树(图)的节点。如果所有节点均被访问，则算法中止。\n\nBFS同样属于盲目搜索。一般用队列数据结构来辅助实现BFS算法。\n\n**算法步骤：**\n\n*   1\\. 首先将根节点放入队列中。\n\n*   2\\. 从队列中取出第一个节点，并检验它是否为目标。如果找到目标，则结束搜寻并回传结果。否则将它所有尚未检验过的直接子节点加入队列中。\n\n*   3\\. 若队列为空，表示整张图都检查过了——亦即图中没有欲搜寻的目标。结束搜寻并回传“找不到目标”。\n\n*   4\\. 重复步骤2。\n\n## **算法八：Dijkstra算法**\n\n戴克斯特拉算法（Dijkstra’s algorithm）是由荷兰计算机科学家艾兹赫尔·戴克斯特拉提出。迪科斯彻算法使用了广度优先搜索解决非负权有向图的单源最短路径问题，算法最终得到一个最短路径树。该算法常用于路由算法或者作为其他图算法的一个子模块。\n\n该算法的输入包含了一个有权重的有向图 G，以及G中的一个来源顶点 S。我们以 V 表示 G 中所有顶点的集合。每一个图中的边，都是两个顶点所形成的有序元素对。(u, v) 表示从顶点 u 到 v 有路径相连。我们以 E 表示G中所有边的集合，而边的权重则由权重函数 w: E → [0, ∞] 定义。\n\n因此，w(u, v) 就是从顶点 u 到顶点 v 的非负权重（weight）。边的权重可以想像成两个顶点之间的距离。任两点间路径的权重，就是该路径上所有边的权重总和。已知有 V 中有顶点 s 及 t，Dijkstra 算法可以找到 s 到 t的最低权重路径(例如，最短路径)。\n\n这个算法也可以在一个图中，找到从一个顶点 s 到任何其他顶点的最短路径。对于不含负权的有向图，Dijkstra算法是目前已知的最快的单源最短路径算法。\n\n**算法步骤：**\n\n*   1\\. 初始时令 S={V0},T={其余顶点}，T中顶点对应的距离值。若存在，d(V0,Vi)为弧上的权值；若不存在，d(V0,Vi)为∞\n\n*   2\\. 从T中选取一个其距离值为最小的顶点W且不在S中，加入S\n\n*   3\\. 对其余T中顶点的距离值进行修改：若加进W作中间顶点，从V0到Vi的距离值缩短，则修改此距离值\n\n重复上述步骤2、3，直到S中包含所有顶点，即W=Vi为止\n\n![23](https://upload-images.jianshu.io/upload_images/18390058-d928c03336d9d358.gif)\n\n## **算法九：动态规划算法**\n\n动态规划（Dynamic programming）是一种在数学、计算机科学和经济学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。\n\n动态规划常常适用于有重叠子问题和最优子结构性质的问题，动态规划方法所耗时间往往远少于朴素解法。\n\n动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。\n\n通常许多 子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个 子问题解之时直接查表。\n\n这种做法在重复子问题的数目关于输入的规模呈指数增长时特别有用。\n\n关于动态规划最经典的问题当属背包问题。\n\n**算法步骤：**\n\n*   1.最优子结构性质。\n\n如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。\n\n最优子结构性质为动态规划算法解决问题提供了重要线索。\n\n*   2\\. 子问题重叠性质。\n\n子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。\n\n动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。\n\n## **算法十：朴素贝叶斯分类算法**\n\n朴素贝叶斯分类算法是一种基于贝叶斯定理的简单概率分类算法。贝叶斯分类的基础是概率推理，就是在各种条件的存在不确定，仅知其出现概率的情况下， 如何完成推理和决策任务。\n\n概率推理是与确定性推理相对应的。而朴素贝叶斯分类器是基于独立假设的，即假设样本每个特征与其他特征都不相关。\n\n朴素贝叶斯分类器依靠精确的自然概率模型，在有监督学习的样本集中能获取得非常好的分类效果。\n\n在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法，换言之朴素贝叶斯模型能工作并没有用到贝叶斯概率或者任何贝叶斯模型。\n","tags":["algorithm"],"categories":["算法"]},{"title":"webrtc-专题-02-WebRTC应用","url":"/2019-06-19/reference/webrtc/webrtc-专题-02/","content":"\n\n## WebRTC浏览器API\n\nWebRTC实现了多个Web API接口，其中三个重要的Web API分别是:\n\n- **[MediaStream](https://www.html5rocks.com/en/tutorials/webrtc/basics/#toc-mediastream)**：通过 MediaStream 的 API 能够通过设备的摄像头及话筒获得视频、音频的同步流。\n- **[RTCPeerConnection](https://www.html5rocks.com/en/tutorials/webrtc/basics/#toc-rtcpeerconnection)**：RTCPeerConnection 是 WebRTC 用于构建点对点之间稳定、高效的流传输的组件。\n- **[RTCDataChannel](https://www.html5rocks.com/en/tutorials/webrtc/basics/#toc-rtcdatachannel)**：RTCDataChannel 使得浏览器之间（点对点）建立一个高吞吐量、低延时的信道，用于传输任意数据。\n\n这里大致介绍一下这三个API：\n\n### MediaStream (aka getUserMedia)\n\nMediaStream API为WebRTC提供了从设备的摄像头、话筒获取视频、音频流数据的功能.\n\n#### W3C标准\n\n详见：<https://w3c.github.io/mediacapture-main/getusermedia.html>\n\n#### 如何调用？\n\n可以通过 `navigator.getUserMedia()` 这个方法来调用，这个方法接受三个参数：\n\n- 一个约束对象（constraints object），这个后面会单独讲。\n\n- 一个调用成功的回调函数，如果调用成功，传递给它一个流对象。\n\n- 一个调用失败的回调函数，如果调用失败，传递给它一个错误对象。\n\n#### 浏览器兼容性处理\n\n由于浏览器实现不同，他们经常会在实现标准版本之前，在方法前面加上前缀，所以一个兼容版本就像这样：\n\n```js\nvar getUserMedia = (navigator.getUserMedia || \n                    navigator.webkitGetUserMedia || \n                    navigator.mozGetUserMedia || \n                    navigator.msGetUserMedia);\n```\n\n####  一个超级简单的例子\n\n这里写一个超级简单的例子，用来展现 `getUserMedia` 的效果：\n\n<details><summary>简单的例子</summary>\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  \t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n    <title>GetUserMedia实例</title>\n</head>\n<body>\n    <video id=\"video\" autoplay></video>\n</body>\n\n<script type=\"text/javascript\">\n    var getUserMedia = (navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia);\n\n    getUserMedia.call(navigator, {\n        video: true,\n        audio: true\n    }, function(localMediaStream) {\n        var video = document.getElementById('video');\n        video.src = window.URL.createObjectURL(localMediaStream);\n        video.onloadedmetadata = function(e) {\n            console.log(\"Label: \" + localMediaStream.label);\n            console.log(\"AudioTracks\" , localMediaStream.getAudioTracks());\n            console.log(\"VideoTracks\" , localMediaStream.getVideoTracks());\n        };\n    }, function(e) {\n        console.log('Rejected!', e);\n    });\n \n/*  \n//或：\n'use strict';\n\nnavigator.getUserMedia = (navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia);\n\nvar constraints = { // 音频、视频约束\n  audio: true, // 指定请求音频Track\n  video: {  // 指定请求视频Track\n      mandatory: { // 对视频Track的强制约束条件\n          width: {min: 320},\n          height: {min: 180}\n      },\n      optional: [ // 对视频Track的可选约束条件\n          {frameRate: 30}\n      ]\n  }\n};\n\nfunction successCallback(localMediaStream) {\n  var video = document.querySelector('video');\n  if (window.URL) {\n    video.src = window.URL.createObjectURL(localMediaStream);\n  } else {\n    video.src = localMediaStream;\n  }\n  video.onloadedmetadata = function(e) {\n            console.log(\"Label: \" + localMediaStream.label);\n            console.log(\"AudioTracks\" , localMediaStream.getAudioTracks());\n            console.log(\"VideoTracks\" , localMediaStream.getVideoTracks());\n        };\n}\n\nfunction errorCallback(error) {\n  console.log('navigator.getUserMedia error: ', error);\n}\n\nnavigator.getUserMedia(constraints, successCallback, errorCallback);\n */ \n</script>\n\n</html>\n```\n\n</details>\n\n将这段内容保存在一个HTML文件中，放在服务器上。用较新版本的Opera、Firefox、Chrome打开，在浏览器弹出询问是否允许访问摄像头和话筒，选同意，浏览器上就会出现摄像头所拍摄到的画面了.\n\n注意，HTML文件要放在服务器上，否则会得到一个 `NavigatorUserMediaError` 的错误，显示 `PermissionDeniedError`。\n\n这里使用 **`getUserMedia`** 获得流之后，需要将其输出，一般是绑定到 **`video`** 标签上输出，需要使用**`window.URL.createObjectURL(localMediaStream)`**来创造能在 `video` 中使用 `src` 属性播放的 Blob URL，注意在 `video` 上加入 `autoplay` 属性，否则只能捕获到一张图片。\n\n流创建完毕后可以通过 **`label`** 属性来获得其唯一的标识，还可以通过 **`getAudioTracks()`** 和 **`getVideoTracks()`** 方法来获得流的追踪对象数组（如果没有开启某种流，它的追踪对象数组将是一个空数组）\n\n在 JS 中，我们通过 `getUserMedia` 函数来处理音频和视频，该函数接收三个参数，分别是音视频的约束，成功的回调以及失败的回调。\n\n在底层，浏览器通过音频和视频引擎对捕获的原始音频和视频流加以处理，除了对画质和音质增强之外，还得保证音频和视频的同步。\n\n由于音频和视频是用来传输的，因此，发送方还要适应不断变化的带宽和客户端之间的网络延迟调整输出的比特率。\n\n对于接收方来说，则必须实时解码音频和视频流，并适应网络抖动和时延。其工作原理如下图所示：\n\n![](/images/imageWebRTC/others/内部WebRTC-API.png)\n\n如上面源码中成功回调的 `localMediaStream` 对象中携带者一个或多个同步的 `Track`，如果你同时在约束中设置了音频和视频为 `true`，则在 `localMediaStream` 中会携带有音频 `Track` 和视频 `Track`，每个 `Track` 在时间上是同步的。\n\n`localMediaStream` 的输出可以被发送到一或多个目的地：本地的音频或视频元素、后期处理的 JavaScript 代理，或者远程另一端。如下图所示：\n\n![](/images/imageWebRTC/others/音视频采集输出.png)\n\n#### 约束对象(Constraints)\n\n约束对象可以被设置在 `getUserMedia()` 和 `RTCPeerConnection` 的 `addStream` 方法中，这个约束对象是WebRTC 用来指定接受什么样的流的，其中可以定义如下属性：\n\n- **video** : 是否接受视频流\n- **audio**：是否接受音频流\n- **MinWidth** : 视频流的最小宽度\n- **MaxWidth**：视频流的最大宽度\n- **MinHeight**：视频流的最小高度\n- **MaxHiehgt**：视频流的最大高度\n- **MinAspectRatio**：视频流的最小宽高比\n- **MaxAspectRatio**：视频流的最大宽高比\n- **MinFramerate**：视频流的最小帧速率\n- **MaxFramerate**：视频流的最大帧速率\n\n### RTCPeerConnection\n\n在获取到音频和视频流后，下一步要做的就是将其发送出去。但这个跟 client-server 模式不同，这是 client-client 之间的传输，因此，在协议层面就必须解决 NAT 穿透问题，否则传输就无从谈起。\n\n另外，由于 WebRTC 主要是用来解决实时通信的问题，可靠性并不是很重要，因此，WebRTC 使用 UDP 作为传输层协议：低延迟和及时性才是关键。\n\n在更深入讲解之前，我们先来思考一下，是不是只要打开音频、视频，然后发送 UDP 包就搞定了？\n\n当然没那么简单，除了要解决我们上面说的 NAT 穿透问题之外，还需要为每个流协商参数，对用户数据进行加密，并且需要实现拥塞和流量控制。\n\n我们来看一张WebRTC的分层协议图：\n\n![](/images/imageWebRTC/others/webrtc分层协议.png)\n\nICE、STUN 和 TURN 是通过 UDP 建立并维护端到端连接所必需的；\n\nSDP 是一种数据格式，用于端到端连接时协商参数；\n\nDTLS 用于保障传输数据的安全；\n\nSCTP 和 SRTP 属于应用层协议，用于在 UDP 之上提供不同流的多路复用、拥塞和流量控制，以及部分可靠的交付和其他服务。\n\nICE（Interactive Connectivity Establishment，交互连接建立）：由于端与端之间存在多层防火墙和 NAT 设备阻隔，因此我们需要一种机制来收集两端之间公共线路的 IP，而 ICE 则是干这件事的好帮手。\n\n- ICE 代理向操作系统查询本地 IP 地址\n- 如果配置了 STUN 服务器，ICE 代理会查询外部 STUN 服务器，以取得本地端的公共 IP 和端口\n- 如果配置了 TURN 服务器，ICE 则会将 TURN 服务器作为一个候选项，当端到端的连接失败，数据将通过指定的中间设备转发。\n\nWebRTC 使用 SDP（Session Description Protocol，会话描述协议）描述端到端连接的参数。 SDP 不包含媒体本身的任何信息，仅用于描述 \"会话状况\"，表现为一系列的连接属性：要交换的媒体类型（音频、视频及应用数据）、网络传输协议、使用的编解码器及其设置、带宽及其他元数据。\n\nDTLS 对 TLS 协议进行了扩展，为每条握手记录明确添加了偏移字段和序号，这样就满足了有序交付的条件，也能让大记录可以被分段成多个分组并在另一端再进行组装。 DTLS 握手记录严格按照 TLS 协议规定的顺序传输，顺序不对就报错。\n\n最后，DTLS 还要处理丢包问题：两端都是用计时器，如果预定时间没有收到应答，就重传握手记录。 为保证过程完整，两端都要生成自己签名的证书，然后按照常规的 TLS 握手协议走。但这样的证书不能用于验证身份，因为没有要验证的信任链。因此，在必要情况下， 应用必须自己参与各端的身份验证：\n\n- 应用可以通过登录来验证用户\n- 每一端也可以在生成 SDP 提议/应答时指定各自的 \"身份颁发机构\"，等对端接收到 SDP 消息后，可以联系指定的身份颁发机构验证收到的证书\n\nSRTP 为通过 IP 网络交付音频和视频定义了标准的分组格式。SRTP 本身并不对传输数据的及时性、可靠性或数据恢复提供任何保证机制， 它只负责把数字化的音频采样和视频帧用一些元数据封装起来，以辅助接收方处理这些流。\n\nSCTP 是一个传输层协议，直接在 IP 协议上运行，这一点跟 TCP 和 UDP 类似。不过在 WebRTC 这里，SCTP 是在一个安全的 DTLS 信道中运行，而这个信道又运行在 UDP 之上。 由于 WebRTC 支持通过 DataChannel API 在端与端之间传输任意应用数据，而 DataChannel 就依赖于 SCTP。\n\n![](/images/imageWebRTC/others/几种传输比较.png)\n\n上讲了这么多，终于到我们的主角 `RTCPeerConnection`，`RTCPeerConnection` 接口负责维护每一个端到端连接的完整生命周期：\n\n- `RTCPeerConnection` 管理穿越 NAT 的完整 ICE 工作流\n- `RTCPeerConnection` 发送自动（STUN）持久化信号\n- `RTCPeerConnection` 跟踪本地流\n- `RTCPeerConnection` 跟踪远程流\n- `RTCPeerConnection` 按需触发自动流协商\n- `RTCPeerConnection` 提供必要的 API，以生成连接提议，接收应答，允许我们查询连接的当前状态，等等\n\n![](/images/imageWebRTC/others/RTCPeerConnection.png)\n\nWebRTC使用RTCPeerConnection来在浏览器之间传递流数据，这个流数据通道是点对点的，不需要经过服务器进行中转。但是这并不意味着我们能抛弃服务器，我们仍然需要它来为我们传递信令（signaling）来建立这个信道。WebRTC没有定义用于建立信道的信令的协议：信令并不是RTCPeerConnection API的一部分。\n\n既然没有定义具体的信令的协议，我们就可以选择任意方式（AJAX、WebSocket），采用任意的协议（SIP、XMPP）来传递信令，建立信道。比如可以使用node的ws模块，在WebSocket上传递信令。\n\n#### 浏览器兼容处理\n\n还是前缀不同的问题，采用和上面类似的方法：\n\n```javascript\nvar PeerConnection = (window.PeerConnection ||\n                    window.webkitPeerConnection00 || \n                    window.webkitRTCPeerConnection || \n                    window.mozRTCPeerConnection);\n```\n\n#### 创建和使用\n\n<details><summary>案例代码</summary>\n\n```javascript\n//使用Google的stun服务器\nvar iceServer = {\n    \"iceServers\": [{\n        \"url\": \"stun:stun.l.google.com:19302\"\n    }]\n};\n//兼容浏览器的getUserMedia写法\nvar getUserMedia = (navigator.getUserMedia ||\n                    navigator.webkitGetUserMedia || \n                    navigator.mozGetUserMedia || \n                    navigator.msGetUserMedia);\n//兼容浏览器的PeerConnection写法\nvar PeerConnection = (window.PeerConnection ||\n                    window.webkitPeerConnection00 || \n                    window.webkitRTCPeerConnection || \n                    window.mozRTCPeerConnection);\n//与后台服务器的WebSocket连接\nvar socket = __createWebSocketChannel();\n//创建PeerConnection实例\nvar pc = new PeerConnection(iceServer);\n//发送ICE候选到其他客户端\npc.onicecandidate = function(event){\n    socket.send(JSON.stringify({\n        \"event\": \"__ice_candidate\",\n        \"data\": {\n            \"candidate\": event.candidate\n        }\n    }));\n};\n//如果检测到媒体流连接到本地，将其绑定到一个video标签上输出\npc.onaddstream = function(event){\n    someVideoElement.src = URL.createObjectURL(event.stream);\n};\n//获取本地的媒体流，并绑定到一个video标签上输出，并且发送这个媒体流给其他客户端\ngetUserMedia.call(navigator, {\n    \"audio\": true,\n    \"video\": true\n}, function(stream){\n    //发送offer和answer的函数，发送本地session描述\n    var sendOfferFn = function(desc){\n            pc.setLocalDescription(desc);\n            socket.send(JSON.stringify({ \n                \"event\": \"__offer\",\n                \"data\": {\n                    \"sdp\": desc\n                }\n            }));\n        },\n        sendAnswerFn = function(desc){\n            pc.setLocalDescription(desc);\n            socket.send(JSON.stringify({ \n                \"event\": \"__answer\",\n                \"data\": {\n                    \"sdp\": desc\n                }\n            }));\n        };\n    //绑定本地媒体流到video标签用于输出\n    myselfVideoElement.src = URL.createObjectURL(stream);\n    //向PeerConnection中加入需要发送的流\n    pc.addStream(stream);\n    //如果是发送方则发送一个offer信令，否则发送一个answer信令\n    if(isCaller){\n        pc.createOffer(sendOfferFn);\n    } else {\n        pc.createAnswer(sendAnswerFn);\n    }\n}, function(error){\n    //处理媒体流创建失败错误\n});\n//处理到来的信令\nsocket.onmessage = function(event){\n    var json = JSON.parse(event.data);\n    //如果是一个ICE的候选，则将其加入到PeerConnection中，否则设定对方的session描述为传递过来的描述\n    if( json.event === \"__ice_candidate\" ){\n        pc.addIceCandidate(new RTCIceCandidate(json.data.candidate));\n    } else {\n         pc.setRemoteDescription(new RTCSessionDescription(json.data.sdp));\n    }\n};\n```\n\n</details>\n\n### RTCDataChannel\n\n既然能建立点对点的信道来传递实时的视频、音频数据流，为什么不能用这个信道传一点其他数据呢？\n\n`RTCDataChannel` API就是用来干这个的，基于它我们可以在浏览器之间传输任意数据。`DataChannel` 是建立在 `PeerConnection` 上的，不能单独使用。建立 `RTCPeerConnection` 连接之后，两端可以打开一或多个信道交换文本或二进制数据。\n\n`DataChannel` 和 `WebSocket` 的区别如下：\n\n![](/images/imageWebRTC/others/DataChannel和WebSocket的区别.png)\n\n#### 使用DataChannel\n\n我们可以使用 `channel = pc.createDataCHannel(“someLabel”)` ;来在 `PeerConnection` 的实例上创建 `Data Channel`，并给与它一个标签。\n\n`DataChannel` 使用方式几乎和 `WebSocket` 一样，有几个事件：\n\n- `onopen`\n- `onclose`\n- `onmessage`\n- `onerror`\n\n同时它有几个状态，可以通过 `readyState` 获取：\n\n- `connecting` : 浏览器之间正在试图建立channel\n- `open`：建立成功，可以使用send方法发送数据了\n- `closing`：浏览器正在关闭channel\n- `closed`：channel已经被关闭了\n\n两个暴露的方法:\n\n- `close()` : 用于关闭channel\n- `send()`：用于通过channel向对方发送数据\n\n<details><summary>示例demo如下：\n```javascript\nvar ice = {\n    'iceServers': [\n        {'url': 'stun:stun.l.google.com:19302'},   // google公共测试服务器\n        // {\"url\": \"turn:user@turnservera.com\", \"credential\": \"pass\"}\n    ]\n};\n\n// var signalingChannel =  new SignalingChannel();\n\nvar pc = new RTCPeerConnection(ice);\n\nnavigator.getUserMedia({'audio': true}, gotStream, logError);\n\nfunction gotStream(stram) {\n    pc.addStream(stram);\n\n    pc.createOffer().then(function(offer){\n        pc.setLocalDescription(offer);\n    });\n}\n\npc.onicecandidate = function(evt) {\n    // console.log(evt);\n    if(evt.target.iceGatheringState == 'complete') {\n        pc.createOffer().then(function(offer){\n            // console.log(offer.sdp);\n            // signalingChannel.send(sdp);\n        })\n    }\n}\n\nfunction handleChannel(chan) {\n    console.log(chan);\n    chan.onerror = function(err) {}\n    chan.onclose = function() {}\n    chan.onopen = function(evt) {\n        console.log('established');\n        chan.send('DataChannel connection established.');\n    }\n\n    chan.onmessage = function(msg){\n        // do something\n    }\n}\n\n// 以合适的交付语义初始化新的DataChannel\nvar dc = pc.createDataChannel('namedChannel', {reliable: false});\n\nhandleChannel(dc);\npc.onDataChannel = handleChannel;\n\nfunction logError(){\n    console.log('error');\n}\n```\n\n</details>\n\n#### 通过Data Channel发送文件大致思路\n\nJavaScript 已经提供了 File API 从 `input[ type= ‘file’]` 的元素中提取文件，并通过 **`FileReader`** 来将文件的转换成 `DataURL`，这也意味着我们可以将 `DataURL` 分成多个碎片来通过 Channel 来进行文件传输。\n\n## WebRTC信令交换\n\n本节讲述了WebRTC中所涉及的信令交换以及聊天室中的信令交换，主要内容来自于：[WebRTC in the real world: STUN, TURN and signaling](http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/)\n\n### WebRTC的服务器\n\nWebRTC提供浏览器之间的点对点信道进行数据传输，但是并不意味着WebRTC不需要服务器，建立这个信道，必须有服务器的参与。WebRTC需要服务器对其进行四方面的功能支持：\n\n1. 用户发现以及通信；\n2. 信令传输：浏览器之间交换建立通信的元数据（信令）；\n3. NAT 防火墙穿越；\n4. 如果点对点通信建立失败，可以作为中转服务器。\n\n### NAT/防火墙穿越技术\n\n#### NAT简介\n\nNAT（Network Address Translation，网络地址转换）属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法 IP 地址的转换技术，主要用于实现私有网络访问公共网络的功能，它被广泛应用于各种类型 Internet 接入方式和各种类型的网络中。原因很简单，NAT 不仅完美地解决了 lP 地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。\n\n#### NAT分类\n\n根据 Stun 协议(RFC3489)，NAT 大致分为下面四类：\n\n**1) Full Cone**（全锥型）\n\n这种 NAT 内部的机器 A 连接过外网机器 C 后，NAT 会打开一个端口。然后外网的任何发到这个打开的端口的 UDP 数据报都可以到达 A，不管是不是 C 发过来的。\n\n例如：\n\n​```shell\nA: 192.168.8.100 \nNAT: 202.100.100.100 \nC: 292.88.88.88 \nA(192.168.8.100:5000) -> NAT(202.100.100.100:8000) -> C(292.88.88.88:2000) \n任何发送到 NAT(202.100.100.100:8000) 的数据都可以到达 A(192.168.8.100:5000)\n```\n\n**2) Restricted Cone**（受限锥型）\n\n这种 NAT 内部的机器 A 连接过外网的机器 C 后，NAT 打开一个端口，然后 C 可以用任何端口和 A 通信，其他的外网机器不行。\n\n例如：\n\n```shell\nA:192.168.8.100 \nNAT:202.100.100.100 \nC:292.88.88.88 \nA(192.168.8.100:5000) -> NAT(202.100.100.100 : 8000) -> C(292.88.88.88:2000) \n任何从 C 发送到 NAT(202.100.100.100:8000)的数据都可以到达 A(192.168.8.100:5000)\n```\n\n**3) Port Restricted Cone**（端口受限锥型）\n\n这种 NAT 内部的机器 A 连接过外网的机器 C 后，NAT打开一个端口，然后 C 可以用原来的端口和 A 通信，其他的外网机器不行。\n\n例如：\n\n```shell\nA:192.168.8.100 \nNAT:202.100.100.100 \nC:292.88.88.88 \nA(192.168.8.100:5000) -> NAT(202.100.100.100 : 8000) -> C(292.88.88.88:2000) C(202.88.88.88:2000)发送到 NAT(202.100.100.100:8000)的数据都可以到达A(192.168.8.100:5000)\n```\n\n以上三种 NAT 通称 Cone NAT(锥型NAT)。我们只能用这种 NAT 进行 UDP 打洞。\n\n**4) Symmetic**（对称型）\n\n对于这种 NAT 连接不同的外部目标，原来 NAT 打开的端口会变化，而 Cone NAT 不会。虽然可以用端口猜测，但是成功的概率很小。因此放弃这种 NAT 的 UDP 打洞。\n\n#### UDP hole punching（UDP打洞）\n\n对于 Cone NAT，要采用 UDP 打洞，需要一个公网机器 C 来充当 ”介绍人”，内网的 A、B 先分别和 C 通信，打开各自的 NAT 端口，C 这个时候知道 A、B 的公网 `IP:Port`，现在 A 和 B 想直接连接，比如 A 给 B 发，除非 B 是 Full Cone，否则不能通信。反之亦然，但是我们可以这样：\n\nA 要连接 B，A 给 B 发一个 UD P包，同时同，A 让那个介绍人给 B 发一个命令，让 B 同时给 A 发一个 UDP 包，这样双方的 NAT 都会记录对方的 IP，然后就会允许互相通信。\n\n#### NAT穿越\n\n我们目前大部分人连接互联网时都处于防火墙后面或者配置私有子网的家庭(NAT)路由器后面, 这就导致我们的计算机的 IP 地址不是广域网 IP 地址, 故而不能相互之间直接通讯。 \n\n正因为这样的一个场景, 我们得想办法去穿越这些防火墙或者家庭(NAT)路由器，让两个同处于私有网络里的计算机能够通讯起来。建立点对点信道的一个常见问题，也就是 NAT 穿越技术问题，即在处于使用了 NAT 设备的私有 TCP/IP 网络中的主机之间需要建立连接时需要使用 NAT 穿越技术。\n\n以往在 VoIP 领域经常会遇到这个问题。目前已经有很多 NAT 穿越技术，但没有一项是完美的，因为 NAT 的行为是非标准化的。这些技术中大多使用了一个公共服务器，这个服务使用了一个从全球任何地方都能访问得到的 IP 地址。\n\n![](/images/imageWebRTC/others/NAT穿越.png)\n\nSTUN(Simple Traversal of UDP over NATs,NAT 的UDP简单穿越)，STUN 协议服务器就是用来解决这些问题:\n\n- 探测和发现通讯对方是否躲在防火墙或者NAT路由器后面。\n- 确定内网客户端所暴露在外的广域网的 IP 和端口以及 NAT 类型等信息; STUN 服务器利用这些信息协助不同内网的计算机之间建立点对点的 UDP 通讯.\n\nSTUN 协议可以很好的解决一般家用(NAT)路由器环境的打洞问题, 但是对于大部分的企业的网络环境就不是很好了。\n\n这时需要一个新的解决方案: TURN（Traversal Using Relay NAT，中继 NAT 实现的穿透）允许在 TCP 或 UDP 的连线上跨越 NAT 或防火墙。\n\nTURN 是一个 Client-Server 协议。TURN 的 NAT 穿透方法与 STUN 类似，都是通过取得应用层中的公有地址达到 NAT 穿透, 但实现 TURN client 的终端必须在通讯开始前与 TURN server 进行交互, 并要求 TURN server 产生 \"relay port\", 也就是 relayed-transport-address. 这时 TURN server 会建立 peer, 即远端端点（remote endpoints）, 开始进行中继（relay）的动作, TURN client 利用 relay port 将资料传送至 peer, 再由 peer 转传到另一方的 TURN client. 通过服务器新产生的 peer 来进行数据的中转。\n\nICE 就是综合前面 2 种协议的综合性 NAT 穿越解决方案。在 `RTCPeeConnection` 中，使用 ICE 框架来保证 `RTCPeerConnection` 能实现 NAT 穿越。\n\nICE，全名叫交互式连接建立（Interactive Connectivity Establishment）, 一种综合性的 NAT 穿越技术，它是一种框架，可以整合各种 NAT 穿越技术如 STUN、TURN。ICE 会先使用 STUN，尝试建立一个基于 UDP 的连接，如果失败了，就会去尝试 TCP（先尝试 HTTP，然后尝试 HTTPS），如果依旧失败 ICE 就会使用一个中继的 TURN 服务器。\n\n通过 `offer/answer` 模型建立基于 UDP 的通讯。ICE 是 `offer/answer` 模型的扩展，通过在 `offer` 和 `answer` 的 SDP(Session Description Protocol)里面包含多种 IP 地址和端口，然后对本地 SDP 和远程 SDP 里面的 IP 地址进行配对，然后通过 P2P 连通性检查进行连通性测试工作，如果测试通过即表明该传输地址对可以建立连接。\n\n其中 IP 地址和端口（也就是地址）有以下几种：\n\n- 本机地址\n- 通过STUN服务器反射后获取的server-reflexive地址（内网地址被NAT映射后的地址）、relayed地址（和TURN转发服务器相对应的地址）及Peer reflexive地址等。\n\n我们可以使用 Google 的 STUN 服器：**`stun:stun.l.google.com:19302`**，于是乎，一个整合了 ICE 框架的架构应该长这个样子 ：\n\n![Finding connection candidates（找到联系候选人）](/images/imageWebRTC/others/Finding_connection_candidates.png)\n\n![WebRTC data pathways（WebRTC数据通路）](/images/imageWebRTC/others/WebRTC_data_pathways.png)\n\n### 为什么需要信令？\n\n我们需要通过一系列的信令来建立浏览器之间的通信。而具体需要通过信令交换哪些内容呢？这里大概列了一下：\n\n- 用来控制通信开启或者关闭的连接控制消息\n- 发生错误时用来彼此告知的消息\n- 媒体适配：媒体流元数据，比如像解码器、解码器的配置、带宽、媒体类型等等\n- 用来建立安全连接的关键数据\n- 网络配置：外界所看到的的网络上的数据，比如 IP 地址、端口等\n\n这些信息的交换应该在点对点的流传输之前就全部完成。在建立连接之前，浏览器之间显然没有办法传递数据。所以我们需要通过服务器的中转，在浏览器之间传递这些数据，然后建立浏览器之间的点对点连接。但是WebRTC API中并没有实现这些。\n\n### 为什么WebRTC不去实现信令交换？\n\n不去由 WebRTC 实现信令交换的原因很简单：\n\n- WebRTC 标准的制定者们希望能够最大限度地兼容已有的成熟技术。具体的连接建立方式由一种叫JSEP（JavaScript Session Establishment Protocol）的协议来规定，使用 JSEP 有两个好处：\n  - 在 JSEP 中，需要交换的关键信息是多媒体会话描述（multimedia session description）。由于开发者在其所开发的应用程序中信令所使用的协议不同（SIP 或是 XMPP 或是开发者自己定义的协议），WebRTC建立呼叫的思想建立在媒体流控制层面上，从而与上层信令传输相分离，防止相互之间的信令污染。只要上层信令为其提供了多媒体会话描述符这样的关键信息就可以建立连接，不管开发者用何种方式来传递\n  - JSEP 的架构同时也避免了在浏览器上保存连接的状态，防止其像一个状态机一样工作。由于页面经常被频繁的刷新，如果连接的状态保存在浏览器中，每次刷新都会丢失。使用 JSEP 能使得状态被保存在服务器上。\n\n![JSEP architecture](/images/imageWebRTC/others/JSEP_architecture.png)\n\n### 会话描述协议（Session Description Protocol）\n\nJSEP 将客户端之间传递的信令分为两种：\n\n- `offer信令`\n- `answer信令`\n\n他们主要内容的格式都遵循会话描述协议（Session Description Protocal，简称 SDP）。一个 SDP 的信令的内容大致上如下：\n\n<details><summary>SDP信令内容</summary>\n\n```shell\nv=0\no=- 7806956 075423448571 2 IN IP4 127.0.0.1\ns=-\nt=0 0\na=group:BUNDLE audio video data\na=msid-semantic: WMS 5UhOcZZB1uXtVbYAU5thB0SpkXbzk9FHo30g\nm=audio 1 RTP/SAVPF 111 103 104 0 8 106 105 13 126\nc=IN IP4 0.0.0.0\na=rtcp:1 IN IP4 0.0.0.0\na=ice-ufrag:grnpQ0BSTSnBLroq\na=ice-pwd:N5i4DZKMM2L7FEYnhO8V7Kg5\na=ice-options:google-ice\na=fingerprint:sha-256 01:A3:18:0E:36:5E:EF:24:18:8C:8B:0C:9E:B0:84:F6:34:E9:42:E3:0F:43:64:ED:EC:46:2C:3C:23:E3:78:7B\na=setup:actpass\na=mid:audio\na=extmap:1 urn:ietf:params:rtp-hdrext:ssrc-audio-level\na=recvonly\na=rtcp-mux\na=crypto:1 AES_CM_128_HMAC_SHA1_80 inline:qzcKu22ar1+lYah6o8ggzGcQ5obCttoOO2IzXwFV\na=rtpmap:111 opus/48000/2\na=fmtp:111 minptime=10\na=rtpmap:103 ISAC/16000\na=rtpmap:104 ISAC/32000\na=rtpmap:0 PCMU/8000\na=rtpmap:8 PCMA/8000\na=rtpmap:106 CN/32000\na=rtpmap:105 CN/16000\na=rtpmap:13 CN/8000\na=rtpmap:126 telephone-event/8000\na=maxptime:60\nm=video 1 RTP/SAVPF 100 116 117\nc=IN IP4 0.0.0.0\na=rtcp:1 IN IP4 0.0.0.0\na=ice-ufrag:grnpQ0BSTSnBLroq\na=ice-pwd:N5i4DZKMM2L7FEYnhO8V7Kg5\na=ice-options:google-ice\na=fingerprint:sha-256 01:A3:18:0E:36:5E:EF:24:18:8C:8B:0C:9E:B0:84:F6:34:E9:42:E3:0F:43:64:ED:EC:46:2C:3C:23:E3:78:7B\na=setup:actpass\na=mid:video\na=extmap:2 urn:ietf:params:rtp-hdrext:toffset\na=extmap:3 http://www.webrtc.org/experiments/rtp-hdrext/abs-send-time\na=sendrecv\na=rtcp-mux\na=crypto:1 AES_CM_128_HMAC_SHA1_80 inline:qzcKu22ar1+lYah6o8ggzGcQ5obCttoOO2IzXwFV\na=rtpmap:100 VP8/90000\na=rtcp-fb:100 ccm fir\na=rtcp-fb:100 nack\na=rtcp-fb:100 goog-remb\na=rtpmap:116 red/90000\na=rtpmap:117 ulpfec/90000\na=ssrc:3162115896 cname:/nERF7Ern+udqf++\na=ssrc:3162115896 msid:5UhOcZZB1uXtVbYAU5thB0SpkXbzk9FHo30g 221b204e-c9a0-4b01-b361-e17e9bf8f639\na=ssrc:3162115896 mslabel:5UhOcZZB1uXtVbYAU5thB0SpkXbzk9FHo30g\na=ssrc:3162115896 label:221b204e-c9a0-4b01-b361-e17e9bf8f639\nm=application 1 DTLS/SCTP 5000\nc=IN IP40.0.0.0\na=ice-ufrag:grnpQ0BSTSnBLroq\na=ice-pwd:N5i4DZKMM2L7FEYnhO8V7Kg5\na=ice-options:google-ice\na=fingerprint:sha-256 01:A3:18:0E:36:5E:EF:24:18:8C:8B:0C:9E:B0:84:F6:34:E9:42:E3:0F:43:64:ED:EC:46:2C:3C:23:E3:78:7B\na=setup:actpass\na=mid:data\na=sctpmap:5000 webrtc-datachannel 1024\n```\n\n</details>\n\n它是一个在点对点连接中描述自己的字符串，我们可以将其封装在 JSON 中进行传输，在 `PeerConnection` 建立后将其通过服务器中转后，将自己的 SDP 描述符和对方的 SDP 描述符交给 `PeerConnection` 就行了。若想深入了解，可以参考[SDP for the WebRTC draft-nandakumar-rtcweb-sdp-04](http://datatracker.ietf.org/doc/draft-nandakumar-rtcweb-sdp/?include_text=1)进行解析。\n\n### 令与RTCPeerConnection建立\n\n在上一章节中介绍过，WebRTC 使用 `RTCPeerConnection` 来在浏览器之间传递流数据，在建立 `RTCPeerConnection` 实例之后，想要使用其建立一个点对点的信道，我们需要做两件事：\n\n- 确定本机上的媒体流的特性，比如分辨率、编解码能力啥的（SDP 描述符）\n\n- 连接两端的主机的网络地址（ICE Candidate）\n\n需要注意的是，由于连接两端的主机都可能在内网或是在防火墙之后，我们需要一种对所有联网的计算机都通用的定位方式。这其中就涉及 NAT/防火墙穿越技术，以及 WebRTC 用来达到这个目的所 ICE 框架。\n\n### 通过offer和answer交换SDP描述符\n\n大致上在两个用户（甲和乙）之间建立点对点连接流程应该是这个样子（这里不考虑错误的情况， `RTCPeerConnection` 简称 PC）：\n\n- 甲和乙各自建立一个 PC 实例\n- 甲通过 PC 所提供的 `createOffer()` 方法建立一个包含甲的 SDP 描述符的 `offer信令`\n- 甲通过 PC 所提供的 `setLocalDescription()` 方法，将甲的 SDP 描述符交给甲的 PC 实例\n- 甲将 `offer信令` 通过服务器发送给乙\n- 乙将甲的 `offer信令` 中所包含的的 SDP 描述符提取出来，通过 PC 所提供的 `setRemoteDescription()` 方法交给乙的 PC 实例\n- 乙通过 PC 所提供的 `createAnswer()` 方法建立一个包含乙的 SDP 描述符 `answer信令`\n- 乙通过 PC 所提供的 `setLocalDescription()` 方法，将乙的 SDP 描述符交给乙的 PC 实例\n- 乙将 `answer信令` 通过服务器发送给甲\n- 甲接收到乙的 `answer信令` 后，将其中乙的 SDP 描述符提取出来，调用 `setRemoteDescripttion()` 方法交给甲自己的 PC 实例\n\n通过在这一系列的信令交换之后，甲和乙所创建的 PC 实例都包含甲和乙的 SDP 描述符了，完成了两件事的第一件。我们还需要完成第二件事——获取连接两端主机的网络地址。\n\n### 通过ICE框架建立NAT/防火墙穿越的连接\n\n这个网络地址应该是能从外界直接访问，WebRTC 使用 ICE 框架来获得这个地址。`RTCPeerConnection` 在创立的时候可以将 ICE 服务器的地址传递进去，如：\n\n```javascript\nvar iceServer = {\n    \"iceServers\": [{\n        \"url\": \"stun:stun.l.google.com:19302\"\n    }]\n};\nvar pc = new RTCPeerConnection(iceServer);\n```\n\n当然这个地址也需要交换，还是以甲乙两位为例，交换的流程如下（`RTCPeerConnection` 简称 PC）：\n\n- 甲、乙各创建配置了 ICE 服务器的 PC 实例，并为其添加 `onicecandidate` 事件回调\n- 当网络候选可用时，将会调用 `onicecandidate` 函数\n- 在回调函数内部，甲或乙将网络候选的消息封装在 ICE Candidate 信令中，通过服务器中转，传递给对方\n- 甲或乙接收到对方通过服务器中转所发送过来 ICE Candidate 信令时，将其解析并获得网络候选，将其通过 PC 实例的 `addIceCandidate()` 方法加入到 PC 实例中\n\n这样连接就创立完成了，可以向 `RTCPeerConnection` 中通过 `addStream()` 加入流来传输媒体流数据。将流加入到 `RTCPeerConnection` 实例中后，对方就可以通过 `onaddstream` 所绑定的回调函数监听到了。调用 `addStream()` 可以在连接完成之前，在连接建立之后，对方一样能监听到媒体流。\n\n### 聊天室中的信令\n\n上面是两个用户之间的信令交换流程，但我们需要建立一个多用户在线视频聊天的聊天室。所以需要进行一些扩展，来达到这个要求。\n\n#### 用户操作\n\n首先需要确定一个用户在聊天室中的操作大致流程：\n\n1. 打开页面连接到服务器上\n2. 进入聊天室\n3. 与其他所有已在聊天室的用户建立点对点的连接，并输出在页面上\n4. 若有聊天室内的其他用户离开，应得到通知，关闭与其的连接并移除其在页面中的输出\n5. 若又有其他用户加入，应得到通知，建立于新加入用户的连接，并输出在页面上\n6. 离开页面，关闭所有连接\n\n从上面可以看出来，除了点对点连接的建立，还需要服务器至少做如下几件事：\n\n1. 新用户加入房间时，发送新用户的信息给房间内的其他用户\n2. 新用户加入房间时，发送房间内的其他用户信息给新加入房间的用户\n3. 用户离开房间时，发送离开用户的信息给房间内的其他用户\n\n#### 实现思路\n\n以使用 WebSocket 为例，上面用户操作的流程可以进行以下修改：\n\n1. 浏览器与服务器建立 WebSocket 连接\n2. 发送一个加入聊天室的信令（`join`），信令中需要包含用户所进入的聊天室名称\n3. 服务器根据用户所加入的房间，发送一个其他用户信令（`peers`），信令中包含聊天室中其他用户的信息，浏览器根据信息来逐个构建与其他用户的点对点连接\n4. 若有用户离开，服务器发送一个用户离开信令（`remove_peer`），信令中包含离开的用户的信息，浏览器根据信息关闭与离开用户的信息，并作相应的清除操作\n5. 若有新用户加入，服务器发送一个用户加入信令（`new_peer`），信令中包含新加入的用户的信息，浏览器根据信息来建立与这个新用户的点对点连接\n6. 用户离开页面，关闭 WebSocket 连接\n\n#### 服务器实现\n\n由于用户可以只是建立连接，可能还没有进入具体房间，所以首先我们需要一个容器来保存所有用户的连接，同时监听用户是否与服务器建立了 WebSocket 的连接：\n\n```javascript\nvar server = new WebSocketServer();\nvar sockets = [];\n\nserver.on('connection', function(socket){\n    socket.on('close', function(){\n        var i = sockets.indexOf(socket);\n        sockets.splice(i, 1);\n        //关闭连接后的其他操作\n    });\n    sockets.push(socket);\n    //连接建立后的其他操作\n});\n```\n\n由于有房间的划分，所以我们需要在服务器上建立一个容器，用来保存房间内的用户信息。显然对象较为合适，键为房间名称，值为用户信息列表。\n\n同时我们需要监听上面所说的用户加入房间的信令（`join`），新用户加入之后需要向新用户发送房间内其他用户信息（`peers`）和向房间内其他用户发送新用户信息（`new_peer`），以及用户离开时向其他用户发送离开用户的信息（`remove_peer`）:\n\n于是乎代码大致就变成这样：\n\n<details><summary>服务器实现</summary>\n\n```java\nvar server = new WebSocketServer();\nvar sockets = [];\nvar rooms = {};\n\n/*\njoin信令所接收的格式\n{\n    \"eventName\": \"join\",\n    \"data\": {\n        \"room\": \"roomName\"\n    }\n}\n*/\nvar joinRoom = function(data, socket) {\n    var room = data.room || \"__default\";\n    var curRoomSockets; //当前房间的socket列表\n    var socketIds = []; //房间其他用户的id\n\n    curRoomSockets = rooms[room] = rooms[room] || [];\n\n    //给所有房间内的其他人发送新用户的id\n    for (var i = curRoomSockets.length; i--;) {\n        socketIds.push(curRoomSockets[i].id);\n        curRoomSockets[i].send(JSON.stringify({\n            \"eventName\": \"new_peer\",\n            \"data\": {\n                \"socketId\": socket.id\n            }\n        }));\n    }\n\n    //将新用户的连接加入到房间的连接列表中\n    curRoomSockets.push(socket);\n    socket.room = room;\n\n    //给新用户发送其他用户的信息，及服务器给新用户自己赋予的id\n    socket.send(JSON.stringify({\n        \"eventName\": \"peers\",\n        \"data\": {\n            \"socketIds\": socketIds,\n            \"you\": socket.id\n        }\n    }));\n};\n\nserver.on('connection', function(socket) {\n    //为socket构建一个特有的id，用来作为区分用户的标记\n    socket.id = getRandomString();\n    //用户关闭连接后，应做的处理\n    socket.on('close', function() {\n        var i = sockets.indexOf(socket);\n        var room = socket.room;\n        var curRoomSockets = rooms[room];\n        sockets.splice(i, 1);\n        //通知房间内其他用户\n        if (curRoomSockets) {\n            for (i = curRoomSockets.length; i--;) {\n                curRoomSockets[i].send(JSON.stringify({\n                    \"eventName\": \"remove_peer\",\n                    \"data\": {\n                        \"socketId\": socket.id\n                    }\n                }));\n            }\n        }\n        //从room中删除socket\n        if (room) {\n            i = this.rooms[room].indexOf(socket);\n            this.rooms[room].splice(i, 1);\n            if (this.rooms[room].length === 0) {\n                delete this.rooms[room];\n            }\n        }\n        //关闭连接后的其他操作\n    });\n    //根据前台页面传递过来的信令进行解析，确定应该如何处理\n    socket.on('message', function(data) {\n        var json = JSON.parse(data);\n        if (json.eventName) {\n            if (json.eventName === \"join\") {\n                joinRoom(data, socket);\n            }\n        }\n    });\n    //将连接保存\n    sockets.push(socket);\n    //连接建立后的其他操作\n});\n```\n\n</details>\n\n最后再加上点对点的信令转发就行了，一份完整的代码可参考[SkyRTC项目源码](https://github.com/LingyuCoder/SkyRTC/blob/master/SkyRTC.js)\n\n## WebRTC点对点通信\n\nWebRTC 给我们带来了浏览器中的视频、音频聊天体验。但个人认为，它最实用的特性莫过于 DataChannel ——在浏览器之间建立一个点对点的数据通道。\n\n在 DataChannel 之前，浏览器到浏览器的数据传递通常是这样一个流程：浏览器 A 发送数据给服务器，服务器处理，服务器再转发给浏览器 B。这三个过程都会带来相应的消耗，占用服务器带宽不说，还减缓了消息从发送到接收的时间。\n\n其实最理想的方式就是浏览器 A 直接与浏览 B 进行通信，服务器不需要参与其中。WebRTC DataChannel 就提供了这样一种方式。当然服务器完全不参与其中，显然是不可能的，用户需要通过服务器上存储的信息，才能确定需要和谁建立连接。这里通过一个故事来讲述建立连接的过程：\n\n### DataChannel连接的建立\n\n**故事一：老刘和老姚去钓鱼**\n\n背景：\n\n- 老刘和老姚都住在同一个小区但不同的片区，小区很破旧，没有电话\n- 片区相互隔离且片区门口有个保安，保安只认识自己片区的人，遇到不认识的人就需要查询凭证才能通过，而凭证需要找物业才能确定\n- 门卫老大爷认识小区里的所有人但是不知道都住哪，有什么消息都可以在出入小区的时候代为传达\n\n现在，老刘听说老姚钓鱼技术高超，想和老姚讨论钓鱼技巧。只要老刘和老姚相互之间知道对方的门牌号以及凭证，就可以串门了:\n\n1. 门卫老大爷认识老刘和老姚\n2. 老刘找物业确定了自己片区的出入凭证，将凭证、自己的门牌号以及意图告诉门卫老大爷，让其转交给老姚\n3. 老姚买菜归来遇到门卫老大爷，门卫老大爷将老刘的消息传达给老姚。于是老姚知道怎么去老刘家了\n4. 老姚很开心，他也找物业获取了自己小区的凭证，并将凭证、自己的门牌号等信息交给门卫老大爷，希望他传达给老刘\n5. 老刘吃早餐回来遇到门卫老大爷，老大爷把老姚的小区凭证、门牌号等信息告诉老刘，这样老刘就知道了怎么去老姚家了\n\n老刘和老姚相互之间知道了对方的门牌号和小区出入凭证，他们相互之间有什么需要交流的直接串门就行了，消息不再需要门卫老大爷来代为传达了\n\n**换个角度**\n\n我们把角色做一个映射：\n\n- 老刘：浏览器A\n- 老姚：浏览器B\n- 片区：不同网段\n- 保安：防火墙\n- 片区凭证：ICE candidate\n- 物业：ICE server\n- 门牌号：session description\n- 门卫老大爷：server\n\n于是乎故事就变成了这样：\n\n1. 浏览器 A 和浏览器 B 在 server 上注册，并保有连接\n2. 浏览器 A 从 ice server 获取 ice candidate 并发送给 server，并生成包含 session description 的 offer，发送给 server\n3. server 发送浏览器 A 的 offer 和 ice candidate 给浏览器 B\n4. 浏览器 B 发送包含 session description 的 answer 和 ice candidate 给 server\n5. server 发送浏览器 B 的 answer 和 ice candidate 给浏览器 A\n\n这样，就建立了一个点对点的信道。\n\n### DataChannel的分片传输\n\n**故事二：老姚送礼物**\n\n老刘和老姚已经可以相互串门了，经过一段时间的交流感情越来越深。老姚的亲友送了20斤葡萄给老姚，老姚决定送10斤给老刘。老姚毕竟年事已高，不可能一次带10斤。于是乎，老姚将葡萄分成了10份，每次去老刘家串门就送一份过去。\n\n这里可以做如下类比：\n\n1. 10斤葡萄：一个文件（尽管文件分片没有意义，葡萄分开还可以单独吃，但是实在找不到啥好的比喻了）\n2. 分成10份：将文件分片，转成多个 chunk\n3. 老姚一次只能带一斤：datachannel 每次传输的数据量不宜太大（[找到最合适的大小](http://stackoverflow.com/questions/15435121/what-is-the-maximum-size-of-webrtc-data-channel-messages)）\n\n这其实就是通过 datachannel 传输文件的方式，首先将文件分片，然后逐个发送，最后再统一的进行组合成一个新的文件\n\n**分片**\n\n通过 HTML5 的 File API 可以将 `type` 为 `file` 的 `input` 选中的文件读取出来，并转换成 `data url` 字符串。这也就为我们提供了很方便的分片方式：\n\n```javascript\nvar reader = new window.FileReader(file);\nreader.readAsDataURL(file);\nreader.onload = function(event, text) {\n    chunkify(event.target.result);//将数据分片\n};\n```\n\n**组合**\n\n通过 datachannel 发送的分片数据，我们需要将其进行组合，由于是 `data url` 字符串，在接收到所有包之后进行拼接就可以了。拼接完成后就得到了一个文件完整的 `data url` 字符串，那么我们如何将这个字符串转换成文件呢？\n\n**方案一：直接跳转下载**\n\n既然是个 `dataurl`，我们直接将其赋值给 `window.location.href` 自然可以下载，但是这样下载是没法设定下载后的文件名的，这想一想都蛋疼\n\n**方案二：通过 `a` 标签下载**\n\n这个原理和跳转下载类似，都是使用 `dataurl` 本身的特性，通过创建一个 `a` 标签，将 `dataurl` 字符串赋值给 `href` 属性，然后使用 download 确定下载后的文件名，就可以完成下载了。但是很快又有新问题了，稍微大一点的文件下载的时候页面崩溃了。这是因为 `dataurl` 有大小限制\n\n**方案三：blob**\n\n其实可以通过给 `a` 标签创建 blob url 的方式来进行下载，这个没有大小限制。但是我们手上是 `dataurl`，所以需要先进行转换：\n\n```javascript\nfunction dataURItoBlob(dataURI, dataTYPE) {\n    var binary = atob(dataURI.split(',')[1]),\n        array = [];\n    for (var i = 0; i < binary.length; i++) array.push(binary.charCodeAt(i));\n    return new Blob([new Uint8Array(array)], {\n        type: dataTYPE\n    });\n}\n```\n\n获得 blob 后，我们就可以通过 URL API 来下载了：\n\n```javascript\nvar a = document.createElement(\"a\");\ndocument.body.appendChild(a);\na.style = \"display: none\";\nvar blob = dataURItoBlob(data, 'octet/stream');\nvar url = window.URL.createObjectURL(blob);\na.href = url;\na.download = filename;\na.click();\n!moz && window.URL.revokeObjectURL(url);\na.parentNode.removeChild(a);\n```\n\n这里有几个点：\n\n- datachannel 其实是可以直接传送 blob 的，但是只有 firefox 支持，所以传 `data url`\n\n- chrome 下载是直接触发的，不会进行询问，firefox 会先询问后下载，在询问过程中如果执行了 `revokeObjectURL`，下载就会取消。\n\n### DataChannel传输的升级\n\n如我们所知，WebRTC 最有特点的地方其实是可以传输 `getUserMedia` 获得的视频、音频流，来实现视频聊天。但事实上我们的使用习惯来看，一般人不会一开始就打开视频聊天，而且视频聊天时很消耗内存的（32 位机上一个连接至少 20M 左右好像，也有可能有出入）。所以常见的需求是，先建立一个包含 datachannel 的连接用于传输数据，然后在需要时升级成可以传输视频、音频。\n\n看看我们之前传输的 session description，它其实来自[Session Description Protocol](http://datatracker.ietf.org/doc/draft-nandakumar-rtcweb-sdp/?include_text=1)。可以看到 wiki 上的介绍：\n\n> The Session Description Protocol (SDP) is a format for describing streaming media initialization parameters.\n\n这意味着什么呢？我们之前建立 datachannel 是没有加视频、音频流的，而这个流的描述是写在 SDP 里面的。现在我们需要传输视频、音频，就需要添加这些描述。所以就得重新获得 SDP，然后构建 offer 和 answer 再传输一次。传输的流程和之前一样，没什么区别。但这一次，我们不需要传输任何的 ice candidate。\n\n> from mattm: You do not need to send ICE candidates on an already established peer connection. The ICE candidates are to make sure the two peers can establish a connection through their potential NAT and firewalls. If you can already send data on the peer connection, ICE candidates will not do anything.\n\n## 有用的链接\n\n### Specifications:\n\n- WebRTC 1.0: Real-time Communication Between Browsers：<https://www.w3.org/TR/webrtc/>\n- Media Capture and Streams：<https://w3c.github.io/mediacapture-main/>\n- Media Capture from DOM Elements：<https://w3c.github.io/mediacapture-fromelement/>\n\n### Getting started:\n\nWebRTC官方网站：<https://webrtc.org/start/>\n\nA Study of WebRTC Security：<http://webrtc-security.github.io/>\n\n### Tutorials:\n\n<https://www.html5rocks.com/en/tutorials/webrtc/basics/>\n\n### WebRTC API:\n\n<https://developer.mozilla.org/zh-CN/docs/Web/API/WebRTC_API>\n\n### WebRTC codelab:\n\nA step-by-step guide that explains how to build a complete video chat app, including a simple signaling server.<https://www.bitbucket.org/webrtc/codelab>\n\n### Javascript frameworks\n\n1. Video chat:\n\n- <https://github.com/andyet/SimpleWebRTC>\n- <https://github.com/priologic/easyrtc>\n- <https://github.com/webRTC-io/webRTC.io>\n\n2. Peer-to-peer data:\n\n- <http://peerjs.com/>\n- <https://github.com/peer5/sharefest>\n\n### Demos:\n\n<https://121.15.167.230:8090/demos/>\n\n<https://webrtc.github.io/samples/>\n\n<https://www.webrtc-experiment.com/>\n\n<https://webcamtoy.com/zh/app/>\n\n<https://idevelop.ro/ascii-camera/>\n\n### WebRTC教程:\n\n1. WebRTC基础:\n\n   - [*Scaledrone*](https://www.scaledrone.com/blog/posts/webrtc-tutorial-simple-video-chat) —这个简单的教程会教你：\n\n     ```shell\n     1) WebRTC 基础知识\n     2) 如何建立一个 1 对 1 视频通话\n     3) 如何使用 Scaledrone 进行信令传输，这样就不需要编写服务器代码了\n     ```\n\n   - [*CodeLabs*](https://codelabs.developers.google.com/codelabs/webrtc-web/#0) —这个教程包括了 WebRTC 介绍，一些简单的代码和 demo。\n\n   - [*Tutorial’s Point*](https://www.tutorialspoint.com/webrtc/index.htm) —这篇教程会帮助所有想要学习如何搭建像实时广告，多人游戏，直播互动，网上学习这些应用的开发人员。\n\n   - [*WebRTC.ventures*](https://webrtc.ventures/webrtc-resources/) Email课程—在这一系列的免费邮件中，你会学到 WebRTC 的基本知识，以及它的优缺点及使用方法。包括这些话题：\n\n     ```shell\n     1) WebRTC 基础\n     2) WebRTC 优点\n     3) WebRTC 缺点\n     4) 信令\n     5) 建立 WebRTC 通话\n     6) WebRTC 中的 DataChannel\n     7) WebRTC 设计\n     ```\n\n   - [*Verto*](http://evoluxbr.github.io/verto-docs/) —在这个教程中，我们会通过例子学到如何建立，调入 verto jQuery 库来创建一个基础的网络视频会议应用。\n\n2. CPASS教程:\n\n   - [*Tokbox*](https://tokbox.com/developer/tutorials/) —这是教程会一步一步指导你建立自己的 OpenTok 实时视频软件中的内容。在完成基本教程之后，你就可以继续跟着 Tokbox 的教程学习进阶内容：\n\n   ```shell\n   1) 归档\n   2) 文字聊天\n   3) 自定义视频渲染\n   4) 自定义音频驱动\n   5) 以及更多内容\n   ```\n\n   - [*Twilio*](https://www.twilio.com/docs/tutorials/browser-calls-node-express) —这个网页应用会向你展示如何使用 Twilio 客户端来创建一个浏览器-手机以及浏览器-浏览器通话。\n   - [*Vidyo.io*](https://support.vidyo.io/hc/en-us/sections/115001457187-Video-Tutorials) —Vidyo.io 通过教育视频聊天教程向我们展示：\n\n   ```shell\n   1) 生成 Vidyo.io Token\n   2) 用几分钟的时间就搭建一个 iOS 版的移动端视频聊天软件\n   3) 用几分钟的时间就搭建一个 Android 版的移动端视频聊天软件\n   ```\n\n3. **信令**\n\n   [*Html5Rocks.com*](https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/) —在这个教程文章中，你会学到如何搭建一个信令服务，以及如何通过使用 STUN 和 TURN 服务器来处理现实世界中连接性产生的各种奇怪问题。还解释了 WebRTC 应用是如何处理多方通话以及与 VoIP 和 PSTN 这些服务互动的。\n\n4. **媒体服务器**\n\n   [*Kurento教程*](http://doc-kurento.readthedocs.io/en/stable/tutorials.html) —这些教程会给你展示如何使用 Kurento 框架搭建不同种类的 WebRTC 及多媒体应用。教程从三个角度出发：Java，浏览器 JavaScript 和 Node.js。\n\n### WebRTC服务提供商:\n\n1. 国外:\n\n- [https://xirsys.com](https://xirsys.com/)\n- <https://tokbox.com/developer/>\n- <https://cloud.aculab.com/documents/webrtcdemo>\n- <https://www.twilio.com/webrtc>\n- <http://www.frafos.com/webrtc/>\n- <http://www.sightcall.com/>\n\n1. 国内:\n\n- 声网：<https://www.agora.io/cn/>\n- 融云：<http://www.rongcloud.cn/>\n- 亲加云：<http://www.gotye.com.cn/>\n- 环信：<https://www.easemob.com/>\n- 野狗通信云：<https://www.wilddog.com/>\n\n\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"webrtc-专题-01-WebRTC框架介绍","url":"/2019-06-16/reference/webrtc/webrtc-专题-01/","content":"\n## 什么是WebRTC？\n\n众所周知，浏览器本身不支持相互之间直接建立信道进行通信，都是通过服务器进行中转。比如现在有两个客户端，甲和乙，他们俩想要通信，首先需要甲和服务器、乙和服务器之间建立信道。甲给乙发送消息时，甲先将消息发送到服务器上，服务器对甲的消息进行中转，发送到乙处，反过来也是一样。这样甲与乙之间的一次消息要通过两段信道，通信的效率同时受制于这两段信道的带宽。同时这样的信道并不适合数据流的传输，如何建立浏览器之间的点对点传输，一直困扰着开发者。WebRTC应运而生。\n\n<!-- more -->\n\nWebRTC，名称源自网页实时通信（Web Real-Time Communication）的缩写，是一项实时通讯技术，它允许网络应用或者站点，在不借助中间媒介的情况下，建立浏览器之间点对点（Peer-to-Peer）的连接，实现视频流和（或）音频流或者其他任意数据的传输，支持网页浏览器进行实时语音对话或视频对话。WebRTC包含的这些标准使用户在无需安装任何插件或者第三方的软件的情况下，创建点对点（Peer-to-Peer）的数据分享和电话会议成为可能。它是谷歌2010年5月以6820万美元收购拥有编解码、回声消除等技术的Global IP Solutions公司而获得的一项技术。该项目是由GIPS项目和libjingle项目融合而成。其中GIPS部分主要提供媒体的处理的功能。libjingle项目部分主要提供P2P传输部分的功能。2011年5月开放了工程的源代码，与相关机构 IETF 和 W3C 制定行业标准，组成了现有的 WebRTC 项目，在行业内得到了广泛的支持和应用，成为下一代视频通话的标准。\n\nWebRTC是一个开源项目，旨在使得浏览器能为实时通信（RTC）提供简单的JavaScript接口。说的简单明了一点就是让浏览器提供JS的即时通信接口。这个接口所创立的信道并不是像WebSocket一样，打通一个浏览器与WebSocket服务器之间的通信，而是通过一系列的信令，建立一个浏览器与浏览器之间（peer-to-peer）的信道，这个信道可以发送任何数据，而不需要经过服务器。并且WebRTC通过实现MediaStream，通过浏览器调用设备的摄像头、话筒，使得浏览器之间可以传递音频和视频。\n\nWebRTC并不是单一的协议， 包含了媒体、加密、传输层等在内的多个协议标准以及一套基于 JavaScript 的 API。通过简单易用的 JavaScript API ，在不安装任何插件的情况下，让浏览器拥有了 P2P音视频和数据分享的能力。同时WebRTC 并不是一个孤立的协议，它拥有灵活的信令，可以便捷的对接现有的SIP 和电话网络的系统。\n\n关键要认识到的是，点对点并不意味着不涉及服务器，这只是意味着正常的数据没有经过它们。至少，两台客户机仍然需要一台服务器来交换一些基本信息（我在网络上的哪些位置，我支持哪些编解码器），以便他们可以建立对等的连接。用于建立对等连接的信息被称为信令，而服务器被称为信令服务器。\n\nWebRTC没有规定您使用什么信令服务器或什么协议。 Websockets是最常见的，但也可以使用长轮询甚至邮件协议。\n\n## WebRTC的目标\n\nWebRTC实现了基于网页的视频会议，标准是WHATWG 协议，目的是通过浏览器提供简单的javascript就可以达到实时通讯（Real-Time Communications (RTC)）能力。\n\nWebRTC（Web Real-Time Communication）项目的最终目的主要是让Web开发者能够基于浏览器（Chrome\\FireFox...）轻易快捷开发出丰富的实时多媒体应用，而无需下载安装任何插件，Web开发者也无需关注多媒体的数字信号处理过程，只需编写简单的Javascript程序即可实现，W3C等组织正在制定Javascript 标准API，目前是[WebRTC 1.0版本](http://w3c.github.io/webrtc-pc/)，Draft状态；另外WebRTC还希望能够建立一个多互联网浏览器间健壮的实时通信的平台，形成开发者与浏览器厂商良好的生态环境。同时，Google也希望和致力于让WebRTC的技术成为HTML5标准之一，可见Google布局之深远。\n\nWebRTC 提供了视频会议的核心技术，包括音视频的采集、编解码、网络传输、显示等功能，并且还支持跨平台：windows，linux，mac，android。\n\n## WebRTC的应用场景\n\nWebRTC的点对点方式能够运用在很多场景：\n\n- 社交平台，如视频聊天室应用\n- 远程实时监控\n- 远程学习，如在线教育、在线培训\n- 远程医疗，如在线医疗\n- 人力资源和招聘，如在线面试\n- 会议和联系中心之间的协作，如客户服务、呼叫中心\n- Web IM，如web qq\n- 游戏娱乐，如双人对战游戏（如象棋这种双人对战游戏，每一步的数据服务器时不关心的，所以完全可以点对点发送）\n- 屏幕共享\n- 人脸检测识别\n- 虚拟现实\n- 市场调研\n- 金融服务\n- 其它即时通信业务\n\n## WebRTC方案在多方实时音视频的应用及局限性\n\n实时音视频有两种会话方式，一种是点对点的，就是2个设备之间进行交流。就像2个人视频聊天这种场景的。另外一种是多方会话，就像视频会议这样的场景。\n\n### WebRTC适合做视频直播或音视频会议吗？\n\n先说结论：**完全可以！**\n\n但是，凡事总有但是，**也没那么简单**。你以为调用几个Chrome的API就能直播了？too simple\n\n那正确的方法是什么呢？\n\n1. 你得有一个实现了WebRTC相关协议的客户端。比如Chrome浏览器。\n2. 架设一个类似MCU系统的服务器。（不知道MCU是什么？看这：[MCU（视频会议系统中心控制设备）](https://baike.baidu.com/item/MCU/3248422#viewPageContent)）\n\n第一步，用你的客户端，比如Chrome浏览器，通过WebRTC相关的媒体API获取图像及声音信源，再用WebRTC中的通信API将图像和声音数据发送到MCU服务器。如果你只是做着玩玩，完全可以直接用Chrome浏览器做你的直播客户端。把摄像头麦克风连上电脑之后，Chrome可以用相关的js的API获取到摄像头和麦克风的数据。缺点就是如果长时间直播，Chrome的稳定性堪忧，因为chrome这样运行24小时以上内存占用很厉害，而且容易崩溃。 第二步，MCU服务器根据你的需求对图像和声音数据进行必要的处理，比如压缩、混音等。你可能要问，WebRTC可以直接在浏览器之间P2P地传输流，为什么还要有中转的MCU服务器？因为Chrome的功能很弱，视频的分辨率控制、多路语音的混音都做不了，所以需要MCU参与。**最重要的是，Chrome同时给6个客户端发视频流就很消耗资源了，所以你如果有超过10个用户收看的话，Chrome很容易崩溃**。 第三步，需要看直播的用户，通过他们的Chrome浏览器，链接上你的MCU服务器，并收取服务器转发来的图像和声音流。\n\n所以，**这就要看你用的客户端是什么**。**如果你是想用浏览器，那WebRTC不是好方案。但如果你是用app，可以肯定回答：可以，而且强烈建议你基于WebRTC**。\n\nWebRTC使web浏览器通过简单的JavaScript api接口实现实时通信功能。在这方面基本已成事实上标准，正如上面写的，它成为标准不是新闻，不成为标准才是新闻。国内就有不少从事和WebRTC相关的开发者，像有的公司就基于WebRTC包做些修改、然后给其它开发者用、号称是视频聊天SDK。这样公司好多，但真正做大却有点难。我想有两个原因：Javascript的限制，浏览器的限制。\n\n**Javascript的限制**。Javascript是脚本语言，能有什么功能取决于实现它的虚拟机，也就是浏览器这个应用程序。由于受限，问题来了，人民群众的需求总是琳琅满目，你都能提供吗？举个例子，直播过程中，要让对方的头上自动加顶红帽子，——当然，修改浏览器代码让加个帽子不是难事，可谁又知道接下会发生什么，难道要一个改一个？聊天往往是娱乐，娱乐经常是没啥规矩。由于这限制，开发者用它时会有这看法：东西是很好，但总是有那么点不足，而且即使是努力了也不可能解决（自个写浏览器除外）。\n\n**浏览器的限制**。这就要涉及到聊天场景。很现实问题，如果我想和你聊天，身边有手机，你认为会用浏览器吗？对PC，网页比app方便，而移动设备却有点反着来，而且将来移动设备会越来越多。关于这个再深入个问题：如果PC用浏览器，手机用app，聊天是否可行？技术实现上没问题，可事实上基本不会做，代价太高划不来。浏览器时，信令走的是Websocket，app用Websocket纯粹是没事找抽，直接C Socket既简单又高效。浏览器时，两socket间没啥心跳包机制，app时心跳包机制可很大提升效率。浏览器时，由于用Javascript开发，功能受限，app时用Native Code，自个想要什么就能实现什么。而且，WebRTC是跨平台包，基于C/C++的跨平台SDK也不是没有，何不在开发时顺便开发出个Windows平台app。以上导致了app不太可能和网页聊天，这又让浏览器少去很多应用场景。\n\n**综合来说，在浏览器不是WebRTC不行，而是其它原因导致有那么点尴尬。想做一个“完美”用户体验的聊天工具，终归还得用app**。\n\n为什么说对App是完全可行呢？浏览器在用的WebRTC其实分两层，底层是个用C++写的库（Native Code），然后上层写个Javascript封装，以便供HTML5调用。既然是写app，那完全不用管上层Js封装，而且Google在开发WebRTC时已考虑用在app，底层C++库的API已做得很完善了。也就是说，一旦直接用Native Code，完全和浏览器无关了，作为C/C++开发者，他就可以用WebRTC去实现自个想实现的所有东西。\n\n对直播使用场景，很多人是用移动设备，移动设备基本都是用app。而WebRTC中的Native Code部分跨平台特性很好，基本不用改，就能写出完全跨iOS、Android、Windows平台的代码，所以有了iOS/Android app，基本不耗成本Windows上的app就出来了。——当然，如果有人在Windows还是坚持要用浏览器，那只能说在Windows不得不留有瑕疵。\n\n为什么有人一想到Windows，直观就认为只有p2p？我猜是和默认的信令服务器是p2p有关。关于这默认的信令服务器是怎么个交互流程，如下图所示：\n\n![信令服务器基本交互流程](/images/imageWebRTC/others/信令服务器基本交互流程.png)\n\n根据这个图，你可以发现，只要换了信令服务器，就有可能变成直播。而事实也的确是这样。就像有人说直播时图像单向就够了（主播传向观众），而WebRTC是双向，只要改信令服务器，立刻就单向了。\n\n为什么强烈建议你基于WebRTC？对直播系统，难的不是服务器，而是客户端。客户端难的地方则主要体现在两个方面，一是网络传输有关，像侦听事件，同步主线程和读线程，穿透；二是流数据有关，像编码、解码、回声消除。而这些正是WebRTC帮你解决了。也正因为如此，现在很多直播系统最早的客户端其实是以WebRTC为根的，只是后面自个不断优化，慢慢地变成自个系统而已。诚然，官方WebRTC是有地方不尽如意，但它们不断更新。\n\n概括的说：\n\n1. **WebRTC整体的技术并不适合做直播**。WebRTC设计的初衷只是为了在两个浏览器/native app之间解决直接连接发送media streaming/data数据的，也就是所谓的peer to peer的通信，大多数的情况下不需要依赖于服务器的中转，因此一般在通信的逻辑上是一对一。而我们现在的直播服务大部分的情况下是一对多的通信，一个主播可能会有成千上万个接收端，这种方式用传统的P2P来实现是不可能的，所以目前直播的方案基本上都是会有直播服务器来做中央管理，主播的数据首先发送给直播服务器，直播服务器为了能够支持非常多用户的同事观看，还要通过边缘节点CDN的方式来做地域加速，所有的接收端都不会直接连接主播，而是从服务器上接收数据。\n2. **WebRTC内部包含的技术模块是非常适合解决直播过程中存在的各种问题的**，而且应该在大多数直播技术框架中都已经得到了部分应用，例如音视频数据的收发、音频处理回音消除降噪等。\n\n所以综上，可以使用WebRTC内部的技术模块来解决直播过程中存在的技术问题，但是不适合直接用WebRTC来实现直播的整体框架。\n\n### 多方会话实现方式\n\nWebRTC针对这多方会话提供了两种实现方式。\n\n**第一种实现方式：实现多个浏览器之间的对等连接——全网状模型**\n\n![全网装模型](/images/imageWebRTC/others/全网装模型.png)\n\n多个浏览器通过Web服务器访问网站，浏览器之间的通话并不通过任何流媒体服务器，而是直接通过对等连接，通过UDP来实现浏览器之间的通信。这个叫做全网状模型。\n\n**第二种实现方式：浏览器和媒体服务器建立对等连接——集中式模型** \n\n服务端除了Web服务器之外还需要架构一个台媒体服务器，媒体服务器和各个浏览器之间实现对点连接。架设媒体服务器的目的在于接收各个浏览器的媒体流，之后通过媒体服务器把媒体流发给各个浏览器。\n\n**两种实现方式的利弊** ：\n\n- 全网状：不需要架设媒体服务器，媒体延迟低质量高。但是如果人数很多的话就会导致浏览器的本地宽带增加，不适合多人会议。\n\n- 集中式：比较适合多人会话，节省本地宽带，但是只有少量浏览器查询的时候，这种体系的效率非常低（因为要走媒体服务器）。\n\n## 业内哪些App在使用WebRTC？\n\n目前，国内外有很多App都在使用WebRTC或者其相关技术，下面仅列举部分案例：\n\n**腾讯QQ和微信**\n\n腾讯的QQ音视频在使用GIPS方案（WebRTC的核心源于GIPS），据说微信内部已大量使用WebRTC组件，其内嵌的浏览器也支持WebRTC。\n\n**陌陌**\n\n由国内知名WebRTC服务提供商声网提供技术支持。\n\n![](/images/imageWebRTC/others/陌陌.png)\n\n**荔枝FM**\n\n由国内知名WebRTC服务提供商声网提供技术支持。\n\n![](/images/imageWebRTC/others/荔枝FM.png)\n\n**MeetMe**\n\n由国内知名WebRTC服务提供商声网提供技术支持。\n\n![](/images/imageWebRTC/others/meetme.png)\n\n**狼人杀**\n\n由国内知名WebRTC服务提供商声网提供技术支持。\n\n![](/images/imageWebRTC/others/狼人杀.png)\n\n**去哪儿**\n\n由国内知名WebRTC服务提供商声网提供技术支持。\n\n![](/images/imageWebRTC/others/去哪儿.png)\n\n## WebRTC的优缺点\n\n**优点：**\n\n- 方便。对于用户来说，在WebRTC出现之前想要进行实时通信就需要安装插件和客户端，但是对于很多用户来说，插件的下载、软件的安装和更新这些操作是复杂而且容易出现问题的，现在WebRTC技术内置于浏览器中，用户不需要使用任何插件或者软件就能通过浏览器来实现实时通信。对于开发者来说，在Google将WebRTC开源之前，浏览器之间实现通信的技术是掌握在大企业手中，这项技术的开发是一个很困难的任务，现在开发者使用简单的HTML标签和JavaScript API就能够实现Web音/视频通信的功能。\n\n- 跨平台。因为基于浏览器，所以可跨浏览器（浏览器支持WebRTC）和跨操作系统平台，windows、Linux、ios、 android......全部支持。\n\n- P2P的优势。使用P2P技术处理数据（音频、视频和文件等）的传输，可减少服务器端的性能压力和带宽成本（这是有条件的，有些网络环境下可能无法使用P2P）\n\n- 一整套的解决方案。从采集，编解码，RTP打包，流量控制，音频处理，多通道混音，都给于了很好的支持，并且是开源的代码，大大节省了开发时间和成本。\n\n- 免费。虽然WebRTC技术已经较为成熟，其集成了最佳的音/视频引擎，十分先进的codec，但是Google对于这些技术不收取任何费用。\n\n- 强大的打洞能力。WebRTC技术包含了使用STUN、ICE、TURN、RTP-over-TCP的关键NAT和防火墙穿透技术，并支持代理。\n\n**缺点：**\n\n- WebRTC中很多的参数都是由GIPS公司的工程师们依靠经验所设定的值，这就会出现卡顿、延时、回声、丢包、多人视频不稳定等问题，并且由于公网的稳定性或机型适配等外在因素，以上问题在项目上线后会更加严重。\n\n- WebRTC缺乏服务器方案的设计和部署。\n\n- 传输质量难以保证。WebRTC的传输设计基于P2P，难以保障传输质量，优化手段也有限，只能做一些端到端的优化，难以应对复杂的互联网环境。比如对跨地区、跨运营商、低带宽、高丢包等场景下的传输质量基本是靠天吃饭，而这恰恰是国内互联网应用的典型场景。\n\n- WebRTC比较适合一对一的单聊，虽然功能上可以扩展实现群聊，但是没有针对群聊，特别是超大群聊进行任何优化。\n\n- 设备端适配，如回声、录音失败等问题层出不穷。这一点在安卓设备上尤为突出。由于安卓设备厂商众多，每个厂商都会在标准的安卓框架上进行定制化，导致很多可用性问题（访问麦克风失败）和质量问题（如回声、啸叫）。\n\n- 对Native开发支持不够。WebRTC顾名思义，主要面向Web应用，虽然也可以用于Native开发，但是由于涉及到的领域知识（音视频采集、处理、编解码、实时传输等）较多，整个框架设计比较复杂，API粒度也比较细，导致连工程项目的编译都不是一件容易的事。\n\n总而言之，WebRTC虽然提供了一套音视频实时通讯的解决方案，但是在实际应用中，由于网络传输、设备适配以及多方通话上都存在很多问题，效果并不理想。\n\n由此可见，WebRTC是一个优缺点兼有的技术，在拥有诱人的优点的同时，其缺点也十分的严重。在进行WebRTC的开发之前，请根据自身的情况来决定是自主开发还是使用第三方SDK。目前在市场上有很多第三方的音视频SDK可供选择，比如声网、腾讯、Intel、天翼RTC、网易云信、环信、融云、anychat等等，虽然这么多厂商提供的服务都大同小异，但他们的技术架构可能完全不同，比如天翼RTC是WebRTC SDK，腾讯是Native SDK。\n\n由于WebRTC的复杂性和尚未完善性，下面的这些建议结合自己的实际参考：\n\n> 1. 音视频不是公司的核心方向，建议使用第三方SDK。\n> 2. 项目时间紧，有多人视频场景，使用场景依赖于手机端，建议使用第三方SDK。\n> 3. 公司没人音视频技术人才，建议使用第三方SDK或者技术外包。\n> 4. 如果公司实力、财力、人力雄厚，时间也不紧急，可考虑WebRTC集成开发，虽然会有很多坑，但总是能填平的。\n> 5. 如果音视频技术是公司的核心方向，但不想花太多时间去研究WebRTC，可直接找熟悉WebRTC的人来培训。\n> 6. 项目时间不紧急、没有多人视频需求且音视频质量要求不高，可考虑WebRTC集成开发。\n\n## 直播领域所用协议的现状及其优缺点比较\n\n直播领域常用到的推送协议主要有：\n\n- HTTP-FLV，即将音视频数据封装成FLV，然后通过HTTP协议传输给客户端。这种直播传输实际上就是利用的flv文件的特点，只需要一个matedata和音视频各自header，后面的音视频数据就可以随意按照时间戳传输，当然视频得按照gop段来传输，这种直播数据实际上就是一个无限大的HTTP传输的flv文件，客户端利用flv特性，可以一边接受数据边解码播放。\n\n- RTMP 是 Real Time Messaging Protocol（实时消息传输协议）的首字母缩写，由Adobe公司为Flash播放器和服务器之间音频、视频传输开发的开放协议。该协议基于 TCP，是一个协议族，包括 RTMP 基本协议及 RTMPT/RTMPS/RTMPE 等多种变种。RTMP 是一种设计用来进行实时数据通信的网络协议，主要用来在 Flash/AIR 平台和支持 RTMP 协议的流媒体/交互服务器之间进行音视频和数据通信。支持该协议的软件包括 Adobe Media Server/Ultrant Media Server/red5 等。RTMP其实实质上也是传输的flv格式的数据，同样是flv tag，只不过RTMP在传输上封装了一层，比如RTMP不仅可以直播，也可以推流。RTMP的直播原理同样也是利用了flv文件的特性，只需要一些头信息，后面就可以随意传输音视频数据，达到边传输边播放。**RTMP 是目前主流的流媒体传输协议，广泛用于直播领域，可以说市面上绝大多数的直播产品都采用了这个协议**。\n\n- HTTP Live Streaming（缩写是HLS）是一个由苹果公司提出的基于HTTP的流媒体网络传输协议。是苹果公司QuickTime X和iPhone软件系统的一部分。它的工作原理是把整个流分成一个个小的基于HTTP的文件来下载，每次只下载一些。当媒体流正在播放时，客户端可以选择从许多不同的备用源中以不同的速率下载同样的资源，允许流媒体会话适应不同的数据速率。在开始一个流媒体会话时，客户端会下载一个包含元数据的extended M3U (m3u8)playlist文件，用于寻找可用的媒体流。HLS只请求基本的HTTP报文，与实时传输协议（RTP)不同，HLS可以穿过任何允许HTTP数据通过的防火墙或者代理服务器。它也很容易使用内容分发网络来传输媒体流。苹果公司把HLS协议作为一个互联网草案（逐步提交），在第一阶段中已作为一个非正式的标准提交到IETF。但是，即使苹果偶尔地提交一些小的更新，IETF却没有关于制定此标准的有关进一步的动作。HLS在大部分的浏览器利用html5video是可以直接播放的。\n\n- WebRTC，名称源自网页即时通信（英语：Web Real-Time Communication）的缩写，是一个支持网页浏览器进行实时语音对话或视频对话的 API。它于 2011 年 6 月 1 日开源并在 Google、Mozilla、Opera 支持下被纳入万维网联盟的 W3C 推荐标准。WebRTC默认使用UDP协议(实际上使用的是RTP/RTCP协议)进行音视频数据的传输，但是也可以通过TCP传输。 目前主要应用于视频会议和连麦中。\n\n这几个协议的优缺点比较如下：\n\n| 协议         | 优点                                                         | 缺点                                                         |\n| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| **HTTP FLV** | 实时性和RTMP相等； 相比于RTMP省去了一些协议交互时间，首屏时间更短，可拓展的功能更多； 将RTMP封装在HTTP协议之上的，可以更好的穿透防火墙等 | 不支持双向互动；目前在网页上只能用flash或者插件的方式解码播放，而且flash在cpu和内存上都是占用很高。 |\n| **RTMP**     | CDN 支持良好，主流的 CDN 厂商都支持；协议简单，在各平台上实现容易，PC flash原生支持；支持双向互动；实时性很好；防HTTP下载。 | 基于TCP，传输成本高，在弱网环境丢包率高的情况下问题显著；不支持浏览器推送；Adobe 私有协议，Adobe已经不再更新； 需要访问1935端口，国内网络情况的恶劣程度，并不是每个网络防火墙都允许1935包通过；目前在网页上只能用flash或者插件的方式解码播放，而且flash在cpu和内存上都是占用很高。 |\n| **HLS**      | 跨平台，支持度高，H5浏览器支持比较好，可以直接打开播放；IOS、安卓原生支持；技术实现简单。 | 延迟性比较大。                                               |\n| **WebRTC**   | W3C 标准，主流浏览器支持程度高；Google 在背后支撑，并在各平台有参考实现；底层基于 SRTP 和 UDP，弱网情况优化空间大；可以实现点对点通信，通信双方延时低。 | 传统CDN没有ICE、STUN、TURN及类似的服务提供                   |\n\n## WebRTC 的核心组件\n\n- 音视频引擎：OPUS、VP8 / VP9、H264\n- 传输层协议：底层传输协议为 UDP\n- 媒体协议：SRTP / SRTCP\n- 数据协议：DTLS / SCTP\n- P2P 内网穿透：STUN / TURN / ICE / Trickle ICE\n- 信令与 SDP 协商：HTTP / WebSocket / SIP、 Offer Answer 模型\n\n## WebRTC架构图\n\n![webrtc架构图](/images/imageWebRTC/others/webrtc架构图.png)\n\n**WebRTC architecture (from webrtc.org)**\n\n架构图颜色标识说明：\n\n（1）紫色部分是Web开发者API层；\n\n（2）蓝色实线部分是面向浏览器厂商的API层（也就是红色框标内模块，也是本人专注研究的部分）;\n\n（3）蓝色虚线部分浏览器厂商可以自定义实现。\n\n将WebRTC架构图分解为内部结构简化图和协议栈后如下：\n\n![图1：webrtc内部结构](/images/imageWebRTC/others/webrtc内部结构.png)\n\n![图2：webrtc协议栈](/images/imageWebRTC/others/webrtc协议栈.png)\n\n图1 为 WebRTC 内部结构简化图，最底层是硬件设备，上面是音频捕获模块和视频捕获模块。中间部分为音视频引擎。音频引擎负责音频采集和传输，具有降噪、回声消除等功能。视频引擎负责网络抖动优化，互联网传输编解码优化。在音视频引擎之上是 一套 C++ API，在 C++ 的 API 之上是提供给浏览器的Javascript API。\n\n图2 是 WebRTC 涉及到的协议栈，WebRTC 核心的协议都是在右侧基于 UDP 基础上搭建起来的。其中：\n\n- ICE、STUN、TURN 用于内网穿透, 解决了获取与绑定外网映射地址，以及 keep alive 机制。DTLS 用于对传输内容进行加密，可以看做是 UDP 版的 TLS。由于 WebRTC 对安全比较重视，这一层是必须的。\n- SRTP 与 SRTCP 是对媒体数据的封装与传输控制协议。\n- SCTP 是流控制传输协议，提供类似 TCP 的特性，SCTP 可以基于 UDP 上构建，在 WebRTC 里是在 DTLS 协议之上。\n- RTCPeerConnection 用来建立和维护端到端连接，并提供高效的音视频流传输。\n- RTCDataChannel 用来支持端到端的任意二进制数据传输。\n\n## WebRTC架构组件介绍\n\n- **1）Your Web App** \t\n\n  Web开发者开发的程序，Web开发者可以基于集成WebRTC的浏览器提供的web API开发基于视频、音频的实时通信应用。\n\n- **2）Web API** \t\n\n  面向第三方开发者的WebRTC标准API（Javascript），使开发者能够容易地开发出类似于网络视频聊天的web应用，最新的标准化进程可以查看[**这里**](http://dev.w3.org/2011/webrtc/editor/webrtc.html)。 \n\n- **3）WebRTC Native C++ API** \t\n\n  本地C++ API层，使浏览器厂商容易实现WebRTC标准的Web API，抽象地对数字信号过程进行处理。\n\n- **4）Transport / Session**\n\n  传输/会话层，会话层组件采用了 libjingle 库的部分组件实现，无须使用 xmpp/jingle 协议\n\n  - **a. RTP Stack协议栈** \t\n\n    Real Time Protocol \t\n\n  - **b. STUN/ICE** \t\n\n    可以通过 STUN 和 ICE 组件来建立不同类型网络间的呼叫连接。 \t\n\n  - **c. Session Management** \t\n\n    一个抽象的会话层，提供会话建立和管理功能。该层协议留给应用开发者自定义实现。\n\n- **5）VoiceEngine** \t\n\n  音频引擎是包含一系列音频多媒体处理的框架，包括从视频采集卡到网络传输端等整个解决方案。 \n\n  PS：VoiceEngine是WebRTC极具价值的技术之一，是Google收购GIPS公司后开源的。在VoIP上，技术业界领先，后面的文章会详细了解\n\n  - **a. iSAC**\n\n    Internet Speech Audio Codec （网络音频编解码）\n\n    针对VoIP和音频流的宽带和超宽带音频编解码器，是WebRTC音频引擎的默认的编解码器 \t\n\n    采样频率：\n\n    - `16khz`\n    - `24kh`\n    - `32khz`（默认为`16khz`） \t\n\n    自适应速率为：\n\n    - `10kbit/s ~ 52kbit/s`\n\n    自适应包大小：\n\n    - `30~60ms` \t\n\n    算法延时：\n\n    - `frame + 3ms`\n\n  - **b. iLBC** \n\n    Internet Low Bitrate Codec \t\n\n    VoIP音频流的窄带语音编解码器 \t\n\n    采样频率：\n\n    - `8khz` \t\n\n    20ms 帧比特率为 `15.2kbps` \n\n    30ms 帧比特率为 `13.33kbps`\n\n    标准由 IETF RFC3951 和 RFC3952 定义\n\n  - **c. NetEQ for Voice**\n\n    针对音频软件实现的语音信号处理元件\n\n    NetEQ算法：**自适应抖动控制算法以及语音包丢失隐藏算法**。使其能够快速且高解析度地适应不断变化的网络环境，确保音质优美且缓冲延迟最小。是GIPS公司独步天下的技术，能够有效的处理由于网络抖动和语音包丢失时候对语音质量产生的影响。\n\n    PS：NetEQ 也是WebRTC中一个极具价值的技术，对于提高VoIP质量有明显效果，加以AEC\\NR\\AGC等模块集成使用，效果更好。\n\n  - **d. Acoustic Echo Canceler (AEC)** \t\n\n    回声消除器是一个基于软件的信号处理元件，能实时的去除mic采集到的回声。\n\n  - **e. Noise Reduction (NR)** \t\n\n    噪声抑制也是一个基于软件的信号处理元件，用于消除与相关VoIP的某些类型的背景噪声（嘶嘶声，风扇噪音等等… …）\n\n- **6）VideoEngine** \t\n\n  WebRTC视频处理引擎 \t\n\n  VideoEngine是包含一系列视频处理的整体框架，从摄像头采集视频到视频信息网络传输再到视频显示整个完整过程的解决方案。\n\n  - **a. VP8** \t\n\n    视频图像编解码器，是WebRTC视频引擎的默认的编解码器 \tVP8 适合实时通信应用场景，因为它主要是针对低延时而设计的编解码器。 \t\n\n    PS:VPx编解码器是Google收购ON2公司后开源的，VPx现在是WebM项目的一部分，而WebM项目是Google致力于推动的HTML5标准之一\n\n  - **b. Video Jitter Buffer** \t\n\n    视频抖动缓冲器，可以降低由于视频抖动和视频信息包丢失带来的不良影。\n\n  - **c. Image enhancements** \t\n\n    图像质量增强模块，对网络摄像头采集到的图像进行处理，包括明暗度检测、颜色增强、降噪处理等功能，用来提升视频质量。\n\n## WebRTC核心模块API\n\n### 网络传输模块：libjingle\n\nWebRTC重用了libjingle的一些组件，主要是network和transport组件，关于libjingle的文档资料可以查看[**这里**](http://code.google.com/apis/talk/libjingle/developer_guide.html)。\n\n### 音频、视频图像处理的主要数据结构\n\n**常量\\VideoEngine\\VoiceEngine**\n\n*注意：以下所有的方法、类、结构体、枚举常量等都在webrtc命名空间里*\n\n| **类、结构体、枚举常量** | **头文件**     | **说明**                                                     |\n| ------------------------ | -------------- | ------------------------------------------------------------ |\n| **Structures**           | common_types.h | Lists the structures common to the VoiceEngine & VideoEngine |\n| **Enumerators**          | common_types.h | List the enumerators common to the VoiceEngine & VideoEngine |\n| **Classes**              | common_types.h | List the classes common to VoiceEngine & VideoEngine         |\n| class **VoiceEngine**    | voe_base.h     | How to allocate and release resources for the VoiceEngine using factory methods in the VoiceEngine class. It also lists the APIs which are required to enable file tracing and/or traces as callback messages |\n| class **VideoEngine**    | vie_base.h     | How to allocate and release resources for the VideoEngine using factory methods in the VideoEngine class. It also lists the APIs which are required to enable file tracing and/or traces as callback messages |\n\n### 音频引擎（VoiceEngine）模块 APIs\n\n*下表列的是目前在 VoiceEngine中可用的sub APIs*\n\n| **sub-API**            | **头文件**             | **说明**                                                     |\n| ---------------------- | ---------------------- | ------------------------------------------------------------ |\n| **VoEAudioProcessing** | voe_audio_processing.h | Adds support for Noise Suppression (NS), Automatic Gain Control (AGC) and Echo Control (EC). Receiving side VAD is also included. |\n| **VoEBase**            | voe_base.h             | Enables full duplex VoIP using G.711.**NOTE:** This API must always be created. |\n| **VoECallReport**      | voe_call_report.h      | Adds support for call reports which contains number of dead-or-alive detections, RTT measurements, and Echo metrics. |\n| **VoECodec**           | voe_codec.h            | Adds non-default codecs (e.g. iLBC, iSAC, G.722 etc.), Voice Activity Detection (VAD) support. |\n| **VoEDTMF**            | voe_dtmf.h             | Adds telephone event transmission, DTMF tone generation and telephone event detection. (Telephone events include DTMF.) |\n| **VoEEncryption**      | voe_encryption.h       | Adds external encryption/decryption support.                 |\n| **VoEErrors**          | voe_errors.h           | Error Codes for the VoiceEngine                              |\n| **VoEExternalMedia**   | voe_external_media.h   | Adds support for external media processing and enables utilization of an external audio resource. |\n| **VoEFile**            | voe_file.h             | Adds file playback, file recording and file conversion functions. |\n| **VoEHardware**        | voe_hardware.h         | Adds sound device handling, CPU load monitoring and device information functions. |\n| **VoENetEqStats**      | voe_neteq_stats.h      | Adds buffer statistics functions.                            |\n| **VoENetwork**         | voe_network.h          | Adds external transport, port and address filtering, Windows QoS support and packet timeout notifications. |\n| **VoERTP_RTCP**        | voe_rtp_rtcp.h         | Adds support for RTCP sender reports, SSRC handling, RTP/RTCP statistics, Forward Error Correction (FEC), RTCP APP, RTP capturing and RTP keepalive. |\n| **VoEVideoSync**       | voe_video_sync.h       | Adds RTP header modification support, playout-delay tuning and monitoring. |\n| **VoEVolumeControl**   | voe_volume_control.h   | Adds speaker volume controls, microphone volume controls, mute support, and additional stereo scaling methods. |\n\n### 视频引擎（VideoEngine）模块 APIs\n\n*下表列的是目前在 VideoEngine中可用的sub APIs*\n\n| **sub-API**          | **头文件**           | **说明**                                                     |\n| -------------------- | -------------------- | ------------------------------------------------------------ |\n| **ViEBase**          | vie_base.h           | Basic functionality for creating a VideoEngine instance, channels and VoiceEngine interaction.**NOTE:** This API must always be created. |\n| **ViECapture**       | vie_capture.h        | Adds support for capture device allocation as well as capture device capabilities. |\n| **ViECodec**         | vie_codec.h          | Adds non-default codecs, codec settings and packet loss functionality. |\n| **ViEEncryption**    | vie_encryption.h     | Adds external encryption/decryption support.                 |\n| **ViEErrors**        | vie_errors.h         | Error codes for the VideoEngine                              |\n| **ViEExternalCodec** | vie_external_codec.h | Adds support for using external codecs.                      |\n| **ViEFile**          | vie_file.h           | Adds support for file recording, file playout, background images and snapshot. |\n| **ViEImageProcess**  | vie_image_process.h  | Adds effect filters, deflickering, denoising and color enhancement. |\n| **ViENetwork**       | vie_network.h        | Adds send and receive functionality, external transport, port and address filtering, Windows QoS support, packet timeout notification and changes to network settings. |\n| **ViERender**        | vie_render.h         | Adds rendering functionality.                                |\n| **ViERTP_RTCP**      | vie_rtp_rtcp.h       | Adds support for RTCP reports, SSRS handling RTP/RTCP statistics, NACK/FEC, keep-alive functionality and key frame request methods. |\n\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"三大系统，让学习不靠意志力","url":"/2019-06-16/三大系统，让学习不靠意志力/","content":"\n\n**本节学习收获：**学会掌控自己的三类方法，让你不靠意志力，也能坚持学习\n\n我们经常会听到诸如此类：如果坚持每天背20个单词，一年词汇量就接近8000了，基本是托福的词汇量；如果坚持每天写2000字，两个月就12万字了，就可以出一本书。\n\n我们都知道这个积少成多、长期坚持的道理，但是，有多少人真正坚持得了呢？\n\n上节课我们提到，能力提升比知识学习更难，当然，因为难所以才有价值，如果非常简单，就没什么竞争力了。但是，这么难的事情，要怎么才能坚持呢？\n\n靠意志力吗？我们都是这么想的，所以经常会说“xx很自律”“xx意志力惊人”。\n\n但实际上，**因为人的意志力是有限的，任何让自己坚持的事情，都会消耗它，从而让你感觉非常累。比如你会发现，如果我们一段时间工作压力很大，就会更容易放弃健身等等良好习惯，因为我们的意志力在坚持工作这方面已经耗费光了，所以就没有多余的意志力用在坚持健身上了。**\n\n ![幻灯片19.JPG](https://upload-images.jianshu.io/upload_images/18390058-fbfebac900a4ade1.JPG)\n\n**学习也是一样，当你坚持学习的时候，不是“学习”这件事情累着你，而是“坚持”这件事情累着你了，因为意志力会耗费大量能量。**\n\n ![幻灯片20.JPG](https://upload-images.jianshu.io/upload_images/18390058-78a806aa1c063865.JPG)\n\n而且在**移动互联网时代，我们会发现，自己的意志力好像越来越薄弱。**以前高中时期，能坚持学习2个小时，但现在看1小时书，就觉得如坐针毡。很简单，因为我们面对的诱惑更多了，游戏、短视频、微博等等这些，都比学习的诱惑力更大，只要我们脑子里有这些东西，就需要不断耗费我们的意志力来抵制他们，才能最终坚持学习，所以我们才会很累。\n\n**威尔海姆·霍夫曼（Wilhelm Hofmann）主持的一项研究表明，人们醒着的时候，把大约1/4的时间用来抵制欲望。**而这个研究是在BP机时代做的，近二十年间，移动互联网兴起带来的诱惑，比如游戏、短视频、段子等等，比过去上千年的诱惑加起来还要多，所以**我们每天需要抵制更多欲望。这也是为什么，我们常常觉得，没做什么事情，也会感到很心累的原因。**\n\n**![幻灯片21.JPG](https://upload-images.jianshu.io/upload_images/18390058-8dd5a95aa61c6845.JPG)** \n\n好了，诱惑那么多，意志力又不够用，那怎么办呢？今天我们就从行为改变的方法论角度，来讲讲，如何不纯靠意志力来学习。这个方法，不只是适用于学习，还适用于其他任何有点“反人性”的事情，比如健身、跑步等等。\n\n心理学家早就对我们的大脑做过研究，学界普遍认为，**人类大脑内部有两个系统：理性和感性。**\n\n**感性跟情感有关，是天性和本能。而理性则跟理智有关，让我们能够深思熟虑。我们决定做任何事情的时候，都是这两部分在起作用。**\n\n比如说，你在刷抖音，感性会告诉你“视频很有趣，继续刷吧，你会很快乐”，而理性却告诉你“别刷了，要去学习，否则将来没有前途”。\n\n理性和感性是你的内部因素，而做一件事情还有外部因素的影响，这就是情境。关于情境我打个比方，如果你周围都是胖子，那么你变胖的概率会大大增加。如果你周围人都爱学习，那么你也会变得更爱学习。\n\n所以，总结来说，**如果要让我们能够坚持做一件事情，比如学习，并不是只能靠意志力的，其实有三个关键因素：理性上知道要学习、情感上愿意去学习、情境上制造适合学习的场景**。接下来，我们从这三个方面去展开，看看有哪些方法，能让我们坚持做成一些事情，而不是白天堕落，晚上后悔然后发誓，第二天继续这个堕落、后悔和发誓的循环。\n\n![幻灯片22.JPG](https://upload-images.jianshu.io/upload_images/18390058-af56b29890852fb7.JPG)\n\n**1\\. 发挥理性的作用：树立明确目标**\n\n假设我让你追一只猫，如果我跟你说：“目标在东北方向！”你能快速找到吗？很难。\n\n但如果跟你说：“目标在东北方向一公里处的圈外办公室！”那你更有可能会找到。\n\n所以，我们的目标需要足够明确。\n\n比如，你跟自己下了无数次决心：要多看书，不能再不学无术了。然后呢？你继续刷剧去了。但如果你跟自己说：每天晚上睡前看10分钟书。那么，你更有可能会做到。\n\n所以，当你有明确目标的时候，同样的事情会有完全不一样的效果，你会更容易坚持。\n\n这里给个数据，在圈外商学院的课程内，会有一个模块是专门用来写学习目标的。我们曾经做过一个数据分析，分析那些写目标的和不写目标的人，最后课程完成率有没有差异。最后的结果是，写目标的人的完成率是没有写的人的1.5倍。\n\n**所以，给自己一个明确的目标，是你坚持学习的第一步。**\n\n**2\\. 发挥感性的作用：利用情绪冲动**\n\n比如，如果我讲很多大道理，从理性的角度去说服你，应该给希望工程捐钱以支持失学儿童，你可能很难做出改变。但是，如果我给你一张失学儿童的照片，看到她渴望学校的眼神，你可能会被触动，然后做出了捐款行为。这就是情绪的力量。\n\n为什么化妆品、医美等等这样的产品，都在不断煽动女性对衰老的恐惧，因为恐惧是一种情绪，会让你有冲动付费。\n\n而**能够促进你行为的情绪不只是恐惧，其实有很多种，比如愧疚感、认同感、成就感，等等。**这里我给你建议几个利用情感的方法，用圈外过去所做的很多尝试为例——\n\n**方法1-截止日期的紧迫感：**\n\n豆瓣9.4分英剧《黑镜》的编剧查理·布洛克，收到过很多邮件，都是问他，怎么写出这么好的剧本。他回复大家说：**“不要谈什么天分、运气，你需要的是一个截稿日，以及一个不交稿就能打爆你狗头的人，然后你就会被自己的才华吓到。”**\n\n圈外商学院的课程，有个规定：在30天之内学完，你可以终身永久回看，而且可以看最新版本，因为我们课程会不断迭代；但如果你30天之内没有学完，课程就会关闭，哪怕你付了钱。很多人抱怨这个政策，但我们一直没改，因为我们看过数据，截止日前3天大家都在拼命学习和写作业，如果取消这个截止日期，谁会去马上学一个终身有效、没有截止日期的课程呢？\n\n**所以，deadline是第一生产力。**\n\n**方法2-比较产生的焦虑感：**\n\n举个我自己的例子，其实我在创业之前也纠结了很久，不知道自己能力是不是足够，总觉得没有准备好。当时我的好朋友，也是现在圈外的联合创始人杰西，就跟我说“你看你都这么大年纪了，现在创始人都很年轻，你再不创业就老了”，后来我就一咬牙出来了，也才有了现在的圈外同学。\n\n顺便说一句，我现在经常给很多人一个建议：**你需要一个在关键时刻踹你一脚的人，因为人都是有惰性的。**\n\n![幻灯片24.JPG](https://upload-images.jianshu.io/upload_images/18390058-1739474fcc7a5fdd.JPG)\n\n**方法3-鼓励带来的成就感：**\n\n在圈外，我们曾经做过一个对比，早期的产品是没有班主任服务的，也没有助教，那时候的完课率只有百分之十几。后来我们先尝试了给学员分班级、配备班主任，主要就是根据学员的学习情况鼓励他们，那时候还没有助教、学习游戏，但是完课率直接提升到了百分之五十多，基本就是社群和班主任鼓励的作用。\n\n当然，其他还有很多情感很有用，比如对未来的期待感，如果你是想从传统行业到互联网行业，想通过学习来实现，那么有一个已经成功转型、而且实现了2倍涨薪的人来告诉你，他是通过学习来实现的，然后给了你他的3个月学习计划，你是不是很有信心学习呢？\n\n**3\\. 发挥情境的作用：创造学习环境**\n\n好了，理性和情感说完，最后说说情境的作用。\n\n还记得我们前面举的例子吗？如果你周围的人都是胖子，那你减肥的成功率会低很多。\n\n这很容易理解，**人是社会动物，我们会观察周围人的行为来调整自己的行为。**\n\n最简单的，你去一个陌生的社交场合，你会多说话还是少说话、多吃还是少吃呢？大多数人的做法是，看周围人怎么做。\n\n学习也是一样的，很多人都发现，跟一群人一起讨论学习，会更容易坚持。\n\n原因很简单，爱学习爱思考的人并不多，现实生活中，大多数人都在看娱乐八卦，如果你在学习，会觉得自己很突兀。但是**到了学习社群就不一样了，大家都在学习，你也会模仿其他人**，这也是为什么，圈外的课程都有线上线下社群的原因，就是为了有这样一个氛围。\n\n情境里除了同伴因素以外，还有其他因素。举个例子，如果你比较容易被抖音干扰学习，那你把手机锁起来，制造一个专注的环境，这个问题就解决了。所以，**如果想让你不看电视，最好的方法是别买电视；如果想让你少刷朋友圈，最好的办法是把小红点功能关掉，甚至，把朋友圈关掉；如果想让你别再剁手，最好的方法是把淘宝卸了，或者把信用卡额度冻结了。**\n\n**如果可以用环境，就不要用你的意志力来抵制欲望。**\n\n**![幻灯片25.JPG](https://upload-images.jianshu.io/upload_images/18390058-fe65e2050918ec43.JPG)** \n\n好了，我们总结一下，学习这件事，是不能纯靠意志力的，因为意志力非常有限，非常耗费能量。所以，你得利用自己的理性和感性两大系统，结合情境，让你愿意学习：首先利用你的理性系统，设定非常清晰量化的目标；然后，利用自己的情绪冲动，包括焦虑感、成就感等等，去促进学习；最后，还要让自己处于一个适合学习的情境。\n\n2018年10月，在中国拥有近五千万职场用户的LinkedIn(领英)，做了一个调研，发布了一份**《职场人转折点报告》。报告中有个有趣的发现：每月收入低于3000元的职场人加班时间最长，而相较之下，每月收入大于4万元的职场人学习时间最长。**可以看出，能提升我们市场价值的，可能不是加班，而是学习。所以，希望你在掌握了这些方法之后，可以坚持学习提升。\n\n到这里，我们的课程已经完成了5小节，如果你走到了这里，恭喜你，因为大部分人可能会在前面几天放弃。\n\n之前的几小节，我们讲了如何选择适合的工作，选了工作之后如何投入时间以获得最大化价值，同时还给出了提升能力的方法，以及告诉你如何坚持学习提升。你有没有想过这些问题，**为什么我们现在都在说要终身学习呢？以及，为什么个人要自己花钱花时间学习呢？为什么企业越来越不愿意培养人了？不仅不愿意培养人，招来人干了几年之后，甚至会裁人**，导致我身边35岁的中年人每天都很焦虑，生怕遭遇中年失业。\n","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"三个建议，让你不做“定制化人才”","url":"/2019-06-16/三个建议，让你不做“定制化人才”/","content":"\n**本节学习收获：**学会3个“不变应万变”的方法，避免成为“企业定制化人才”。\n\n我的一位中学同学，在某个耳熟能详的大外企做工程师。工作10年，接受了很多公司培训，还经常有国外出差的机会，自己的表现也一直都属于中上游。3年前，他所在的业务开始缩减人员，最近一个关系好的同事也离开公司去创业了，他也觉得自己要做些改变，打算转行。\n\n但是，信心满满地投了几十份简历，几乎没什么回音。唯一拿到的一个offer，对方公司开出的薪酬只有现在的70%。\n\n照理说，我这位同学也算是大公司的优秀人才，怎么出了公司，竟然找不到合适的工作呢？\n\n《国富论》里亚当·斯密提到的一个扣针工厂的例子：**一个工人无论如何努力，一天也生产不了20枚扣针，但有了分工之后，经过前后十几道工序，每人每天平均可以生产48000枚扣针。**这就是专业化分工的高效性！\n\n任何一家公司，从老板的角度肯定是要提高效率、多赚钱，所以必然走向专业化分工，**把一个工作切成很多块，每个人都终日重复其中某一块，这样能提高效率、降低风险，同时也降低对人的依赖。**\n\n但是，这对人才来说是灾难性的。因为亚当·斯密也说过**“如果一个人的全部心思都用在一只扣针的十几分之一时，见识必然有限”。**\n\n**![幻灯片26.JPG](https://upload-images.jianshu.io/upload_images/18390058-2ca176e7eeab8d3f.JPG)** \n\n精细化分工的结果是，你成为了一个企业定制化人才，极端一点说，就像一颗螺丝钉，尺寸和材质只能用在一个产品上，挪到别处去，就成了废铁，被这个产品牢牢困住。\n\n这是我那位同学之所以跳槽失败的原因。\n\n**那么，我们要如何避免被定制化呢？**\n\n从我过去帮助7个行业的近百家头部企业设计人才管理体系的经验来说，我给你三个建议**。第一，调整主体，给自己定好发展方向；第二，提升能力，让自己成为横向可迁移的人才；第三，提升认知高度，让自己成为纵向可拓展的人才。**\n\n**1、调整主体，给自己定好发展方向**\n\n我之前公司做的一个调研数据很有意思：**10年前，一个人在一家公司所待的时间是3-4年，而现在，一个人在一家公司平均只会待1年多****。**\n\n**这是人才侧的数据，而从企业来说，我10多年前刚做咨询的时候，企业做战略规划都是5年甚至10年，而现在，能够拿得出清晰3年战略的企业，已经不多了。而中国企业的平均寿命不到3年。**\n\n在这种情况下，**企业没有任何动力培养人才**，一方面人才只在公司平均待1年多，培养好之后还来不及给公司提供足够多的贡献，对企业来说根本不划算；另一方面，外部环境竞争激烈，培养人才所需要的时间，企业根本耗不起。所以，企业越来越倾向于用现成的人。而当所有企业都想用现成人才的时候，其实就是企业将过去所承担的人才培养成本，逐渐转移到了个人身上。\n\n**在这种趋势下，个人不可能把安全感寄托于企业。**\n\n**![幻灯片27.JPG](https://upload-images.jianshu.io/upload_images/18390058-0290e0f290811c84.JPG)** \n\n所以，我们现在倡导所谓“终身学习”，不是因为人们这两年突然爱学习了，是因为学习变成了生存下去所必须做的事情。企业定制化人才越来越没有竞争力，只有主动学习、提升能力才能适应时代。\n\n怎么转移主体呢？听起来好像很虚，如何落地？很简单，**当你进入一家公司的时候，就要想好：如果我离开这里，还能干嘛？**就像我前面课程所说的，你去领英等等招聘网站上，搜索一下这个职位出去的人都在做什么，就知道将来是否有出路了。\n\n![幻灯片28.JPG](https://upload-images.jianshu.io/upload_images/18390058-967b335ba4dee414.JPG)\n\n**2、提升能力，让自己成为横向可迁移的人才**\n\n亚马逊创始人贝佐斯曾说：“人们常问我，未来十年，什么会改变；人们几乎从来不曾问我，未来十年，什么不会变。”\n\n他觉得，这个世界不管怎么变，都很难想象，人们有一天会希望物品价格越高越好，物流速度越慢越好。所以，这两个因素就是不变的因素，做好这些，比每天追逐变化要有效得多。\n\n**作为人才也是一样，要想不被企业定制化，让自己成为万能零件，不受环境影响，也是要找到那些不变的因素。**\n\n**那对我们来说，什么是那个不变的因素呢？就是我们之前课程提到的，可迁移能力。**\n\n当然，**不同发展阶段的能力要求有不同**。在很多公司内部，针对不同阶段的员工，都会有不同的能力模型，晋升以及招聘的时候就是用这一套标准来筛人的，而且各家公司其实有很多共性，这就是可迁移能力。\n\n**如果是职场新人**，要成为资深员工或者主管，需要具备：独立思考的能力、多任务高效工作的能力、清晰表达的能力、基础的人际拓展能力。\n\n**如果到了要带一个小团队、或者在公司内部管理一些项目的阶段**，则需要具备：解决复杂问题和创新的思维能力、辅导和激励下属让团队提升效率的能力、在公开场合演讲表达能力、处理复杂人际冲突的沟通能力。\n\n**而如果是成为管理者，开始带整个部门的时候，需要具备的能力包括**：在思维方面，掌握战略思维以及理性的经济学思维；在效率方面，需要掌握一些运营和财务的知识，更好地配置资源；同时，需要掌握营销思维，贯穿在工作全流程中；最后，在人际方面，需要全面的领导力。\n\n\n**3、提升认知高度，让自己成为纵向可拓展的人才**\n\n上面讲的是提升可迁移能力，这样不局限在某个垂直领域。**而除了横向可迁移之外，还得纵向可迁移**，也就是说，提升自己的认知高度。就像亚当·斯密所说的，如果你全部精力都在某个产品的某个工序，见识一定是有限的，所以你得跳出这道工序，了解整个产品。\n\n**你需要去思考：**\n\n*   **这个行业和公司的关键成功要素是什么？**\n*   **对用户对社会的价值是什么？**\n*   **你所在的部门和岗位，对公司的核心价值是什么？**\n\n从更加宏观的角度去看问题，至少成为一个比螺丝钉更大的零件。\n\n而且在职场，职位越高，对专业的要求会降低，而对战略全局观、商业敏感度和领导力的要求是越来越高的。\n\n那如何去提高这些能力呢？ \n\n大多数人的第一选择是去读个MBA，领英在2017年的报告显示：高达79%的职场白领都想过读MBA。但实际上，去读的只占中国职场人比例的不到1%。几十万的学费、2年的时间，都是极大的机会成本。\n\n\n**好了，总结一下这节课程，要想不成为“定制化人才”，我们需要：第一，调整主体，给自己定好发展方向；第二，提升能力，让自己成为横向可迁移的人才；第三，提升认知高度，让自己成为纵向可拓展的人才。**\n\n**到这里呢，你已经完成了这个课程的学习。**\n\n**我们稍微总结一下：**首先我们学习了冰山模型，知道了自己为什么会不喜欢一份工作，以及怎么找到跟自己匹配的工作；接着，继续从冰山模型出发，分析应该把时间花在什么要素上面，才能有市场价值的提升，我们得出了提升能力的结论；然后我们通过两小节讲了能力如何提升，首先是找到自己的天赋，也就是隐藏的能力，其次是掌握3个方法、将学到的知识内化成能力；而要做到这些，离不开对学习的坚持，坚持学习很难，而意志力又很有限，所以要掌握理性、感性和情境三类方法。最后，关于如何应对未来的加速变化，答案是不要成为企业定制化人才。\n\n通过这6小节课程，我想要告诉你的是：无论你对未来迷茫也好、遇到发展瓶颈也好，其实都是有解决方案的。**社会发展到今天，你所遇到的几乎每个问题，这个世界上都有人曾经成功解决过它，我们要做的，就是去学习****，没有太多的问题需要我们重复造轮子。**克林顿曾提到，世界上几乎每个问题都已经在某个地方被某个人解决，我们的挑战是找到那些有效的解决方案。\n\n![幻灯片29.JPG](https://upload-images.jianshu.io/upload_images/18390058-d888ba9ee1481290.JPG)\n\n\n**马斯洛说：“教育是让一个人成为最好版本的自己”**，给自己一个机会，用几个月的时间，真正用正确的方法，来训练一下自己，看看自己最好的状态到底是什么样的！\n\n![幻灯片30.JPG](https://upload-images.jianshu.io/upload_images/18390058-6a4b0bfb8f38f2fa.JPG)\n","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"三种方法，将知识内化成能力","url":"/2019-06-14/三种方法，将知识内化成能力/","content":"\n\n **本节学习收获：**掌握将所学知识内化成能力的3个方法，盘活你脑中的知识\n\n在课程正式开始之前，我先提个问题，我们第一节课介绍了冰山模型，在这几天里，你有没有用它来解决过问题呢？\n\n如果没有的话，那么冰山模型对你来说，就是没有用的**“惰性知识”，它只是待在你的脑海里，在一些明明可以发挥作用的场合，却不能及时被调取出来，白白占据了你大脑的内存**。\n\n![幻灯片13.JPG](https://upload-images.jianshu.io/upload_images/18390058-502c53284b21f84c.JPG)\n\n陈铭在《奇葩说》中举过一个例子，很好地解释了知识和能力的关系。他说：\n\n水在零度的时候会结冰，这是一个知识，是对外部客观规律的归纳和总结。在未来的时间中，我在什么时候、把什么味道的水、变成什么味道的冰棒、卖给谁，叫做**智慧和能力，它是指对知识的处理和运用**。  \n\n我在咨询公司的时候，很多客户都在自己所在的行业里摸爬滚打了几十年，行业知识和经验肯定比工作几年的咨询顾问丰富，但为什么我们能帮他解决问题、收取那么高的咨询费呢？这也是因为，咨询顾问掌握的是一套思维方式和思维能力，能够更好地用知识来解决问题，发挥每个知识的最大效用。\n\n所以，如果你只是知道冰山模型这个概念，但在实际的工作和生活中用不上它，没能让它成为自己思维的一部分，那就是没用的。那么，**究竟该怎么做，才能让知识内化成能力，让自己的能力快速提升呢？今天给你三个方法：1.掌握20%的核心；2.知识和问题互相靠；3.做系统化训练。**\n\n**1.掌握20%的核心**\n\n做咨询的时候，需要快速地在1周之内了解一个行业。很多人会觉得：咨询顾问很聪明，学习能力超强。\n\n但在我看来，只是因为他们能够拿到优质信息资源、所以掌握了那个领域的核心而已，倘若把这个核心教给任何一个智商尚可的人，他们也可以做到。\n\n不信的话，回想一下我们大学的考试，虽然**一学期的课很复杂，但大多数人也是用考前一周的时间去突击的**，只要抓住了核心，最后的考试成绩其实不会差。\n\n我们常说的“二八原则”，其实这里也适用：**一个领域20%的核心内容，能够解决这个领域80%的问题。**\n\n**![幻灯片14.JPG](https://upload-images.jianshu.io/upload_images/18390058-33a3c6fb3f508c4b.JPG)** \n\n所以，掌握这20%的核心，就是我们快速提升的关键。无论是知识也好，能力也罢，想要快速提升，都是如此。**那什么是20%的核心呢？**\n\n分享一个故事。我在刚做咨询顾问的时候，有一次被项目经理狠批：你PPT为什么做得那么难看？有没有审美？\n\n当时非常沮丧，因为熬了无数个夜晚做出来的方案，最后却得到这样的评价。所以我下决心去美化一下PPT。\n\n先说结果：项目经理周五说的这话，我在后面周一的时候，PPT就全改了，彻底把她惊到了。后来这个事儿在公司广为流传，大概是：一个学习能力超强而且超级拼命的顾问，用2天时间提升了100页PPT的逼格。\n\n怎么做到的？其实很简单，我当时就是**拿了一些高手的PPT，跟我自己的做对比，然后再问这些高手请教经验，最后找出了PPT美观的三个关键点**：1）饱和度；2）边框；3）行间距。然后我把自己的PPT做了三个改动：1）把原先填充颜色改成半透明；2）把表格的边框改细；3）把字的行间距拉到1.5倍。这样改好，结合配色一点调整，逼格就高很多。\n\n**这三点，其实就是让咨询PPT变得更加美观的那20%核心。**\n\n当然，比起真正的PPT高手来说，我差得远，但我并不靠做PPT谋生，这20%已经够用了。\n\n上面说的是技能，其实能力也是一样的。比如，我们经常说一个人思维混乱，然后觉得思维混乱是个很大的事情，感觉一辈子都提升不了。但实际上，**大部分的思维混乱，都是因为缺乏结构化思维，而结构化思维只要掌握三个特征就可以，包括主题鲜明、归类分组、逻辑递进，不管写作、演讲还是会议发言，做到这三点就可以显得思路很清晰。**按照一定的方法论来训练，根据天赋不同，2周到2个月不等，就能有明显改变。这不是我瞎说，我写过文章、开过课程，按这个方式训练的没有一万也有八千。之所以取得这样的效果，是因为结构化思维属于思维的20%核心，而我说的3个特征，又是结构化思维的20%核心。\n\n但是，**每个领域里面，哪些是那20%的核心，其实是需要这个领域的专家来指点的**，因为在你对这个领域一无所知的情况下，你没有方向，绕一大圈也不知道门在哪里，等终于入门了才发现走了不少弯路。**大多数的成功人士，都很擅长找“师傅”**，他们都在自己的传记当中提到，受谁谁谁的启发和提携。而我理解的**好师傅所起到的作用，除了提供资源之外，另一个重大作用就在于，他们会告诉你，一个领域里面最核心的是什么，从而让你快速入门**。而我所认为的好书、好课程，除了需要大而全的教科书之外，也应该是这样的定位，不是把所有理论都照搬过来，而应该帮助读者和学员找到那20%，至于入门之后他是否需要或者愿意深度提升，那是他自己的选择。\n\n关于20%，圈外的公众号里面会有一些文章，涵盖了一些常见能力领域，这些文章都是提取了该领域里20%左右的核心，可以在班主任那里获取文章链接。\n\n![幻灯片15.JPG](https://upload-images.jianshu.io/upload_images/18390058-e09551ce1c4bcfe1.JPG)\n\n**2.知识和问题互相靠**\n\n我们经常有一种感觉，平时明明学了很多知识，但在遇到问题的时候，苦思冥想，试图从记忆里找知识来解决问题，可就是想不出来，没有任何思路。但是，也有极少数人，学一个知识，就可以举一反三地解决很多问题。\n\n实际上，**之所以在解决问题的时候想不起用什么知识，是因为知识和问题之间是脱节的，他们是分别存储的**。\n\n比如你学习了马斯洛五层次需求的知识，你在学这个知识的时候，就没想过它可以用来解决什么样的问题，只是知道了一个概念，那么当你遇到团队管理问题的时候，自然也想不起来能用这个知识解决，也就是说，你的问题很难把你脑中的知识调用出来。这种现象，是造成我们所学的知识无法内化成能力的关键原因。\n\n那么，**如何让知识和问题链接起来、解决问题的时候可以轻易调用知识呢？答案是：知识向问题靠，以及问题向知识靠。**\n\n**![2.jpg](https://upload-images.jianshu.io/upload_images/18390058-c7ca5d136ea94ee3.jpg)** \n\n先说**知识向问题靠，就是说，每看到一个知识的时候，就去思考这个知识可以用来解决什么问题。**\n\n比如，你学习了马斯洛的五层次需求，就马上思考一下它可以用来解决什么问题。因为这个知识讲的是人类的需求，而我们跟任何人相处其实都需要知道对方的需求。所以你就会想到，它可以用来管理下属（分析他们的需求层次，并据此激励他们），也可以用来找另一半（分析你自己所处的需求层次，从而确定你需要什么样的人），还可以用来分析奢侈品为什么可以卖那么贵（买奢侈品，满足的不只是物质需求，还有被人尊重的需求等等）。\n\n这就是知识向问题靠。\n\n而**问题向知识靠呢？就是当你遇到问题的时候，抛弃第一反应，不要先按照自己的思维定式来解决，而要去想有什么方法论模型可以用。**\n\n为什么不能按照第一反应的方式解决呢？因为在我们碰到问题的时候，我们的下意识第一反应，是未经思考的，是在遵循我们过去的模式，但如果我们处理所有问题都靠过去的模式，那么刚刚学习的新知识、新解决方案，要怎么才能掌握呢？所以，试着抛开思维定式，用你学到的知识去解决，这样一来，你的知识就会被不断盘活，你解决问题的能力会越来越强。\n\n**具体怎么落地呢？分享我的做法。**当我看书或者学习课程看到一个很有用的理论或者模型，我会记下来，然后思考至少3个用该模型解决问题的场景，也一起记下来。下次当我遇到一些难题的时候，我不会下意识反应，而是翻一下这个笔记，对照一下，哪些方法模型能解决这个问题，如果是笔记中没有的场景，这个问题是某个模型的新场景，我就补充进去。长此以往，积累了很多知识和问题之间的链接，自己解决问题的能力也得到了极大提升。\n\n**3.系统化训练**\n\n**为什么说要系统化训练呢？一方面，不同的能力并不是相互割裂的，它们之间是有关联的。**比如说，你要提升自己的演讲能力。而演讲要做得好，首先需要结构化思维，你得表达清晰；在此基础上，你需要会讲故事，能够打动听众，而讲故事也是一种能力。再比如说，很多人说我表达不清晰，那我就去学表达，但表达其实本质上是大脑在说话，大多数人表达不清晰，并不是因为表达能力的问题，而是本身思维就不清晰，所以思维和表达得一起提升。所以说，能力的训练是需要系统化的，因为它们本身相互关联。\n\n**另一方面，能力需要刻意练习，而刻意练习需要时间。**我在前面说到的结构化思维的例子，虽然已经帮你挑出那20%最关键的部分了，但它也不是一蹴而就的，也是个系统提升。比如，1个月的计划中，先是每天练习讲三段式，再是练习提炼主题，然后是积累结构，基本上每天是不间断的。\n\n这样的系统训练需要一段时间的，但很多时候**我们的习惯，是高估几天的变化，而低估几个月的变化**，所以很难坚持系统化训练。\n\n ![幻灯片17.JPG](https://upload-images.jianshu.io/upload_images/18390058-31cdc3aba1d734db.JPG)\n\n比如说减肥，你这周有兴致就跑步2次，下周有兴致就节食1次，再下一周从外卖改自己做饭，之后再参加了几次减肥训练营。你会发现，几个月之后，也不会有什么变化。因为减肥是个系统工程，你得有一个3个月计划，按照计划，今天跑步、明天健身房，同时辅助饮食调整。\n\n学习是一样的，今天学点结构化思维，明天练一下演讲，很难有真正的提升。知识的学习是大脑增加了一段记忆，但能力的提升是行为和思维方式发生改变，而行为和思维方式是短时间内很难改变的。\n\n所以，**圈外商学院里面，针对能力提升的学习，只做系统化学习项目**，包括课程、练习及反馈，也有线上线下学习社群，整个学习持续4-8个月。效果如何呢？我在开学的时候，会寄给大家一封入学欢迎信，要求大家在毕业的时候给我回信。我收到过上百封回信，以及近千个微信消息，都提到说，**原以为几个月很长，但真正结束后，回头看，没有想到几个月可以有这些改变**。\n\n好了，总结一下，今天提到了将知识内化成能力的三个方法：掌握核心的20%、知识和问题互相靠以及系统化训练。具体到我们的行动中，如果我们有需要提升的能力，**首先要先找到专业人士，了解这个领域的核心20%是什么，先去学习那20%；然后在学习过程中，思考各个知识点的应用场景，并记录下来，遇到问题再返回去找；最后，能力提升是个系统性过程，所以需要坚持至少1个月。**\n\n我经常用开车来打比方，你拿着一本开车指南然后背下来，里面的知识背得再熟，一点价值都没有；你把指南里的知识落地，真正上车然后开起来，才是有价值的。知识不能被内化成能力，就像守着一本开车指南却不会开车一样，除了浪费大脑内存之外，毫无意义。互联网时代，我们要记住一句话：**大脑不单是用来记忆的，更是用来思考的。**\n\n**![幻灯片18.JPG](https://upload-images.jianshu.io/upload_images/18390058-4c3d1e1de021f51d.JPG)** \n\n好了，现在我们知道了如何通过学习将知识内化成能力，但学习说到底是一件反人性的事情，很难坚持，更何况是要系统化训练。实际上，**学习这件事，如果纯靠意志力，的确是不可持续的**，那如何能够让自己坚持学习呢？这个领域有什么样的研究和方法呢？我们将在下节课揭晓。**这个方法不仅对学习有用，对任何需要坚持的正面行为，比如健身，都会非常有用。**","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"四类迹象，发现你的隐藏能力","url":"/2019-06-13/四类迹象，发现你的隐藏能力/","content":"\n\n**本节学习收获：**掌握SIGN模型，从而找到自己的天赋，并借此进入发展快车道\n\n**我还在咨询公司的时候，带过两个相似起点、但结局完全不同的顾问。**\n\nA顾问大学是学行政管理的，实习期间执行力很强，自己做实习生团队的小头目，但有个很大的问题，她总是点状思考，没有框架。虽然后来我还是争取破格留下了她，但是，我一直担心她未来上升会有瓶颈。\n\n入职不到一年的时候，我有次让她写份报告，做好了大量修改的心理准备，但我惊讶地发现，结构竟然非常清晰，丝毫不比其他人差。\n\n后来我问她：你为什么一下子进步了呢？她说：我不知道，好像某天突然顿悟了。\n\n另一个是T顾问，她之前在甲方工作过很短时间。转行过来以后，她的表现跟A顾问很像，执行力不错，但常常找不到思路。\n\n本来以为她也能像A顾问一样成长起来，所以我经常带她讨论，一起复盘。\n\n经过一段时间，她的思维能力有所进步，但是，涉及到新问题，还是不得要领。之后一年，T顾问每天加班到深夜，觉得很难撑下去，就离开咨询公司回到了甲方。\n\n当时我一直不明白，两个人的差异在哪里。后来，随着我管理经验增多，加之对人才发展领域更加了解，才找到了答案。结合我们这一节课的主题，相信你也猜到了，答案跟天赋有关。\n\n优势心理学之父唐纳德·克里夫顿博士，曾经带领团队进行了一项长达50年、基于200万人的研究。研究表明，**虽然成功的道路有千万条，但成功人士基本上都遵循了一个原则，就是将自身天赋发挥到了极致。**\n\n ![幻灯片9.JPG](https://upload-images.jianshu.io/upload_images/18390058-a74356c065cdba81.JPG)\n\n更重要的是，他指出：**天赋并不是少数人的专属，每个人都有自己的天赋。**\n\n这里的天赋是什么呢？**天赋就是隐藏的能力，让一个人可以在同样起点的情况下，更加快速地成长。**\n\n也就是说，在某个领域内天赋高的人与该领域里的一般人，他们的努力与水平之间的关系，类似于图片里的两条实线。\n\n![天赋.png](https://upload-images.jianshu.io/upload_images/18390058-6ca4f6ae95fd7be1.png)\n\n但是我们大多数人，往往以为天赋=能力，以为天赋是那条虚线，以为只要有天赋，不需要努力就可以达成结果。实际上，**天赋只是个催化剂，是加快反应的。**可以说，天赋是一种隐藏能力，是否能够转化为显性的能力，则是需要后天刻意练习的。\n\n ![幻灯片11.JPG](https://upload-images.jianshu.io/upload_images/18390058-7465c6cc958a855a.JPG)\n\n这也就解释了开头那个故事，为什么A顾问可以进步得这么迅速，因为系统化思维并非她天生的弱势，只是她之前专业和实习里面，没有让她刻意练习的机会，所以展现不出来。\n\n而T顾问呢？天赋上欠缺系统化思维，后天的练习确实可以起到作用，但没有A顾问那么快进步，时间一长，对比身边人，很容易沮丧挫败，从而没有动力去提升。\n\n当然，A顾问的事业就一定会比T顾问更成功吗？不是，因为世界上不是只有这一份工作。事实上，T顾问回到甲方后，工作做得很不错。\n\n所以，找到你的天赋，并且培养它，发挥它，就能够让你事半功倍地获得提升，而且，你做事会更有成就感。\n\n好了，你肯定很想知道：我的天赋是什么呢？我有什么隐藏能力呢？\n\n**接下来我给你一个探索方法，让你更快发现自己的天赋。**\n\n**天赋有四个表现，简称为“SIGN”，可以帮你在日常生活和工作中发现它：**\n\n**表现一：自我效能（Self-efficacy）**\n\n字母S对应的表现是Self-efficacy，自我效能。它的意思是对于某些任务，你的信心很强，觉得自己肯定能做好。\n\n比如，每当面对细节琐碎的事情时，还没开始，我就觉得自己做不好，但每当要我去解决一个难题时，我会充满信心。\n\n所以，**当你对某类事非常有信心，觉得自己可以做成功，这就是天赋的其中一个表现。**\n\n**表现二：本能（Instinct）**\n\n字母I对应的表现是Instinct，本能。意思是当你还没有开始做这件事的时候，你就迫不及待地想要尝试了。\n\n比如，当我决定要开发课程、教给别人的时候，我会感到异常兴奋，会思考很多方法，会想如何让学员更愿意以及更容易掌握这些内容。\n\n所以，**那些让你迫不及待、跃跃欲试的事情，可能意味着你天赋的所在。**\n\n**表现三：成长（Growth）**\n\n字母G对应的表现是Growth，成长。正如前面提到的天赋曲线那样，你发现自己学得很快，相同的时间投入，会带来更多的成长。\n\n比如，当我为了搞清楚某个商业模式，去阅读大量相关资料的时候，一上午对我来说，就跟20分钟一样。\n\n**如果在某个领域，你一接触就明显比别人进步得要快一些，这也是天赋给你的一个信号。**\n\n**表现四：满足（Needs）**\n\n字母N对应的是Needs，也就是满足。**做完这件事之后，就算感到疲劳和困倦，你依然会有满足感。**比如，之前做咨询的时候，每次给客户汇报方案、被各种问题砸过来的时候，我都累到不想再继续了，然而结束之后，又觉得充满了成就感。杨丽萍说“跳舞就是跳舞最好的回报”，描述的就是这种感受。\n\n以上就是天赋的四个表现，了解到天赋的表现之后，你就可以开始有意识地去挖掘了。当然，你**总结出这些事件之后，还需要再次提炼共性**，比如我总结出这些事件的共性是数据分析，那么说明我的数据分析方面有很大天赋，所以就可以着力培养，以成为自己的核心竞争力。我曾经写过一篇文章，描述我利用自己的优势，挑选客户，成为晋升最快咨询顾问的故事，感兴趣的话可以跟班主任要链接。\n\n好了，讲完这些，你可能还是有个疑问：这些事件我要怎么发现呢？因为**我们对自己的行为太熟悉，日常工作生活可能很难发现符合SIGN的特征，所以这里给两个方法，分别是向自己提问和向他人提问**：\n\n**1. 问自己**\n\n你可以通过问自己一些问题，帮自己找到符合天赋表现的事件。\n\n当然，问题还是围绕天赋的四个特征SIGN。\n\n**第一类问题：自我效能S相关**\n\n你认为，自己能够教别人什么？或者，别人常常向你请教什么？\n\n你跟他人聊天的时候，倾向聊什么？以及，聊什么话题你会更有自信？\n\n你在做什么事情的时候，不会感到焦虑和担心？\n\n**第二类问题：本能I相关**\n\n你在做什么事情的时候，很少拖延？\n\n长时间休假后，你最想念工作的哪个方面、哪个内容？\n\n你宁愿放弃休息时间，也要做的事情，是什么？\n\n**第三类问题：成长/专注G相关**\n\n有什么事情，让你沉浸其中忘记吃饭/睡觉？\n\n你在做什么事情的时候，会暂时忘记刷社交网络？\n\n你在做什么事情的时候，不容易感到疲倦和厌烦？\n\n**第四类问题：满足N相关**\n\n过去的工作和生活中，有什么让你获得巨大的成就感和满足感？\n\n**2. 问他人**\n\n我们之前说过，天赋是天生的、下意识的，所以很多时候，你很难辨别出来。但是从别人的角度，他们跟自己有对比，会更容易帮你辨别。\n\n所以，你可以**把这些基于SIGN模型的问题，发给你亲近的朋友或者同事，让他们帮你回答：**\n\n你觉得，我身上有什么不同于别人的特质？\n\n你最欣赏或者佩服我的方面是什么？\n\n在你看来，我做什么事情的时候，看起来最兴奋？\n\n你曾经看到我做过哪件事情，让你印象深刻？\n\n在以下这些方面，你觉得我哪些更加擅长？思维方式：条理清晰、逻辑严密、脑洞很大、专注专业；沟通协调：化解冲突、争取资源、知人善任；计划执行：执行力强、追求完美、目标导向，等等。\n\n**你会发现，你眼中的自己，和别人眼中的自己，常常有很大的差异。**\n\n好了，关于天赋的方法，我们了解到这里。\n\n我们都应该听过一句话**“大多数人的努力程度之低，根本达不到拼天赋”，这并不正确。**虽然这句话是为了安慰我们不要怪先天条件，而要靠后天努力，初衷是好的**。但天赋不是达到一定程度之后才发挥作用的东西，而是一个在起点就发挥作用的因素**，它让你快速掌握某项能力，而快速掌握之后产生的成就感和信心，又会让你继续下去，从而形成一个良性循环。\n\n ![幻灯片12.JPG](https://upload-images.jianshu.io/upload_images/18390058-dd4d6fcf44ac8a08.JPG)\n\n这里顺便说一下，**努力固然重要，但没有方法的、无效的努力反而会让你丧失信心，是有害的。**实际上，我自己是很努力的，可关键问题是，我们都是一天24小时，无非是有人工作学习8小时、有人工作学习16小时，最多也是2倍的差异，可最后不同人的市场价值、能达到的高度，远远不是2倍，而是200倍、20000倍，而那些最成功的人，也并非智商超群、天赋异禀，那是什么因素导致的差异呢？方法一定是其中很关键的因素。\n\n所以我一直强调努力要有方法，而**这些方法，并不需要你自己去摸索，前人早有很多总结，你只需要站在他们的肩膀上就可以**了，就像今天所讲到的天赋一样。这也是我做圈外、做这门课程的原因，初衷之一，就是希望自己作为人才发展领域的专业人士，能够把这些研究和正确的方法，教给更多人，让我们付出同样时间的前提下，能有更多的效果和产出。\n\n","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"四大要素，决定了你的市场价值","url":"/2019-06-11/四大要素，决定了你的市场价值/","content":"\n\n**本节学习收获：**明确冰山模型各个要素的投入产出比，从而更好地分配自己的时间，让自己变得更值钱；同时，理解不同类型的岗位所对应的门槛和天花板，帮助择业\n\n我们都会发现，同样学校、同样专业的两个人，在毕业几年之后，薪资差异可能非常大。大家起点差不多、毕业时间差不多、每天投入的工作时间也差不多，但最后产出却不一样，到底是什么原因呢？我们自己的时间应该要投入在哪里，才能最大化我们的市场价值也就是薪资呢？今天我们用科学的方法做个分析。\n\n首先，薪资，也就是一个人的市场价值，无非是跟内部和外部两类因素相关。外部就是机遇、运气、选择等等；而内部，其实就是我们在冰山模型各个要素上的表现，我们的知识技能越多、能力越强、价值观/性格特质/动机跟所做的事情越匹配，你的市场价值就越高。\n\n所以，抛开运气、选择这些很难掌控的因素不谈，我们今天从内部角度，也就是冰山模型的各个要素，来聊聊市场价值怎么提升。\n\n说具体方法之前，我们先得有一个认识，那就是**整个冰山模型越往下的要素，越难培养、越难发现**，比如学一个知识，冰山模型，我跟你讲一下马上就知道，但是提升一种能力，比如解决问题的能力，那不是听讲就行的。可是呢，**也因为越往下的要素越难，相对也越能成为竞争优势**。同时，你应该也发现了，这几个要素并不是独立的，**冰山下面的要素会影响上面的要素**。举例来说，一个人的知识储备有多少，跟ta的学习能力、动机等等都非常相关。如果你的学习和思考能力更强、并以追求真理作为价值观，知识储备就会更高。根据研究，冰山底层的要素中，光个性特质就能够解释员工绩效差异的35%，而**冰山底层的因素加起来，差不多决定了一个人的70%**。\n\n好了，有了这个认知，我们接下来挨个分析，**把时间投入在不同的要素上，会有什么结果**。\n\n**首先是投入到知识。**\n\n我们可能觉得知识很值钱，毕竟“知识付费”嘛。但说实话，单纯用知识很难赚钱。\n\n为什么呢？\n\n其一，当今社会，**你想知道什么，网上搜索就可以了，可替代性太高**。比如说，你就算把百科全书背下来，最后可能还得靠卖记忆力课程赚钱，而没法通过这个知识直接赚钱。\n\n其二，**知识跟思维，是有差别的**。有句古语叫**“学富五车”**，出自《庄子·天下》，形容一个人知识渊博，但那时候不是纸质书、是用竹简的，而且车也是马车，不是火车，所以**五车竹简能装多少知识呢？最多十几二十万字，也就一本书。**所以，现代社会，随便一个人，知识储备都超过孔子、老子、亚里士多德。但你为什么还在学他们呢？因为知识跟思维是不一样的，你学的是他们的思维。而思维是一种能力，用知识解释以及解决问题的能力。\n\n所以，你会发现，**单纯的知识储备，如果不能结合思维能力去解决一些具体问题，是很难提升你的市场价值的**。\n\n**第二种选择是投入到技能。**\n\n有一些职业是有专业门槛的，比如程序员、设计师，这属于技能型岗位，因为这些岗位有一个进入门槛，往往会让人觉得非常有安全感。反过来，像是做销售、市场、运营类工作的人，或者大学学习经管这种万金油专业的同学，常常会觉得“哎呀，我没有一技之长，好没安全感”。\n\n但实际上，**门槛高，不代表天花板也高，毕竟没人想要一直拿每个工作的门槛工资**，我们是往上看的。\n\n ![幻灯片4.JPG](https://upload-images.jianshu.io/upload_images/18390058-ab7e0f155d4ac4df.JPG)\n\n**技能的定价如何？天花板在哪里呢？取决于该技能的稀缺性。**\n\n10多年前，很多大企业都会给有CPA证书的人支付一个叫做Market Premium（市场溢价）的额外工资，因为这个技能当时相对稀缺，所以公司需要支付溢价才能招到人。可是，这个技能的市场价格高，就会有很多人进入到这个领域，人才供需逐渐稳定，那么溢价就消失了。程序员岗位也一样，在移动互联网红利期的时候，行业增长快，所以对程序员的需求非常旺盛，而市场供给还没来得及跟上，所以带来了程序员的高工资。但是，这两年红利消失，增长放缓，对程序员的需求就会下降，可供给没有下降，所以会导致工资水平上不去，但因为不能降薪，所以很多公司规定的工作时间越来越长，以此来变相降薪，“996事件”的爆发跟这个趋势有很大关系。\n\n当然，那些有管理技能、商业思维、能解决复杂问题的程序员，薪资依然很高，但他们已经不单单是靠技能吃饭了，而是具备了很高的能力。\n\n**一个很有意思的现象，在圈外的付费学员中，工作5年以下的人群技术人员占比不高，但是5年以上的人群中，就有特别多技术人员**。因为他们发现技能带来的薪资提升是有天花板的，所以需要提升管理能力、解决问题的能力，来获得更高的职业回报、突破发展瓶颈。\n\n所以，**技能在稀缺的时期很值钱，一旦行业变动、人才供给增多，市场价值就会下跌，不得不学习新的技能。**\n\n**第三种选择是投入到能力。**\n\n有一些岗位，**对技能没有特殊要求，看起来好像门槛不高，但薪资天花板却很高，而且薪资范围很宽**，例如互联网运营、产品经理、项目经理、还有大多数的管理岗位。厉害的产品经理像上亿年薪的张小龙，而一般的产品经理可能月薪才一万，他们的薪资差异并不在于技能的不同，比如谁画的产品原型图更好，而是能力高低。所以，**这些岗位也被称为是能力导向型岗位**。\n\n圈外商学院有一位深圳的学员，在一家传统行业的500强企业做工程师，带公司内部的一些技术项目。工作5年，薪资一万多，他觉得支撑不了家庭未来的需要，想要晋升，但他发现，公司的部门经理都在35-45岁之间，而自己才27岁，还需要熬很多年，于是纠结要不要投身互联网。但他也担心跨行会不会成功，以及不知道如何着手。后来他在圈外学了近半年，知道了不同行业所需要的很多能力是相通的，所以坚定了信心，而且找到圈外的朋友了解目标岗位的情况，最后在一家互联网大厂拿到了项目管理的offer，薪资翻了倍。他后来跟大家分享的时候提到，**面试中问了很多如何带团队、带项目的问题，都是课程里面学过的**。当然，想要了解他的故事，可以跟班主任要文章链接，他太太有在圈外公众号写过一篇文章，提到过这段经历。\n\n所以，**能力提升是可以跨行业跨职业的，一旦积累到一定高度，哪怕行业不行，换个地方一样可以值钱**。\n\n**第四种选择是投入到冰山模型底层的自我发现。**\n\n冰山底部的性格特质、动机、价值观这些要素，虽然难以改变和发现，但是，**如果我们对自己能有一个清晰的认识，然后找到跟这些要素相匹配的工作，其实也能大大提升我们的市场价值**。\n\n比如说，我是一个高成就动机的人，在亲和动机方面比较弱，从这个角度看，咨询公司就非常适合我，因为工作挑战大、容易有成就感。但如果我去某个注重关系和家庭氛围的国有企业，那我就会感到不适，最终很难得到提升。环境其实是一种外部驱动力，你选到了适合的地方，自然成长效率就会更高。所谓“橘生淮南则为橘，生于淮北则为枳”。\n\n好了，从上面的四种选择我们可以看到，从长期来说，**想要提升自己的市场价值，把大多数时间花在提升能力和认识自己冰山底层要素上面，是最好的选择**。但事实上，**大多数人是怎么做的呢？每天打开各种学习产品、不断学很多碎片化知识，练习一些并不稀缺的技能、考各种几个月就能拿下来的证**，最后收入还是上不去，反而会觉得很挫败。\n\n**为什么我们大多数人会做出这样的选择呢？**\n\n**第一，知识和技能的学习最容易。**它们的获得门槛很低，听一堂课、学一个知识，这是几乎所有人都能做到的事情。而能力就不一样了，需要持续投入、刻意练习。**人们总是倾向于做容易的事情，而不是正确的事情。**\n\n ![幻灯片5.JPG](https://upload-images.jianshu.io/upload_images/18390058-7f51c3f988b9b01d.JPG)\n\n**第二，受限于目前岗位的定位。**前面有提到，其实一些岗位的性质，本身就是技能导向型的，对你没什么能力要求，只需要表格做得越快越好、流程越熟练越好。而人都是有惰性的，在这种岗位要求下，也就没有动力去提升眼前岗位不需要、但长远更有价值的能力了。所以有时候我会说：**你选的不是一份工作，而是一个天花板。**\n\n ![幻灯片6.JPG](https://upload-images.jianshu.io/upload_images/18390058-79ad0c1a8a60fde7.JPG)\n\n**第三，没有看清楚工作的本质。**很多人以为值钱的是知识和经验，但并非如此。本质来说，**任何工作都是在解决问题**，营销解决的是如何让用户知道我们的产品、HR解决的是如何让公司有充足的人才供给等等，往大了说，不仅我们的工作，**一家公司也是在解决某类用户问题**，滴滴解决打不到车的问题，饿了么解决不想出门吃饭的问题，等等。\n\n ![幻灯片7.JPG](https://upload-images.jianshu.io/upload_images/18390058-58f5bdecd1166cd9.JPG)\n\n既然本质是解决问题，所以你单有一个知识就是没用的。你会背很多营销理论，但没法帮公司宣传和卖出更多产品，就是没价值的。\n\n好了，我们现在知道了，要花时间去认识自己和提升能力。\n\n关于认识自己，圈外商学院里有这个课程，但自我认知是一个持续的、慢慢发现的过程，课程也只是供参考，我们这次先不展开。\n\n关于提升能力，这是任何职业阶段的人都应该去做的，而且你投入了就一定会有回报。\n\n但你有没有发现，**同样两个人，都培养自己的某项能力，比如结构化思维，一个人很快就能提升，但另一个人好像努力了很久也没有太大效果？**这跟我们每个人的天赋有关，我**下一节课告诉你，关于天赋的一些研究成果，以及大多数人对天赋的误解，并且会帮你找出自己的天赋**，让你能够更快速地提升能力，进入发展快车道。\n\n好，今天这节课，我们知道了：\n\n**知识容易获取跟习得，如果不能用它解决问题，几乎就没什么竞争力；**\n\n**技能有进入门槛，其市场价值取决于稀缺程度，但长期来说，所有技能都会走向供需平衡，高收入不可持续；**\n\n**能力可迁移，并且对知识和技能也有很大促进，值得我们多投入；**\n\n**而每个人的性格、动机和价值观不同，所以做不同的工作，也会有产出的不同。**\n\n总之来说，我们的时间应该多投入在后两者上面，但现实是大多数人都在提升前两者。**我们的时间，永远应该花在正确的事情上，而不是容易的事情上。**\n","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"一个模型，帮你找到真正热爱的工作","url":"/2019-06-10/一个模型，帮你找到真正热爱的工作/","content":"\n\n**本节学习收获：**掌握冰山模型的用法，能够用它来分析自己跟工作的匹配度，从而做出正确的职业选择，并能精准找到自己喜欢并有成就感的工作。\n\n“我感觉目前的工作状态不是我最想要的，做得也还行，但没有太多成就感，也不知道未来发展会怎么样……想有一份真正热爱并且有前途的工作，但不知道应该做什么，感觉很迷茫。”\n\n这段话，要么是我们当下的状态，要么是曾经有过的状态。\n\n刚工作发现不喜欢想转行、到了新岗位发现不胜任想提升、人到中年发现这个工作的天花板等等，都是常态。**因为我们对自己的事业有期许，但当下的工作可能无法承载这样的期许，所以会觉得“不喜欢”“很迷茫”。**\n\n可是，“不喜欢”“很迷茫”只是一种情绪，情绪背后的原因是什么呢？我们喜欢、又有前途的工作到底在哪里呢？这节课，我给你介绍一个模型，然后用它来帮你做好职业选择和定位，让你不再迷茫。\n\n这个模型称为冰山模型，你可能听过，但我还是会简单介绍一下，然后告诉你这个模型有哪些妙用，用好它，差不多可以解决你一半以上的学习和发展问题。\n\n冰山模型是美国著名心理学家麦克利兰提出来的，它全面地描述了一个人的个体素质要素，也就是说，**你跟一个岗位是不是匹配、匹配程度如何、市场薪资值多少，都是这个模型可以解释的，几乎所有大公司都会用它来进行人才招聘和培养。**\n\n![1.jpg](https://upload-images.jianshu.io/upload_images/18390058-44ed087541330a89.jpg)\n\n**第一部分，知识和技能**\n\n**冰山模型从上到下有很多要素，最上面的要素是知识和技能。**\n\n知识，就是我们在学习和实践中获得的认知和经验，比如财务知识、人力资源知识等等，包括我们现在学习的冰山模型，也属于知识。这跟你大学所学的专业、常看的书、从事的工作、甚至业余爱好都有关系。\n\n技能是指你所具备的某项专门技术，比如骑自行车、编程、使用Excel等等。\n\n一个人的知识和技能是可以后也是非常显性，容易看出来的。所以，我们称为冰山上的部分。\n\n知识技能跟工作之间的关系是什么呢？简单来说，**如果你的工作中有很多陌生的内容，觉得每天都信息量很大、来不及接收，感到慌乱和焦虑，很可能就是你的知识技能跟岗位不匹配。**但这不是什么大问题，因为知识和技能比较容易补齐，上上课、看看书、跟资深同事学，一段时间之后就能提升。\n\n**第二部分，能力**\n\n**冰山模型中间的要素是能力，或叫通用能力，比如学习和思考能力、人际交往能力等。**相对知识和技能来说，能力高低不是一眼就能看出来的。比如，一个人的创新能力到底如何，很难用一个证书、几道题目来考察，而需要看他在处理很多问题时候的行为。\n\n能力跟知识技能最大的区别在于：**知识和技能属于特定领域，而能力则更多是通用领域的。**比如，知识会分财务、人力资源、金融等等，但是“创新”这样的能力，是适用于任何领域的，一旦掌握，是能够迁移的。\n\n那么，**如果能力不匹配，在工作中会如何呢？工作效率、沟通效率较低，面对复杂的问题无从下手，缺乏成就感，力不从心。**能力的培养周期相对长一些，一般要几个月时间，我们后面几节会讲方法。\n\n**第三部分，价值观、性格、动机**\n\n**冰山模型最底下包括价值观、性格特质、动机。这些要素在成年之后很难被改变**，它们会受基因、家庭教育、童年经历等等的影响。简单介绍一下各个要素：\n\n**价值观是你判断事物的标准**，比如说，当你在择业的时候，自由和稳定产生了冲突，你选择哪个、放弃哪个；在事业和家庭产生冲突的时候，你怎么处理，等等。**如果你在工作中经常陷入矛盾和纠结，对所做的事情很难发自内心地认同，很可能就是价值观上不匹配**，比如你做自媒体，公司为了赚钱让你写一些低俗内容，但你觉得那对用户没有价值。 \n\n**性格特质则是个人的行为偏好**，比如，你是偏内向还是外向，更关注宏观还是细节，等等。**如果你在工作中发现，自己好像工作量没有很大，但却觉得心累，很有可能是性格不匹配**。比如，你是内向性格的人，是从独处中获得能量，但你做了一份每天都要不断跟陌生人沟通的工作。 \n\n至于动机，其实动机的分类方法有很多，最常见的是麦克利兰的理论，分为成就动机、权力动机和亲和动机。成就动机的人，喜欢挑战；权力动机的人，希望影响他人；而亲和动机的人，希望维持更好的团队关系。**如果你感觉自己没有动力，做事提不起劲儿来，那很可能就是现在的工作跟你的动机不匹配**，比如你明明是成就感动机，喜欢一定挑战，但你的工作却高度重复。\n\n好了，我们讲完整个冰山模型，现在你知道了，**工作跟伴侣是一样的，“匹配”比“优秀”重要**，而你跟一份工作的匹配要素，其实就是冰山模型中的要素。所以，我们**感觉一份工作不喜欢，有可能是缺乏知识技能导致的慌乱和焦虑、缺乏能力导致的挫败和低效、价值观不匹配导致的矛盾和纠结、动机不匹配导致的没热情，或者性格特质不匹配导致的心累。**\n\n**![幻灯片1.JPG](https://upload-images.jianshu.io/upload_images/18390058-402ae4ae1a256cfc.JPG)** \n\n而且，所有匹配要素里面，冰山底层的隐性要素起到了更大的作用，就像找伴侣一样，尽管看起来表面条件差不多，但你就是对一个人有感觉而对另一个人没有，这就是冰山底层的隐性要素在发挥作用。\n\n所以，知道了这个模型，你就可以分析了，如果目前的工作自己不是很喜欢，那么到底是哪里不喜欢。\n\n好了，现在我们分析的是现有岗位是否喜欢，那么假如我现在是销售，发现并不喜欢，想要转行做互联网运营，面对这样一个陌生岗位，不知道自己是否匹配，这时候应该怎么办呢？或者说，我不喜欢销售工作，但我面对运营、产品经理、市场分析等等这些可能感兴趣的岗位，不知道该去哪一个，又该怎么办呢？**这世界上有几千种岗位，我们总不能全都干一遍然后再选。**\n\n其实很简单，你**把目标岗位用冰山模型分析一下**，就可以了。\n\n具体该怎么做呢？分成四步：\n\n**第一步，先确定一个你要分析的岗位。**\n\n比如你有一个目标岗位，那么很简单，就用这个岗位来分析自己是不是匹配。但如果你没有，可以先从自己的相关经验出发，框定你能做的几个岗位，一个个分析。这里**教你一个技巧**，比如你在某个行业做技术服务，但目前的工作不喜欢，想要换，可又不知道自己能换什么，这时候可以去领英等等这类招聘平台去搜索，就**搜这个行业的技术服务岗位，然后会出来一堆人，你看这些人的履历，看看他们后来都去了哪里，你就知道潜在的方向了**。而且，**这个方法也能帮你判断，未来你要去的这个行业和岗位，到底有没有前途**，这些原先做该岗位的人，后面的出路是什么。\n\n**第二步，在招聘网站上，搜索这个岗位的招聘要求。**\n\n这时候，你会搜到很多公司对这个岗位的招聘要求，找出几份不同的招聘需求。因为不同公司会有自己的细分要求，为了提取出共性要求，你可以尽可能多看几份，找到共性。\n\n**第三步，按照冰山模型，综合分析这些招聘需求。**\n\n你会发现，**招聘需求基本上也是按照冰山模型来写的**，以新媒体运营岗位来说，有对知识的要求，比如传播学或文学专业毕业、了解内容营销的知识、熟悉新媒体平台；有对技能的要求，比如精通公众号排版工具使用；也有对能力的要求，比如文案写作能力、用户思维、学习能力等等；最后，还有对性格特质、动机等等的要求，比如细心、热爱挑战等等。把它们汇总在一起，就可以得到这份岗位的需求模型了。\n\n**第四步，按照岗位需求模型，与自己进行对比。**\n\n岗位需求模型已经出来了，那么自己就可以对照一下，自己到底是不是符合，是否应该换工作或转行。当然，我们很难找到完全匹配的工作，**如果是冰山底层的因素不匹配，那么不太建议选择这个职业，因为这些要素后天很难改变；但如果是知识、技能方面不匹配，未必不能选，因为大部分知识和技能都是可以后天学习的**，只是看你是否愿意投入时间罢了。\n\n我们平时看一份工作，大都是看冰山表层，比如职责、薪资、办公环境、学历、专业等等因素，可最终你这份工作能不能做好、是否有成就感，其实是很多隐性要素所决定的，今天的内容，其实是把你对工作的感觉给表述出来了，被分解成了你的性格特质、动机、价值观，也就避免了我们一个个去尝试然后看喜不喜欢。\n\n我一直认为，**搞明白“我应该选择什么样的工作”，比“我应该如何在别人认为的好工作里面成功”，要重要得多**。毕竟，**想要让自己获得成就感和满足感，就不应该把它绑在别人的记分牌上**。\n\n![幻灯片2.JPG](https://upload-images.jianshu.io/upload_images/18390058-cf810a2e107ecb6f.JPG)\n\n好了，今天我们讲了冰山模型，你会发现，**它不只是能用在职业选择方面，还可以用在其他很多方面**，比如今天稍微提到一点的伴侣选择，还有子女教育，很多父母都给孩子报很多培训班，但孩子时间有限，所以应该选什么培训班呢？是教孩子背背单词和儿歌，还是开发思维的课程呢？如果你知道了冰山模型，相信会有答案。\n\n所以，我想告诉你的是，**我们所学习的每个方法论和模型，都不是只能解决一个问题**，而是可以解决几百甚至几千个问题，**查理芒格在《穷查理宝典》里面提到，掌握一定数量的思维模型，能解决这世上90%的问题**。你会发现，同样的知识量，如果你能用知识解决更多问题，那你的学习投入产出比就是最高的。\n\n![幻灯片3.JPG](https://upload-images.jianshu.io/upload_images/18390058-646d251f02ca5ce3.JPG)\n\n\n好了，正如我们刚才说的，**冰山模型有很多作用，除了帮你匹配工作，其实它还可以用来衡量你的市场价值，也就是薪资**。公司会通过简历、面试来考察候选人在冰山模型各个要素的表现情况，来确定对方的薪资。\n\n\n\n","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"实时音视频互动系列","url":"/2019-06-09/reference/webrtc/01-实时音视频互动系列/","content":"\n## 又拍云UTUN网络详解\n\n### 如何定义实时音视频互动, 延迟 400ms 内才能无异步感\n\n实时音视频互动如果存在1秒左右的延时会给交流者带来异步感，必须将视频播放延迟限制在 400ms 以内，才能给用户较好的交互体验。\n\n当延迟控制在 400ms 以内时，两个人音视频互动是实时的，不会有异步感存在，即实时音视频互动。\n\n<!-- more -->\n\n### 实时音视频互动产生延迟的原因\n\n音视频互动的延迟是如何产生的？\n\n我们先假设这样一个场景：位于北京的A客户端与位于广州的B客户端进行实时音视频互动。\n\n该场景会有以下几个产生延迟的原因：\n\n- 光的传输耗时 30ms；\n- 网络处理耗时 10ms；\n- 应用服务处理耗时 10ms；\n- 客户端发送处理耗时 50ms（采集、编码、缓冲…）；\n- 客户端接收处理耗时 50ms（缓冲、解码、渲染…）；\n\n网络层面，在跨地区、跨运营商等情况下，传输延时会非常高并且不稳定，尤其在晚高峰或者网络拥堵的情况下延时更加无法把控。单纯通讯环境导致超过100ms的延迟时间，因此需要在技术层面达到较高的性能才能将延迟控制在200ms以内。\n\n### 又拍云 UTUN 通讯网，数据传输耗时低于50ms\n\n![](/images/imageWebRTC/others/utun通信网.png)\n\n为了解决这个问题，又拍云设计了基于公网的通讯网 UTUN，以此实现所有客户端接入又拍云通讯网之后再进行交互。\n\nUTUN 是一个分布式网络路由器，加入 UTUN 可以将数据以最快的速度传达到目的地，同时无需担心跨地区、跨 ISP、负载均衡、容灾等问题。\n\n![](/images/imageWebRTC/others/utun通信网-01.png)\n\n又拍云 UTUN 网络基于又拍云 CDN 网络部署，同时拥有200多个边缘接入节点、4000多台服务器、覆盖3大运营商、3个小运营商。\n\n通过又拍云 UTUN 网络进行数据传输，国内可以做到传输低于50ms，海外传输低于200ms。计算入上文提到的应用层产生延时的点，50ms加上其他因素所导致的延时，又拍云国内传输可以做到100~200ms音视频互动。国际传输音视频互动延时等于应用层所消耗掉的100~200ms再加上网络传输的延时，又拍云能够做到400ms之内。\n\n## 基于 WebRTC 技术的实战解析\n\n在 WebRTC 项目中，又拍云团队做到了覆盖系统全局，保证项目进程流畅。这牵涉到主要三大块技术点：\n\n- 网络端、服务端的开发和传输算法\n- WebRTC 协议中牵扯到服务端的应用协议和信令服务\n- 客户端iOS、安卓 H.264 编解码技术\n\n![WebRTC 技术点](/images/imageWebRTC/others/utun通信网-02.png)\n\n### 实时音视频互动必须遵守三大点\n\n- **必须基于 UDP 协议，否则不要谈实时**\n\n  因为 TCP 协议的重传机制（传输保障）会导致累积延迟问题，用 UDP 协议没有传输保障机制，但需要自行完善丢包容错逻辑。\n\n  又拍云音视频互动方案是基于UDP 协议，使用 TCP 协议无法保障实时性。\n\n  TCP 协议有包重传机制，保证传输内容100%传输到目的地，这个特性导致延时增加。当然，由于UDP协议没有包重传机制，需要完善业务的容错性。目前来说，UTUN 网络提供的两种配置，都可以保证数据100%传输。\n\n  在极差的网络状态下，可以选择容忍丢包，使用算法保障90%以上的数据包正常到达，以此达到200ms以内延迟。\n\n  UDP协议相比TCP协议具有多链路传输的优势。\n\n  TCP协议只支持单一链路传输。当连麦、音画同时需要传输时，TCP协议只有一条通道进行数据传输。而通过UDP协议，音视频可以通过两个节点将数据一分为二来传输，A路传输50%数据包，B路传输50%数据包。终端收到两路数据流，再合并放到应用层做解码处理。\n\n- **考虑多终端适配，使用 WebRTC 协议**\n\n  客户端网络跨地区和跨运营商信号很差，所以不能使用 P2P 模式。目前包括苹果Safari 在内的所有的桌面端浏览器都已支持 WebRTC 协议。\n\n  网络层使用 P2P 模式无法解决跨地域、跨 ISP 的跨运营商网络问题，会导致延时过高的情况产生。如果一直纠结于P2P模式，那么QOS码率控制、包容忍等问题就无法在算法上有所突破。\n\n- **云服务化**\n\n  单机、单机房存在硬件瓶颈，唯有云服务化才能按需做到横向扩展。\n\n  随着用户量的提升，单台服务器所能支撑的并发量直播有限，RTMP Server、WebRTC Server一般八核服务器能承受的并发量只有2000~4000路，单机房也会成为硬件瓶颈，而公有云能承受几十万甚至上百万的数量压力，所以机房中不能存在单点，必须是云服务化分布式的。\n\n  云服务化非常重要，上文提到的 UTUN 网络属于完全分布式网络，分布在又拍云两百多个节点，四千台服务器上。只需要接入又拍云任意边缘服务器，就可以做到自主服务，自动选择出一条甚至数条路径，让用户与通讯网中任何地点的人交互。\n\n### 又拍云 WebRTC 架构中遇到的经验和问题\n\n又拍云 WebRTC 相比外部的 WebRTC 有较大的差别。即使你在同一个地方、同一个服务商、同一个无线信号下，又拍云都没有使用P2P模式，都是通过云服务来进行网络传输的。\n\n我们严格遵循官方标准搭建包括服务端、客户端在内的 WebRTC 体系。目前 WebRTC 版本为可变性非常大的1.0版本，未来该技术可能会有革命性的迭代。如果采用自研的方式，会有无法跟进版本技术更新的风险。再者如果完全自主编写 Server 端或者客户端势必要投入非常大的精力和研发时间。\n\n因此又拍云选择紧跟官方的步伐，无论官方有何种bug修复，都选择同步更新。\n\n**在实践中遇到的问题：**\n\n- 当 iOS 端使用新版本 WebRTC 时，由于音频处理部分导致的 Bug，会导致 CPU 占用率过高；\n- 服务 Server 端由于编码传输时 WebRTC 是可变码率、可变帧率的，但是内核代码在进行传输时却使用了固定帧率操作，时间戳不一致的 Bug 导致了音视频不同步的情况，声音与画面不同步最大延时可以达到数十秒，不断累积。为了解决这个 Bug 需要把视频时间戳进行修正，统一使用音频的时间戳，来保证音视频同步；\n- Android 端不支持高通外的芯片硬解码，又拍云在近期把各个 Android 端编解码功能完善，目前已经能够适配华为、MTK、三星等品牌的机型；\n- 目前客户端解码能力有限，会话人数最好控制在8个人以内；\n- 自动根据参与人数控制总带宽在2Mbps以内；\n- 美颜、滤镜等功能的接入会增加延迟，加入额外功能不能过度消耗客户端 CPU 资源。\n\n### 音视频互动最大的难点——业务信令\n\n目前业务信令还没有一套完整的解决方法，业务信令在 WebRTC 中虽然是开源的，但是没有形成标准的信令协议，这个部分需要我们自行构建。\n\n架构网络电话场景时，牵扯到三个信令：呼叫、等待接听、通话。\n\n但是实际中会有更多信令，假设一个会议场景，A邀请参会B，A会设置多个邀请途径：1.A直接将B拉到会议室；2.A把会议室号码给B，B自行进入；3.A配置房间权限控制，需要得到授权才能进入房间等。随着业务的发展，业务信令会不断增加，我们需要构建一套完善的信令体系显得非常重要。\n\n我们在编写信令系统时，把信令系统分成了两类：1.底层系统信令，2.公共业务信令。\n\n底层系统信令只需编写公共业务信令的总通道协议和 API 接口，让应用程序对接，将业务信令进行统一标准化。比如在房间里，发送一条广播给所有参会者的业务信令S，而业务信令S只想传达给B，但是C在同一个会议室也听到了，C会选择性的对业务信令S忽略以此达成这个业务功能。\n\n### 目前来说必须面临的现实问题：\n\n- 客户端硬件性能未能支持高清码率：多人互动不可能做到720P分辨率，一般来说都是在320P或者460P分辨率。一般手机因为客户端的解码能力支撑不了多路高清解码，达到6路以上码率只能做到300K以下；\n\n- 硬编解码兼容性差：Android 机型太多，仅能有限支持H.264硬编解码，同时iOS和Android 端均不支持 H.265 硬编解码；\n\n- 手机发热、耗电量大：参加会议iPhone电量支撑两、三个小时。桌面端耗电、发热最严重，测试时使用Chrome硬解码电量只能支持两个小时。\n\n以上三点是目前整个业内所都要面临的最大的问题，只能等待终端的解码能力提升，相信到明年手机解码能力就可以支持多路高清互联。\n\n\n  \n\n\n\n\n\n\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"个人提升课程简介","url":"/2019-06-08/个人提升课程简介/","content":"\n\n1. 我对现在的工作没什么热情，应该是因为我不够擅长\n2. 我认同“以大多数人的努力程度之低，根本达不到拼天赋”\n3. 做技术的，应该专注在做技术上，其他能力不重要\n4. 时代变化太快，我需要紧密关注和学习应对所有变化，避免被淘汰\n5. 我觉得自己好像没什么天赋\n6. 学习一个领域的知识，应该从概念开始，全面地学习\n7. 多学点知识，一定会有用的\n8. 工作让我感觉很心累，肯定是工作量太大了\n9. 很难坚持学习、阅读、健身这类事情，是因为不够自律\n \n现在，我来公布答案了，你先记录下来，再看答案。\n \n答案是以上说法全部错误。\n \n实际上，这些问题，都是我经常碰到的，无论是我的读者、用户还是团队的年轻人，在这些问题上都有很大的认识误区。很多人在职业发展和个人提升方面的无策略、无方法程度，让我感到惊讶。\n \n但跟很多人聊过之后，我发现，我之所以认识到这些，其实是因为我自身职业的优势，使我具备企业、管理者、咨询顾问、教育创业者四种视角。而这种角度，恰好是大多数人所不具备的。\n\n企业视角：\n我曾经在全球排名第一的人力资源咨询公司，美世咨询（Mercer），工作了八年多，为7个行业、几十家头部企业，设计人才发展战略。如果你在航空、消费品、互联网等等行业的一些头部公司，很可能我设计过的薪资和晋升体系就直接影响到你了（我希望你不要顺着wifi掐死我）。我设计的人才管理体系，直接影响到的职场人士有数百万。而一般人并没有这样的咨询经历，无法看到各类企业的全局。\n \n管理者视角：\n一方面，因为此前做咨询的缘故，接触的都是各大企业的高管，因此，我能够知道管理者在想什么，他们如何看待人才。另一方面，我自己也有过很多年带团队的经验，而且我带过的人，都比相同岗位起点的同龄人薪资高出2-3倍，所以在这方面有切身体会。\n \n咨询师视角：\n作为咨询顾问，最重要的能力就是解决问题。所以我养成了一个习惯，就是每当面临问题的时候，用结构化的方式去解决。比如个人规划，很多人不知道应该如何规划，那我就会将个人类比企业，将企业战略规划的框架进行改良，变成个人规划方法，然后在使用过程中不断完善，最终形成适合自己的方法论。所以，职业发展、自我认知、个人提升等话题，我都可以摸索出一套套实操性较强的方法论，而较少陷入困扰。\n \n教育创业者视角：\n我是在线商学院【圈外同学】的创始人，同时也是我们平台上的老师之一，教授过几十门课程。目前有几十万人都上过我们的课程，数万人进入到我们的商学院培养项目。在商学院项目学员里，97%的学员认为课程对他们带来了明显改变，仅仅3个月学习后就有19%的人实现了顺利转行跳槽或升职加薪。\n \n我还写过一本个人发展类的干货畅销书，叫《请停止无效努力：如何用正确的方法快速进阶》，黄色封面，人称“小黄书”，豆瓣评分是8.4分，差不多是同类书籍万分之一的水平。所以，我知道应该怎么教。\n \n正因为这些特殊的视角，我在个人发展方面习以为常的认知，或许在他人看来，却是颠覆性的。\n \n但是，我们现在不得不拥有这样的认知，掌握正确的发展方法论了。\n \n我前公司有过一个调研数据：\n10年前，一个人在一家公司所待的时间是3-4年，而现在，一个人在一家公司平均只会待1年多。\n\n在这种情况下，企业没有任何动力培养人才，只在公司平均待1年多，培养好之后还来不及给公司做贡献就走了，对企业来说根本不划算，更何况外部环境竞争激烈，培养人才所需要的时间，企业根本耗不起。所以，企业越来越倾向于用现成的人。而当所有企业都想用现成人才的时候，其实就是企业将过去所承担的人才培养成本，逐渐转移到了个人身上。\n \n个人从现在开始，不得不开始培养自己，进入到所谓的终身学习状态。这正是知识付费兴起的大背景之一。\n \n但是，从未有人教过我们，离开了学校这种应试教育的地方，在职场中应该如何正确地学习和提升。所以，大家一窝蜂买知识付费课程，一窝蜂去学英语，一窝蜂去考证，最后发现好像没用，于是陷入了焦虑和迷茫。\n \n这个大背景，是我从2年多前开始创业做圈外的初衷。\n \n圈外目前有一些4-6个月的体系化学习项目，也有针对单项能力的特训营，进入门槛都相对高，也并非适合每个人的发展阶段。所以我希望通过这个非常短的课程，普及一些基本的职业发展和提升方法，不再让一些毫无专业基础的成功学老师误导我们，每个人都应该有自己的发展策略，当好自己的CEO。\n \n好了，那这门课程到底如何帮你呢？\n \n我会分为4个维度、6节内容来展开叙述：\n \n一、诊断现状\n \n1）一个模型，帮你找到真正热爱的工作：分析个人和工作匹配的要素有哪些，知道自己为什么会不喜欢一份工作，以及怎么找到跟自己匹配的工作。\n \n2）四大要素，决定了你的市场价值：分析应该把时间花在什么要素上面，才能有市场价值的提升，并不是所有努力都有效\n \n二、发现潜能\n \n3）四类迹象，发现你的隐藏能力：找到自己的天赋，也就是隐藏的能力，才能事半功倍\n \n三、学习提升\n \n4）三种方法，将知识内化成能力：知识是死的，能力才能盘活它\n \n5）三大系统，让学习不靠意志力：靠意志力学习是不可能的，需要依靠三大系统\n \n四、规划未来\n \n6）三个建议，让你不做“定制化人才：如何找到不变的因素，应对未来的加速变化，避免成为企业定制化人才\n \n课程只有6小节，但不是听完就算，我会在每小节给你一个作业，让你进行练习。我们的班主任，也会在微信群里发起很多讨论，跟同学一起帮助你，如果你有什么困惑，欢迎“骚扰”他们。\n \n我们每个人遇到的问题和烦恼看似不同，但你要知道，你不是这个世界上第一个遇到这些问题的人。人才发展这个话题，本身就是一门科学，已经有很多前人帮你研究过这些问题了，你需要做的，不是自己慢慢摸索、重新造轮子，而是找到最科学的方法，进行学习。\n \n","tags":["planning"],"categories":["职业规划","个人提升"]},{"title":"WebRTC视频统计信息之延迟抖动与丢包","url":"/2019-06-07/reference/webrtc/webrtc的视频统计信息之延迟抖动与丢包/","content":"\n## 前言\n\n> 这篇文章主要想说明的是WebRTC内部对视频`上下行延时、抖动、丢包`如何更新，上层又怎么获取到这些统计信息的。对应的`WebRTC版本：63`。\n\n## 背景\n\n> 最近在内网情况下测试视频会议，视频下行延时很大，很多时候超过`100ms`。另外，视频的上下行抖动总是稳定在`30~40ms`这个区间。这些统计在内网环境下是不正常的，于是决定看看是哪里导致这些问题的。\n>\n> 在解决这些问题的过程中，也对WebRTC内部视频统计数据做了一次梳理。\n>\n> 阅读这篇文章之前，最好对RTP、RTCP、SR、RR有一些了解。这里就不过多展开，可以参考以下文章：\n>\n> [RTP Data Transfer Protocol](<https://tools.ietf.org/html/rfc3550#section-5>)\n>\n> [RTP Control Protocol – RTC](<https://tools.ietf.org/html/rfc3550#section-6>)\n>\n> [RTP/RTSP/RTCP有什么区别](<https://www.zhihu.com/question/20278635/answer/14590945>)\n\n<!-- more -->\n\n## 综述\n\n下图是WebRTC内部获取视频统计信息和统计信息如何被更新的流程图：（其中的箭头代表函数调用）\n\n![WebRTC内部获取视频统计信息和统计信息如何被更新的流程图](/images/imageWebRTC/others/WebRTC内部获取视频统计信息和统计信息如何被更新的流程图.png)\n\n上图共有两个大的模块，**如何取** 和 **如何更新**：\n\n### 如何取\n\n上面部分“客户端视频数据统计入口”中，左下角的`WebRtcVideoChannel::GetStats`是WebRTC对外暴露的获取统计信息的入口，视频的上下行统计数据最终分别使用右上角`SendStatisticsProxy::stats_`、`ReceiveStatisticsProxy::stats_`和`CallStats::avg_rtt_ms_`来填充返回。\n\n### 如何更新\n\n下面部分“延时、抖动、丢包更新流程”部分，从网络接收到RTP/RTCP之后，使用三个不同颜色代表三种统计信息的更新流程，比如红色代表下行抖动/丢包更新流程、蓝色代表RTT的更新流程等。\n\n统计信息大多不是由一条调用流程完成的（这就是下文会说到的“阶段”），会有几次类似缓冲区的“中转”，然后由另外的线程或函数继续做统计信息的整理，最终达到上一步的 `SendStatisticsProxy::stats_`、`ReceiveStatisticsProxy::stats_` 和 `CallStats::avg_rtt_ms_`，等待上层获取。\n\n## 几个统计信息详细介绍\n\n### 延时\n\n> 这里统计的延时指的是往返延时 rtt。`WebRTC使用SR/RR来计算rtt`。\n\n#### (1) 延时的计算\n\n##### 1) SR和RR报文格式\n\n| Sender Report RTCP Packet                             | Receiver Report RTCP Packet                           |\n| ----------------------------------------------------- | ----------------------------------------------------- |\n| ![](/images/imageWebRTC/others/SR和RR报文格式-01.png) | ![](/images/imageWebRTC/others/SR和RR报文格式-02.png) |\n\n##### 2) 计算rtt\n\n> 以下流程通过结合SR/RR包报文格式，浏览`RTCPReceiver::HandleReceiverReport`、`RTCPReceiver::HandleReportBlock`、`ModuleRtpRtcpImpl::SendCompoundRTCP`、`RTCPSender::BuildSR`、`RTCPSender::BuildRR`函数。前面2个函数是接收端计算rtt，后面3个函数是对端在构造RR时LSR/DLSR如何设置的。\n\n- 首先，发送端构造SR时，`sender info`部分的NTP字段被设置为当前ntp时间戳；\n- 接收端收到最新的SR之后，使用`last_received_sr_ntp_`字段记录当前ntp时间戳；\n- 接收端构造RR时，设置RR的DLSR字段为`当前ntp时间戳 - last_received_sr_ntp_`，之后发出RR包；\n- 发送端在接收到RR包之后，记录RR包到达时间A；\n- 使用公式 `A - LSR - DLSR` 计算rtt。\n\n##### 3) 用一个图描述上述RTT计算流程\n\n![RTT计算流程](/images/imageWebRTC/others/RTT计算流程.png)\n\n> SR与RR的个数并不完全相同，因为RR并不是对SR的回应，它们的发送各自独立；另外丢包也会导致一部分SR/RR没有被对方接收。因此上图中，SR和RR传输中，实线代表发了一次SR/RR，并且被被对方接收了。这里想说明的是：**即便SR或RR丢失一部分，只要发送端收到了RR，它总能计算出rtt，因为RR中使用的LSR和DLSR字段都是从最近一次收到的SR中取到的。**\n\n#### (2) 延时的更新流程\n\n> 下文所说的第一阶段、第二阶段等，都是指 **数据从一个位置转移到另一个位置的过程，或者说是一次推或拉模式**。比如：F1函数把数据从A点转移到B点就返回了，F2函数把数据从B点转移到C点就返回了，那A->B就是第一阶段，B->C就是第二阶段。如下：\n\n![延时的更新流程](/images/imageWebRTC/others/延时的更新流程.png)\n\n##### 1) rtt统计第一阶段\n\n由上文可知：从RR可以计算出往返延时rtt，这个rtt最终保存在`RTCPReceiver::received_report_blocks_`。\n\n![rtt统计第一阶段](/images/imageWebRTC/others/rtt统计第一阶段.png)\n\n##### 2) rtt统计第二阶段\n\n`ModuleRtpRtcpImpl::Process`会定时把rtt从`RTCPReceiver::received_report_blocks_`更新到`CallStats::reports_`，这个更新过程，`CallStats::reports_`中每个rtt都会与一个更新时间戳绑定。参考`CallStats::OnRttUpdate` 函数。\n\n![rtt统计第二阶段](/images/imageWebRTC/others/rtt统计第二阶段.png)\n\n##### 3) rtt统计第三阶段\n\n`CallStats`继承`Module`，`CallStats::Process`函数会定时做以下三个步骤：\n\n- 根据第二阶段绑定的时间戳，清理掉 `reports_` 中距当前时间1.5s以前的rtt；\n- 计算1.5s内的平均rtt；\n- 使用平均rtt，更新 `avg_rtt_ms` 成员；\n\n![rtt统计第三阶段](/images/imageWebRTC/others/rtt统计第三阶段.png)\n\n#### (3) 获取延时\n\n调用`CallStats::avg_rtt_ms`函数获取rtt时，直接返回 `avg_rtt_ms_` ;\n\n### 下行抖动和丢包\n\n> 下行抖动和丢包，通过在接收端根据收到的RTP包来计算和更新。\n\n#### (1) 抖动和丢包的计算\n\n##### 1) 抖动定义\n\n抖动被定义为：一对数据包在接收端与发送端的数据包时间间距之差。如下：\n\n![一对数据包在接收端与发送端的数据包时间间距之差](/images/imageWebRTC/others/抖动定义.png)\n\n如果Si代表第i个包的发送时间戳，Ri代表第i个包的接收时间戳。Sj、Rj同理。\n`抖动(i, j)` = `|(Rj - Ri) - (Sj - Si)|` = `|(Rj - Sj) - (Ri - Si)|`\n\nWebRTC为了统一抖动，并且为了很好的降噪、降低突发抖动的影响，把上面的`抖动(i, j)`定义为`D(i, j)`，`抖动J(i)`定义为:\n`J(i) = J(i-1) + (|D(i-1, i)| - J(i - 1)) / 16`\n\n我虽然看不出J(i)和D(i)的关系，但是`D(i-1, j)`是唯一引起`J(i)`变化的因素，是需要重点关注的。\n\n##### 2) 抖动计算存在的问题：\n\nRTP报文头部，有timestamp字段，该字段用来表示该RTP包所属帧的`capture time`。接收RTP包时如果记录接收时间戳，再根据头部的`timestamp`字段，D(i, j)就可以计算出来，J也就有了。（事实上webrtc原本也是这样干的，而且这种方式计算的抖动还对外暴露，可以参考`StreamStatisticianImpl::UpdateJitter`函数）\n\n但是这样计算抖动是存在问题的：**每一帧的视频数据放进多个RTP包之后，这些RTP包的头部timestamp字段都是一样的（都是帧的capture time），但是实际发送时间不一样，到达时间也不同。**\n\n##### 3) 如何正确计算抖动：\n\n计算D(i, j)时，Si不能只使用RTP timestamp，而是应该使用该RTP实际发送到网络的时间戳。这种抖动被命名为`jitter_q4_transmission_time_offset`，意为考虑了transmission_time_offset的jitter。\n\n- **a. transmission_time_offset是什么?**\n\n> transmission_time_offset是一段时间间隔，该时间间隔代表属于同一帧的RTP的`实际发送时间`距离帧的`capture time`的 **偏移量** 。下图是对transmission_offset_time的解释：\n\n![transmission_time_offset](/images/imageWebRTC/others/transmission_time_offset.png)\n\n> 其中，箭头代表一个RTP，发送端的竖线代表时间轴，虚线代表帧的capture time。\n>\n> 最开始三个RTP包在距离capture time `offset1`时间之后发送到网络，因此这三个RTP包的transmission_time_offset应该是offset1。同理第四个RTP包的transmission_time_offset应该是offset2，第五个RTP包的transmission_time_offset应该是offset3。\n\n- **b. transmission_time_offset在RTP包的哪里放着?**\n\ntransmission_time_offset存在于RTP的扩展头部，设置该扩展头可以参考`RTPSender::SendToNetwork`函数，但使用之前该扩展头之前需要注册，否则在设置transmission_time_offset扩展头会失败。\n\n下面的代码段是WebRTC中`D(i, j)`的计算：\n\n```c++\n// Extended jitter report, RFC 5450.\n// Actual network jitter, excluding the source-introduced jitter.\nint32_t time_diff_samples_ext =\n  (receive_time_rtp - last_receive_time_rtp) -\n  ((header.timestamp +\n    header.extension.transmissionTimeOffset) -\n   (last_received_timestamp_ +\n    last_received_transmission_time_offset_));\n```\n\n其中：\n\n> - `receive_time_rtp` 代表当前RTP的到达时间戳；\n> - `last_receive_time_rtp` 是上一个RTP到达时记录的时间戳；\n> - `header.timestamp + header.extension.transmissionTimeOffset` 前者是capture time，后者是对应的transmission time offset，两者相加代表该RTP实际发送到网络的时间戳；\n> - `last_received_timestamp_ + last_received_transmission_time_offset_` 含义同上，但是代表的是**上一个**RTP的实际发送到网络的时间戳；\n\n#### (2) 下行抖动的更新流程\n\n##### 1) 抖动统计第一阶段\n\n接收端收到的RTP包，会经过`StreamStatisticianImpl::UpdateJitter`函数，该函数内部会计算经过这个RTP包之后的抖动值，并更新到成员`jitter_q4_transmission_time_offset_`成员中。\n\n![抖动统计第一阶段](/images/imageWebRTC/others/抖动统计第一阶段.png)\n\n##### 2) 抖动统计第二阶段\n\n`ModuleRtpRtcpImpl::Process`会定时发送RR，在构建RR的Report Block时，会搜集本地接收报告并把第一阶段保存的`jitter_q4_transmission_time_offset_`信息更新到`ReceiveStatisticsProxy::stats_` 。\n\n![抖动统计第二阶段](/images/imageWebRTC/others/抖动统计第二阶段.png)\n\n#### (3) 下行丢包的更新流程\n\n##### 1) 丢包统计第一阶段\n\n接收端收到的RTP包，会经过`StreamStatisticianImpl::UpdateCounters` 函数，在该函数内部，会累加接收到的RTP包的个数和重传包的个数，以及当前收到的最大的sequence。\n\n##### 2) 丢包统计第二阶段\n\n下图是WebRTC内部计算下行丢包：\n\n![WebRTC内部计算下行丢包](/images/imageWebRTC/others/WebRTC内部计算下行丢包.png)\n\n丢包率更新的周期是发送一次RR，在发送RR时，会根据第一阶段记录的数据统计丢包，丢包根据下面的公式：\n\n`fraction_lost` = `RTP包丢失个数` / `期望接收的RTP包个数`\n\n> 其中：\n>\n> `包丢失个数` = `期望接收的RTP包个数` - `实际收到的RTP包个数`\n>\n> `期望接收的RTP包个数` = `当前最大sequence` - `上次最大sequence`\n>\n> `实际收到的RTP包个数` = `正常有序RTP包` + `重传包`\n\n计算出来的丢包，连同抖动一起被更新到`ReceiveStatisticsProxy::stats_`。\n\n![下行丢包计算](/images/imageWebRTC/others/下行丢包计算.png)\n\n#### (3) 获取下行抖动和丢包\n\n下行抖动和丢包最终会从`ReceiveStatisticsProxy::stats_` 获取。\n\n### 上行抖动和丢包\n\n> 下行抖动和丢包，从对方发来的RR包中获取。RR包格式参考上文链接。\n\n#### (1) 上行抖动和丢包的更新流程\n\n本地上行抖动和丢包，就是对端下行抖动和丢包，对端按照上面介绍的方式计算下行抖动和丢包，然后通过RR返回。\n\n从RR获取抖动和丢包，没有太多阶段，只有一次`推`过程。接收端在收到RR之后，就把内部的抖动和丢包更新到`SendStatisticsProxy::stats_`中，这里就是客户端主动获取上行抖动和丢包时最终的数据源。\n\n#### (2) 获取上行抖动和丢包\n\n上行抖动和丢包最终会从`SendStatisticsProxy::stats_` 获取。\n\n---\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"WebRTC 镜像源","url":"/2019-05-30/reference/webrtc/webrtc-src/","content":"\n# WebRTC 镜像源\n\n> [WebRTC 镜像源](https://webrtc.agora.io/mirror/)\n\n> [webrtc src](http://120.92.49.206:3232/chromiumsrc/webrtc)\n\n> [WebRTC-编译以及运行IOS的Demo](<https://www.jianshu.com/p/1b4c79b45055>)\n>\n> [WebRTC iOS&OSX 库的编译](<http://www.enkichen.com/2017/05/12/webrtc-ios-build/>)\n>\n> [生成WebRTC的DEMO并运行](<https://www.binss.me/blog/build-webrtc-demo-and-run/>)\n>\n> [使用xcode来生成webrtc的Demo](<https://www.binss.me/blog/use-xcode-to-bulid-webrtc-demo/>)\n>\n> [webrtc ios client 源码拉取和编译](<https://blog.csdn.net/liwenlong_only/article/details/79422673>)\n\n> [webrtc视频jitterbuffer原理机制(描述版)](<https://www.jianshu.com/p/bd10d60cebcd>)\n>\n> [jitter buffer QoS的解决方案](https://www.cnblogs.com/lidabo/p/6846548.html)\n>\n> [webrtc中的码率控制](<https://blog.csdn.net/chinabinlang/article/details/78294464?locationNum=7&fps=1>)\n\n> [WebRTC介绍](<https://github.com/bovinphang/WebRTC>)\n>\n> [WebRTC源码解读一](<https://blog.csdn.net/languobeibei/article/details/77447081>)\n>\n> [WebRTC架构简介](<https://blog.csdn.net/fishmai/article/details/69681595>)\n>\n> [C/C++ Linux 程序员必须了解的 10 个工具](<https://blog.csdn.net/temotemo/article/details/7917010>)\n>\n> [C++设计一个类不能偷懒的地方](<https://blog.csdn.net/temotemo/article/details/7763040>)\n>\n> [设计性能良好系统的指导思想](<https://blog.csdn.net/temotemo/article/details/7696215>)\n>\n> [WebRTC实时音视频技术的整体架构介绍](<http://www.52im.net/thread-284-1-1.html>)\n>\n> [WebRTC 开发（二）源码下载与编译](<https://depthlove.github.io/2019/05/02/webrtc-development-2-source-code-download-and-build/>)\n\n# 查看和下载特定版本的webrtc代码\n\n**注**：*这个方法已经不适用了*\n\n`gclient`：如果不知道 gclient 是什么东西 。。。 就别再往下看了。\n\n下载特定版本的代码：\n\n```shell\n$ gclient sync --revision src@31000\n```\n\n**其中31000是版本号**\n\n查看自己下载代码的版本号：\n\n```shell\n$ gclient revinfo -a\nwebrtc@ubuntu:~/code/webrtc/src/talk$ gclient revinfo -a\nsrc: http://webrtc.googlecode.com/svn/trunk@**7706**\nsrc/third_party/gflags/src: http://gflags.googlecode.com/svn/trunk/src@84\nsrc/third_party/junit/:http://webrtc.googlecode.com/svn/deps/third_party/junit@3367\n```\n\n**其中7706是版本号**\n\n如何在官网上浏览特定版本的代码：\n\nhttps://code.google.com/p/webrtc/source/browse/?r=7643\n\n**其中7643是版本号**\n\n```shell\n# 同步第三方依赖库\n$ gclient sync\n```\n\n\n\n\n\n创建xcode的mac 工程\n\n```shell\n$ export GYP_GENERATOR_FLAGS=\"xcode_project_version=7.2 xcode_ninja_target_pattern=All_mac xcode_ninja_executable_target_pattern=AppRTCDemo output_dir=out_mac\"\n$ export GYP_GENERATORS=\"ninja,xcode-ninja\"\n$ ./webrtc/build/gyp_webrtc.py \n```\n\n创建xcode的iOS工程 \n\n```shell\n$ export GYP_GENERATOR_FLAGS=\"xcode_project_version=7.2 xcode_ninja_target_pattern=All_iOS xcode_ninja_executable_target_pattern=AppRTCDemo output_dir=out_ios\"\n$ export GYP_GENERATORS=\"ninja,xcode-ninja\"\n$ ./webrtc/build/gyp_webrtc.py \n```\n\n运行后在 webrtc 根目录下生成 `all.ninja.xcodeproj` 和 `sources_for_indexing.xcodeproj`， 分别用来编译和浏览源代码。\n\n\n\n\n\n> [webrtc技术难点笔记 --- 带github工程](<https://blog.csdn.net/zhangkai19890929/article/details/84590332>)\n>\n> [webrtc代码研究](https://blog.csdn.net/zhangkai19890929/article/category/7955356)\n\nwebrtc工程有点大，自己强攻了一个多月，基本被拖进了无穷无尽多工程结构梳理中。\n\n现在的思路就是：\n\n总结webrtc里面的工程难点，然后到对应的github上去找开源项目，然后一个一个项目的研究，然后再回过头去研究工程.\n\nwebrtc的研究点包括:\n\n1.音视频的网络抖动缓冲策略\n\n2.网络的拥塞处理策略\n\n3.丢包重传策略\n\n4.\n\n对应的开源github工程:\n\n1.video jitter buffer https://github.com/TaoistKing/Video-Jitter-Buffer\n\n介绍video jitter buffer设计原理的文章: https://blog.csdn.net/u012635648/article/details/72953237\n\n2.网络拥塞流控 https://github.com/yuanrongxi/razor\n\n相关文档: https://blog.csdn.net/chinabinlang/article/details/78294464?locationNum=7&fps=1\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"实用的计算机工具库","url":"/2019-05-29/reference/实用的计算机工具库/","content":"\n在我们平时的工作过程中，经常会用到各种工具，每次遇到问题都得各种百度搜索，今天给大家带来几个私藏多年的工具库，有了它，你再也不用到处找工具了，里面包含了大量的使用工具。\n\n## MikuTools\n\n[网站地址](https://miku.tools/)\n\n<!-- more -->\n\n一个轻量的工具集合，里面包含有媒体类，图片类，文字处理类，编程开发类，日常实用类工具，基本覆盖了我们所有的需求，登录后还有更多的隐藏功能。\n\n![mikutools](/images/imageOther/mikutools.png)\n\n## atoolbox\n\n[网站地址](http://www.atoolbox.net)\n\n一个工具箱，里面包含了143个在线工具，包括加密解密，文字编辑，编程开发，单位换算，日期时间，图形图像，金融理财，生活日常等各个门类，应该算是很全面了。\n\n![atoolbox](/images/imageOther/atoolbox.png)\n\n## 在线工具\n\n[网站地址](https://tool.lu/)\n\n程序员必备，里面包含各种常用的开发工具。\n\n![在线工具](/images/imageOther/在线工具.png)\n\n## FrontEndNav\n\n[网站地址](http://nav.web-hub.cn/)\n\n前端程序员必备，收集了大量高质量的前端相关资源。\n\n![frontEndNav](/images/imageOther/frontEndNav.png)","tags":["other"],"categories":["other"]},{"title":"FFmpeg的H.264解码器源代码简单分析","url":"/2019-05-28/reference/FFmpeg/FFmpeg的H.264解码器源代码简单分析/","content":"\n> 文章参考汇总至[雷神笔记](<https://blog.csdn.net/leixiaohua1020/article/details/45536607>)\n\n# 编码 - x264\n\n## 概述\n\n最近正在研究H.264和HEVC的编码方式，因此分析了一下最常见的H.264编码器——x264的源代码。本文简单梳理一下它的结构。X264的源代码量比较大而且涉及到很多的算法，目前还有很多不懂的地方，因此也不能保证分析的完全正确。目前打算先把已经理解的部分整理出来以作备忘。\n\n### 函数调用关系图\n\n<!-- more -->\n\n![X264的函数调用关系图](/images/imageFFmpeg/Thor/X264的函数调用关系图.png)\n\n下面解释一下图中关键标记的含义。\n\n#### 函数背景色\n\n函数在图中以方框的形式表现出来。不同的背景色标志了该函数不同的作用：\n\n- **白色背景的函数**：不加区分的普通内部函数。\n- **浅红背景的函数**：libx264类库的接口函数（API）。\n- **粉红色背景函数**：滤波函数（Filter）。用于环路滤波，半像素插值，SSIM/PSNR的计算。\n- **黄色背景函数**：分析函数（Analysis）。用于帧内预测模式的判断，或者帧间预测模式的判断。\n- **绿色背景的函数**：宏块编码函数（Encode）。通过对残差的DCT变换、量化等方式对宏块进行编码。\n- **紫色背景的函数**：熵编码函数（Entropy Coding）。对宏块编码后的数据进行CABAC或者CAVLC熵编码。\n- **蓝色背景函数**：汇编函数（Assembly）。做过汇编优化的函数。图中主要画出了这些函数的C语言版本，此外这些函数还包含MMX版本、SSE版本、NEON版本等。\n\n- **浅蓝色背景函数**：码率控制函数（Rate Control）。对码率进行控制的函数。具体的方法包括了ABR、CBR、CRF等。\n\n#### 区域\n\n整个关系图可以分为以下几个区域：\n\n- **最左边区域**——x264命令行程序函数区域。\n- **左边中间区域**——libx264内部函数区域。\n- **右上方粉红色区域**——滤波模块。其中包括了环路滤波，半像素插值，SSIM/PSNR计算。\n- **右上方黄色区域**——分析模块。其中包含了帧内预测模式分析以及帧间运动估计等。\n- **右中间绿色区域**——宏块编码模块。其中包含了针对编码帧的DCT变换，量化，Hadamard变换等；以及针对重建帧的DCT反变换，反量化，Hadamard反变换等。\n- **右下方紫色区域**——熵编码模块。其中包含了CABAC或者CAVLC熵编码。\n\n#### 箭头线\n\n箭头线标志了函数的调用关系：\n\n- **黑色箭头线**：不加区别的调用关系。\n- **粉红色的箭头线**：滤波函数（Filter）之间的调用关系。\n- **黄色箭头线**：分析函数（Analysis）之间的调用关系。\n- **绿色箭头线**：宏块编码函数（Encode）之间的调用关系。\n- **紫色箭头线**：熵编码函数（Entropy Coding）之间的调用关系。\n\n#### 函数所在的文件\n\n每个函数标识了它所在的文件路径。\n\n### 几个关键的部分\n\n下文简单记录图中几个关键的部分。\n\n#### x264命令行程序\n\nx264命令行程序指的是x264项目提供的控制台程序。通过这个程序可以调用libx264编码YUV为H.264码流。该程序的入口函数为 `main()`。`main()` 函数首先调用 `parse()` 解析输入的参数，然后调用 `encode()` 编码YUV数据。\n\n<font style=\"color:red;\">**parse()**</font>首先调用 `x264_param_default()` 为保存参数的 `x264_param_t` 结构体赋默认值；然后在一个大循环中通过 `getopt_long()` 解析通过命令行传递来的存储在 `argv[]` 中的参数，并作相应的设置工作；最后调用 `select_input()` 和 `select_output()` 完成输入文件格式（yuv，y4m等）和输出文件格式（裸流，mp4，mkv，FLV等）的设置。\n\n<font style=\"color:red;\">**encode()**</font>首先调用 `x264_encoder_open()` 打开编码器；接着在一个循环中反复调用 `encode_frame()` 一帧一帧地进行编码；最后在编码完成后调用 `x264_encoder_close()` 关闭编码器。\n\n<font style=\"color:red;\">**encode_frame()**</font>则调用 `x264_encoder_encode()` 将存储YUV数据的 `x264_picture_t` 编码为存储H.264数据的 `x264_nal_t`。\n\n#### <font style=\"color:#ff6600;\">libx264类库的接口</font>\n\n在一个x264编码流程中，至少需要调用如下API函数（参考文章《[最简单的视频编码器：基于libx264（编码YUV为H.264）](http://blog.csdn.net/leixiaohua1020/article/details/42078645)》）：\n\n```c\nx264_param_default() \t// 设置参数集结构体x264_param_t的缺省值。\nx264_picture_alloc() \t// 为图像结构体x264_picture_t分配内存。\nx264_encoder_open() \t// 打开编码器。\nx264_encoder_encode()\t// 编码一帧图像。\nx264_encoder_close()\t// 关闭编码器。\nx264_picture_clean()\t// 释放x264_picture_alloc()申请的资源。\n```\n\n#### libx264主干函数\n\nlibx264主干函数指的是编码API之后，`x264_slice_write()` 之前的函数。这一部分函数较多，暂时不详细分析，仅仅举几个例子列一下它们的功能。\n\n```c\nx264_encoder_open() \t\t// 调用了下面的函数：\nx264_validate_parameters()\t// 检查输入参数（例如输入图像的宽高是否为正数）。\nx264_predict_16x16_init()\t// 初始化Intra16x16帧内预测汇编函数。\nx264_predict_4x4_init()\t\t// 初始化Intra4x4帧内预测汇编函数。\nx264_pixel_init()\t\t\t// 初始化像素值计算相关的汇编函数（包括SAD、SATD、SSD等）。\nx264_dct_init()\t\t\t\t// 初始化DCT变换和DCT反变换相关的汇编函数。\nx264_mc_init() \t\t\t\t// 初始化运动补偿相关的汇编函数。\nx264_quant_init()\t\t\t// 初始化量化和反量化相关的汇编函数。\nx264_deblock_init()\t\t\t// 初始化去块效应滤波器相关的汇编函数。\nx264_lookahead_init()\t\t// 初始化Lookahead相关的变量。\nx264_ratecontrol_new()\t\t// 初始化码率控制模块。\n```\n\n`x264_encoder_headers()` 调用了下面的函数：\n\n```c\nx264_sps_write()\t\t\t// 输出SPS\nx264_pps_write()\t\t\t// 输出PPS\nx264_sei_version_write()\t// 输出SEI\n```\n\nx264_encoder_encode()调用了下面的函数：\n\n```c\nx264_frame_pop_unused()\t// 获取1个x264_frame_t类型结构体fenc。如果frames.unused[]队列不为空，就调用x264_frame_pop()从unused[]队列取1个现成的；否则就调用x264_frame_new()创建一个新的。\nx264_frame_copy_picture()\t// 将输入的图像数据拷贝至fenc。\nx264_lookahead_put_frame()\t// 将fenc放入lookahead.next.list[]队列，等待确定帧类型。\nx264_lookahead_get_frames()\t// 通过lookahead分析帧类型。该函数调用了x264_slicetype_decide()，x264_slicetype_analyse()和x264_slicetype_frame_cost()等函数。经过一些列分析之后，最终确定了帧类型信息，并且将帧放入frames.current[]队列。\nx264_frame_shift()\t// 从frames.current[]队列取出一帧用于编码。\nx264_reference_update() // 更新参考帧列表。\nx264_reference_reset() // 如果为IDR帧，调用该函数清空参考帧列表。\nx264_reference_hierarchy_reset() // 如果是I（非IDR帧）、P帧、B帧（可做为参考帧），调用该函数（还没研究）。\nx264_reference_build_list() // 创建参考帧列表list0和list1。\nx264_ratecontrol_start() // 开启码率控制。\nx264_slice_init() // 创建 Slice Header。\nx264_slices_write() // 编码数据（最关键的步骤）。其中调用了x264_slice_write()完成了编码的工作（注意“x264_slices_write()”和“x264_slice_write()”名字差了一个“s”）。\nx264_encoder_frame_end() // 编码结束后做一些后续处理，例如释放一些中间变量以及打印输出一些统计信息。其中调用了x264_frame_push_unused()将fenc重新放回frames.unused[]队列，并且调用x264_ratecontrol_end()关闭码率控制。\n```\n\n#### x264_slice_write()\n\n`x264_slice_write()` 用于编码 Slice。该函数中包含了一个很长的 `for()` 循环。该循环每执行一遍编码一个宏块。`x264_slice_write()` 中以下几个函数比较重要：\n\n```c\nx264_nal_start() \t// 开始写一个NALU。\nx264_macroblock_thread_init() // 初始化存储宏块的重建数据缓存fdec_buf[]和编码数据缓存fenc_buf[]。\nx264_slice_header_write() \t// 输出 Slice Header。\nx264_fdec_filter_row() \t\t// 滤波模块。该模块包含了环路滤波，半像素插值，SSIM/PSNR的计算。\nx264_macroblock_cache_load() \t// 将要编码的宏块的周围的宏块的信息读进来。\nx264_macroblock_analyse()\t\t// 分析模块。该模块包含了帧内预测模式分析以及帧间运动估计等。\nx264_macroblock_encode() // 宏块编码模块。该模块通过对残差的DCT变换、量化等方式对宏块进行编码。\nx264_macroblock_write_cabac() // CABAC熵编码模块。\nx264_macroblock_write_cavlc() // CAVLC熵编码模块。\nx264_macroblock_cache_save() // 保存当前宏块的信息。\nx264_ratecontrol_mb() // 码率控制。\nx264_nal_end() // 结束写一个NALU。\n```\n\n#### <font style=\"color:#ffcccc;\">滤波模块</font>\n\n滤波模块对应的函数是 `x264_fdec_filter_row()`。该函数完成了环路滤波，半像素插值，`SSIM/PSNR` 的计算的功能。该函数调用了以下及个比较重要的函数：\n\n```c\nx264_frame_deblock_row()\t// 去块效应滤波器。\nx264_frame_filter()\t\t\t// 半像素插值。\nx264_pixel_ssd_wxh()\t\t// PSNR计算。\nx264_pixel_ssim_wxh()\t\t// SSIM计算。\n```\n\n#### <font style=\"color:#ffcc33;\">分析模块</font>\n\n分析模块对应的函数是 `x264_macroblock_analyse()`。该函数包含了帧内预测模式分析以及帧间运动估计等。该函数调用了以下比较重要的函数（只列举了几个有代表性的函数）：\n\n```c\nx264_mb_analyse_init()\t\t\t// Analysis模块初始化。\nx264_mb_analyse_intra()\t\t\t// I 宏块帧内预测模式分析。\nx264_macroblock_probe_pskip()\t// 分析是否是skip模式。\nx264_mb_analyse_inter_p16x16()\t// P16x16宏块帧间预测模式分析。\nx264_mb_analyse_inter_p8x8()\t// P8x8宏块帧间预测模式分析。\nx264_mb_analyse_inter_p16x8()\t// P16x8宏块帧间预测模式分析。\nx264_mb_analyse_inter_b16x16()\t// B16x16宏块帧间预测模式分析。\nx264_mb_analyse_inter_b8x8()\t// B8x8宏块帧间预测模式分析。\nx264_mb_analyse_inter_b16x8()\t// B16x8宏块帧间预测模式分析。\n```\n\n#### <font style=\"color:#33cc00;\">宏块编码模块</font>\n\n宏块编码模块对应的函数是 `x264_macroblock_encode()`。该模块通过对残差的 DCT 变换、量化等方式对宏块进行编码。对于 `Intra16x16` 宏块，调用 `x264_mb_encode_i16x16()` 进行编码，对于 `Intra4x4`，调用 `x264_mb_encode_i4x4()` 进行编码。对于Inter类型的宏块则直接在函数体里面编码。\n\n#### <font style=\"color:#cc33cc;\">熵编码模块</font>\n\nCABAC 熵编码对应的函数是 `x264_macroblock_write_cabac()`。CAVLC 熵编码对应的函数是 `x264_macroblock_write_cavlc()`。`x264_macroblock_write_cavlc()` 调用了以下几个比较重要的函数：\n\n```c\nx264_cavlc_mb_header_i()\t\t// 写入I宏块MB Header数据。包含帧内预测模式等。\nx264_cavlc_mb_header_p()\t\t// 写入P宏块MB Header数据。包含MVD、参考帧序号等。\nx264_cavlc_mb_header_b()\t\t// 写入B宏块MB Header数据。包含MVD、参考帧序号等。\nx264_cavlc_qp_delta()\t\t\t// 写入QP。\nx264_cavlc_block_residual()\t\t// 写入残差数据。\n```\n\n#### <font style=\"color:#66cccc;\">码率控制模块</font>\n\n码率控制模块函数分布在x264源代码不同的地方，包含了以下几个比较重要的函数：\n\n```c\nx264_encoder_open() \t中的 x264_ratecontrol_new()\t\t// 创建码率控制。\nx264_encoder_encode() \t中的 x264_ratecontrol_start()\t\t// 开始码率控制。\nx264_slice_write()\t\t中的 x264_ratecontrol_mb()\t\t// 码率控制算法。\nx264_encoder_encode()\t中的 x264_ratecontrol_end()\t\t// 结束码率控制。\nx264_encoder_close()\t中的 x264_ratecontrol_summary()\t// 码率控制信息。\nx264_encoder_close()\t中的 x264_ratecontrol_delete()\t// 释放码率控制。\n```\n\n## x264命令行工具\n\n该命令行工具可以调用 libx264 将 YUV 格式像素数据编码为 H.264 码流。\n\n### 函数调用关系图\n\n![X264命令行工具的源代码的调用关系](/images/imageFFmpeg/Thor/X264命令行工具的源代码的调用关系.png)\n\n从图中可以看出，X264命令行工具调用了libx264的几个API完成了H.264编码工作。使用libx264的API进行编码可以参考《[最简单的视频编码器：基于libx264（编码YUV为H.264）](http://blog.csdn.net/leixiaohua1020/article/details/42078645)》，这个流程中最关键的API包括：\n\n```c\nx264_param_default()\t// 设置参数集结构体x264_param_t的缺省值。\nx264_encoder_open()\t\t// 打开编码器。\nx264_encoder_headers()\t// 输出SPS，PPS，SEI等信息。\nx264_encoder_encode()\t// 编码输出一帧图像。\nx264_encoder_close()\t// 关闭编码器。\n```\n\n在X264命令行工具中，`main()` 首先调用 `parse()` 解析输入的命令行参数，然后调用 `encode()` 进行编码。 \n\n`parse()` 首先调用 `x264_param_default()` 为存储参数的结构体 `x264_param_t` 赋默认值；然后在一个大循环中调用 `getopt_long()` 逐个解析输入的参数，并作相应的处理；最后调用 `select_input()` 和 `select_output()` 解析输入文件格式（例如yuv，y4m…）和输出文件格式（例如raw，flv，MP4…）。\n\n`encode()` 首先调用 `x264_encoder_open()` 打开H.264编码器，然后调用 `x264_encoder_headers()` 输出H.264码流的头信息（例如SPS、PPS、SEI），接着进入一个循环并且调用 `encode_frame()` 逐帧编码视频，最后调用 `x264_encoder_close()` 关闭解码器。其中 `encode_frame()` 中又调用了 `x264_encoder_encode()` 完成了具体的编码工作。下文将会对上述流程展开分析。\n\n#### main()\n\n`main()` 的定义很简单，它主要调用了两个函数：`parse()` 和 `encode()` 。`main()` 首先调用 `parse()` 解析输入的命令行参数，然后调用 `encode()` 进行编码。下面分别分析这两个函数。\n\n#### parse()\n\n`parse()` 用于解析命令行输入的参数（存储于 `argv[]` 中）\n\n下面简单梳理 `parse()` 的流程：\n\n（1）调用 `x264_param_default()` 为存储参数的结构体 `x264_param_t` 赋默认值\n\n（2）调用 `x264_param_default_preset()` 为 `x264_param_t` 赋值\n\n（3）在一个大循环中调用 `getopt_long()` 逐个解析输入的参数，并作相应的处理。举几个例子：\n\n- a) “-h”：调用 `help()` 打开帮助菜单。\n- b) “-V” 调用 `print_version_info()` 打印版本信息。\n- c)对于长选项，调用 `x264_param_parse()` 进行处理。\n\n（4）调用 `select_input()` 解析输出文件格式（例如raw，flv，MP4…）\n\n（5）调用 `select_output()` 解析输入文件格式（例如yuv，y4m…）\n\n下文按照顺序记录parse()中涉及到的函数：\n\n```c\nx264_param_default()\nx264_param_default_preset()\nhelp()\nprint_version_info()\nx264_param_parse()\nselect_input()\nselect_output()\n```\n\n`x264_param_default()` 是一个x264的API。该函数用于设置x264中 `x264_param_t` 结构体的默认值。\n\n`x264_param_default_preset()` 是一个 libx264 的 API，用于设置 x264 的 preset 和 tune。\n\n从源代码可以看出，`x264_param_default_preset()` 调用 `x264_param_apply_preset()` 设置 preset，调用 `x264_param_apply_tune()` 设置 tune。记录一下这两个函数。\n\n`help()` 用于打印帮助菜单。在 x264 命令行程序中添加 “-h” 参数后会调用该函数。\n\n`print_version_info()` 用于打印 x264 的版本信息。在x264命令行程序中添加 “-V” 参数后会调用该函数。\n\n`x264_param_parse()` 是一个 x264 的 API。该函数以字符串键值对的方式设置 `x264_param_t` 结构体的一个成员变量。\n\n`x264_param_parse()` 中判断参数的宏 `OPT()` 和 `OPT2()` 实质上就是 `strcmp()`。由此可见该函数的流程首先是调用 `strcmp()` 判断当前输入参数的名称 name，然后再调用 `atoi()`，`atof()`，或者 `atobool()` 等将当前输入参数值 value 转换成相应类型的值并赋值给对应的参数。\n\n`x264_param_apply_profile()` 是一个 x264 的 API。该函数用于设置 x264 的 profile\n\n`select_output()` 用于设定输出的文件格式。\n\n`select_input()` 用于设定输入的文件格式。\n\n#### encode()\n\n`encode()` 编码 YUV 为 H.264 码流\n\n从源代码可以梳理出来 `encode()` 的流程：\n\n（1）调用 `x264_encoder_open()` 打开 H.264 编码器。\n\n（2）调用 `x264_encoder_parameters()` 获得当前的参数集 `x264_param_t`，用于后续步骤中的一些配置。\n\n（3）调用输出格式（H.264裸流、FLV、mp4等）对应 `cli_output_t` 结构体的 `set_param()` 方法，为输出格式的封装器设定参数。其中参数源自于上一步骤得到的 `x264_param_t`。\n\n（4）如果不是在每个keyframe前面都增加 SPS/PPS/SEI 的话，就调用 `x264_encoder_headers()` 在整个码流前面加 SPS/PPS/SEI。\n\n（5）进入一个循环中进行一帧一帧的将 YUV 编码为 H.264：\n\n- a)调用输入格式（YUV、Y4M等）对应的 `cli_vid_filter_t` 结构体 `get_frame()` 方法，获取一帧YUV数据。\n- b)调用 `encode_frame()` 编码该帧YUV数据为H.264数据，并且输出出来。该函数内部调用`x264_encoder_encode()` 完成编码工作，调用输出格式对应 `cli_output_t` 结构体的 `write_frame()` 完成了输出工作。\n- c)调用输入格式（YUV、Y4M等）对应的 `cli_vid_filter_t` 结构体 `release_frame()` 方法，释放刚才获取的 YUV 数据。\n- d)调用 `print_status()` 输出一些统计信息。\n\n（6）编码即将结束的时候，进入另一个循环，输出编码器中缓存的视频帧：\n\n- a)不再传递新的YUV数据，直接调用 `encode_frame()`，将编码器中缓存的剩余几帧数据编码输出出来。\n- b)调用 `print_status()` 输出一些统计信息。\n\n（7）调用 `x264_encoder_close()` 关闭 H.264 编码器。\n\n`encode()` 的流程中涉及到 libx264 的几个关键的 API：\n\n```c\nx264_encoder_open()\t\t// 打开H.264编码器。\nx264_encoder_headers()\t// 输出SPS/PPS/SEI。\nx264_encoder_encode()\t// 编码一帧数据。\nx264_encoder_close()\t// 关闭H.264编码器。\n```\n\n此外上述流程中涉及到两个比较简单的函数：`encode_frame()` 和 `print_status()`。其中 `encode_frame()` 用于编码一帧数据，而 `print_status()` 用于输出一帧数据编码后的统计信息。下文记录一下这两个函数的定义。\n\n`encode_frame()` 内部调用 `x264_encoder_encode()` 完成编码工作，调用输出格式对应 `cli_output_t` 结构体的 `write_frame() `完成了输出工作。\n\nprint_status()的代码不再详细记录，它的输出效果如下图中红框中的文字。\n\n![print_status输出效果](/images/imageFFmpeg/Thor/print_status.png)\n\n### X264 控制台程序中和输入输出相关的结构体\n\n在x264控制台程序中有3个和输入输出相关的结构体：\n\n```c\ncli_output_t\t\t// 输出格式对应的结构体。输出格式一般为H.264裸流、FLV、MP4等。\ncli_input_t\t\t\t// 输入格式对应的结构体。输入格式一般为纯YUV像素数据，Y4M格式数据等。\ncli_vid_filter_t\t// 输入格式滤镜结构体。滤镜可以对输入数据做一些简单的处理，例如拉伸、裁剪等等（当然滤镜也可以不作任何处理，直接读取输入数据）。\n```\n\n在 x264 的编码过程中，调用 `cli_vid_filter_t` 结构体的 `get_frame()` 读取 YUV 数据，调用 `cli_output_t` 的 `write_frame()` 写入数据。\n\n## 编码器主干部分\n\n“主干部分”指的就是libx264中最核心的接口函数—— `x264_encoder_encode()` ，以及相关的几个接口函数`x264_encoder_open()`，`x264_encoder_headers()`，和 `x264_encoder_close()`。\n\n### 函数调用关系图\n\n![X264编码器主干部分的函数调用关系](/images/imageFFmpeg/Thor/X264编码器主干部分的函数调用关系.png)\n\n从图中可以看出，x264 主干部分最复杂的函数就是 `x264_encoder_encode()`，该函数完成了编码一帧 YUV 为H.264 码流的工作。与之配合的还有打开编码器的函数 `x264_encoder_open()`，关闭编码器的函数 `x264_encoder_close()`，以及输出 SPS/PPS/SEI 这样的头信息的 `x264_encoder_headers()`。\n\n`x264_encoder_open()` 用于打开编码器，其中初始化了 libx264 编码所需要的各种变量。它调用了下面的函数：\n\n```c\nx264_validate_parameters()\t// 检查输入参数（例如输入图像的宽高是否为正数）。\nx264_predict_16x16_init()\t// 初始化Intra16x16帧内预测汇编函数。\nx264_predict_4x4_init()\t\t// 初始化Intra4x4帧内预测汇编函数。\nx264_pixel_init()\t\t\t// 初始化像素值计算相关的汇编函数（包括SAD、SATD、SSD等）。\nx264_dct_init()\t\t\t\t// 初始化DCT变换和DCT反变换相关的汇编函数。\nx264_mc_init()\t\t\t\t// 初始化运动补偿相关的汇编函数。\nx264_quant_init()\t\t\t// 初始化量化和反量化相关的汇编函数。\nx264_deblock_init()\t\t\t// 初始化去块效应滤波器相关的汇编函数。\nx264_lookahead_init()\t\t// 初始化Lookahead相关的变量。\nx264_ratecontrol_new()\t\t// 初始化码率控制相关的变量。\n```\n\n`x264_encoder_headers()` 输出 SPS/PPS/SEI 这些 H.264 码流的头信息。它调用了下面的函数：\n\n```c\nx264_sps_write()\t\t\t// 输出SPS\nx264_pps_write()\t\t\t// 输出PPS\nx264_sei_version_write()\t// 输出SEI\n```\n\n`x264_encoder_encode()` 编码一帧 YUV 为 H.264 码流。它调用了下面的函数：\n\n```c\nx264_frame_pop_unused()\t// 获取1个x264_frame_t类型结构体fenc。如果frames.unused[]队列不为空，就调用x264_frame_pop()从unused[]队列取1个现成的；否则就调用x264_frame_new()创建一个新的。\nx264_frame_copy_picture() // 将输入的图像数据拷贝至fenc。\nx264_lookahead_put_frame()  // 将fenc放入lookahead.next.list[]队列，等待确定帧类型。\nx264_lookahead_get_frames() // 通过lookahead分析帧类型。该函数调用了x264_slicetype_decide()，x264_slicetype_analyse()和x264_slicetype_frame_cost()等函数。经过一些列分析之后，最终确定了帧类型信息，并且将帧放入frames.current[]队列。\nx264_frame_shift() // 从frames.current[]队列取出1帧用于编码。\nx264_reference_update() // 更新参考帧列表。\nx264_reference_reset() // 如果为IDR帧，调用该函数清空参考帧列表。\nx264_reference_hierarchy_reset() // 如果是I（非IDR帧）、P帧、B帧（可做为参考帧），调用该函数。\nx264_reference_build_list() // 创建参考帧列表list0和list1。\nx264_ratecontrol_start() // 开启码率控制。\nx264_slice_init() // 创建 Slice Header。\nx264_slices_write() // 编码数据（最关键的步骤）。其中调用了x264_slice_write()完成了编码的工作（注意“x264_slices_write()”和“x264_slice_write()”名字差了一个“s”）。\nx264_encoder_frame_end() // 编码结束后做一些后续处理，例如记录一些统计信息。其中调用了x264_frame_push_unused()将fenc重新放回frames.unused[]队列，并且调用x264_ratecontrol_end()关闭码率控制。\n```\n\n`x264_encoder_close()` 用于关闭解码器，同时输出一些统计信息。它调用了下面的函数：\n\n```c\nx264_lookahead_delete()\t\t// 释放Lookahead相关的变量。\nx264_ratecontrol_summary()\t// 汇总码率控制信息。\nx264_ratecontrol_delete()\t// 关闭码率控制。\n```\n\n#### x264_encoder_open()\n\n`x264_encoder_open()` 是一个 libx264 的 API。该函数用于打开编码器，其中初始化了 libx264 编码所需要的各种变量。\n\n根据函数调用的顺序，看一下 `x264_encoder_open()` 调用的下面几个函数：\n\n```c\nx264_sps_init()\t\t\t\t// 根据输入参数生成H.264码流的SPS信息。\nx264_pps_init()\t\t\t\t// 根据输入参数生成H.264码流的PPS信息。\nx264_predict_16x16_init()\t// 初始化Intra16x16帧内预测汇编函数。\nx264_predict_4x4_init()\t\t// 初始化Intra4x4帧内预测汇编函数。\nx264_pixel_init()\t\t\t// 初始化像素值计算相关的汇编函数（包括SAD、SATD、SSD等）。\nx264_dct_init()\t\t\t\t// 初始化DCT变换和DCT反变换相关的汇编函数。\nx264_mc_init()\t\t\t\t// 初始化运动补偿相关的汇编函数。\nx264_quant_init()\t\t\t// 初始化量化和反量化相关的汇编函数。\nx264_deblock_init()\t\t\t// 初始化去块效应滤波器相关的汇编函数。\nmbcmp_init()\t\t\t\t// 决定像素比较的时候使用SAD还是SATD。\n```\n\n#### 相关知识简述\n\n简单记录一下帧内预测的方法。帧内预测根据宏块左边和上边的边界像素值推算宏块内部的像素值，帧内预测的效果如下图所示。其中左边的图为图像原始画面，右边的图为经过帧内预测后没有叠加残差的画面。\n\n![帧内预测-01](/images/imageFFmpeg/Thor/帧内预测-01.png)\n\nH.264 中有两种帧内预测模式：`16x16` 亮度帧内预测模式和 `4x4` 亮度帧内预测模式。其中 `16x16` 帧内预测模式一共有 4 种，如下图所示。\n\n![帧内预测-02](/images/imageFFmpeg/Thor/帧内预测-02.png)\n\n这 4 种模式列表如下。\n\n| 模式       | 描述                                 |\n| ---------- | ------------------------------------ |\n| Vertical   | 由上边像素推出相应像素值             |\n| Horizontal | 由左边像素推出相应像素值             |\n| DC         | 由上边和左边像素平均值推出相应像素值 |\n| Plane      | 由上边和左边像素推出相应像素值       |\n\n`4x4` 帧内预测模式一共有 9 种，如下图所示。\n\n![帧内预测-03](/images/imageFFmpeg/Thor/帧内预测-03.png)\n\n简单记录几个像素计算中的概念。SAD 和 SATD 主要用于帧内预测模式以及帧间预测模式的判断。有关 SAD、SATD、SSD 的定义如下：\n\n> SAD（Sum of Absolute Difference）也可以称为SAE（Sum of Absolute Error），即绝对误差和。它的计算方法就是求出两个像素块对应像素点的差值，将这些差值分别求绝对值之后再进行累加。\n>\n> SATD（Sum of Absolute Transformed Difference）即Hadamard变换后再绝对值求和。它和SAD的区别在于多了一个“变换”。\n>\n> SSD（Sum of Squared Difference）也可以称为SSE（Sum of Squared Error），即差值的平方和。它和SAD的区别在于多了一个“平方”。\n\nH.264中使用SAD和SATD进行宏块预测模式的判断。早期的编码器使用SAD进行计算，近期的编码器多使用SATD进行计算。为什么使用SATD而不使用SAD呢？关键原因在于编码之后码流的大小是和图像块DCT变换后频域信息紧密相关的，而和变换前的时域信息关联性小一些。SAD只能反应时域信息；SATD却可以反映频域信息，而且计算复杂度也低于DCT变换，因此是比较合适的模式选择的依据。\n\n使用SAD进行模式选择的示例如下所示。下面这张图代表了一个普通的 `Intra16x16` 的宏块的像素。它的下方包含了使用Vertical，Horizontal，DC和Plane四种帧内预测模式预测的像素。通过计算可以得到这几种预测像素和原始像素之间的SAD（SAE）分别为3985，5097，4991，2539。由于Plane模式的SAD取值最小，由此可以断定Plane模式对于这个宏块来说是最好的帧内预测模式。\n\n![帧内预测-04](/images/imageFFmpeg/Thor/帧内预测-04.png)\n\n![帧内预测-05](/images/imageFFmpeg/Thor/帧内预测-05.png)\n\n简单记录一下DCT相关的知识。DCT变换的核心理念就是把图像的低频信息（对应大面积平坦区域）变换到系数矩阵的左上角，而把高频信息变换到系数矩阵的右下角，这样就可以在压缩的时候（量化）去除掉人眼不敏感的高频信息（位于矩阵右下角的系数）从而达到压缩数据的目的。二维 `8x8` DCT变换常见的示意图如下所示。\n\n![帧内预测-06](/images/imageFFmpeg/Thor/帧内预测-06.png)\n\n早期的DCT变换都使用了 `8x8` 的矩阵（变换系数为小数）。在 H.264 标准中新提出了一种 `4x4` 的矩阵。这种 `4x4` DCT变换的系数都是整数，一方面提高了运算的准确性，一方面也利于代码的优化。`4x4` 整数DCT变换的示意图如下所示（作为对比，右侧为 `4x4` 块的Hadamard变换的示意图）。\n\n![帧内预测-07](/images/imageFFmpeg/Thor/帧内预测-07.png)\n\n简单记录一下半像素插值的知识。《H.264标准》中规定，运动估计为 `1/4` 像素精度。因此在H.264编码和解码的过程中，需要将画面中的像素进行插值——简单地说就是把原先的 1 个像素点拓展成 `4x4` 一共16个点。下图显示了H.264编码和解码过程中像素插值情况。可以看出原先的 G 点的右下方通过插值的方式产生了a、b、c、d等一共 16 个点。\n\n![帧内预测-08](/images/imageFFmpeg/Thor/帧内预测-08.png)\n\n如图所示，`1/4` 像素内插一般分成两步：\n\n（1）半像素内插。这一步通过 6 抽头滤波器获得 5 个半像素点。\n\n（2）线性内插。这一步通过简单的线性内插获得剩余的 `1/4` 像素点。\n\n图中半像素内插点为 b、m、h、s、j 五个点。半像素内插方法是对整像素点进行 6 抽头滤波得出，滤波器的权重为( `1/32, -5/32, 5/8, 5/8, -5/32, 1/32` )。例如 b 的计算公式为：\n\n**`b=round( (E - 5F + 20G + 20H - 5I + J ) / 32)`**\n\n剩下几个半像素点的计算关系如下：\n\n```shell\nm：由B、D、H、N、S、U计算\nh：由A、C、G、M、R、T计算\ns：由K、L、M、N、P、Q计算\nj：由cc、dd、h、m、ee、ff计算。需要注意j点的运算量比较大，因为cc、dd、ee、ff都需要通过半像素内插方法进行计算。\n```\n\n在获得半像素点之后，就可以通过简单的线性内插获得 `1/4` 像素内插点了。`1/4` 像素内插的方式如下图所示。例如图中 a 点的计算公式如下：\n\n**`A=round( (G+b)/2 )`**\n\n在这里有一点需要注意：位于 4 个角的e、g、p、r 四个点并不是通过 j 点计算计算的，而是通过b、h、s、m四个半像素点计算的。\n\n![帧内预测-09](/images/imageFFmpeg/Thor/帧内预测-09.png)\n\n#### x264_encoder_headers()\n\n`x264_encoder_headers()` 是libx264的一个API函数，用于输出 SPS/PPS/SEI 这些 H.264 码流的头信息。\n\n#### x264_encoder_close()\n\n`x264_encoder_close()` 是libx264的一个API函数。该函数用于关闭编码器，同时输出一些统计信息。\n\n#### x264_encoder_encode()\n\n`x264_encoder_encode()` 是libx264的API函数，用于编码一帧 YUV 为 H.264 码流。\n\n`x264_encoder_encode()` 的流程大致如下：\n\n（1）调用 `x264_frame_pop_unused` 获取一个空的 `fenc`（x264_frame_t类型）用于存储一帧编码像素数据。\n\n（2）调用 `x264_frame_copy_picture()` 将外部结构体的 `pic_in`（`x264_picture_t`类型）的数据拷贝给内部结构体的 `fenc`（`x264_frame_t` 类型）。\n\n（3）调用 `x264_lookahead_put_frame()` 将 `fenc` 放入 Lookahead 模块的队列中，等待确定帧类型。\n\n（4）调用 `x264_lookahead_get_frames()` 分析 Lookahead 模块中一个帧的帧类型。分析后的帧保存在`frames.current[]` 中。\n\n（5）调用 `x264_frame_shift()` 从 `frames.current[]` 中取出分析帧类型之后的 `fenc`。\n\n（6）调用 `x264_reference_update()` 更新参考帧队列 `frames.reference[]`。\n\n（7）如果编码帧 `fenc` 是 `IDR` 帧，调用 `x264_reference_reset()` 清空参考帧队列 `frames.reference[]`。\n\n（8）调用 `x264_reference_build_list()` 创建参考帧列表 `List0` 和 `List1`。\n\n（9）根据选项做一些配置：\n\n- a) 如果 `b_aud` 不为 0，输出 AUD 类型 NALU\n- b) 在当前帧是关键帧的情况下，如果 `b_repeat_headers` 不为 0，调用 `x264_sps_write()` 和 `x264_pps_write()` 输出 SPS 和 PPS。\n- c) 输出一些特殊的 SEI 信息，用于适配各种解码器。\n\n（10）调用 `x264_slice_init()` 初始化 Slice Header 信息。\n\n（11）调用 `x264_slices_write()` 进行编码。该部分是 libx264 的核心，在后续文章中会详细分析。\n\n（12）调用 `x264_encoder_frame_end()` 做一些编码后的后续处理。\n\n`x264_slice_write()` 是完成编码工作的函数。该函数中包含了去块效应滤波，运动估计，宏块编码，熵编码等模块。\n\n## x264_slice_write()\n\n`x264_slice_write()` 是 x264 项目的核心，它完成了编码了一个 Slice 的工作。根据功能的不同，该函数可以分为滤波（Filter），分析（Analysis），宏块编码（Encode）和熵编码（Entropy Encoding）几个子模块。\n\n### 函数调用关系图\n\n![x264_slice_write](/images/imageFFmpeg/Thor/x264_slice_write.png)\n\nx264_slice_write()调用了如下函数：\n\n```c\nx264_nal_start()\t// 开始写一个NALU。\nx264_macroblock_thread_init() // 初始化宏块重建数据缓存fdec_buf[]和编码数据缓存fenc_buf[]。\nx264_slice_header_write()\t// 输出 Slice Header。\nx264_fdec_filter_row()\t// 滤波模块。该模块包含了环路滤波，半像素插值，SSIM/PSNR的计算。\nx264_macroblock_cache_load() \t// 将要编码的宏块的周围的宏块的信息读进来。\nx264_macroblock_analyse()\t// 分析模块。该模块包含了帧内预测模式分析以及帧间运动估计等。\nx264_macroblock_encode()\t// 宏块编码模块。该模块通过对残差的DCT变换、量化等方式对宏块进行编码。\nx264_macroblock_write_cabac()\t// CABAC熵编码模块。\nx264_macroblock_write_cavlc()\t// CAVLC熵编码模块。\nx264_macroblock_cache_save()\t// 保存当前宏块的信息。\nx264_ratecontrol_mb()\t// 码率控制。\nx264_nal_end() // 结束写一个NALU。\n```\n\n根据源代码简单梳理了 `x264_slice_write()` 的流程，如下所示：\n\n（1）调用 `x264_nal_start()` 开始输出一个 NALU。\n\n（2）`x264_macroblock_thread_init()`：初始化宏块重建像素缓存 `fdec_buf[]` 和编码像素缓存 `fenc_buf[]`。\n\n（3）调用 `x264_slice_header_write()` 输出 Slice Header。\n\n（4）进入一个循环，该循环每执行一遍编码一个宏块：\n\n- a) 每处理一行宏块，调用一次 `x264_fdec_filter_row()` 执行滤波模块。\n- b) 调用 `x264_macroblock_cache_load_progressive()` 将要编码的宏块的周围的宏块的信息读进来。\n- c) 调用 `x264_macroblock_analyse()` 执行分析模块。\n- d) 调用 `x264_macroblock_encode()` 执行宏块编码模块。\n- e) 调用 `x264_macroblock_write_cabac()/x264_macroblock_write_cavlc()` 执行熵编码模块。\n- f) 调用 `x264_macroblock_cache_save()` 保存当前宏块的信息。\n- g) 调用 `x264_ratecontrol_mb()` 执行码率控制。\n- h) 准备处理下一个宏块。\n\n（5）调用 `x264_nal_end()` 结束输出一个 NALU。\n\n### 重要的数据结构\n\nX264在宏块编码方面涉及到下面几个比较重要的结构体：\n\n宏块像素存储缓存  `fenc_buf[]` 和 `fdec_buf[]` ——位于 `x264_t.mb.pic` 中，用于存储宏块的亮度和色度像素。\n宏块各种信息的缓存 Cache——位于 `x264_t.mb.pic` 中，用于存储宏块的信息例如 `4x4` 帧内预测模式、DCT 的非 0 系数个数、运动矢量、参考帧序号等。\n\n图像半像素点存储空间 `filtered[]` ——位于 `x264_frame_t` 中，用于存储半像素插值后的点。\n\n#### 宏块像素存储缓存 fenc_buf[] 和 fdec_buf[]\n\n`fenc_buf[]` 和 `fdec_buf[]` 为 `x264_t.mb.cache` 中的结构体，用于存储一个宏块的像素数据。其中 `fenc_buf[]` 用于存储宏块编码像素数据，而 `fdec_buf[]` 用于存储宏块重建像素数据。他们的定义如下所示。\n\n```c\n/* space for p_fenc and p_fdec */\n#define FENC_STRIDE 16\n#define FDEC_STRIDE 32\n//存储编码宏块fenc和重建宏块fdec的内存\nuint8_t fenc_buf[48*FENC_STRIDE]\nuint8_t fdec_buf[52*FDEC_STRIDE]\n```\n\n从定义可以看出，`fenc_buf[]` 每行 16 个数据；而 `fdec_buf[]` 每行 32 个数据。在 `x264_t.mb.cache` 中和 `fenc_buf[]` 和 `fdec_buf[]` 相关的指针数组还有 `p_fenc[3]` 和 `p_fdec[3]` ，它们中的 3 个元素 `[0]、[1]、[2]` 分别指向分别指向对应缓存 buf 的 Y、U、V 分量。下图画出了像素格式为 YUV420P 的时候 `fenc_buf[]` 的存储示意图。图中灰色区域存储 Y，蓝色区域存储 U，粉红区域存储 V。`p_fenc[0]` 指向 Y 的存储区域，`p_fenc[1]` 指向 U 的存储区域，`p_fenc[2]` 指向 V 的存储区域，在图中以方框的形式标注了出来。\n\n![像素格式为 YUV420P 的时候 fenc_buf 的存储](/images/imageFFmpeg/Thor/x264-01.png)\n\n下图画出了像素格式为 YUV420P 的时候 `fdec_buf[]` 的存储示意图。图中灰色区域存储 Y，蓝色区域存储 U，粉红区域存储 V。`p_fenc[0]` 指向 Y 的存储区域，`p_fenc[1]` 指向 U 的存储区域，`p_fenc[2]` 指向 V 的存储区域，在图中以方框的形式标注了出来。\n\n![像素格式为 YUV420P 的时候 fdec_buf 的存储](/images/imageFFmpeg/Thor/x264-02.png)\n\n从图中可以看出，`fdec_buf[]` 和 `fenc_buf[]` 主要的区别在于 `fdec_buf[]` 像素块的左边和上边包含了左上方相邻块用于预测的像素。\n\n### 宏块各种信息的缓存Cache \n\n在 x264 中 `x264_t.mb.cache` 结构体中包含了存储宏块信息的各种各样的缓存 Cache。例如：\n\n- **intra4x4_pred_mode**：`Intra4x4` 帧内预测模式的缓存\n- **non_zero_count**：DCT 的非 0 系数个数的缓存\n- **mv**：运动矢量缓存\n- **ref**：运动矢量参考帧的缓存\n\n## 滤波（Filter）部分\n\n`x264_fdec_filter_row()` 对应着 x264 中的滤波模块。滤波模块主要完成了下面 3 个方面的功能：\n\n（1）环路滤波（去块效应滤波）\n\n（2）半像素内插\n\n（3）视频质量指标PSNR和SSIM的计算\n\n### 函数调用关系图\n\n![滤波（Filter）部分的函数调用关系](/images/imageFFmpeg/Thor/x264_fdec_filter_row.png)\n\n从图中可以看出，滤波模块对应的x264_fdec_filter_row()调用了如下函数：\n\n```c\nx264_frame_deblock_row()\t// 去块效应滤波器。\nx264_frame_filter()\t\t\t// 半像素插值。\nx264_pixel_ssd_wxh()\t\t// PSNR计算。\nx264_pixel_ssim_wxh()\t\t// SSIM计算。\n```\n\n从源代码可以看出，`x264_fdec_filter_row()` 完成了三步工作：\n\n（1）环路滤波（去块效应滤波）。通过调用 `x264_frame_deblock_row()` 实现。\n\n（2）半像素内插。通过调用 `x264_frame_filter()` 实现。\n\n（3）视频质量 SSIM 和 PSNR 计算。PSNR在这里只计算了 SSD，通过调用 `x264_pixel_ssd_wxh()` 实现；SSIM 的计算则是通过 `x264_pixel_ssim_wxh()` 实现。\n\n## 宏块分析（Analysis）部分-帧内宏块（Intra）\n\n`x264_macroblock_analyse()` 对应着 x264 中的分析模块。分析模块主要完成了下面 2 个方面的功能：\n\n（1）对于帧内宏块，分析帧内预测模式\n\n（2）对于帧间宏块，进行运动估计，分析帧间预测模式\n\n### 函数调用关系图\n\n![宏块分析（Analysis）部分的函数调用关系](/images/imageFFmpeg/Thor/x264_macroblock_analyse.png)\n\n从图中可以看出，分析模块的 `x264_macroblock_analyse()` 调用了如下函数（只列举了几个有代表性的函数）：\n\n```c\nx264_mb_analyse_init()\t\t\t// Analysis模块初始化。\nx264_mb_analyse_intra()\t\t\t// Intra宏块帧内预测模式分析。\nx264_macroblock_probe_pskip()\t// 分析是否是skip模式。\nx264_mb_analyse_inter_p16x16()\t// P16x16宏块帧间预测模式分析。\nx264_mb_analyse_inter_p8x8()\t// P8x8宏块帧间预测模式分析。\nx264_mb_analyse_inter_p16x8()\t// P16x8宏块帧间预测模式分析。\nx264_mb_analyse_inter_b16x16()\t// B16x16宏块帧间预测模式分析。\nx264_mb_analyse_inter_b8x8()\t// B8x8宏块帧间预测模式分析。\nx264_mb_analyse_inter_b16x8()\t// B16x8宏块帧间预测模式分析。\n```\n\n尽管 `x264_macroblock_analyse()` 的源代码比较长，但是它的逻辑比较清晰，如下所示：\n\n（1）如果当前是 `I` Slice，调用 `x264_mb_analyse_intra()` 进行 Intra 宏块的帧内预测模式分析。\n\n（2）如果当前是 `P` Slice，则进行下面流程的分析：\n\n- a)调用 `x264_macroblock_probe_pskip()` 分析是否为 Skip 宏块，如果是的话则不再进行下面分析。\n- b)调用 `x264_mb_analyse_inter_p16x16()` 分析 `P16x16` 帧间预测的代价。\n- c)调用 `x264_mb_analyse_inter_p8x8()` 分析 `P8x8` 帧间预测的代价。\n- d)如果 `P8x8` 代价值小于 `P16x16`，则依次对 4 个 `8x8` 的子宏块分割进行判断：\n  - i.调用 `x264_mb_analyse_inter_p4x4()` 分析 `P4x4` 帧间预测的代价。\n  - ii.如果 `P4x4` 代价值小于 `P8x8` ，则调用 `x264_mb_analyse_inter_p8x4()` 和`x264_mb_analyse_inter_p4x8()` 分析 `P8x4` 和 `P4x8` 帧间预测的代价。\n- e)如果 `P8x8` 代价值小于 `P16x16`，调用 `x264_mb_analyse_inter_p16x8()` 和`x264_mb_analyse_inter_p8x16()` 分析 `P16x8` 和 `P8x16` 帧间预测的代价。\n- f)此外还要调用 `x264_mb_analyse_intra()` ，检查当前宏块作为 Intra 宏块编码的代价是否小于作为 `P` 宏块编码的代价（`P` Slice中也允许有 Intra 宏块）。\n\n（3）如果当前是 `B` Slice，则进行和 `P` Slice类似的处理。\n\n总体说来 `x264_mb_analyse_intra()` 通过计算 `Intra16x16`，`Intra8x8`（暂时没有研究），`Intra4x4` 这 3 中帧内预测模式的代价，比较后得到最佳的帧内预测模式。该函数的等流程大致如下：\n\n（1）进行 `Intra16X16` 模式的预测\n\n- a)调用 `predict_16x16_mode_available()` 根据周围宏块的情况判断其可用的预测模式（主要检查左边和上边的块是否可用）。\n- b)循环计算 4 种 `Intra16x16` 帧内预测模式：\n  - i.调用 `predict_16x16[]()` 汇编函数进行 `Intra16x16` 帧内预测\n  - ii.调用 `x264_pixel_function_t` 中的 `mbcmp[]()` 计算编码代价（`mbcmp[]()` 指向 SAD 或者 SATD 汇编函数）。\n- c)获取最小代价的 `Intra16x16` 模式。\n\n（2）进行 `Intra8x8` 模式的预测（未研究，流程应该类似）\n\n（3）进行 `Intra4X4` 块模式的预测\n\n- a)循环处理 16 个 `4x4` 的块：\n  - i.调用 `x264_mb_predict_intra4x4_mode()` 根据周围宏块情况判断该块可用的预测模式。\n  - ii.循环计算 9 种 `Intra4x4` 的帧内预测模式：\n    - 1)调用 `predict_4x4 []()` 汇编函数进行 `Intra4x4` 帧内预测\n    - 2)调用 `x264_pixel_function_t` 中的 `mbcmp[]()` 计算编码代价（`mbcmp[]()` 指向 SAD 或者 SATD 汇编函数）。\n  - iii.获取最小代价的 `Intra4x4` 模式。\n- b)将 16 个 `4X4` 块的最小代价相加，得到总代价。\n\n（4）将上述 3 中模式的代价进行对比，取最小者为当前宏块的帧内预测模式。\n\n## 宏块分析（Analysis）部分-帧间宏块（Inter）\n\n`x264_macroblock_analyse()` 对应着 x264 中的分析模块。分析模块主要完成了下面 2 个方面的功能：\n\n（1）对于帧内宏块，分析帧内预测模式\n\n（2）对于帧间宏块，进行运动估计，分析帧间预测模式\n\n> [详细功能说明](<https://blog.csdn.net/leixiaohua1020/article/details/45936267>)\n\n## 宏块编码（Encode）部分\n\n`x264_macroblock_encode()` 对应着 x264 中的宏块编码模块。宏块编码模块主要完成了 DCT 变换和量化两个步骤。\n\n### 函数调用关系图\n\n![宏块编码（Encode）部分的函数调用关系](/images/imageFFmpeg/Thor/x264_macroblock_encode.png)\n\n从图中可以看出，宏块编码模块的 `x264_macroblock_encode()` 调用了 `x264_macroblock_encode_internal()` ，而 `x264_macroblock_encode_internal()` 完成了如下功能：\n\n```c\nx264_macroblock_encode_skip()\t// 编码Skip类型宏块。\nx264_mb_encode_i16x16()\t\t\t// 编码Intra16x16类型的宏块。该函数除了进行DCT变换之外，还对16个小块的DC系数进行了Hadamard变换。\nx264_mb_encode_i4x4()\t\t\t// 编码Intra4x4类型的宏块。\n// 帧间宏块编码：这一部分代码直接写在了函数体里面。\nx264_mb_encode_chroma()\t\t\t// 编码色度块。\n```\n\n`x264_macroblock_encode()` 用于编码宏块。该函数的定义位于 `encoder\\macroblock.c`\n\n`x264_macroblock_encode_internal()` 的流程大致如下：\n\n（1）如果是 Skip 类型，调用 `x264_macroblock_encode_skip()` 编码宏块。\n\n（2）如果是 `Intra16x16` 类型，调用 `x264_mb_encode_i16x16()` 编码宏块。\n\n（3）如果是 `Intra4x4` 类型，循环 16 次调用 `x264_mb_encode_i4x4()` 编码宏块。\n\n（4）如果是 Inter 类型，则不再调用子函数，而是直接进行编码：\n\n- a)对 `16x16` 块调用 `x264_dct_function_t` 的 `sub16x16_dct()` 汇编函数，求得编码宏块数据 `p_fenc` 与重建宏块数据 `p_fdec` 之间的残差（“sub”），并对残差进行 DCT 变换。\n- b)分成 4 个 `8x8` 的块，对每个 `8x8` 块分别调用 `x264_quant_function_t` 的 `quant_4x4x4()` 汇编函数进行量化。\n- c)分成 16 个 `4x4` 的块，对每个 `4x4` 块分别调用 `x264_quant_function_t` 的 `dequant_4x4()` 汇编函数进行反量化（用于重建帧）。\n- d)分成 4 个 `8x8` 的块，对每个 `8x8` 块分别调用 `x264_dct_function_t` 的 `add8x8_idct()` 汇编函数，对残差进行 DCT 反变换，并将反变换后的数据叠加（“add”）至预测数据上（用于重建帧）。\n\n（5）\t如果对色度编码，调用 `x264_mb_encode_chroma()` 。\n\n从 Inter 宏块编码的步骤可以看出，编码就是 “DCT变换+量化” 两步的组合。\n\n简单整理一下 `x264_mb_encode_i16x16()` 的逻辑，如下所示：\n\n（1）调用 `predict_16x16[]()` 汇编函数对重建宏块数据 `p_fdec` 进行帧内预测。\n\n（2）调用 `x264_dct_function_t` 的 `sub16x16_dct()` 汇编函数，计算重建宏块数据 `p_fdec` 与编码宏块数据`p_fenc` 之间的残差，然后对残差做 DCT 变换。\n\n（3）抽取出来 16 个 `4x4DCT` 小块的 DC 系数，存储于 `dct_dc4x4[]`。\n\n（4）分成 4 个 `8x8` 的块，对每个 `8x8` 块分别调用 `x264_quant_function_t` 的 `quant_4x4x4()` 汇编函数进行量化。\n\n（5）分成 16 个 `4x4` 的块，对每个 `4x4` 块分别调用 `x264_quant_function_t` 的 `dequant_4x4()` 汇编函数进行反量化（用于重建帧）。\n\n（6）对于 `dct_dc4x4[]` 中 16 个小块的 DC 系数作如下处理：\n\n- a)调用 `x264_dct_function_t` 的 `dct4x4dc()` 汇编函数进行 Hadamard 变换。\n- b)调用 `x264_quant_function_t` 的 `quant_4x4_dc()` 汇编函数进行 DC 系数的量化。\n- c)调用 `x264_dct_function_t` 的 `idct4x4dc()` 汇编函数进行 Hadamard 反变换。\n- d)调用 `x264_quant_function_t` 的 `dequant_4x4_dc()` 汇编函数进行 DC 系数的反量化。\n- e)将反量化后的 DC 系数重新放到 `16x16` 块对应的位置上。\n\n（7）调用 `x264_dct_function_t` 的 `add16x16_idct()` 汇编函数，对残差进行 DCT 反变换，并将反变换后的数据叠加（“add”）至预测数据上（用于重建帧）。\n\n可以看出 `Intra16x16` 编码的过程就是一个 “DCT变换 + 量化 + Hadamard变换” 的流程。其中 “DCT变换 + 量化” 是一个通用的编码步骤，而 “Hadamard变换” 是专属于 `Intra16x16` 宏块的步骤。\n\n简单整理一下 `x264_mb_encode_i4x4()` 的逻辑，如下所示：\n\n（1）调用 `predict_4x4[]()` 汇编函数对重建宏块数据 `p_fdec` 进行帧内预测。\n\n（2）调用 `x264_dct_function_t` 的 `sub4x4_dct ()` 汇编函数，计算重建宏块数据 `p_fdec` 与编码宏块数据 `p_fenc` 之间的残差，然后对残差做 DCT 变换。\n\n（3）调用 `x264_quant_function_t` 的 `quant_4x4()` 汇编函数进行量化。\n\n（4）调用 `x264_quant_function_t` 的 `dequant_4x4()` 汇编函数进行反量化（用于重建帧）。\n\n（5）调用 `x264_dct_function_t` 的 `add4x4_idct()` 汇编函数，对残差进行 DCT 反变换，并将反变换后的数据叠加（“add”）至预测数据上（用于重建帧）。\n\n可以看出 `Intra4x4` 编码的过程就是一个 “DCT变换 + 量化” 的流程。\n\n## 熵编码（Entropy Encoding）部分\n\n`x264_macroblock_write_cavlc()` 对应着x264中的熵编码模块。熵编码模块主要完成了编码数据输出的功能。\n\n### 函数调用关系图\n\n![熵编码（Entropy Encoding）部分的函数调用关系](/images/imageFFmpeg/Thor/x264_macroblock_write_cavlc.png)\n\n从图中可以看出，熵编码模块包含两个函数 `x264_macroblock_write_cabac()` 和`x264_macroblock_write_cavlc()`。如果输出设置为 CABAC 编码，则会调用`x264_macroblock_write_cabac()`；如果输出设置为 CAVLC 编码，则会调用 `x264_macroblock_write_cavlc()` 。本文选择 CAVLC 编码输出函数 `x264_macroblock_write_cavlc()` 进行分析。该函数调用了如下函数：\n\n```c\nx264_cavlc_mb_header_i()\t\t// 写入I宏块MB Header数据。包含帧内预测模式等。\nx264_cavlc_mb_header_p()\t\t// 写入P宏块MB Header数据。包含MVD、参考帧序号等。\nx264_cavlc_mb_header_b()\t\t// 写入B宏块MB Header数据。包含MVD、参考帧序号等。\nx264_cavlc_qp_delta()\t\t\t// 写入QP。\nx264_cavlc_block_residual()\t\t// 写入残差数据。\n```\n\n从源代码可以看出，`x264_macroblock_write_cavlc()` 的流程大致如下：\n\n（1）根据 Slice 类型的不同，调用不同的函数输出宏块头（MB Header）：\n\n- a)对于 `P Slice`，调用 `x264_cavlc_mb_header_p()`\n- b)对于 `B Slice`，调用 `x264_cavlc_mb_header_b()`\n- c)对于 `I Slice`，调用 `x264_cavlc_mb_header_i()`\n\n（2）调用 `x264_cavlc_qp_delta()` 输出宏块 QP 值\n\n（3）调用 `x264_cavlc_block_residual()` 输出 CAVLC 编码的残差数据\n\n## FFmpeg与libx264接口源代码简单分析\n\n本文简单记录一下 FFmpeg 的 libavcodec 中与 libx264 接口部分的源代码。该部分源代码位于 “libavcodec/libx264.c” 中。正是有了这部分代码，使得 FFmpeg 可以调用 libx264 编码 H.264 视频。\n\n### 函数调用关系图\n\n![FFmpeg的libavcodec中的libx264.c的函数调用关系](/images/imageFFmpeg/Thor/[FFmpeg的libavcodec中的libx264的函数调用关系.png)\n\n从图中可以看出，libx264 对应的 AVCodec 结构体 `ff_libx264_encoder` 中设定编码器初始化函数是 `X264_init()`，编码一帧数据的函数是 `X264_frame()`，编码器关闭函数是 `X264_close()`。\n\n`X264_init()` 调用了如下函数：\n\n```c\n[libx264 API] x264_param_default()\t\t\t// 设置默认参数。\n[libx264 API] x264_param_default_preset()\t// 设置默认preset。\nconvert_pix_fmt() \t// 将FFmpeg像素格式转换为libx264像素格式。\n[libx264 API] x264_param_apply_profile()\t// 设置Profile。\n[libx264 API] x264_encoder_open()\t\t\t// 打开编码器。\n[libx264 API] x264_encoder_headers()\t\t// 需要全局头的时候，输出头信息。\n```\n\nX264_frame()调用了如下函数：\n\n```c\n[libx264 API] x264_encoder_encode()\t\t\t\t// 编码一帧数据。\n[libx264 API] x264_encoder_delayed_frames()\t\t// 输出编码器中缓存的数据。\nencode_nals() \t// 将编码后得到的x264_nal_t转换为AVPacket。\n```\n\n`X264_close()` 调用了如下函数：\n\n```c\n[libx264 API] x264_encoder_close()\t// 关闭编码器。\n```\n\n# 解码 - libavcodec H.264 解码器\n\n## 概述\n\n本文简单记录 FFmpeg 中 libavcodec 的 H.264 解码器（H.264 Decoder）的源代码。这个 H.264 解码器十分重要，可以说 FFmpeg 项目今天可以几乎“垄断”视音频编解码技术，很大一部分贡献就来自于这个 H.264 解码器。这个 H.264 解码器一方面功能强大，性能稳定；另一方面源代码也比较复杂，难以深入研究。本文打算梳理一下这个 H.264 解码器的源代码结构，以方便以后深入学习 H.264 使用。\n\n> PS：这部分代码挺复杂的，还有不少地方还比较模糊，还需要慢慢学习......\n\n### 函数调用关系图\n\nH.264解码器的函数调用关系图如下所示。\n\n![H.264解码器的函数调用关系图](/images/imageFFmpeg/Thor/H.264解码器的函数调用关系图.png)\n\n下面解释一下图中关键标记的含义。\n\n#### 作为接口的结构体\n\nFFmpeg和H.264解码器之间作为接口的结构体有2个：\n\n- `ff_h264_parser`：用于解析 H.264 码流的 AVCodecParser 结构体。\n- `ff_h264_decoder`：用于解码 H.264 码流的 AVCodec 结构体。\n\n#### 函数背景色\n\n函数在图中以方框的形式表现出来。不同的背景色标志了该函数不同的作用：\n\n- 白色背景的函数：普通内部函数。\n- 粉红色背景函数：解析函数（Parser）。这些函数用于解析SPS、PPS等信息。\n- 紫色背景的函数：熵解码函数（Entropy Decoding）。这些函数读取码流数据并且进行CABAC或者CAVLC熵解码。\n- 绿色背景的函数：解码函数（Decode）。这些函数通过帧内预测、帧间预测、DCT反变换等方法解码压缩数据。\n- 黄色背景的函数：环路滤波函数（Loop Filter）。这些函数对解码后的数据进行滤波，去除方块效应。\n- 蓝色背景函数：汇编函数（Assembly）。这些函数是做过汇编优化的函数。图中主要画出了这些函数的C语言版本，此外这些函数还包含MMX版本、SSE版本、NEON版本等。\n\n#### 箭头线\n\n箭头线标志了函数的调用关系：\n\n- 黑色箭头线：不加区别的调用关系。\n- 粉红色的箭头线：解析函数（Parser）之间的调用关系。\n- 紫色箭头线：熵解码函数（Entropy Decoding）之间的调用关系。\n- 绿色箭头线：解码函数（Decode）之间的调用关系。\n- 黄色箭头线：环路滤波函数（Loop Filter）之间的调用关系。\n\n#### 函数所在的文件\n\n每个函数标识了它所在的文件路径。\n\n### 几个关键部分\n\n下文简单记录几个关键的部分。\n\n#### FFmpeg和H.264解码器之间作为接口的结构体\n\nFFmpeg和H.264解码器之间作为接口的结构体有2个：ff_h264_parser和ff_h264_decoder。\n\n**ff_h264_parser**\n\nff_h264_parser是用于解析H.264码流的AVCodecParser结构体。AVCodecParser中包含了几个重要的函数指针：\n\n- **parser_init()：初始化解析器。**\n- **parser_parse()：解析。**\n- **parser_close()：关闭解析器。**\n\n在ff_h264_parser结构体中，上述几个函数指针分别指向下面几个实现函数：\n\n- **init()：初始化H.264解析器。**\n- **h264_parse()：解析H.264码流。**\n- **close()：关闭H.264解析器。**\n\n**ff_h264_decoder**\n\nff_h264_decoder是用于解码H.264码流的AVCodec结构体。AVCodec中包含了几个重要的函数指针：\n\n- **init()：初始化解码器。**\n- **decode()：解码。**\n- **close()：关闭解码器。**\n\n在ff_h264_decoder结构体中，上述几个函数指针分别指向下面几个实现函数：\n\n- **ff_h264_decode_init()：初始化H.264解码器。**\n- **h264_decode_frame()：解码H.264码流。**\n\n- **h264_decode_end()：关闭H.264解码器。**\n\n#### 普通内部函数\n\n普通内部函数指的是H.264解码器中还没有进行分类的函数。下面举几个例子。\n\nff_h264_decoder中ff_h264_decode_init()调用的初始化函数：\n\n- **ff_h264dsp_init()：初始化DSP相关的函数。包含了IDCT、环路滤波函数等。**\n- **ff_h264qpel_init()：初始化四分之一像素运动补偿相关的函数。**\n- **ff_h264_pred_init()：初始化帧内预测相关的函数。**\n- **ff_h264_decode_extradata()：解析AVCodecContext中的extradata。**\n\nff_h264_decoder中h264_decode_frame()逐层调用的和解码Slice相关的函数：\n\n- **decode_nal_units()，ff_h264_execute_decode_slices()，decode_slice()等。**\n\nff_h264_decoder中h264_decode_end()调用的清理函数：\n\n- **ff_h264_remove_all_refs()：移除所有参考帧。**\n- **ff_h264_free_context()：释放在初始化H.264解码器的时候分配的内存。**\n\nff_h264_parser中h264_parse()逐层调用的和解析Slice相关的函数：\n\n- **h264_find_frame_end()：查找NALU的结尾。**\n\n- **parse_nal_units()：解析一个NALU。**\n\n#### <font style=\"color:rgb(255,153,255);\">解析函数（Parser）</font>\n解析函数（Parser）用于解析H.264码流中的一些信息（例如SPS、PPS、Slice Header等）。在parse_nal_units()和decode_nal_units()中都调用这些解析函数完成了解析。下面举几个解析函数的例子。\n\n- **ff_h264_decode_nal()：解析NALU。这个函数是后几个解析函数的前提。**\n- **ff_h264_decode_slice_header()：解析Slice Header。**\n- **ff_h264_decode_sei()：解析SEI。**\n- **ff_h264_decode_seq_parameter_set()：解析SPS。**\n- **ff_h264_decode_picture_parameter_set()：解析PPS。**\n\n#### <font style=\"color:#993399;\">熵解码函数（Entropy Decoding）</font>\n\n熵解码函数（Entropy Decoding）读取码流数据并且进行CABAC或者CAVLC熵解码。CABAC解码函数是ff_h264_decode_mb_cabac()，CAVLC解码函数是ff_h264_decode_mb_cavlc()。熵解码函数中包含了很多的读取指数哥伦布编码数据的函数，例如get_ue_golomb_long()，get_ue_golomb()，get_se_golomb()，get_ue_golomb_31()等等。\n\n在获取残差数据的时候需要进行CAVLC/CABAC解码。例如解码CAVLC的时候，会调用decode_residual()函数，而decode_residual()会调用get_vlc2()函数，get_vlc2()会调用OPEN_READER()，UPDATE_CACHE()，GET_VLC()，CLOSE_READER()几个函数读取CAVLC格式的数据。\n此外，在获取运动矢量的时候，会调用pred_motion()以及类似的几个函数获取运动矢量相关的信息。\n\n#### <font style=\"color:#009900;\">解码函数（Decode）</font>\n\n解码函数（Decode）通过帧内预测、帧间预测、DCT反变换等方法解码压缩数据。解码函数是`ff_h264_hl_decode_mb()`。其中跟宏块类型的不同，会调用几个不同的函数，最常见的就是调用`hl_decode_mb_simple_8()`。\n\n`hl_decode_mb_simple_8()` 的定义是无法在源代码中直接找到的，这是因为它实际代码的函数名称是使用宏的方式写的（以后再具体分析）。`hl_decode_mb_simple_8()`的源代码实际上就是 `FUNC(hl_decode_mb)()` 函数的源代码。\n\n`FUNC(hl_decode_mb)()`根据宏块类型的不同作不同的处理：如果宏块类型是INTRA，就会调用`hl_decode_mb_predict_luma()` 进行帧内预测；如果宏块类型不是INTRA，就会调用`FUNC(hl_motion_422)()` 或者 `FUNC(hl_motion_420)()` 进行四分之一像素运动补偿。\n\n随后 `FUNC(hl_decode_mb)()` 会调用 `hl_decode_mb_idct_luma()` 等几个函数对数据进行DCT反变换工作。\n\n#### <font style=\"color:#ffcc00;\">环路滤波函数（Loop Filter）</font>\n\n环路滤波函数（Loop Filter）对解码后的数据进行滤波，去除方块效应。环路滤波函数是loop_filter()。其中调用了ff_h264_filter_mb()和ff_h264_filter_mb_fast()。ff_h264_filter_mb_fast()中又调用了h264_filter_mb_fast_internal()。而h264_filter_mb_fast_internal()中又调用了下面几个函数进行滤波：\n\n- **filter_mb_edgeh()：亮度水平滤波**\n- **filter_mb_edgev()：亮度垂直滤波**\n- **filter_mb_edgech()：色度水平滤波**\n\n- **filter_mb_edgecv()：色度垂直滤波**\n\n#### <font style=\"color:#3333ff;\">汇编函数（Assembly）</font>\n\n汇编函数（Assembly）是做过汇编优化的函数。为了提高效率，整个H.264解码器中（主要在解码部分和环路滤波部分）包含了大量的汇编函数。实际解码的过程中，FFmpeg会根据系统的特性调用相应的汇编函数（而不是C语言函数）以提高解码的效率。如果系统不支持汇编优化的话，FFmpeg才会调用C语言版本的函数。例如在帧内预测的时候，对于16x16亮度DC模式，有以下几个版本的函数：\n\n- C语言版本的pred16x16_dc_8_c()\n- NEON版本的ff_pred16x16_dc_neon()\n- MMXEXT版本的ff_pred16x16_dc_8_mmxext()\n- SSE2版本的ff_pred16x16_dc_8_sse2()\n\n### 附录\n\n在网上找到一张图（出处不详），分析了FFmpeg的H.264解码器每个函数运行的耗时情况，比较有参考意义，在这里附上。\n\n![H.264解码器每个函数运行的耗时情况](/images/imageFFmpeg/Thor/H.264解码器每个函数运行的耗时情况.png)\n\n从图中可以看出，熵解码、宏块解码、环路滤波耗时比例分别为：23.64%、51.85%、22.22%。\n\n## 解析器（Parser）部分\n\n本文继续分析FFmpeg中libavcodec的H.264解码器（H.264 Decoder）。上篇文章概述了FFmpeg中H.264解码器的结构；从这篇文章开始，具体研究H.264解码器的源代码。本文分析H.264解码器中解析器（Parser）部分的源代码。这部分的代码用于分割H.264的NALU，并且解析SPS、PPS、SEI等信息。解析H.264码流（对应AVCodecParser结构体中的函数）和解码H.264码流（对应AVCodec结构体中的函数）的时候都会调用该部分的代码完成相应的功能。\n\n### 函数调用关系图\n\n![解析器（Parser）部分的源代码的调用关系](/images/imageFFmpeg/Thor/解析器部分的源代码的调用关系.png)\n\n从图中可以看出，H.264的解析器（Parser）在解析数据的时候调用 `h264_parse()`，`h264_parse()` 调用了`parse_nal_units()`，`parse_nal_units()` 则调用了一系列解析特定 NALU 的函数。H.264 的解码器（Decoder）在解码数据的时候调用 `h264_decode_frame()`，`h264_decode_frame()` 调用了`decode_nal_units()`，`decode_nal_units()` 也同样调用了一系列解析不同 NALU 的函数。\n\n图中简单列举了几个解析特定 NALU 的函数：\n\n```c\nff_h264_decode_nal()\t\t\t\t\t// 解析 NALU Header\nff_h264_decode_seq_parameter_set()\t\t// 解析 SPS\nff_h264_decode_picture_parameter_set()\t// 解析 PPS\nff_h264_decode_sei()\t\t\t\t\t// 解析 SEI\n```\n\nH.264 解码器与 H.264 解析器最主要的不同的地方在于它调用了 `ff_h264_execute_decode_slices()` 函数进行了解码工作。这篇文章只分析 H.264 解析器的源代码，至于 H.264 解码器的源代码，则在后面几篇文章中再进行分析。\n\n#### h264_find_frame_end()\n\n`h264_find_frame_end()` 用于查找 H.264 码流中的 “起始码”（start code）。在 H.264 码流中有两种起始码： `0x000001` 和 `0x00000001`。其中 4Byte 的长度的起始码最为常见。只有当一个完整的帧被编为多个 slice 的时候，包含这些 slice 的 NALU 才会使用 3Byte 的起始码。`h264_find_frame_end()` 的定义位于`libavcodec\\h264_parser.c`\n\n从源代码可以看出，`h264_find_frame_end()` 使用了一种类似于状态机的方式查找起始码。函数中的 `for()` 循环每执行一遍，状态机的状态就会改变一次。该状态机主要包含以下几种状态：\n\n```shell\n7 - 初始化状态\n2 - 找到1个0\n1 - 找到2个0\n0 - 找到大于等于3个0\n4 - 找到2个0和1个1，即001（即找到了起始码）\n5 - 找到至少3个0和1个1，即0001等等（即找到了起始码）\n>=8 - 找到2个Slice Header\n```\n\n这些状态之间的状态转移图如下所示。图中粉红色代表初始状态，绿色代表找到“起始码”的状态。\n\n![状态之间的状态转移](/images/imageFFmpeg/Thor/h264_find_frame_end状态转移.png)\n\n如图所示，`h264_find_frame_end()` 初始化时候位于状态 “7”；当找到 1 个 “0” 之后，状态从 “7” 变为 “2”；在状态 “2” 下，如果再次找到 1 个 “0”，则状态变为 “1”；在状态 “1” 下，如果找到 “1”，则状态变换为 “4”，表明找到了 “0x000001” 起始码；在状态 “1” 下，如果找到 “0”，则状态变换为 “0”；在状态 “0” 下，如果找到 “1”，则状态变换为 “5” ，表明找到了 “0x000001” 起始码。\n\n`parse_nal_units()` 主要做了以下几步处理：\n\n（1）对于所有的 NALU，都调用 `ff_h264_decode_nal` 解析 NALU 的 Header，得到 nal_unit_type 等信息\n\n（2）根据 nal_unit_type 的不同，调用不同的解析函数进行处理。例如：\n\n- a)解析 SPS 的时候调用 `ff_h264_decode_seq_parameter_set()`\n- b)解析 PPS 的时候调用 `ff_h264_decode_picture_parameter_set()`\n- c)解析 SEI 的时候调用 `ff_h264_decode_sei()`\n\n- d)解析 IDR Slice / Slice 的时候，获取 slice_type 等一些信息。\n\n## 解码器主干部分\n\n本文分析FFmpeg的H.264解码器的主干部分。“主干部分” 是相对于 “熵解码”、“宏块解码”、“环路滤波” 这些细节部分而言的。它包含了 H.264 解码器直到 `decode_slice()` 前面的函数调用关系（`decode_slice()` 后面就是H.264解码器的细节部分，主要包含了 “熵解码”、“宏块解码”、“环路滤波” 3个部分）。\n\n### 函数调用关系图\n\n![解码器主干部分的源代码的调用关系](/images/imageFFmpeg/Thor/解码器主干部分的源代码的调用关系.png)\n\n从图中可以看出，H.264解码器（Decoder）在初始化的时候调用了 `ff_h264_decode_init()`，`ff_h264_decode_init()` 又调用了下面几个函数进行解码器汇编函数的初始化工作（仅举了几个例子）：\n\n```c\nff_h264dsp_init()\t\t// 初始化DSP相关的汇编函数。包含了IDCT、环路滤波函数等。\nff_h264qpel_init()\t\t// 初始化四分之一像素运动补偿相关的汇编函数。\nff_h264_pred_init()\t\t// 初始化帧内预测相关的汇编函数。\n```\n\nH.264 解码器在关闭的时候调用了 `h264_decode_end()`，`h264_decode_end()` 又调用了`ff_h264_remove_all_refs()`，`ff_h264_free_context()` 等几个函数进行清理工作。\nH.264 解码器在解码图像帧的时候调用了 `h264_decode_frame()`，`h264_decode_frame()` 调用了 `decode_nal_units()`，`decode_nal_units()` 调用了两类函数——解析函数和解码函数，如下所示。\n\n（1）解析函数（获取信息）：\n\n```c\nff_h264_decode_nal()\t\t\t\t// 解析NALU Header。\nff_h264_decode_seq_parameter_set()\t// 解析SPS。\nff_h264_decode_picture_parameter_set()\t// 解析PPS。\nff_h264_decode_sei()\t// 解析SEI。\nff_h264_decode_slice_header()\t// 解析Slice Header。\n```\n\n（2）解码函数（解码获得图像）：\n\n```c\nff_h264_execute_decode_slices() \t// 解码Slice。\n```\n\n其中 `ff_h264_execute_decode_slices()` 调用了 `decode_slice()`，而 `decode_slice()` 中调用了解码器中细节处理的函数（暂不详细分析）：\n\n```c\nff_h264_decode_mb_cabac()\t// CABAC熵解码函数。\nff_h264_decode_mb_cavlc()\t// CAVLC熵解码函数。\nff_h264_hl_decode_mb()\t\t// 宏块解码函数。\nloop_filter()\t\t\t\t// 环路滤波函数。\n```\n\n`h264_decode_frame()` 根据输入的 AVPacket 的 data 是否为空作不同的处理：\n\n（1）若果输入的 AVPacket 的 data 为空，则调用 `output_frame()` 输出 `delayed_pic[]` 数组中的H264Picture，即输出解码器中缓存的帧（对应的是通常称为 “Flush Decoder” 的功能）。\n\n（2）若果输入的 AVPacket 的 data 不为空，则首先调用 `decode_nal_units()` 解码 AVPacket 的 data，然后再调用 `output_frame()` 输出解码后的视频帧（有一点需要注意：由于帧重排等因素，输出的 AVFrame 并非对应于输入的 AVPacket）。\n\n`decode_nal_units()` 首先调用 `ff_h264_decode_nal()` 判断 NALU 的类型，然后根据 NALU 类型的不同调用了不同的处理函数。这些处理函数可以分为两类——解析函数和解码函数，如下所示。\n\n（1）解析函数（获取信息）：\n\n```c\nff_h264_decode_seq_parameter_set()\t\t// 解析SPS。\nff_h264_decode_picture_parameter_set()\t// 解析PPS。\nff_h264_decode_sei()\t\t\t\t\t// 解析SEI。\nff_h264_decode_slice_header()\t\t\t// 解析Slice Header。\n```\n\n（2）解码函数（解码得到图像）：\n\n```c\nff_h264_execute_decode_slices()\t// 解码Slice。\n```\n\n`decode_slice()` 按照宏块（`16x16`）的方式处理输入的视频流。每个宏块的压缩数据经过以下 3 个基本步骤的处理，得到解码后的数据：\n\n（1）熵解码。如果熵编码为 CABAC，则调用 `ff_h264_decode_mb_cabac()`；如果熵编码为 CAVLC，则调用 `ff_h264_decode_mb_cavlc()`\n\n（2）宏块解码。这一步骤调用 `ff_h264_hl_decode_mb()`\n\n（3）环路滤波。这一步骤调用 `loop_filter()`\n\n此外，还有可能调用错误隐藏函数 `er_add_slice()`。\n\n至此，`decode_nal_units()` 函数的调用流程就基本分析完毕了。`h264_decode_frame()` 在调用完 `decode_nal_units()` 之后，还需要把解码后得到的 H264Picture 转换为 AVFrame 输出出来，这时候会调用一个相对比较简单的函数 `output_frame()`。\n\n## 熵解码（Entropy Decoding）部分\n\nFFmpeg的H.264解码器调用 `decode_slice()` 函数完成了解码工作。这些解码工作可以大体上分为3个步骤：熵解码，宏块解码以及环路滤波。本文分析这3个步骤中的第1个步骤。\n\n### 函数调用关系图\n\n![熵解码（Entropy Decoding）部分的源代码的调用关系](/images/imageFFmpeg/Thor/熵解码部分的源代码的调用关系.png)\n\n从图中可以看出，FFmpeg的熵解码方面的函数有两个：`ff_h264_decode_mb_cabac()` 和 `ff_h264_decode_mb_cavlc()`。\n\n- `ff_h264_decode_mb_cabac()` 用于解码 CABAC 编码方式的 H.264 数据，\n- `ff_h264_decode_mb_cavlc()`用于解码 CAVLC 编码方式的 H.264 数据。\n\n本文挑选了`ff_h264_decode_mb_cavlc()` 函数进行分析。\n\n`ff_h264_decode_mb_cavlc()` 调用了很多的读取指数哥伦布编码数据的函数，例如 `get_ue_golomb_long()`，`get_ue_golomb()，get_se_golomb()`，`get_ue_golomb_31()` 等。此外在解码残差数据的时候，调用了 `decode_residual()`函数，而 `decode_residual()` 会调用 `get_vlc2()` 函数读取 CAVLC 编码数据。\n\n总而言之，“熵解码” 部分的作用就是按照 H.264 语法和语义的规定，读取数据（宏块类型、运动矢量、参考帧、残差等）并且赋值到 FFmpeg H.264 解码器中相应的变量上。需要注意的是，“熵解码” 部分并不使用这些变量还原视频数据。还原视频数据的功能在下一步 “宏块解码” 步骤中完成。\n\n在开始看 `ff_h264_decode_mb_cavlc()` 之前先回顾一下 `decode_slice()` 函数。\n\n`decode_slice()` 的的流程如下：\n\n（1）判断 H.264 码流是 CABAC 编码还是 CAVLC 编码，进入不同的处理循环。\n\n（2）如果是 CABAC 编码，首先调用 `ff_init_cabac_decoder()` 初始化 CABAC 解码器。然后进入一个循环，依次对每个宏块进行以下处理：\n\n- a)调用 `ff_h264_decode_mb_cabac()`进行 CABAC 熵解码\n\n- b)调用 `ff_h264_hl_decode_mb()` 进行宏块解码\n\n- c)解码一行宏块之后调用 `loop_filter()` 进行环路滤波\n\n- d)此外还有可能调用 `er_add_slice()` 进行错误隐藏处理\n\n（3）如果是 CABAC 编码，直接进入一个循环，依次对每个宏块进行以下处理：\n\n- a)调用 `ff_h264_decode_mb_cavlc()` 进行 CAVLC 熵解码\n\n- b)调用 `ff_h264_hl_decode_mb()` 进行宏块解码\n\n- c)解码一行宏块之后调用 `loop_filter()` 进行环路滤波\n\n- d)此外还有可能调用 `er_add_slice()` 进行错误隐藏处理\n\n可以看出，出了熵解码以外，宏块解码和环路滤波的函数是一样的。\n\n`ff_h264_decode_mb_cavlc()` 的定义有将近 1000 行代码，算是一个比较复杂的函数了。我在其中写了不少注释，因此不再对源代码进行详细的分析。下面先简单梳理一下它的流程：\n\n（1）解析 Skip 类型宏块\n\n（2）获取 `mb_type`\n\n（3）填充当前宏块左边和上边宏块的信息（后面的预测中会用到）\n\n（4）根据 `mb_type` 的不同，分成三种情况进行预测工作：\n\n- a)宏块是帧内预测\n  - i.如果宏块是 `Intra4x4` 类型，则需要单独解析帧内预测模式。\n  - ii.如果宏块是 `Intra16x16` 类型，则不再做过多处理。\n\n- b)宏块划分为 4 个块（此时每个 `8x8` 的块可以再次划分为 4 种类型）\n\n  这个时候每个 `8x8` 的块可以再次划分为 `8x8、8x4、4x8、4x4` 几种子块。需要分别处理这些小的子块：\n\n  - i.解析子块的参考帧序号\n  - ii.解析子块的运动矢量\n\n- c)其它类型（包括 `16x16，16x8，8x16` 几种划分，这些划分不可再次划分）\n\n  这个时候需要判断宏块的类型为 `16x16，16x8` 还是 `8x16`，然后作如下处理：\n\n  - i.解析子宏块的参考帧序号\n  - ii.解析子宏块的运动矢量\n\n（5）解码残差信息\n\n（6）将宏块的各种信息输出到整个图片相应的变量中\n\n#### 各种 Cache（缓存）\n在 H.264 解码器中包含了各种各样的 Cache（缓存）。例如：\n\n```c\nintra4x4_pred_mode_cache\t// Intra4x4帧内预测模式的缓存\nnon_zero_count_cache\t\t// 每个4x4块的非0系数个数的缓存\nmv_cache\t\t\t\t\t// 运动矢量缓存\nref_cache\t\t\t\t\t// 运动矢量参考帧的缓存\n```\n\n> [其他知识查看](<https://blog.csdn.net/leixiaohua1020/article/details/45114453>)\n\n## 宏块解码（Decode）部分-帧内宏块（Intra）\n\nFFmpeg的H.264解码器调用 `decode_slice()` 函数完成了解码工作。这些解码工作可以大体上分为3个步骤：熵解码，宏块解码以及环路滤波。本文分析这3个步骤中的第2个步骤。由于宏块解码部分的内容比较多，因此将本部分内容拆分成两篇文章：一篇文章记录帧内预测宏块（Intra）的宏块解码，另一篇文章记录帧间预测宏块（Inter）的宏块解码。\n\n### 函数调用关系图\n\n![宏块解码（Decode）部分的源代码的调用关系](/images/imageFFmpeg/Thor/宏块解码部分的源代码的调用关系.png)\n\n宏块解码函数（Decode）通过帧内预测、帧间预测、DCT 反变换等方法解码压缩数据。解码函数是 `ff_h264_hl_decode_mb()`。其中跟宏块类型的不同，会调用几个不同的函数，最常见的就是调用 `hl_decode_mb_simple_8()`。\n\n`hl_decode_mb_simple_8()` 的定义是无法在源代码中直接找到的，这是因为它实际代码的函数名称是使用宏的方式写的。`hl_decode_mb_simple_8()` 的源代码实际上就是 `FUNC(hl_decode_mb)()` 函数的源代码。\n\n从函数调用图中可以看出，`FUNC(hl_decode_mb)()` 根据宏块类型的不同作不同的处理：\n\n- 如果帧内预测宏块（INTRA），就会调用 `hl_decode_mb_predict_luma()` 进行帧内预测；\n- 如果是帧间预测宏块（INTER），就会调用 `FUNC(hl_motion_422)()` 或者 `FUNC(hl_motion_420)()` 进行四分之一像素运动补偿。\n\n经过帧内预测或者帧间预测步骤之后，就得到了预测数据。随后 `FUNC(hl_decode_mb)()` 会调用 `hl_decode_mb_idct_luma()` 等几个函数对残差数据进行 DCT 反变换工作，并将变换后的数据叠加到预测数据上，形成解码后的图像数据。\n\n由于帧内预测宏块和帧间预测宏块的解码工作都比较复杂，因此分成两篇文章记录这两部分的源代码。本文记录帧内预测宏块解码时候的源代码。\n\n下面简单梳理一下 `FUNC(hl_decode_mb)` 的流程（在这里只考虑亮度分量的解码，色度分量的解码过程是类似的）：\n\n（1）预测\n\n- a)如果是帧内预测宏块（Intra），调用 `hl_decode_mb_predict_luma()` 进行帧内预测，得到预测数据。\n- b)如果不是帧内预测宏块（Inter），调用 `FUNC(hl_motion_420)()` 或者 `FUNC(hl_motion_422)()` 进行帧间预测（即运动补偿），得到预测数据。\n\n（2）残差叠加\n\n- a)调用 `hl_decode_mb_idct_luma()` 对 DCT 残差数据进行 DCT 反变换，获得残差像素数据并且叠加到之前得到的预测数据上，得到最后的图像数据。\n\nPS：该流程中有一个重要的贯穿始终的内存指针 `dest_y`，其指向的内存中存储了解码后的亮度数据。\n\n根据原代码梳理一下 `hl_decode_mb_predict_luma()` 的主干：\n\n（1）如果宏块是4x4帧内预测类型（Intra4x4），作如下处理：\n\n- a)循环遍历 16 个 `4x4` 的块，并作如下处理：\n  - i.从 `intra4x4_pred_mode_cache` 中读取 `4x4` 帧内预测方法\n  - ii.根据帧内预测方法调用 H264PredContext 中的汇编函数 `pred4x4()` 进行帧内预测\n  - iii.调用 H264DSPContext 中的汇编函数 `h264_idct_add()` 对 DCT 残差数据进行 `4x4DCT` 反变换；如果DCT 系数中不包含 AC 系数的话，则调用汇编函数 `h264_idct_dc_add()` 对残差数据进行 `4x4DCT` 反变换（速度更快）。\n\n（2）如果宏块是 `16x16` 帧内预测类型（`Intra4x4`），作如下处理：\n\n- a)通过 `intra16x16_pred_mode` 获得 `16x16` 帧内预测方法\n- b)根据帧内预测方法调用 H264PredContext 中的汇编函数 `pred16x16 ()` 进行帧内预测\n- c)调用 H264DSPContext 中的汇编函数 `h264_luma_dc_dequant_idct ()` 对 16 个小块的 DC 系数进行Hadamard 反变换\n\n在这里需要注意，帧内 `4x4` 的宏块在执行完 `hl_decode_mb_predict_luma()` 之后实际上已经完成了 “帧内预测+DCT反变换” 的流程（解码完成）；而帧内 `16x16` 的宏块在执行完 `hl_decode_mb_predict_luma()` 之后仅仅完成了 “帧内预测+Hadamard反变换 ”的流程，而并未进行 “DCT反变换” 的步骤，这一步骤需要在后续步骤中完成。\n\n下文记录上述流程中涉及到的汇编函数（此处暂不记录DCT反变换的函数，在后文中再进行叙述）：\n\n- `4x4`帧内预测汇编函数：`H264PredContext -> pred4x4[dir`]()\n- `16x16` 帧内预测汇编函数：`H264PredContext -> pred16x16[dir]()`\n\n- Hadamard反变换汇编函数：`H264DSPContext->h264_luma_dc_dequant_idct()`\n\n下面根据源代码简单梳理一下 `hl_decode_mb_idct_luma()` 的流程：\n\n（1）判断宏块是否属于 `Intra4x4` 类型，如果是，函数直接返回（`Intra4x4` 比较特殊，它的 DCT 反变换已经前文所述的 “帧内预测” 部分完成）。\n\n（2）根据不同的宏块类型作不同的处理：\n\n- a) `Intra16x16`：调用 H264DSPContext 的汇编函数 `h264_idct_add16intra()` 进行 DCT 反变换\n- b) Inter类型：调用 H264DSPContext 的汇编函数 `h264_idct_add16()` 进行 DCT 反变换\n\nPS：需要注意的是 `h264_idct_add16intra()` 和 `h264_idct_add16()` 只有微小的区别，它们的基本逻辑都是把 `16x16` 的块划分为 16 个 `4x4` 的块再进行 DCT 反变换。此外还有一点需要注意：函数名中的 “add” 的含义是将 DCT 反变换之后的残差像素数据直接叠加到已有数据之上。\n\n## 宏块解码（Decode）部分-帧间宏块（Inter）\n\n本文分析FFmpeg的H.264解码器的宏块解码（Decode）部分。FFmpeg的H.264解码器调用 `decode_slice()` 函数完成了解码工作。这些解码工作可以大体上分为3个步骤：熵解码，宏块解码以及环路滤波。本文分析这3个步骤中的第2个步骤：宏块解码。上一篇文章已经记录了帧内预测宏块（Intra）的宏块解码，本文继续上一篇文章的内容，记录帧间预测宏块（Inter）的宏块解码。\n\n### 函数调用关系图\n\n参考宏块解码（Decode）部分的源代码的调用关系图\n\n`MCFUNC(hl_motion)` 根据子宏块的划分类型的不同，传递不同的参数调用 `mc_part()` 函数。\n\n（1）如果子宏块划分为 `16x16`（等同于没有划分），直接调用 `mc_part()` 并且传递如下参数：\n\n- a)单向预测汇编函数集：`qpix_put[0]` （`qpix_put[0]`中的函数进行 `16x16` 块的四分之一像素运动补偿）。\n- b)双向预测汇编函数集：`qpix_avg[0]`。\n- c) square 设置为 1，delta 设置为 0。\n- d) x_offset 和 y_offset 都设置为 0。\n\n（2）如果子宏块划分为 `16x8`，分两次调用 `mc_part()` 并且传递如下参数：\n\n- a)单向预测汇编函数集：`qpix_put[1]` （`qpix_put[1]` 中的函数进行 `8x8` 块的四分之一像素运动补偿）。\n- b)双向预测汇编函数集：`qpix_avg[1]`。\n- c) square 设置为 0，delta 设置为 8。\n\n其中第 1 次调用 `mc_part()` 的时候 x_offset 和 y_offset 都设置为 0，第 2 次调用 `mc_part()` 的时候 x_offset 设置为 0，y_offset 设置为 4。\n\n（3）如果子宏块划分为 `8x16`，分两次调用 `mc_part()` 并且传递如下参数：\n\n- a)单向预测汇编函数集：`qpix_put[1]` （`qpix_put[1]` 中的函数进行 `8x8` 块的四分之一像素运动补偿）。\n- b)双向预测汇编函数集：`qpix_avg[1]`。\n- c) square设置为 0，delta 设置为 `8 * h->mb_linesize`。\n\n其中第 1 次调用 `mc_part()` 的时候 x_offset 和 y_offset 都设置为 0，第 2 次调用 `mc_part()` 的时候 x_offset 设置为 4，y_offset 设置为 0。\n\n（4）如果子宏块划分为 `8x8`，说明此时每个 `8x8` 子宏块还可以继续划分为 `8x8，8x8，4x8，4x4` 几种类型，此时根据上述的规则，分成 4 次分别对这些小块做类似的处理。\n\n`qpix_put[4][16]` 实际上指向了 H264QpelContex 的 `put_h264_qpel_pixels_tab[4][16]` ，其中存储了所有单向预测方块的四分之一像素运动补偿函数。其中：\n\n```shell\nqpix_put[0]存储的是16x16方块的运动补偿函数；\nqpix_put[1]存储的是8x8方块的运动补偿函数；\nqpix_put[2]存储的是4x4方块的运动补偿函数；\nqpix_put[3]存储的是2x2方块的运动补偿函数；\n```\n\n从源代码可以看出，`mc_part_std()` 首先计算了几个关键的用于确定子宏块位置的参数，然后根据预测类型的不同（单向预测或者双向预测），把不同的函数指针传递给 `mc_dir_part()`：如果仅仅使用了 list0（单向预测），则只传递 `qpix_put()`；如果使用了 list0 和 list1（双向预测），则调用两次 `mc_dir_part()`，第一次传递 `qpix_put()`，第二次传递 `qpix_avg()`。\n\n`mc_part_std()` 中赋值了 3 个重要的变量（只考虑亮度）：\n\n（1）`dest_y`：指向子宏块亮度数据指针。这个值是通过 x_offset 和 y_offset 计算得来的。在这里需要注意一点：x_offset 和 y_offset 是以色度为基本单位的，所以在计算亮度相关的变量的时候需要乘以 2。\n\n（2）`x_offset`：传入的 x_offset 本来是子宏块相对于整个宏块位置的横坐标，在这里加上 `8 * h->mb_x` 之后，变成了子宏块相对于整个图像的位置的横坐标（以色度为基本单位）。\n\n（3）`y_offset`：传入的 y_offset 本来是子宏块相对于整个宏块位置的纵坐标，在这里加上 `8 * h->mb_y` 之后，变成了子宏块相对于整个图像的位置的纵坐标（以色度为基本单位）。\n\n通过源代码，简单梳理一下 `mc_dir_part()` 的流程（只考虑亮度，色度的流程类似）：\n\n（1）计算 mx 和 my。mx 和 my 是当前宏块的匹配块的位置坐标。需要注意的是该坐标是以 `1/4` 像素（而不是整像素）为基本单位的。\n\n（2）计算 offset。offset 是当前宏块的匹配块相对于图像的整像素偏移量，由 mx、my 计算而来。\n\n（3）计算 luma_xy。luma_xy 决定了当前宏块的匹配块采用的四分之一像素运动补偿的方式，由 mx、my 计算而来。\n\n（4）调用运动补偿汇编函数 `qpix_op[luma_xy]()` 完成运动补偿。在这里需要注意，如果子宏块不是正方形的（square 取 0），则还会调用 1 次 `qpix_op[luma_xy]()` 完成另外一个方块的运动补偿。\n\n总而言之，首先找到当前宏块的匹配块的整像素位置，然后在该位置的基础上进行四分之一像素的内插，并将结果输出出来。\n\n前文中曾经提过，由于 H.264 解码器中只提供了正方形块的四分之一像素运动补偿函数，所以如果子宏块不是正方形的（例如 `16x8，8x16`），就需要先将子宏块划分为正方形的方块，然后再进行两次运动补偿（两个正方形方块之间的位置关系用 delta 变量记录）。例如 `16x8` 的宏块，就会划分成两个 `8x8` 的方块，调用两次相同的运动补偿函数\n\n下面可以看一下 C 语言版本的四分之一像素运动补偿函数的源代码。由于 `1/4` 像素内插比较复杂，其中还用到了整像素赋值函数以及 `1/2` 像素线性内插函数，所以需要从简到难一步一步的看这些源代码。打算按照顺序一步一步分析这些源代码：\n\n（1）pel_template.c（展开“ `DEF_PEL(put, op_put)` ”宏）：整像素赋值（用于整像素的单向预测）\n\n（2）pel_template.c（展开“ `DEF_PEL(avg, op_avg)` ”宏）：整像素求平均（写这个为了举一个双向预测的例子）\n\n（3）hpel_template.c(（展开“`DEF_HPEL(put, op_put)`”宏）：`1/2` 像素线性内插\n\n（4）h264qpel_template.c（展开“ `H264_LOWPASS(put_, op_put, op2_put)`”宏）：半像素内插（注意不是1/2像素线性内插，而是需要滤波的）\n\n（5）h264qpel_template.c（展开“`H264_MC(put_, 8)`”宏）：`1/4`像素运动补偿\n\n## 环路滤波（Loop Filter）部分\n\n本文分析FFmpeg的H.264解码器的环路滤波（Loop Filter）部分。FFmpeg的H.264解码器调用decode_slice()函数完成了解码工作。这些解码工作可以大体上分为3个步骤：熵解码，宏块解码以及环路滤波。本文分析这3个步骤中的第3个步骤。\n\n### 函数调用关系图\n\n![环路滤波（Loop Filter）部分的源代码的调用关系](/images/imageFFmpeg/Thor/环路滤波部分的源代码的调用关系.png)\n\n环路滤波主要用于滤除方块效应。`decode_slice()` 在解码完一行宏块之后，会调用 `loop_filter()` 函数完成环路滤波功能。`loop_filter()` 函数会遍历该行宏块中的每一个宏块，并且针对每一个宏块调用 `ff_h264_filter_mb_fast()`。`ff_h264_filter_mb_fast()` 又会调用 `h264_filter_mb_fast_internal()`。\n\n`h264_filter_mb_fast_internal()` 完成了一个宏块的环路滤波工作。该函数调用 `filter_mb_edgev()` 和 `filter_mb_edgeh()` 对亮度垂直边界和水平边界进行滤波，或者调用 `filter_mb_edgecv()` 和 `filter_mb_edgech()` 对色度的的垂直边界和水平边界进行滤波。\n\n通过源代码整理出来 `h264_filter_mb_fast_internal()` 的流程如下：\n\n（1）读取 QP 等几个参数，用于推导滤波门限值 alpha，beta。\n\n（2）如果是帧内宏块（Intra），作如下处理：\n\n- a)对于水平的边界，调用 `filter_mb_edgeh()` 进行滤波。\n\n- b)对于垂直的边界，调用 `filter_mb_edgev()` 进行滤波。\n\n  帧内宏块滤波过程中，对于在宏块边界上的边界（最左边的垂直边界和最上边的水平边界），采用滤波强度 Bs 为 4 的滤波；对于其它边界则采用滤波强度 Bs 为 3 的滤波。\n\n（3）如果是其他宏块，作如下处理：\n\n- a)对于水平的边界，调用 `filter_mb_edgeh()` 进行滤波。\n\n- b)对于垂直的边界，调用 `filter_mb_edgev()` 进行滤波。\n\n  此类宏块的滤波强度需要另作判断。\n\n总体说来，一个宏块内部的滤波顺序如下图所示。图中的 “0”、“1”、“2”、“3” 为滤波的顺序。可以看出首先对垂直边界进行滤波，然后对水平边界进行滤波。垂直边界滤波按照从左到右的顺序进行，而水平边界的滤波按照从上到下的顺序进行。\n\n![宏块内部的滤波顺序](/images/imageFFmpeg/Thor/宏块内部的滤波顺序.png)\n\n#  H.264 中的 NAL 技术\n## NAL 技术\n\n### NAL 概述\n\nNAL 全称 Network Abstract Layer，即网络抽象层。在 H.264/AVC 视频编码标准中，整个系统框架被分为了两个层面：视频编码层面（VCL）和网络抽象层面（NAL）。其中，前者负责有效表示视频数据的内容，而后者则负责格式化数据并提供头信息，以保证数据适合各种信道和存储介质上的传输。\n\n现实中的传输系统是多样化的，其可靠性，服务质量，封装方式等特征各不相同，NAL 这一概念的提出提供了一个视频编码器和传输系统的友好接口，使得编码后的视频数据能够有效地在各种不同的网络环境中传输。\n\n### NAL 单元\n\nNAL 单元是 NAL 的基本语法结构，它包含一个字节的头信息和一系列来自 VCL 的称为原始字节序列载荷\n（RBSP）的字节流。头信息中包含着一个可否丢弃的指示标记，标识着该 NAL 单元的丢弃能否引起错误扩散，一般，如果 NAL 单元中的信息不用于构建参考图像，则认为可以将其丢弃；最后包含的是NAL 单元的类型信息，暗示着其内含有效载荷的内容。 送到解码器端的 NAL 单元必须遵守严格的顺序，如果应用程序接收到的 NAL 单元处于乱序，则必须提供一种恢复其正确顺序的方法。\n\n### NAL 实现编解码器与传输网络的结合\n\nNAL 提供了一个编解码器与传输网络的通用接口，而对于不同的网络环境，具体的实现方案是不同的。对于基于流的传输系统如 H.320、MPEG 等，需要按照解码顺序组织 NAL 单元，并为每个 NAL 单元增加若干比特字节对齐的前缀以形成字节流；对于 RTP/UDP/IP 系统，则可以直接将编码器输出的 NAL 单元作为 RTP 的有效载荷；而对于同时提供多个逻辑信道的传输系统，甚至可以根据重要性将不同类型的NAL 单元在不同服务质量的信道中传输。\n\n### 结论\n为了实现编解码器良好的网络适应性，需要做两方面的工作：\n\n第一、在 Codec 中将 NAL 这一技术完整而有效的实现；\n\n第二、在遵循 H.264/AVC NAL 规范的前提下设计针对不同网络的最佳传输方案。\n\n如果实现了以上两个目标，所实现的就不仅仅是一种视频编解码技术，而是一套适用范围很广的多媒体传输方案，该方案适用于如视频会议，数据存储，电视广播，流媒体，无线通信，远程监控等多种领域。\n\n## NALU 类型\n\n标识 NAL 单元中的 RBSP 数据类型，其中，nal_unit_type 为 1， 2， 3， 4， 5 的 NAL 单元称为 VCL 的 NAL单元，其他类型的 NAL 单元为非 VCL 的 NAL 单元。\n\n- 0：未规定\n- 1：非 IDR 图像中不采用数据划分的片段\n- 2：非 IDR 图像中 A 类数据划分片段\n- 3：非 IDR 图像中 B 类数据划分片段\n- 4：非 IDR 图像中 C 类数据划分片段\n- 5：IDR 图像的片段\n- 6：补充增强信息（SEI）\n- 7：序列参数集（SPS）\n- 8：图像参数集（PPS）\n- 9：分割符\n- 10：序列结束符\n- 11：流结束符\n- 12：填充数据\n- 13：序列参数集扩展\n- 14：带前缀的 NAL 单元\n- 15：子序列参数集\n- 16 – 18：保留\n- 19：不采用数据划分的辅助编码图像片段\n- 20：编码片段扩展\n- 21 – 23：保留\n- 24 – 31：未规定\n\n### SPS 详析\n\nTODO\n\n### PPS 详析\n\nTODO\n\n### SEI 详析\n\nTODO\n\n## NAL 在多媒体传输、存储系统中的应用\n\nNAL 的头占用了一个字节，按照比特自高至低排列可以表示如下：\n\n```shell\n0AABBBBB\n```\n\n其中，AA 用于表示该 NAL 是否可以丢弃（有无被其后的 NAL 参考），00b 表示没有参考作用，可丢弃，如 B slice、SEI 等，非零——包括 01b、10b、11b——表示该 NAL 不可丢弃，如 SPS、PPS、I Slice、P Slice 等。\n\n常用的 NAL 头的取值如：\n\n```shell\n0x67: SPS\n0x68: PPS\n0x65: IDR\n0x61: non-IDR Slice\n0x01: B Slice\n0x06: SEI\n0x09: AU Delimiter\n```\n\n由于 NAL 的语法中没有给出长度信息，实际的传输、存储系统需要增加额外的头实现各个 NAL 单元的定界。其中，AVI 文件和 MPEG TS 广播流采取的是字节流的语法格式，即在 NAL 单元之前增加 0x00000001 的同步码，则从 AVI 文件或 MPEG TS PES 包中读出的一个 H.264 视频帧以下面的形式存在：\n\n```shell\n00 00 00 01 06 ... 00 00 00 01 67 ... 00 00 00 01 68 ... 00 00 00 01 65 ...\nSEI 信息 \t\t\t  SPS \t\t\t\t PPS \t\t\t\tIDR Slice\n```\n\n而对于 MP4 文件，NAL 单元之前没有同步码，却有若干字节的长度码，来表示 NAL 单元的长度，这个长度码所占用的字节数由 MP4 文件头给出；此外，从 MP4 读出来的视频帧不包含 PPS 和 SPS，这些信息位于 MP4的文件头中，解析器必须在打开文件的时候就获取它们。从 MP4 文件读出的一个 H.264 帧往往是下面的形式（假设长度码为 2 字节）：\n\n```shell\n00 19 06 [... 25 字节...] 24 aa 65 [... 9386 字节...]\nSEI 信息 \t\t\t\t\tIDR Slice\n```\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"WebRTC开源src","url":"/2019-05-27/reference/webrtc/webrtc-opensrc/","content":"\n\nweb端用webRTC实现的一对一视频，互动直播和会议。<https://github.com/starrtc/android-demo>\n\nios源码<https://github.com/starrtc/ios-demo> \n\nweb端源码 <https://github.com/starrtc/webrtc-demo>  \n\n<!-- more -->\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"FFmpeg框架详解","url":"/2019-05-27/reference/FFmpeg/FFmpeg框架函数分析/","content":"\n> [[总结]FFMPEG视音频编解码零基础学习方法](<https://blog.csdn.net/leixiaohua1020/article/details/84499632>)\n\n# 架构图\n\n## FFMPEG+SDL的视频播放器\n\n> [最简单的基于FFMPEG+SDL的视频播放器 ver2 （采用SDL2.0）](<https://blog.csdn.net/leixiaohua1020/article/details/38868499>)\n\n**FFmpeg 解码一个视频流程：**\n\n<!-- more -->\n\n![FFmpeg解码一个视频流程](/images/imageFFmpeg/Thor/播放器解码的流程用图.png)\n\n**SDL2.0 显示 YUV 的流程：**\n\n![SDL2.0显示YUV的流程](/images/imageFFmpeg/Thor/SDL2.0显示YUV的流程图.png)\n\n## FFMPEG的视频编码器（YUV编码为H.264）\n\n> [最简单的基于FFMPEG的视频编码器（YUV编码为H.264）](<https://blog.csdn.net/leixiaohua1020/article/details/25430425>)\n>\n> [最简单的基于FFmpeg的视频编码器-更新版（YUV编码为HEVC(H.265)）](<https://blog.csdn.net/leixiaohua1020/article/details/39770947>)\n>\n> [最简单的基于FFmpeg的编码器-纯净版（不包含libavformat）](<https://blog.csdn.net/leixiaohua1020/article/details/42181271>)\n\n### FFmpeg编码视频的流程图\n\n通过该流程，不仅可以编码H.264/H.265的码流，而且可以编码MPEG4/MPEG2/VP9/VP8等多种码流。实际上使用FFmpeg编码视频的方式都是一样的。图中蓝色背景的函数是实际输出数据的函数。浅绿色的函数是视频编码的函数。\n\n![FFmpeg编码视频的流程图](/images/imageFFmpeg/Thor/FFmpeg编码视频的流程图.png)\n\n简单介绍一下流程中各个函数的意义：\n\n```c\nav_register_all()  // 注册FFmpeg所有编解码器。\navformat_alloc_output_context2()  // 初始化输出码流的AVFormatContext。\navio_open()  // 打开输出文件。\nav_new_stream()  // 创建输出码流的AVStream。\navcodec_find_encoder()  // 查找编码器。\navcodec_open2()  // 打开编码器。\navformat_write_header()  // 写文件头（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。\navcodec_encode_video2()  // 编码一帧视频。即将AVFrame（存储YUV像素数据）编码为AVPacket（存储H.264等格式的码流数据）。\nav_write_frame()  // 将编码后的视频码流写入文件。\nflush_encoder()  // 输入的像素数据读取完成后调用此函数。用于输出编码器中剩余的AVPacket。\nav_write_trailer()  // 写文件尾（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。\n```\n\n### “纯净”的基于FFmpeg的视频编码器\n\n以下记录一个更加 “纯净” 的基于 FFmpeg 的视频编码器。此前记录过一个基于 FFmpeg 的视频编码器：\n\n[ 《最简单的基于FFmpeg的视频编码器-更新版（YUV编码为HEVC(H.265)）》](http://blog.csdn.net/leixiaohua1020/article/details/39770947)\n\n这个视频编码器调用了 FFmpeg 中的 libavformat 和 libavcodec 两个库完成了视频编码工作。但是这不是一个 “纯净” 的编码器。\n\n上述两个库中 libavformat 完成封装格式处理，而 libavcodec 完成编码工作。\n\n一个 “纯净” 的编码器，理论上说只需要使用 libavcodec 就足够了，并不需要使用 libavformat。一下记录的编码器就是这样的一个 “纯净” 的编码器，它仅仅通过调用 libavcodec 将 YUV 数据编码为 H.264/HEVC 等格式的压缩视频码流。\n\n**仅使用libavcodec（不使用libavformat）编码视频的流程：**\n\n![仅使用libavcodec（不使用libavformat）编码视频的流程](/images/imageFFmpeg/Thor/仅使用libavcodec编码视频的流程.png)\n\n流程图中关键函数的作用如下所列：\n\n```c\navcodec_register_all()  // 注册所有的编解码器。\navcodec_find_encoder()  // 查找编码器。\navcodec_alloc_context3()  // 为AVCodecContext分配内存。\navcodec_open2()  // 打开编码器。\navcodec_encode_video2()  // 编码一帧数据。\n```\n\n两个存储数据的结构体如下所列：\n\n```c\nAVFrame  // 存储一帧未编码的像素数据。\nAVPacket  // 存储一帧压缩编码数据。\n```\n\n**对比：**\n\n简单记录一下这个只使用 libavcodec 的 “纯净版” 视频编码器和使用 libavcodec+libavformat 的视频编码器的不同。\n\n（1）\t下列与libavformat相关的函数在“纯净版”视频编码器中都不存在。\n\n```c\nav_register_all注册所有的编解码器，复用/解复用器等等组件。其中调用了\navcodec_register_all()  // 注册所有编解码器相关的组件。\navformat_alloc_context()  // 创建AVFormatContext结构体。\navformat_alloc_output_context2()  // 初始化一个输出流。\navio_open()  // 打开输出文件。\navformat_new_stream()  // 创建AVStream结构体。avformat_new_stream()中会调用\navcodec_alloc_context3()  // 创建AVCodecContext结构体。\navformat_write_header()  // 写文件头。\nav_write_frame()  // 写编码后的文件帧。\nav_write_trailer()  // 写文件尾。\n```\n\n（2）\t新增了如下几个函数\n\n```c\navcodec_register_all()  // 只注册编解码器有关的组件。\navcodec_alloc_context3()  // 创建AVCodecContext结构体。\n```\n\n可以看出，相比于“完整”的编码器，这个纯净的编码器函数调用更加简单，功能相对少一些，相对来说更加的“轻量”。\n\n## 解码框架图\n\n![FFmpeg解码](/images/imageFFmpeg/Thor/FFmpeg源码API结构图-解码.png)\n\n## 编码框架图\n\n![FFmpeg编码](/images/imageFFmpeg/Thor/FFmpeg源码API结构图-编码.png)\n\n# 通用函数解析\n\n> [函数解析](<https://blog.csdn.net/leixiaohua1020/article/details/44220151>)\n\n## av_register_all()\n\nffmpeg 注册复用器，编码器等的函数 `av_register_all()`。该函数在所有基于ffmpeg的应用程序中几乎都是第一个被调用的。只有调用了该函数，才能使用复用器，编码器等。\n\n函数调用关系图如下图所示。`av_register_all()` 调用了 `avcodec_register_all()`。`avcodec_register_all()` 注册了和编解码器有关的组件：硬件加速器，解码器，编码器，Parser，Bitstream Filter。`av_register_all()` 除了调用 `avcodec_register_all()` 之外，还注册了复用器，解复用器，协议处理器。\n\n![av_register_all](/images/imageFFmpeg/Thor/av_register_all.png)\n\n## 内存的分配和释放（av_malloc()、av_free()等）\n\n内存操作的常见函数位于 `libavutil\\mem.c` 中。本文记录FFmpeg开发中最常使用的几个函数：`av_malloc()`，`av_realloc()`，`av_mallocz()`，`av_calloc()`，`av_free()`，`av_freep()`。\n\n`av_malloc()` 就是简单的封装了系统函数malloc()，并做了一些错误检查工作。\n\n### 关于size_t\n\nsize _t  这个类型在 FFmpeg 中多次出现，简单解释一下其作用。size _t 是为了增强程序的可移植性而定义的。不同系统上，定义 size_t 可能不一样。它实际上就是 unsigned int。\n\n### 为什么要内存对齐？\n\nFFmpeg 内存分配方面多次涉及到 “内存对齐”（memory alignment）的概念。\n\n这方面内容在 IBM 的网站上有一篇文章，讲的挺通俗易懂的，在此简单转述一下。\n\n程序员通常认为内存就是一个字节数组，每次可以一个一个字节存取内存。例如在 C 语言中使用 `char *` 指代 “一块内存”，Java 中使用 `byte[]` 指代一块内存。如下所示。\n\n![](/images/imageFFmpeg/Thor/内存对齐-01.png)\n\n但那实际上计算机处理器却不是这样认为的。处理器相对比较 “懒惰”，它会以 2 字节，4 字节，8 字节，16 字节甚至 32 字节来存取内存。例如下图显示了以 4 字节为单位读写内存的处理器 “看待” 上述内存的方式。\n\n![](/images/imageFFmpeg/Thor/内存对齐-02.png)\n\n上述的存取单位的大小称之为内存存取粒度。\n\n下面看一个实例，分别从地址0，和地址 1 读取 4 个字节到寄存器。\n\n从程序员的角度来看，读取方式如下图所示。\n\n![](/images/imageFFmpeg/Thor/内存对齐-03.png)\n\n而 2 字节存取粒度的处理器的读取方式如下图所示。\n\n![](/images/imageFFmpeg/Thor/内存对齐-04.png)\n\n可以看出 2 字节存取粒度的处理器从地址 0 读取 4 个字节一共读取 2 次；从地址 1 读取 4 个字节一共读取了 3 次。由于每次读取的开销是固定的，因此从地址 1 读取 4 字节的效率有所下降。\n\n4 字节存取粒度的处理器的读取方式如下图所示。\n\n![](/images/imageFFmpeg/Thor/内存对齐-05.png)\n\n可以看出 4 字节存取粒度的处理器从地址 0 读取 4 个字节一共读取 1 次；从地址 1 读取 4 个字节一共读取了 2 次。从地址 1 读取的开销比从地址 0 读取多了一倍。由此可见内存不对齐对 CPU 的性能是有影响的。 \n\n```c\nav_malloc()  // 是FFmpeg中最常见的内存分配函数, av_malloc()就是简单的封装了系统函数malloc()\nav_realloc()  // 用于对申请的内存的大小进行调整。\nav_mallocz()  // 可以理解为av_malloc()+zeromemory\nav_calloc()  // 则是简单封装了av_mallocz()\nav_free()  // 用于释放申请的内存\nav_freep()  // 简单封装了av_free()。并且在释放内存之后将目标指针设置为NULL\n```\n\n## 常见结构体的初始化和销毁（AVFormatContext，AVFrame等）\n\n> [FFMPEG中最关键的结构体之间的关系](http://blog.csdn.net/leixiaohua1020/article/details/11693997)\n\n常见的结构体如下：\n\n```c\n// 统领全局的基本结构体。主要用于处理封装格式（FLV/MKV/RMVB 等）\nAVFormatContext\n\n// 输入输出对应的结构体，用于输入输出（读写文件，RTMP 协议等）\nAVIOContext\n\n// 视音频流对应的结构体，用于视音频编解码\nAVStream，AVCodecContext\n\n// 存储非压缩的数据（视频对应 RGB/YUV 像素数据，音频对应 PCM 采样数据）\nAVFrame\n\n// 存储压缩数据（视频对应 H.264 等码流数据，音频对应 AAC/MP3 等码流数据）\nAVPacket\n```\n\n他们之间的关系如下图所示：\n\n![常见结构体之间的关系](/images/imageFFmpeg/Thor/常见结构体之间的关系.png)\n\n简单分析一下上述几个结构体的初始化和销毁函数。这些函数列表如下。\n\n| 结构体          | 初始化                                        | 销毁                    |\n| --------------- | --------------------------------------------- | ----------------------- |\n| AVFormatContext | avformat_alloc_context()                      | avformat_free_context() |\n| AVIOContext     | avio_alloc_context()                          |                         |\n| AVStream        | avformat_new_stream()                         |                         |\n| AVCodecContext  | avcodec_alloc_context3()                      |                         |\n| AVFrame         | av_frame_alloc();<br />av_image_fill_arrays() | av_frame_free()         |\n| AVPacket        | av_init_packet();<br />av_new_packet()        | av_free_packet()        |\n\n### avformat_alloc_context()\n\n`avformat_alloc_context()` 的定义位于 `libavformat\\options.c`。\n\n`avformat_alloc_context()` 调用 `av_malloc()` 为 AVFormatContext 结构体分配了内存，而且同时也给 AVFormatContext 中的 `internal` 字段分配内存（这个字段是 FFmpeg 内部使用的，先不分析）。此外调用了一个 `avformat_get_context_defaults()` 函数。该函数用于设置 AVFormatContext 的字段的默认值。它的定义也位于 `libavformat\\options.c`，确切的说就位于 `avformat_alloc_context()`上面\n\n`avformat_get_context_defaults()` 首先调用 `memset()` 将 AVFormatContext 的所有字段置 0。而后调用了一个函数 `av_opt_set_defaults()` 。`av_opt_set_defaults()` 用于给字段设置默认值。\n\n`avformat_alloc_context()` 代码的函数调用关系如下图所示。\n\n![avformat_alloc_context](/images/imageFFmpeg/Thor/avformat_alloc_context.png)\n\n`avformat_free_context()` 的声明位于 `libavformat\\avformat.h`\n\n`avformat_free_context()` 的定义位于 `libavformat\\options.c`\n\n`avformat_free_context()` 调用了各式各样的销毁函数：`av_opt_free()`，`av_freep()`，`av_dict_free()`。这些函数分别用于释放不同种类的变量，在这里不再详细讨论。\n\n在这里看一个释放 AVStream 的函数 `ff_free_stream()`。该函数的定义位于 `libavformat\\options.c`（其实就在 `avformat_free_context()` 上方）, 与释放 AVFormatContext 类似，释放 AVStream 的时候，也是调用了 `av_freep()`，`av_dict_free()` 这些函数释放有关的字段。如果使用了 parser 的话，会调用 `av_parser_close()` 关闭该 parser。\n\n### avio_alloc_context()\n\nAVIOContext 的初始化函数是 `avio_alloc_context()`，销毁的时候使用 `av_free()` 释放掉其中的缓存即可。它的声明位于 `libavformat\\avio.h` 中\n\n`avio_alloc_context()` 定义位于 `libavformat\\aviobuf.c` 中\n\n`avio_alloc_context()` 首先调用 `av_mallocz()` 为 AVIOContext 分配内存。而后调用了一个函数 `ffio_init_context()` 。该函数完成了真正的初始化工作\n\n### avformat_new_stream()\n\n`avformat_new_stream()` 的声明位于 `libavformat\\avformat.h` 中\n\nAVStream 的初始化函数是 `avformat_new_stream()`，销毁函数使用销毁 AVFormatContext 的 `avformat_free_context()` 就可以了。\n\n`avformat_new_stream()` 的定义位于 `libavformat\\utils.c` 中\n\n`avformat_new_stream()` 首先调用 `av_mallocz()`  为 AVStream 分配内存。接着给新分配的AVStream 的各个字段赋上默认值。然后调用了另一个函数 `avcodec_alloc_context3()` 初始化 AVStream 中的 AVCodecContext。\n\n### avcodec_alloc_context3()\n\n`avcodec_alloc_context3()` 的声明位于 `libavcodec\\avcodec.h` 中\n\n`avcodec_alloc_context3()` 的定义位于 `libavcodec\\options.c` 中\n\n`avcodec_alloc_context3()` 首先调用 `av_malloc()` 为 AVCodecContext 分配存储空间，然后调用了一个函数 `avcodec_get_context_defaults3()` 用于设置该 AVCodecContext 的默认值\n\n`avformat_new_stream()` 函数的调用结构如下所示：\n\n![avformat_new_stream](/images/imageFFmpeg/Thor/avformat_new_stream.png)\n\n### av_frame_alloc()\n\nAVFrame 的初始化函数是 `av_frame_alloc()`，销毁函数是 `av_frame_free()`。在这里有一点需要注意，旧版的 FFmpeg 都是使用 `avcodec_alloc_frame()` 初始化 AVFrame 的，但是我在写这篇文章的时候，`avcodec_alloc_frame()` 已经被标记为 “过时的” 了，为了保证与时俱进，决定分析新的`API——av_frame_alloc()`。\n\n`av_frame_alloc()` 的声明位于 `libavutil\\frame.h`\n\n`av_frame_alloc()` 的定义位于 `libavutil\\frame.c`\n\n`av_frame_alloc()` 首先调用 `av_mallocz()` 为 AVFrame 结构体分配内存。而后调用了一个函数`get_frame_defaults()` 用于设置一些默认参数\n\n从 `av_frame_alloc()` 的代码我们可以看出，该函数并没有为 AVFrame 的像素数据分配空间。因此AVFrame 中的像素数据的空间需要自行分配空间，例如使用 `avpicture_fill()`， `av_image_fill_arrays()` 等函数。\n\n`av_frame_alloc()` 函数的调用结构如下所示：\n\n![av_frame_alloc](/images/imageFFmpeg/Thor/av_frame_alloc.png)\n\n#### avpicture_fill()\n\n`avpicture_fill()` 的声明位于 `libavcodec\\avcodec.h`\n\n`avpicture_fill()` 的定义位于 `libavcodec\\avpicture.c`\n\n`avpicture_fill()` 仅仅是简单调用了一下 `av_image_fill_arrays()`。也就是说这两个函数实际上是等同的\n\n#### av_image_fill_arrays()\n\n`av_image_fill_arrays()` 的声明位于 `libavutil\\imgutils.h` 中\n\n`av_image_fill_arrays()` 的定义位于 `libavutil\\imgutils.c` 中\n\n`av_image_fill_arrays()` 函数中包含 3 个函数：`av_image_check_size()`，`av_image_fill_linesizes()`，`av_image_fill_pointers()`。`av_image_check_size()` 用于检查输入的宽高参数是否合理，即不能太大或者为负数。`av_image_fill_linesizes()` 用于填充dst_linesize。`av_image_fill_pointers()` 则用于填充 dst_data。它们的定义相对比较简单，不再详细分析。\n\n`avpicture_fill()` 函数调用关系如下图所示：\n\n![avpicture_fill](/images/imageFFmpeg/Thor/avpicture_fill.png)\n\n### av_init_packet()\n\n`av_init_packet()` 的声明位于 `libavcodec\\avcodec.h`\n\n`av_init_packet()` 的定义位于 `libavcodec\\avpacket.c`\n\n### av_new_packet()\n\n`av_new_packet()` 的声明位于 `libavcodec\\avcodec.h`\n\n`av_new_packet()` 的定义位于 `libavcodec\\avpacket.c`\n\n`av_new_packet()` 调用了 `av_init_packet(pkt)`。此外还调用了一个函数 `packet_alloc()`\n\n`packet_alloc()` 中调用 `av_buffer_realloc()` 为 AVPacket 分配内存。然后调用 `memset()` 将分配的内存置 0。\n\nPS：发现 AVPacket 的结构随着 FFmpeg 的发展越发复杂了。原先 AVPacket 中的数据仅仅存在一个 uint8_t 类型的数组里，而现在已经使用一个专门的结构体 AVBufferRef 存储数据。\n\n`av_new_packet()` 代码的函数调用关系如下图所示：\n\n![av_new_packet](/images/imageFFmpeg/Thor/av_new_packet.png)\n\n`av_free_packet()` 的声明位于 `libavcodec\\avcodec.h`\n\n`av_free_packet()` 的定义位于 `libavcodec\\avpacket.c`\n\n`av_free_packet()` 调用 `av_buffer_unref()` 释放 AVPacket 中的数据，而后还调用了`av_packet_free_side_data()` 释放了 side_data（存储封装格式可以提供的额外的数据）。\n\n## avio_open2() \n\n该函数用于打开 FFmpeg 的输入输出文件。`avio_open2()` 的声明位于 `libavformat\\avio.h` 文件中\n\n```c\nint avio_open2(AVIOContext **s, const char *url, int flags,\n               const AVIOInterruptCB *int_cb, AVDictionary **options);\n```\n\n`avio_open2()` 函数参数的含义如下：\n\n```shell\ns：函数调用成功之后创建的AVIOContext结构体。\nurl：输入输出协议的地址（文件也是一种“广义”的协议，对于文件来说就是文件的路径）。\nflags：打开地址的方式。可以选择只读，只写，或者读写。取值如下。\nAVIO_FLAG_READ：只读。\nAVIO_FLAG_WRITE：只写。\nAVIO_FLAG_READ_WRITE：读写。\nint_cb：目前还没有用过。\noptions：目前还没有用过。\n```\n\n函数调用结构图：\n\n![avio_open2](/images/imageFFmpeg/Thor/avio_open2.png)\n\n## av_find_decoder() 和 av_find_encoder()\n\n`avcodec_find_encoder()` 用于查找 FFmpeg 的编码器，\n\n`avcodec_find_decoder()` 用于查找 FFmpeg 的解码器。\n\n`avcodec_find_encoder()` 的声明位于 `libavcodec\\avcodec.h`\n\n```c\nAVCodec *avcodec_find_encoder(enum AVCodecID id);\n```\n\n函数的参数是一个编码器的 ID，返回查找到的编码器（没有找到就返回NULL）。\n\n`avcodec_find_decoder()` 的声明也位于 `libavcodec\\avcodec.h`\n\n```c\nAVCodec *avcodec_find_decoder(enum AVCodecID id);\n```\n\n函数的参数是一个解码器的 ID，返回查找到的解码器（没有找到就返回NULL）。\n\n`avcodec_find_encoder()` 和 `avcodec_find_decoder()` 的函数调用关系图如下所示：\n\n![函数调用关系图](/images/imageFFmpeg/Thor/avcodec_find_encoder.png)\n\n`avcodec_find_encoder()` 的源代码位于 `libavcodec\\utils.c`\n\n`avcodec_find_encoder()` 调用了一个 `find_encdec()`，注意它的第二个参数是 1。\n\n`find_encdec()` 的源代码位于 `libavcodec\\utils.c`\n\n`find_encdec()` 中有一个循环，该循环会遍历 AVCodec 结构的链表，逐一比较输入的 ID 和每一个编码器的 ID，直到找到 ID 取值相等的编码器。\n\n在这里有几点需要注意：\n\n（1）first_avcodec 是一个全局变量，存储 AVCodec 链表的第一个元素。\n\n（2）`remap_deprecated_codec_id()` 用于将一些过时的编码器 ID 映射到新的编码器 ID。\n\n（3）函数的第二个参数 encoder 用于确定查找编码器还是解码器。当该值为 1 的时候，用于查找编码器，此时会调用 `av_codec_is_encoder()` 判断 AVCodec 是否为编码器；当该值为 0 的时候，用于查找解码器，此时会调用 `av_codec_is_decoder()` 判断 AVCodec 是否为解码器。\n\n`avcodec_find_decoder()` 的源代码位于 `libavcodec\\utils.c`\n\n`avcodec_find_decoder()` 同样调用了 `find_encdec()`，只是第 2 个参数设置为 0。\n\n## avcodec_open2()\n\n该函数用于初始化一个视音频编解码器的 AVCodecContext。\n\n`avcodec_open2()` 的声明位于 `libavcodec\\avcodec.h`\n\n```c\nint avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options);\n```\n\n用中文简单转述一下avcodec_open2()各个参数的含义：\n\n```shell\navctx：需要初始化的 AVCodecContext。\ncodec：输入的 AVCodec\noptions：一些选项。例如使用 libx264 编码的时候，“preset”，“tune”等都可以通过该参数设置。\n```\n\n`avcodec_open2()` 函数调用关系非常简单，如下图所示：\n\n![avcodec_open2](/images/imageFFmpeg/Thor/avcodec_open2.png)\n\n`avcodec_open2()` 的定义位于 `libavcodec\\utils.c`\n\n`avcodec_open2()` 的源代码量是非常长的，但是它的调用关系非常简单——它只调用了一个关键的函数，即 AVCodec 的 `init()`，后文将会对这个函数进行分析。\n\n我们可以简单梳理一下 `avcodec_open2()` 所做的工作，如下所列：\n\n（1）为各种结构体分配内存（通过各种 `av_malloc()` 实现）。\n\n（2）将输入的 AVDictionary 形式的选项设置到 AVCodecContext。\n\n（3）其他一些零零碎碎的检查，比如说检查编解码器是否处于 “实验” 阶段。\n\n（4）如果是编码器，检查输入参数是否符合编码器的要求\n\n（5）调用 AVCodec 的 `init()` 初始化具体的解码器。\n\n前几步比较简单，不再分析。在这里我们分析一下第4步和第5步。\n\n### 检查输入参数是否符合编码器要求\n\n在这里简单分析一下第 4 步，即 “检查输入参数是否符合编码器的要求”。这一步中检查了很多的参数，在这里我们随便选一个参数 pix_fmts（像素格式）看一下，如下所示。\n\n<details><summary>代码：</summary>\n\n```c\n//检查像素格式\n        if (avctx->codec->pix_fmts) {\n            for (i = 0; avctx->codec->pix_fmts[i] != AV_PIX_FMT_NONE; i++)\n                if (avctx->pix_fmt == avctx->codec->pix_fmts[i])\n                    break;\n            if (avctx->codec->pix_fmts[i] == AV_PIX_FMT_NONE\n                && !((avctx->codec_id == AV_CODEC_ID_MJPEG || avctx->codec_id == AV_CODEC_ID_LJPEG)\n                     && avctx->strict_std_compliance <= FF_COMPLIANCE_UNOFFICIAL)) {\n                char buf[128];\n                snprintf(buf, sizeof(buf), \"%d\", avctx->pix_fmt);\n                av_log(avctx, AV_LOG_ERROR, \"Specified pixel format %s is invalid or not supported\\n\",\n                       (char *)av_x_if_null(av_get_pix_fmt_name(avctx->pix_fmt), buf));\n                ret = AVERROR(EINVAL);\n                goto free_and_end;\n            }\n            if (avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ420P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ411P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ422P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ440P ||\n                avctx->codec->pix_fmts[i] == AV_PIX_FMT_YUVJ444P)\n                avctx->color_range = AVCOL_RANGE_JPEG;\n        }\n```\n\n</details>\n\n可以看出，该代码首先进入了一个 `for()` 循环，将 AVCodecContext 中设定的 `pix_fmt` 与编码器AVCodec 中的 `pix_fmts` 数组中的元素逐一比较。\n\n先简单介绍一下 AVCodec 中的 `pix_fmts` 数组。AVCodec 中的 `pix_fmts` 数组存储了该种编码器支持的像素格式，并且规定以 AV_PIX_FMT_NONE（AV_PIX_FMT_NONE 取值为 -1）为结尾。例如，libx264 的 `pix_fmts` 数组的定义位于 `libavcodec\\libx264.c`，如下所示。\n\n<details><summary>代码：</summary>\n\n```c\nstatic const enum AVPixelFormat pix_fmts_8bit[] = {\n    AV_PIX_FMT_YUV420P,\n    AV_PIX_FMT_YUVJ420P,\n    AV_PIX_FMT_YUV422P,\n    AV_PIX_FMT_YUVJ422P,\n    AV_PIX_FMT_YUV444P,\n    AV_PIX_FMT_YUVJ444P,\n    AV_PIX_FMT_NV12,\n    AV_PIX_FMT_NV16,\n    AV_PIX_FMT_NONE\n};\n```\n\n</details>\n\n从 `pix_fmts_8bit` 的定义可以看出 libx264 主要支持的是以 YUV 为主的像素格式。\n\n现在回到 “检查输入 `pix_fmt` 是否符合编码器的要求” 的那段代码。如果 `for()` 循环从 `AVCodec->pix_fmts` 数组中找到了符合 `AVCodecContext->pix_fmt` 的像素格式，或者完成了 `AVCodec->pix_fmts` 数组的遍历，都会跳出循环。如果发现 `AVCodec->pix_fmts` 数组中索引为 `i` 的元素是 AV_PIX_FMT_NONE（即最后一个元素，取值为 -1）的时候，就认为没有找到合适的像素格式，并且最终提示错误信息。\n\n### AVCodec->init()\n`avcodec_open2()` 中最关键的一步就是调用 AVCodec 的 `init()` 方法初始化具体的编码器。AVCodec 的 `init()` 是一个函数指针，指向具体编解码器中的初始化函数。这里我们以 libx264 为例，看一下它对应的 AVCodec 的定义。\n\nlibx264 对应的 AVCodec 的定义位于 `libavcodec\\libx264.c`\n\n<details><summary>代码：</summary>\n\n```c\nAVCodec ff_libx264_encoder = {\n    .name             = \"libx264\",\n    .long_name        = NULL_IF_CONFIG_SMALL(\"libx264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10\"),\n    .type             = AVMEDIA_TYPE_VIDEO,\n    .id               = AV_CODEC_ID_H264,\n    .priv_data_size   = sizeof(X264Context),\n    .init             = X264_init,\n    .encode2          = X264_frame,\n    .close            = X264_close,\n    .capabilities     = CODEC_CAP_DELAY | CODEC_CAP_AUTO_THREADS,\n    .priv_class       = &x264_class,\n    .defaults         = x264_defaults,\n    .init_static_data = X264_init_static,\n};\n```\n\n</details>\n\n可以看出在 `ff_libx264_encoder` 中 `init()` 指向 `X264_init()` 。`X264_init()` 的定义同样位于`libavcodec\\libx264.c`\n\n`X264_init()` 的代码以后研究 X264 的时候再进行细节的分析，在这里简单记录一下它做的两项工作：\n\n（1）设置 X264Context 的参数。X264Context 主要完成了 libx264 和 FFmpeg 对接的功能。可以看出代码主要在设置一个 params 结构体变量，该变量的类型即是 x264 中存储参数的结构体 `x264_param_t`。\n（2）调用 libx264 的 API 进行编码器的初始化工作。例如调用 `x264_param_default()` 设置默认参数，调用 `x264_param_apply_profile()` 设置 profile，调用 `x264_encoder_open()` 打开编码器等等。\n\n最后附上 X264Context 的定义，位于 `libavcodec\\libx264.c`\n\n## avcodec_close()\n\n该函数用于关闭编码器。`avcodec_close()` 函数的声明位于 `libavcodec\\avcodec.h`\n\n```c\nint avcodec_close(AVCodecContext *avctx);\n```\n\n该函数只有一个参数，就是需要关闭的编码器的 AVCodecContext。\n\n函数的调用关系图如下所示：\n\n![avcodec_close](/images/imageFFmpeg/Thor/avcodec_close.png)\n\n`avcodec_close()` 的定义位于 `libavcodec\\utils.c`\n\n从 `avcodec_close()` 的定义可以看出，该函数释放 AVCodecContext 中有关的变量，并且调用了 AVCodec 的 `close()` 关闭了解码器。\n\n# 解码\n\n## 图解 FFMPEG 打开媒体的函数 avformat_open_input\n\nFFMPEG打开媒体的的过程开始于avformat_open_input，因此该函数的重要性不可忽视。\n\n在该函数中，FFMPEG完成了：\n\n- 输入输出结构体 AVIOContext 的初始化；\n\n- 输入数据的协议（例如 RTMP，或者 file）的识别（通过一套评分机制）:\n  - 判断文件名的后缀 \n  - 读取文件头的数据进行比对；\n\n- 使用获得最高分的文件协议对应的 URLProtocol，通过函数指针的方式，与 FFMPEG 连接（非专业用词）；\n\n剩下的就是调用该 URLProtocol 的函数进行 open, read 等操作了\n\n以下是通过 eclipse+MinGW 调试 FFMPEG 源代码获得的函数调用关系图：\n\n![](/images/imageFFmpeg/Thor/图解FFMPEG打开媒体的函数avformat_open_input.png)\n\n可见最终都调用了 URLProtocol 结构体中的函数指针。\n\nURLProtocol 结构如下，是一大堆函数指针的集合（avio.h文件）\n\n<details><summary>代码</summary>\n\n```cpp\ntypedef struct URLProtocol {\n    const char *name;\n    int (*url_open)(URLContext *h, const char *url, int flags);\n    int (*url_read)(URLContext *h, unsigned char *buf, int size);\n    int (*url_write)(URLContext *h, const unsigned char *buf, int size);\n    int64_t (*url_seek)(URLContext *h, int64_t pos, int whence);\n    int (*url_close)(URLContext *h);\n    struct URLProtocol *next;\n    int (*url_read_pause)(URLContext *h, int pause);\n    int64_t (*url_read_seek)(URLContext *h, int stream_index,\n                             int64_t timestamp, int flags);\n    int (*url_get_file_handle)(URLContext *h);\n    int priv_data_size;\n    const AVClass *priv_data_class;\n    int flags;\n    int (*url_check)(URLContext *h, int mask);\n} URLProtocol;\n```\n\n</details>\n\nURLProtocol 功能就是完成各种输入协议的读写等操作\n\n但输入协议种类繁多，它是怎样做到 “大一统” 的呢？\n\n原来，每个具体的输入协议都有自己对应的 URLProtocol。\n\n比如 file 协议（FFMPEG 把文件也当做一种特殊的协议）（`*file.c` 文件）\n\n<details><summary>代码：</summary>\n\n```cpp\nURLProtocol ff_pipe_protocol = {\n    .name                = \"pipe\",\n    .url_open            = pipe_open,\n    .url_read            = file_read,\n    .url_write           = file_write,\n    .url_get_file_handle = file_get_handle,\n    .url_check           = file_check,\n};\n```\n\n</details>\n\n或者rtmp协议（此处使用了librtmp）（librtmp.c文件）\n\n<details><summary>代码：</summary>\n\n```cpp\nURLProtocol ff_rtmp_protocol = {\n    .name                = \"rtmp\",\n    .url_open            = rtmp_open,\n    .url_read            = rtmp_read,\n    .url_write           = rtmp_write,\n    .url_close           = rtmp_close,\n    .url_read_pause      = rtmp_read_pause,\n    .url_read_seek       = rtmp_read_seek,\n    .url_get_file_handle = rtmp_get_file_handle,\n    .priv_data_size      = sizeof(RTMP),\n    .flags               = URL_PROTOCOL_FLAG_NETWORK,\n};\n```\n\n</details>\n\n可见它们把各自的函数指针都赋值给了 URLProtocol 结构体的函数指针\n\n因此 `avformat_open_input` 只需调用 url_open, url_read 这些函数就可以完成各种具体输入协议的 open, read 等操作了\n\n## avformat_open_input()\n\n> [FFMPEG源码分析：avformat_open_input()（媒体打开函数）](<https://blog.csdn.net/leixiaohua1020/article/details/11885813>)\n>\n> [avformat_open_input()](<https://blog.csdn.net/leixiaohua1020/article/details/44064715>)\n\n个人感觉这个函数确实太重要了，可以算作 FFmpeg 的 “灵魂”\n\n函数用于打开多媒体数据并且获得一些相关的信息。它的声明位于 `libavformat\\avformat.h`\n\n```c\nint avformat_open_input(AVFormatContext **ps, const char *filename, AVInputFormat *fmt, AVDictionary **options);\n```\n\n参数说明：\n\n```shell\nps：函数调用成功之后处理过的 AVFormatContext 结构体。\nfile：打开的视音频流的 URL。\nfmt：强制指定 AVFormatContext 中 AVInputFormat 的。这个参数一般情况下可以设置为 NULL，这样 FFmpeg 可以自动检测 AVInputFormat。\ndictionay：附加的一些选项，一般情况下可以设置为 NULL。\n```\n\n函数执行成功的话，其返回值大于等于 0。\n\n函数调用结构图如下所示：\n\n![avformat_open_input](/images/imageFFmpeg/Thor/avformat_open_input.png)\n\n`avformat_open_input()` 定义位于 `libavformat\\utils.c` 中\n\n`avformat_open_input()` 源代码比较长，一部分是一些容错代码，比如说如果发现传入的 AVFormatContext 指针没有初始化过，就调用 `avformat_alloc_context()` 初始化该结构体；还有一部分是针对一些格式做的特殊处理，比如 id3v2 信息的处理等等。有关上述两种信息不再详细分析，在这里只选择它关键的两个函数进行分析：\n\n- **`init_input()`**：绝大部分初始化工作都是在这里做的。\n\n- **`s->iformat->read_header()`**：读取多媒体数据文件头，根据视音频流创建相应的 AVStream。\n\n### init_input()\n\n`init_input()` 作为一个内部函数，竟然包含了一行注释（一般内部函数都没有注释），足可以看出它的重要性。它的主要工作就是打开输入的视频数据并且探测视频的格式。该函数的定义位于 `libavformat\\utils.c`\n\n<details><summary>代码：</summary>\n\n```cpp\n/* Open input file and probe the format if necessary. */\nstatic int init_input(AVFormatContext *s, const char *filename,\n                      AVDictionary **options)\n{\n    int ret;\n    AVProbeData pd = { filename, NULL, 0 };\n    int score = AVPROBE_SCORE_RETRY;\n \n    if (s->pb) {\n        s->flags |= AVFMT_FLAG_CUSTOM_IO;\n        if (!s->iformat)\n            return av_probe_input_buffer2(s->pb, &s->iformat, filename,\n                                         s, 0, s->format_probesize);\n        else if (s->iformat->flags & AVFMT_NOFILE)\n            av_log(s, AV_LOG_WARNING, \"Custom AVIOContext makes no sense and \"\n                                      \"will be ignored with AVFMT_NOFILE format.\\n\");\n        return 0;\n    }\n \n    if ((s->iformat && s->iformat->flags & AVFMT_NOFILE) ||\n        (!s->iformat && (s->iformat = av_probe_input_format2(&pd, 0, &score))))\n        return score;\n \n    if ((ret = avio_open2(&s->pb, filename, AVIO_FLAG_READ | s->avio_flags,\n                          &s->interrupt_callback, options)) < 0)\n        return ret;\n    if (s->iformat)\n        return 0;\n    return av_probe_input_buffer2(s->pb, &s->iformat, filename,\n                                 s, 0, s->format_probesize);\n}\n```\n\n</details>\n\n这个函数在短短的几行代码中包含了好几个 return，因此逻辑还是有点复杂的，我们可以梳理一下：\n\n在函数的开头的 score 变量是一个判决 AVInputFormat 的分数的门限值，如果最后得到的 AVInputFormat 的分数低于该门限值，就认为没有找到合适的 AVInputFormat 。\n\nFFmpeg 内部判断封装格式的原理实际上是对每种 AVInputFormat 给出一个分数，满分是 100 分，越有可能正确的 AVInputFormat 给出的分数就越高。最后选择分数最高的 AVInputFormat 作为推测结果。score 的值是一个宏定义 AVPROBE_SCORE_RETRY，我们可以看一下它的定义：\n\n```cpp\n#define AVPROBE_SCORE_RETRY (AVPROBE_SCORE_MAX/4)\n```\n\n其中 AVPROBE_SCORE_MAX 是 score 的最大值，取值是 100：\n\n```cpp\n#define AVPROBE_SCORE_MAX       100 ///< maximum score\n```\n\n由此我们可以得出 score 取值是 25，即如果推测后得到的最佳 AVInputFormat 的分值低于 25，就认为没有找到合适的 AVInputFormat。\n\n整个函数的逻辑大体如下：\n\n（1）当使用了自定义的 AVIOContext 的时候（AVFormatContext 中的 AVIOContext 不为空，即 `s->pb!=NULL`），如果指定了 AVInputFormat 就直接返回，如果没有指定就调用 `av_probe_input_buffer2()` 推测 AVInputFormat。这一情况出现的不算很多，但是当我们从内存中读取数据的时候（需要初始化自定义的 AVIOContext），就会执行这一步骤。\n\n（2）在更一般的情况下，如果已经指定了 AVInputFormat，就直接返回；如果没有指定 AVInputFormat，就调用 `av_probe_input_format(NULL,…)` 根据文件路径判断文件格式。这里特意把 `av_probe_input_format()` 的第 1 个参数写成 “NULL”，是为了强调这个时候实际上并没有给函数提供输入数据，此时仅仅通过文件路径推测 AVInputFormat。\n\n（3）如果发现通过文件路径判断不出来文件格式，那么就需要打开文件探测文件格式了，这个时候会首先调用 `avio_open2()` 打开文件，然后调用 `av_probe_input_buffer2()` 推测 AVInputFormat。\n\n## avformat_find_stream_info()\n\n该函数可以读取一部分视音频数据并且获得一些相关的信息。\n\n`avformat_find_stream_info()` 的声明位于 `libavformat\\avformat.h`\n\n```c\nint avformat_find_stream_info(AVFormatContext *ic, AVDictionary **options);\n```\n\n简单解释一下它的参数的含义：\n\n```shell\nic：输入的 AVFormatContext。\noptions：额外的选项，目前没有深入研究过。\n```\n\n函数正常执行后返回值大于等于 0。\n\nPS：由于该函数比较复杂，所以只看了一部分代码，以后有时间再进一步分析。\n\n函数的调用关系如下图所示：\n\n![avformat_find_stream_info](/images/imageFFmpeg/Thor/avformat_find_stream_info.png)\n\n`avformat_find_stream_info()` 的定义位于 `libavformat\\utils.c`\n\n由于`avformat_find_stream_info()` 代码比较长，难以全部分析，在这里只能简单记录一下它的要点。该函数主要用于给每个媒体流（音频/视频）的 AVStream 结构体赋值。我们大致浏览一下这个函数的代码，会发现它其实已经实现了解码器的查找，解码器的打开，视音频帧的读取，视音频帧的解码等工作。换句话说，该函数实际上已经“走通”的解码的整个流程。下面看一下除了成员变量赋值之外，该函数的几个关键流程。\n\n- 查找解码器：`find_decoder()`\n\n- 打开解码器：`avcodec_open2()`\n\n- 读取完整的一帧压缩编码的数据：`read_frame_internal()`\n\n  注：`av_read_frame()` 内部实际上就是调用的 `read_frame_internal()`。\n\n- 解码一些压缩编码数据：`try_decode_frame()`\n\n## av_read_frame()\n\nffmpeg 中的 `av_read_frame()` 的作用是读取码流中的音频若干帧或者视频一帧。例如，解码视频的时候，每解码一个视频帧，需要先调用 `av_read_frame()` 获得一帧视频的压缩数据，然后才能对该数据进行解码（例如 H.264 中一帧压缩数据通常对应一个 NAL）。\n\n上代码之前，先参考了其他人对 `av_read_frame()` 的解释，在此做一个参考：\n\n> 通过 `av_read_packet()`，读取一个包，需要说明的是此函数必须是包含整数帧的，不存在半帧的情况，以 ts 流为例，是读取一个完整的 PES 包（一个完整 pes 包包含若干视频或音频 es 包），读取完毕后，通过 `av_parser_parse2()` 分析出视频一帧（或音频若干帧），返回，下次进入循环的时候，如果上次的数据没有完全取完，则 `st = s->cur_st` ; 不会是 NULL，即再此进入 `av_parser_parse2()` 流程，而不是下面的 `av_read_packet（）` 流程，这样就保证了，如果读取一次包含了 N 帧视频数据（以视频为例），则调用 `av_read_frame（）` N 次都不会去读数据，而是返回第一次读取的数据，直到全部解析完毕。\n\n`av_read_frame()` 的声明位于 `libavformat\\avformat.h`\n\n```c\nint av_read_frame(AVFormatContext *s, AVPacket *pkt);\n```\n\n`av_read_frame()` 使用方法在注释中写得很详细，用中文简单描述一下它的两个参数：\n\n```shell\ns：输入的AVFormatContext\npkt：输出的AVPacket\n```\n\n如果返回 0 则说明读取正常。\n\n函数调用结构图如下所示：\n\n![av_read_frame](/images/imageFFmpeg/Thor/av_read_frame.png)\n\n`av_read_frame()` 的定义位于 `libavformat\\utils.c`\n\n`read_frame_internal()` 代码比较长，这里只简单看一下它前面的部分。它前面部分有 2 步是十分关键的：\n\n（1）调用了 `ff_read_packet()` 从相应的 AVInputFormat 读取数据。\n\n（2）如果媒体频流需要使用 AVCodecParser，则调用 `parse_packet()` 解析相应的 AVPacket。\n\n`ff_read_packet()` 中最关键的地方就是调用了 AVInputFormat 的 `read_packet()` 方法。 AVInputFormat 的 `read_packet()` 是一个函数指针，指向当前的 AVInputFormat 的读取数据的函数。在这里我们以 FLV 封装格式对应的 AVInputFormat 为例，看看 `read_packet()` 的实现函数是什么样子的。\n\nFLV 封装格式对应的 AVInputFormat 的定义位于 `libavformat\\flvdec.c`\n\n<details><summary>代码：</summary>\n\n```c\nAVInputFormat ff_flv_demuxer = {\n    .name           = \"flv\",\n    .long_name      = NULL_IF_CONFIG_SMALL(\"FLV (Flash Video)\"),\n    .priv_data_size = sizeof(FLVContext),\n    .read_probe     = flv_probe,\n    .read_header    = flv_read_header,\n    .read_packet    = flv_read_packet,\n    .read_seek      = flv_read_seek,\n    .read_close     = flv_read_close,\n    .extensions     = \"flv\",\n    .priv_class     = &flv_class,\n};\n```\n\n</details>\n\n从 `ff_flv_demuxer` 的定义可以看出，`read_packet()` 对应的是 `flv_read_packet()` 函数。在看 `flv_read_packet()` 函数之前，我们先回顾一下 FLV 封装格式的结构，如下图所示。\n\nPS：原图是网上找的，感觉画的很清晰，比官方的 Video File Format Specification 更加通俗易懂。但是图中有一个错误，就是 TagHeader 中的 StreamID 字段的长度写错了（查看了一下官方标准，应该是 3 字节，现在已经改过来了）。\n\n![FLV封装格式](/images/imageFFmpeg/Thor/FLV封装格式.png)\n\n从图中可以看出，FLV 文件体部分是由一个一个的 Tag 连接起来的（中间间隔着 Previous Tag Size）。每个 Tag 包含了 Tag Header 和 Tag Data 两个部分。\n\nTag Data 根据 Tag 的 Type 不同而不同：可以分为音频 Tag Data，视频 Tag Data 以及 Script Tag Data。下面简述一下音频 Tag Data 和视频 Tag Data。\n\n### Audio Tag Data\n\nAudio Tag在官方标准中定义如下。\n\n![Audio Tag](/images/imageFFmpeg/Thor/AudioTag.png)\n\nAudio Tag 开始的第 1 个字节包含了音频数据的参数信息，从第 2 个字节开始为音频流数据。\n第 1 个字节的前 4 位的数值表示了音频数据格式：\n\n```shell\n0 = Linear PCM, platform endian\n1 = ADPCM\n2 = MP3\n3 = Linear PCM, little endian\n4 = Nellymoser 16-kHz mono\n5 = Nellymoser 8-kHz mono\n6 = Nellymoser\n7 = G.711 A-law logarithmic PCM\n8 = G.711 mu-law logarithmic PCM\n9 = reserved\n10 = AAC\n14 = MP3 8-Khz\n15 = Device-specific sound\n```\n\n第 1 个字节的第 5-6 位的数值表示采样率：`0 = 5.5kHz，1 = 11KHz，2 = 22 kHz，3 = 44 kHz`。\n\n第 1 个字节的第7位表示采样精度：`0 = 8bits，1 = 16bits`。\n\n第 1 个字节的第8位表示音频类型：`0 = sndMono，1 = sndStereo`。\n\n其中，当音频编码为 AAC 的时候，第一个字节后面存储的是 AACAUDIODATA，格式如下所示。\n\n![AACAUDIODATA格式](/images/imageFFmpeg/Thor/AACAUDIODATA格式.png)\n\n### Video Tag Data\n\nVideo Tag在官方标准中的定义如下：\n\n![Video Tag](/images/imageFFmpeg/Thor/VideoTag.png)\n\nVideo Tag 也用开始的第 1 个字节包含视频数据的参数信息，从第 2 个字节为视频流数据。\n\n第 1 个字节的前 4 位的数值表示帧类型（FrameType）：\n\n```shell\n1: keyframe (for AVC, a seekableframe)（关键帧）\n2: inter frame (for AVC, a nonseekableframe)\n3: disposable inter frame (H.263only)\n4: generated keyframe (reservedfor server use only)\n5: video info/command frame\n```\n\n第 1 个字节的后 4 位的数值表示视频编码 ID（CodecID）：\n\n```shell\n1: JPEG (currently unused)\n2: Sorenson H.263\n3: Screen video\n4: On2 VP6\n5: On2 VP6 with alpha channel\n6: Screen video version 2\n7: AVC\n```\n\n其中，当音频编码为 AVC（H.264）的时候，第一个字节后面存储的是 AVCVIDEOPACKET，格式如下所示。\n\n![AVCVIDEOPACKET格式](/images/imageFFmpeg/Thor/AVCVIDEOPACKET格式.png)\n\n了解了 FLV 的基本格式之后，就可以看一下 FLV 解析 Tag 的函数 `flv_read_packet()了`。\n\n`flv_read_packet()` 的定义位于 `libavformat\\flvdec.c`\n\n`flv_read_packet()` 的代码比较长，但是逻辑比较简单。它的主要功能就是根据 FLV 文件格式的规范，逐层解析 Tag 以及 TagData，获取 Tag 以及 TagData 中的信息。比较关键的地方已经写上了注释，不再详细叙述。\n\n`parse_packet()` 给需要 AVCodecParser 的媒体流提供解析 AVPacket 的功能。\n\n从代码中可以看出，最终调用了相应 AVCodecParser 的 `av_parser_parse2()` 函数，解析出来 AVPacket。此后根据解析的信息还进行了一系列的赋值工作，不再详细叙述。\n\n## avcodec_decode_video2()\n\nffmpeg 中的 `avcodec_decode_video2()` 的作用是解码一帧视频数据。输入一个压缩编码的结构体 AVPacket，输出一个解码后的结构体 AVFrame。该函数的声明位于 `libavcodec\\avcodec.h`\n\n```c\nint avcodec_decode_video2(AVCodecContext *avctx, AVFrame *picture,\n                         int *got_picture_ptr,\n                         const AVPacket *avpkt);\n```\n\n查看源代码之后发现，这个函数竟然十分的简单，源代码位于 `libavcodec\\utils.c`\n\n从代码中可以看出，`avcodec_decode_video2()` 主要做了以下几个方面的工作：\n\n（1）对输入的字段进行了一系列的检查工作：例如宽高是否正确，输入是否为视频等等。\n\n（2）通过 `ret = avctx->codec->decode(avctx, picture, got_picture_ptr,&tmp)` 这句代码，调用了相应 AVCodec 的 `decode()` 函数，完成了解码操作。\n\n（3）对得到的 AVFrame 的一些字段进行了赋值，例如宽高、像素格式等等。\n\n其中第二部是关键的一步，它调用了 AVCodec 的 `decode()` 方法完成了解码。AVCodec 的 `decode()` 方法是一个函数指针，指向了具体解码器的解码函数。在这里我们以 H.264 解码器为例，看一下解码的实现过程。H.264 解码器对应的 AVCodec 的定义位于 `libavcodec\\h264.c`，如下所示。\n\n<details><summary>代码：</summary>\n\n```cpp\nAVCodec ff_h264_decoder = {\n    .name                  = \"h264\",\n    .long_name             = NULL_IF_CONFIG_SMALL(\"H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10\"),\n    .type                  = AVMEDIA_TYPE_VIDEO,\n    .id                    = AV_CODEC_ID_H264,\n    .priv_data_size        = sizeof(H264Context),\n    .init                  = ff_h264_decode_init,\n    .close                 = h264_decode_end,\n    .decode                = h264_decode_frame,\n    .capabilities          = /*CODEC_CAP_DRAW_HORIZ_BAND |*/ CODEC_CAP_DR1 |\n                             CODEC_CAP_DELAY | CODEC_CAP_SLICE_THREADS |\n                             CODEC_CAP_FRAME_THREADS,\n    .flush                 = flush_dpb,\n    .init_thread_copy      = ONLY_IF_THREADS_ENABLED(decode_init_thread_copy),\n    .update_thread_context = ONLY_IF_THREADS_ENABLED(ff_h264_update_thread_context),\n    .profiles              = NULL_IF_CONFIG_SMALL(profiles),\n    .priv_class            = &h264_class,\n};\n```\n\n</details>\n\n从 `ff_h264_decoder` 的定义可以看出，`decode()` 指向了 `h264_decode_frame()` 函数。\n\n从 `h264_decode_frame()` 的定义可以看出，它调用了 `decode_nal_units()` 完成了具体的 H.264 解码工作。\n\n## avformat_close_input()\n\n该函数用于关闭一个 AVFormatContext，一般情况下是和 `avformat_open_input()` 成对使用的。\n\n函数的调用关系如下图所示：\n\n![avformat_close_input](/images/imageFFmpeg/Thor/avformat_close_input.png)\n\n`avformat_close_input()` 的源代码位于 `libavformat\\utils.c`\n\n从源代码中可以看出，`avformat_close_input()` 主要做了以下几步工作：\n\n（1）调用 AVInputFormat 的 `read_close()` 方法关闭输入流\n\n（2）调用 `avformat_free_context()` 释放 AVFormatContext\n\n（3）调用 `avio_close()` 关闭并且释放 AVIOContext\n\n# 编码\n\n## avformat_alloc_output_context2()\n\n在基于 FFmpeg 的视音频编码器程序中，该函数通常是第一个调用的函数（除了组件注册函数 `av_register_all()`）。\n\n`avformat_alloc_output_context2()` 函数可以初始化一个用于输出的 AVFormatContext 结构体。它的声明位于 `libavformat\\avformat.h`\n\n```c\nint avformat_alloc_output_context2(AVFormatContext **ctx, AVOutputFormat *oformat,\n                                   const char *format_name, const char *filename);\n```\n\n代码中的英文注释写的已经比较详细了，在这里拿中文简单叙述一下。\n\n```shell\nctx：函数调用成功之后创建的AVFormatContext结构体。\noformat：指定AVFormatContext中的AVOutputFormat，用于确定输出格式。如果指定为NULL，可以设定后两个参数（format_name或者filename）由FFmpeg猜测输出格式。\nPS：使用该参数需要自己手动获取AVOutputFormat，相对于使用后两个参数来说要麻烦一些。\nformat_name：指定输出格式的名称。根据格式名称，FFmpeg会推测输出格式。输出格式可以是“flv”，“mkv”等等。\nfilename：指定输出文件的名称。根据文件名称，FFmpeg会推测输出格式。文件名称可以是“xx.flv”，“yy.mkv”等等。\n```\n\n函数执行成功的话，其返回值大于等于0。\n\n首先贴出来最终分析得出的函数调用结构图，如下所示：\n\n![avformat_alloc_output_context2](/images/imageFFmpeg/Thor/avformat_alloc_output_context2.png)\n\n`avformat_alloc_output_context2()` 的函数定义位于 `libavformat\\mux.c`\n\n从代码中可以看出，`avformat_alloc_output_context2()` 的流程如要包含以下 2 步：\n\n1)\t调用 `avformat_alloc_context()` 初始化一个默认的 AVFormatContext。\n\n2)\t如果指定了输入的 AVOutputFormat，则直接将输入的 AVOutputFormat 赋值给AVOutputFormat 的 oformat。如果没有指定输入的 AVOutputFormat，就需要根据文件格式名称或者文件名推测输出的 AVOutputFormat。无论是通过文件格式名称还是文件名推测输出格式，都会调用一个函数 `av_guess_format()`。\n\n`avformat_alloc_context()` 首先调用 `av_malloc()` 为 AVFormatContext 分配一块内存。然后调用了一个函数 `avformat_get_context_defaults()` 用于给 AVFormatContext 设置默认值\n\n`avformat_alloc_context()` 首先调用 `memset()` 将 AVFormatContext 的内存置零；然后指定它的AVClass（指定了 AVClass 之后，该结构体就支持和 AVOption 相关的功能）；最后调用 `av_opt_set_defaults()` 给 AVFormatContext 的成员变量设置默认值（`av_opt_set_defaults()` 就是和 AVOption 有关的一个函数，专门用于给指定的结构体设定默认值，此处暂不分析）。\n\n`av_guess_format()` 中使用一个整型变量 score 记录每种输出格式的匹配程度。函数中包含了一个 `while()` 循环，该循环利用函数 `av_oformat_next()` 遍历 FFmpeg 中所有的 AVOutputFormat，并逐一计算每个输出格式的 score。具体的计算过程分成如下几步：\n\n1)\t如果封装格式名称匹配，score 增加 100。匹配中使用了函数 `av_match_name()`。\n\n2)\t如果 mime 类型匹配，score 增加 10。匹配直接使用字符串比较函数 `strcmp()`。\n\n3)\t如果文件名称的后缀匹配，score 增加 5。匹配中使用了函数 `av_match_ext()`。\n\n`while()` 循环结束后，得到得分最高的格式，就是最匹配的格式。\n\n下面看一下一个 AVOutputFormat 的实例，就可以理解 “封装格式名称”，“mine类型”，“文件名称后缀” 这些概念了。下面是 flv 格式的视音频复用器（Muxer）对应的 AVOutputFormat 格式的变量 `ff_flv_muxer`。\n\n<details><summary>代码：</summary>\n\n```c\nAVOutputFormat ff_flv_muxer = {\n    .name           = \"flv\",\n    .long_name      = NULL_IF_CONFIG_SMALL(\"FLV (Flash Video)\"),\n    .mime_type      = \"video/x-flv\",\n    .extensions     = \"flv\",\n    .priv_data_size = sizeof(FLVContext),\n    .audio_codec    = CONFIG_LIBMP3LAME ? AV_CODEC_ID_MP3 : AV_CODEC_ID_ADPCM_SWF,\n    .video_codec    = AV_CODEC_ID_FLV1,\n    .write_header   = flv_write_header,\n    .write_packet   = flv_write_packet,\n    .write_trailer  = flv_write_trailer,\n    .codec_tag      = (const AVCodecTag* const []) {\n                          flv_video_codec_ids, flv_audio_codec_ids, 0\n                      },\n    .flags          = AVFMT_GLOBALHEADER | AVFMT_VARIABLE_FPS |\n                      AVFMT_TS_NONSTRICT,\n};\n```\n\n</details>\n\n## avformat_write_header()\n\nFFmpeg 的写文件用到的 3 个函数：\n\n- **`avformat_write_header()`**\n- **`av_write_frame()`**\n- **`av_write_trailer()`**\n\n其中 `av_write_frame()` 用于写视频数据，`avformat_write_header()` 用于写视频文件头，而 `av_write_trailer()` 用于写视频文件尾。\n\n本文首先分析`avformat_write_header()`。\n\nPS：需要注意的是，尽管这 3 个函数功能是配套的，但是它们的前缀却不一样，写文件头 Header 的函数前缀是“`avformat_`”，其他两个函数前缀是“`av_`”（不太明白其中的原因）。\n\n`avformat_write_header()` 的声明位于 `libavformat\\avformat.h`\n\n```c\nint avformat_write_header(AVFormatContext *s, AVDictionary **options);\n```\n\n简单解释一下它的参数的含义：\n\n```shell\ns：用于输出的AVFormatContext。\noptions：额外的选项，目前没有深入研究过，一般为NULL。\n```\n\n函数正常执行后返回值等于 0。\n\n`avformat_write_header()` 的调用关系如下图所示：\n\n![avformat_write_header](/images/imageFFmpeg/Thor/avformat_write_header.png)\n\n`avformat_write_header()` 的定义位于 `libavformat\\mux.c`\n\n从源代码可以看出，`avformat_write_header()` 完成了以下工作：\n\n（1）调用 `init_muxer()` 初始化复用器\n\n（2）调用 AVOutputFormat 的 `write_header()`\n\n`init_muxer()` 代码很长，但是它所做的工作比较简单，可以概括成两个字：检查。函数的流程可以概括成以下几步：\n\n（1）将传入的 AVDictionary 形式的选项设置到 AVFormatContext\n\n（2）遍历 AVFormatContext 中的每个 AVStream，并作如下检查：\n\n- a) AVStream 的 time_base 是否正确设置。如果发现 AVStream 的 time_base 没有设置，则会调用 `avpriv_set_pts_info()` 进行设置。\n- b) 对于音频，检查采样率设置是否正确；对于视频，检查宽、高、宽高比。\n\n- c) 其他一些检查，不再详述。\n\n**AVOutputFormat->write_header()**\n\n`avformat_write_header()` 中最关键的地方就是调用了 AVOutputFormat 的 `write_header()`。\n\n`write_header()` 是 AVOutputFormat 中的一个函数指针，指向写文件头的函数。不同的AVOutputFormat 有不同的 `write_header()` 的实现方法。在这里我们举例子看一下 FLV 封装格式对应的 AVOutputFormat，它的定义位于 `libavformat\\flvenc.c`\n\n从 `ff_flv_muxer` 的定义中可以看出，`write_header()` 指向的函数为 `flv_write_header()`。我们继续看一下 `flv_write_header()` 函数。`flv_write_header()` 的定义同样位于 `libavformat\\flvenc.c`\n\n从源代码可以看出，`flv_write_header()` 完成了FLV文件头的写入工作。该函数的工作可以大体分为以下两部分：\n\n（1）给 FLVContext 设置参数\n\n（2）写文件头，以及相关的 Tag\n\n可以参考下图中 FLV 文件头的定义比对一下上面的代码。\n\n![FLV Header.png](/images/imageFFmpeg/Thor/FLVHeader.png)\n\n## avcodec_encode_video()\n\n该函数用于编码一帧视频数据。`avcodec_encode_video2()` 函数的声明位于 `libavcodec\\avcodec.h`\n\n```c\nint avcodec_encode_video2(AVCodecContext *avctx, AVPacket *avpkt,\n                          const AVFrame *frame, int *got_packet_ptr);\n```\n\n该函数每个参数的含义在注释里面已经写的很清楚了，在这里用中文简述一下：\n\n```shell\navctx：编码器的AVCodecContext。\navpkt：编码输出的AVPacket。\nframe：编码输入的AVFrame。\ngot_packet_ptr：成功编码一个AVPacket的时候设置为1。\n```\n\n函数返回0代表编码成功。\n\n函数的调用关系如下图所示：\n\n![avcodec_encode_video](/images/imageFFmpeg/Thoreavcodec_encode_video.png)\n\n`avcodec_encode_video2()` 的定义位于 `libavcodec\\utils.c`\n\n从函数的定义可以看出，`avcodec_encode_video2()` 首先调用了 `av_image_check_size()` 检查设置的宽高参数是否合理，然后调用了 AVCodec 的 `encode2()` 调用具体的解码器。\n\n`av_image_check_size()` 主要是要求图像宽高必须为正数，而且取值不能太大。\n\nAVCodec 的 `encode2()` 是一个函数指针，指向特定编码器的编码函数\n\n从 `ff_libx264_encoder` 的定义可以看出，`encode2()` 函数指向的是 `X264_frame()` 函数。\n\n`X264_frame()` 函数的定义位于 `libavcodec\\libx264.c`\n\n## av_write_frame()\n\n`av_write_frame()` 用于输出一帧视音频数据，它的声明位于 `libavformat\\avformat.h`\n\n```c\nint av_write_frame(AVFormatContext *s, AVPacket *pkt);\n```\n\n简单解释一下它的参数的含义：\n\n```shell\ns：用于输出的AVFormatContext。\npkt：等待输出的AVPacket。\n```\n\n函数正常执行后返回值等于 0。\n\n`av_write_frame()` 的调用关系如下图所示：\n\n![av_write_frame](/images/imageFFmpeg/Thor/av_write_frame.png)\n\n`av_write_frame()` 的定义位于 `libavformat\\mux.c`\n\n从源代码可以看出，`av_write_frame()` 主要完成了以下几步工作：\n\n（1）调用 `check_packet()` 做一些简单的检测\n\n（2）调用 `compute_pkt_fields2()` 设置 AVPacket 的一些属性值\n\n（3）调用 `write_packet()` 写入数据\n\n`check_packet()` 的功能比较简单：首先检查一下输入的 AVPacket 是否为空，如果为空，则是直接返回；然后检查一下 AVPacket 的 `stream_index`（标记了该 AVPacket 所属的 AVStream）设置是否正常，如果为负数或者大于 AVStream 的个数，则返回错误信息；最后检查 AVPacket 所属的 AVStream 是否属于 attachment stream，这个地方没见过，目前还没有研究。\n\n`compute_pkt_fields2()` 函数的定义位于 `libavformat\\mux.c`\n\n`compute_pkt_fields2()` 主要有两方面的功能：\n\n- 一方面用于计算 AVPacket 的 duration， dts 等信息；\n- 另一方面用于检查 pts、dts 这些参数的合理性（例如 PTS 是否一定大于 DTS）。具体的代码还没有细看，以后有时间再进行分析。\n\n`write_packet()` 函数的定义位于 `libavformat\\mux.c`\n\n`write_packet()` 函数最关键的地方就是调用了 AVOutputFormat 中写入数据的方法。如果 AVPacket 中的 flag 标记中包含 AV_PKT_FLAG_UNCODED_FRAME，就会调用 AVOutputFormat 的 `write_uncoded_frame()` 函数；如果不包含那个标记，就会调用 `write_packet()` 函数。 `write_packet()` 实际上是一个函数指针，指向特定的 AVOutputFormat 中的实现函数。例如，我们看一下 FLV 对应的 AVOutputFormat，位于 `libavformat\\flvenc.c`\n\n从 `ff_flv_muxer` 的定义可以看出，`write_packet()` 指向的是 `flv_write_packet()` 函数。在看 `flv_write_packet()` 函数的定义之前，先回顾一下 FLV 封装格式的结构。\n\n## av_write_trailer()\n\n`av_write_trailer()` 用于输出文件尾，它的声明位于 `libavformat\\avformat.h`\n\n```c\nint av_write_trailer(AVFormatContext *s);\n```\n\n它只需要指定一个参数，即用于输出的 AVFormatContext。\n\n函数正常执行后返回值等于 0。\n\n`av_write_trailer()` 的调用关系如下图所示：\n\n![av_write_trailer](/images/imageFFmpeg/Thor/av_write_trailer.png)\n\n`av_write_trailer()` 的定义位于 `libavformat\\mux.c`\n\n从源代码可以看出 `av_write_trailer()` 主要完成了以下两步工作：\n\n（1）循环调用 `interleave_packet()` 以及 `write_packet()`，将还未输出的 AVPacket 输出出来。\n\n（2）调用 AVOutputFormat 的 `write_trailer()`，输出文件尾。\n\n其中第一步和 `av_write_frame()` 中的步骤大致是一样的（`interleave_packet()` 这一部分在并不包含在 `av_write_frame()` 中，而是包含在 `av_interleaved_write_frame()` 中，这一部分源代码还没有分析）\n\nAVOutputFormat 的 `write_trailer()` 是一个函数指针，指向特定的 AVOutputFormat 中的实现函数。我们以 FLV 对应的 AVOutputFormat 为例，看一下它的定义\n\n从 FLV 对应的 AVOutputFormat 结构体的定义我们可以看出，`write_trailer()` 指向了`flv_write_trailer()` 函数。\n\n`flv_write_trailer()` 函数的定义位于 `libavformat\\flvenc.c`\n\n从 `flv_write_trailer()` 的源代码可以看出该函数做了以下两步工作：\n\n（1）如果视频流是 H.264，则添加包含 EOS（End Of Stream） NALU 的 Tag。\n\n（2）更新 FLV 的时长信息，以及文件大小信息。\n\n其中，`put_avc_eos_tag()` 函数用于添加包含 EOS NALU 的 Tag（包含结尾的一个PreviousTagSize）\n\n可以参考 FLV 封装格式理解上述函数。由于前面的文章中已经描述过 FLV 封装格式，在这里不再重复叙述，在这里仅在此记录一下 AVCVIDEOPACKET 的格式，如下所示。\n\n![AVCVIDEOPACKET格式](/images/imageFFmpeg/Thor/AVCVIDEOPACKET格式.png)\n\n可以看出包含 EOS NALU 的 AVCVIDEOPACKET 的 AVCPacketType 为 2。在这种情况下， AVCVIDEOPACKET 的 CompositionTime 字段取 0，并且无需包含 Data 字段。\n\n# 日志输出系统\n\n> [日志输出系统](<https://blog.csdn.net/leixiaohua1020/article/details/44243155>)\n\n## av_log()\n\n本文分析一下 FFmpeg 的日志（Log）输出系统的源代码。日志输出部分的核心函数只有一个： `av_log()`。使用 `av_log()` 在控制台输出日志的效果如下图所示。\n\n![av_log控制台日志输出](/images/imageFFmpeg/Thor/av_log控制台日志输出.png)\n\nFFmpeg 日志输出系统的函数调用结构图如图所示：\n\n![FFmpeg 日志输出系统的函数调用结构图](/images/imageFFmpeg/Thor/av_log.png)\n\n`av_log()` 是 FFmpeg 中输出日志的函数。随便打开一个 FFmpeg 的源代码文件，就会发现其中遍布着 `av_log()` 函数。一般情况下 FFmpeg 类库的源代码中是不允许使用 `printf()` 这种的函数的，所有的输出一律使用 `av_log()`。\n\nav_log()的声明位于libavutil\\log.h\n\n```c\nvoid av_log(void *avcl, int level, const char *fmt, ...) av_printf_format(3, 4);\n```\n\n这个函数的声明有两个地方比较特殊：\n\n（1）函数最后一个参数是 “…”。\n\n在 C 语言中，在函数参数数量不确定的情况下使用 “…” 来代表参数。例如 `printf()` 的原型定义如下\n\n```c\nint printf (const char*, ...);\n```\n\n（2）它的声明后面有一个 `av_printf_format(3, 4)`。有关这个地方的左右还没有深入研究，网上资料中说它的作用是按照 `printf()` 的格式检查 `av_log()` 的格式。\n\nav_log()每个字段的含义如下：\n\n- avcl：指定一个包含 AVClass 的结构体。\n- level：log 的级别\n- fmt：和 `printf()` 一样。\n\n由此可见，`av_log()` 和 `printf()` 的不同主要在于前面多了两个参数。其中第一个参数指定该 log 所属的结构体，例如 AVFormatContext、AVCodecContext 等等。第二个参数指定 log 的级别，源代码中定义了如下几个级别。\n\n```c\n#define AV_LOG_QUIET    -8\n#define AV_LOG_PANIC     0\n#define AV_LOG_FATAL     8\n#define AV_LOG_ERROR    16\n#define AV_LOG_WARNING  24\n#define AV_LOG_INFO     32\n#define AV_LOG_VERBOSE  40\n#define AV_LOG_DEBUG    48\n```\n\n从定义中可以看出来，随着严重程度逐渐下降，一共包含如下级别：\n\n- AV_LOG_PANIC，\n- AV_LOG_FATAL，\n- AV_LOG_ERROR，\n- AV_LOG_WARNING，\n- AV_LOG_INFO，\n- AV_LOG_VERBOSE，\n- AV_LOG_DEBUG。\n\n每个级别定义的数值代表了严重程度，数值越小代表越严重。默认的级别是 AV_LOG_INFO。此外，还有一个级别不输出任何信息，即 AV_LOG_QUIET。\n\n当前系统存在着一个 “Log级别”。所有严重程度高于该级别的 Log 信息都会输出出来。例如当前的 Log 级别是 AV_LOG_WARNING，则会输出 AV_LOG_PANIC，AV_LOG_FATAL，AV_LOG_ERROR，AV_LOG_WARNING 级别的信息，而不会输出 AV_LOG_INFO 级别的信息。可以通过 `av_log_get_level()` 获得当前 Log 的级别，通过另一个函数 `av_log_set_level()` 设置当前的 Log 级别。\n\n可以通过 `av_log_set_level()` 设置当前 Log 的级别。\n\n# 接头体成员管理系统\n\n## AVClass\n\n> [FFmpeg源代码简单分析：结构体成员管理系统-AVClass](<https://blog.csdn.net/leixiaohua1020/article/details/44268323>)\n\nTODO\n\n## AVOption\n\n> [FFmpeg源代码简单分析：结构体成员管理系统-AVOption](<https://blog.csdn.net/leixiaohua1020/article/details/44279329>)\n\nTODO\n\n# libswscale\n\n## sws_getContext()\n\n> [FFmpeg源代码简单分析：libswscale的sws_getContext()](<https://blog.csdn.net/leixiaohua1020/article/details/44305697>)\n\nTODO\n\n## sws_scale()\n\n> [FFmpeg源代码简单分析：libswscale的sws_scale()](<https://blog.csdn.net/leixiaohua1020/article/details/44346687>)\n\nTODO\n\n# libavdevice\n\n## avdevice_register_all()\n\n> [FFmpeg源代码简单分析：libavdevice的avdevice_register_all()](<https://blog.csdn.net/leixiaohua1020/article/details/41211121>)\n\n## gdigrab\n\n> [FFmpeg源代码简单分析：libavdevice的gdigrab](<https://blog.csdn.net/leixiaohua1020/article/details/44597955>)\n\n\n\n![](/images/imageFFmpeg/Thor/)\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"FFmpeg 源代码结构（编解码解析）","url":"/2019-05-25/reference/FFmpeg/FFmpeg源代码结构note/","content":"\n> 特别说明，此文参考至[雷神笔记](<https://blog.csdn.net/leixiaohua1020/article/details/44220151>)，做一个备忘录。\n\n## FFmpeg源代码结构图 - 解码\n\n下图表明了 FFmpeg 在解码一个视频的时候的函数调用流程。为了保证结构清晰，其中仅列出了最关键的函数，剔除了其它不是特别重要的函数。\n\n<!-- more -->\n\n![FFmpeg源代码结构图 - 解码](/images/imageFFmpeg/Thor/FFmpeg源码API结构图-解码.png)\n\n下面解释一下图中关键标记的含义。\n\n### 函数背景色\n\n函数在图中以方框的形式表现出来。不同的背景色标志了该函数不同的作用：\n\n- 粉红色背景函数：FFmpeg 的 API函数。\n- 白色背景的函数：FFmpeg 的内部函数。\n- 黄色背景的函数：URLProtocol 结构体中的函数，包含处理协议（Protocol）的功能。\n- 绿色背景的函数：AVInputFormat 结构体中的函数，包含处理封装格式（Format）的功能。\n- 蓝色背景的函数：AVCodec 结构体中的函数，包含了编解码器（Codec）的功能。\n\n> PS：URLProtocol，AVInputFormat，AVCodec在FFmpeg开始运行并且注册完组件之后，都会分别被连接成一个个的链表。因此实际上是有很多的URLProtocol，AVInputFormat，AVCodec的。图中画出了解码一个输入协议是“文件”（其实就是打开一个文件。“文件”也被当做是一种广义的协议），封装格式为FLV，视频编码格式是H.264的数据的函数调用关系。\n\n### 区域\n\n整个架构图可以分为以下几个区域：\n\n- **左边区域——架构函数区域**：这些函数并不针对某一特定的视频格式。\n- **右上方黄色区域——协议处理函数区域**：不同的协议（RTP，RTMP，FILE）会调用不同的协议处理函数。\n- **右边中间绿色区域——封装格式处理函数区域**：不同的封装格式（MKV，FLV，MPEGTS，AVI）会调用不同的封装格式处理函数。\n- **右边下方蓝色区域——编解码函数区域**：不同的编码标准（HEVC，H.264，MPEG2）会调用不同的编解码函数。\n\n### 箭头线\n\n为了把调用关系表示的更明显，图中的箭头线也使用了不同的颜色：\n\n- 黑色箭头线：标志了函数之间的调用关系。\n\n- 红色的箭头线：标志了解码的流程。\n\n- 其他颜色的箭头线：标志了函数之间的调用关系。其中：\n  - 调用 URLProtocol 结构体中的函数用**黄色箭头线**标识；\n  - 调用 AVInputFormat 结构体中的函数用**绿色箭头线**标识；\n  - 调用 AVCodec 结构体中的函数用**蓝色箭头线**标识。\n\n### 函数所在的文件\n\n每个函数旁边标识了它所在的文件的路径。\n\n此外，还有一点需要注意的是，一些 API 函数内部也调用了另一些API函数。也就是说，API函数并不一定全部都调用FFmpeg的内部函数，他也有可能调用其他的API函数。例如从图中可以看出来， `avformat_close_input()` 调用了 `avformat_free_context()` 和 `avio_close()`。这些在内部代码中被调用的API函数也标记为粉红色。\n\n### 函数调用关系\n\n下面简单列出几个区域中函数之间的调用关系（函数之间的调用关系使用缩进的方式表现出来）。详细的函数分析可以参考相关的《FFmpeg源代码分析》系列文章。\n\n#### 左边区域（FFmpeg架构函数）\n\n**<font color=red>1.  av_register_all()【函数简单分析】</font>>**\n\n- **<font color=red>1)  avcodec_register_all()</font>**\n  - **(a) REGISTER_HWACCEL()**\n  - **(b) REGISTER_ENCODER()**\n  - **(c) REGISTER_DECODER()**\n  - **(d) REGISTER_PARSER()**\n  - **(e) REGISTER_BSF()**\n- **2)  REGISTER_MUXER()**\n- **3)  REGISTER_DEMUXER()**\n- **4)  REGISTER_PROTOCOL()**\n\n**<font color=red>2.  avformat_alloc_context()【函数简单分析】</font>**\n\n- **1) av_malloc(sizeof(AVFormatContext))**\n\n- **2) avformat_get_context_defaults()**\n  - **(a) av_opt_set_defaults()**\n\n**<font color=red>3.  avformat_open_input()【函数简单分析】</font>**\n\n- **1) init_input()**\n  - **<font color=red>(a) avio_open2()【函数简单分析】</font>**\n    - **a) ffurl_open()**\n      - **i. ffurl_alloc()**\n        - **url_find_protocol()**\n        - **url_alloc_for_protocol()**\n      - **ii. ffurl_connect()**\n        - **<font color=#FFC000>URLProtocol->url_open()</font>**\n    - **b) ffio_fdopen()**\n      - **i. av_malloc(buffer_size)**\n      - **ii. <font color=red>avio_alloc_context()</font>**\n        - **av_mallocz(sizeof(AVIOContext))**\n        - **ffio_init_context()**\n  - **<font color=red>(b) av_probe_input_buffer2()</font>**\n    - **<font color=red>a) avio_read()</font>**\n      - **i.  <font color=#009900>AVInputFormat->read_packet()</font>**\n    - **<font color=red>b) av_probe_input_format2()</font>**\n    - **<font color=red>c) av_probe_input_format3()</font>**\n      - **i. <font color=red>av_iformat_next()</font>**\n      - **ii. <font color=red>av_match_name()</font>**\n      - **iii. <font color=red>av_match_ext()</font>**\n      - **iv. <font color=#009900>AVInputFormat->read_probe()</font>**\n- **2) <font color=#009900>AVInputFormat->read_header()</font>**\n\n**<font color=red>4. avformat_find_stream_info()【函数简单分析】</font>**\n\n- **1) find_decoder()**\n  - **<font color=red>(a) avcodec_find_decoder()</font>**\n- **<font color=red>2) avcodec_open2()</font>**\n- **3) read_frame_internal()**\n- **4) try_decode_frame()**\n  - **<font color=red>(a) avcodec_decode_video2()</font>**\n- **<font color=red>5) avcodec_close()</font>**\n- **6) estimate_timings()**\n  - **(a)  estimate_timings_from_pts()**\n  - **(b)  estimate_timings_from_bit_rate()**\n  - **(c)  update_stream_timings()**\n\n**<font color=red>5. avcodec_find_decoder()【函数简单分析】</font>**\n\n- **1) find_encdec()**\n\n**<font color=red>6. avcodec_open2()【函数简单分析】</font>**\n\n- **<font color=#3072C2>1) AVCodec->init()</font>**\n\n**<font color=red>7. av_read_frame()【函数简单分析】</font>**\n\n- **1) read_from_packet_buffer()**\n\n- **2) read_frame_internal()**\n  - **(a) ff_read_packet()**\n    - **<font color=#009900>a) AVInputFormat->read_packet()</font>**\n  - **(b) parse_packet()**\n    - **a) av_parser_parse2()**\n\n**<font color=red>8. avcodec_decode_video2()【函数简单分析】</font>**\n\n- **1) av_packet_split_side_data()**\n\n- **2) <font color=#3072C2>AVCodec</font>-> <font color=red>decode()</font>**\n\n- **3) av_frame_set_pkt_pos()**\n\n- **4) av_frame_set_best_effort_timestamp()**\n\n**<font color=red>9. avcodec_close()【函数简单分析】</font>**\n\n- **<font color=#3072C2>1) AVCodec->close()</font>**\n\n**<font color=red>10. avformat_close_input()【函数简单分析】</font>**\n\n- **<font color=#009900>1) AVInputFormat->read_close()</font>**\n\n- **2) avformat_free_context()**\n  - **(a) ff_free_stream()**\n- **3) avio_close()**\n  - **(a) avio_flush()**\n    - **a) flush_buffer()**\n  - **(b) ffurl_close()**\n    - **a) ffurl_closep()**\n      - **<font color=#FFC000>URLProtocol->url_close()</font>**\n\n#### 右上区域（URLProtocol协议处理函数）\n\nURLProtocol结构体包含如下协议处理函数指针：\n\n- **<font color=#FFC000>url_open()：打开</font>**\n- **<font color=#FFC000>url_read()：读取</font>**\n- **<font color=#FFC000>url_write()：写入</font>**\n- **<font color=#FFC000>url_seek()：调整进度</font>**\n- **<font color=#FFC000>url_close()：关闭</font>**\n\n【例子】不同的协议对应着上述接口有不同的实现函数，举几个例子：\n\n**File协议（即文件）对应的URLProtocol结构体 `ff_file_protocol`：**\n\n```c\nurl_open() -> file_open() -> open()\nurl_read() -> file_read() -> read()\nurl_write() -> file_write() -> write()\nurl_seek() -> file_seek() -> lseek()\nurl_close() -> file_close() -> close()\n```\n\n**RTMP协议（libRTMP）对应的URLProtocol结构体 `ff_librtmp_protocol`：**\n\n```c\nurl_open() -> rtmp_open() -> RTMP_Init(), RTMP_SetupURL(), RTMP_Connect(), RTMP_ConnectStream()\nurl_read() -> rtmp_read() -> RTMP_Read()\nurl_write() -> rtmp_write() -> RTMP_Write()\nurl_seek() -> rtmp_read_seek() -> RTMP_SendSeek()\nurl_close() -> rtmp_close() -> RTMP_Close()\n```\n\n**UDP协议对应的URLProtocol结构体 `ff_udp_protocol`：**\n\n```c\nurl_open() -> udp_open()\nurl_read() -> udp_read()\nurl_write() -> udp_write()\nurl_seek() -> udp_close()\nurl_close() -> udp_close()\n```\n\n#### 右中区域（AVInputFormat封装格式处理函数）\n\nAVInputFormat包含如下封装格式处理函数指针：\n\n- **<font color=#009900>read_probe()：检查格式</font>**\n- **<font color=#009900>read_header()：读取文件头</font>**\n- **<font color=#009900>read_packet()：读取一帧数据</font>**\n- **<font color=#009900>read_seek()：调整进度</font>**\n- **<font color=#009900>read_close()：关闭</font>**\n\n【例子】不同的封装格式对应着上述接口有不同的实现函数，举几个例子：\n\n**FLV封装格式对应的AVInputFormat结构体 `ff_flv_demuxer`：**\n\n```c\nread_probe() -> flv_probe() –> probe()\nread_header() -> flv_read_header() -> create_stream() -> avformat_new_stream()\nread_packet() -> flv_read_packet()\nread_seek() -> flv_read_seek()\nread_close() -> flv_read_close()\n```\n\n**MKV封装格式对应的AVInputFormat结构体 `ff_matroska_demuxer`：**\n\n```c\nread_probe() -> matroska_probe()\nread_header() -> matroska_read_header()\nread_packet() -> matroska_read_packet()\nread_seek() -> matroska_read_seek()\nread_close() -> matroska_read_close()\n```\n\n**MPEG2TS封装格式对应的AVInputFormat结构体 `ff_mpegts_demuxer`：**\n\n```c\nread_probe() -> mpegts_probe()\nread_header() -> mpegts_read_header()\nread_packet() -> mpegts_read_packet() \nread_close() -> mpegts_read_close()\n```\n\n**AVI封装格式对应的AVInputFormat结构体 `ff_avi_demuxer`：**\n\n```c\nread_probe() -> avi_probe()\nread_header() -> avi_read_header()\nread_packet() -> avi_read_packet()\nread_seek() -> avi_read_seek()\nread_close() -> avi_read_close()\n```\n\n#### 右下区域（AVCodec编解码函数）\n\nAVCodec包含如下编解码函数指针：\n\n- **<font color=#3072C2>init()：初始化</font>**\n- **<font color=red>decode()</font>：解码一帧数据**\n- **<font color=#3072C2>close()：关闭</font>**\n\n【例子】不同的编解码器对应着上述接口有不同的实现函数，举几个例子：\n\n**HEVC解码对应的AVCodec结构体 `ff_hevc_decoder`：**\n\n```c\ninit() -> hevc_decode_init()\ndecode() -> hevc_decode_frame() -> decode_nal_units()\nclose() -> hevc_decode_free()\n```\n\n**H.264解码对应的AVCodec结构体 `ff_h264_decoder`：**\n\n```c\ninit() -> ff_h264_decode_init()\ndecode() -> h264_decode_frame() -> decode_nal_units()\nclose() -> h264_decode_end()\n```\n\n**VP8解码（libVPX）对应的AVCodec结构体 `ff_libvpx_vp8_decoder`：**\n\n```c\ninit() -> vpx_init() -> vpx_codec_dec_init()\ndecode() -> vp8_decode() -> vpx_codec_decode(), vpx_codec_get_frame()\nclose() -> vp8_free() -> vpx_codec_destroy()\n```\n\n**MPEG2解码对应的AVCodec结构体 `ff_mpeg2video_decoder`：**\n\n```c\ninit() -> mpeg_decode_init()\ndecode() -> mpeg_decode_frame()\nclose() -> mpeg_decode_end()\n```\n\n### avformat_open_input() 函数\n\n![avformat_open_input](/images/imageFFmpeg/Thor/avformat_open_input.png)\n\n## FFmpeg源代码结构图 - 编码\n\n### 函数调用关系图\n\n下图表明了FFmpeg在编码一个视频的时候的函数调用流程。为了保证结构清晰，其中仅列出了最关键的函数，剔除了其它不是特别重要的函数。\n\n![FFmpeg源代码结构图 - 编码](/images/imageFFmpeg/Thor/FFmpeg源码API结构图-编码.png)\n\n下面解释一下图中关键标记的含义。\n\n### 函数背景色\n\n函数在图中以方框的形式表现出来。不同的背景色标志了该函数不同的作用：\n\n- 粉红色背景函数：FFmpeg 的 API 函数。\n- 白色背景的函数：FFmpeg 的内部函数。\n- 黄色背景的函数：URLProtocol 结构体中的函数，包含了读写各种协议的功能。\n- 绿色背景的函数：AVOutputFormat 结构体中的函数，包含了读写各种封装格式的功能。\n- 蓝色背景的函数：AVCodec 结构体中的函数，包含了编解码的功能。\n\n### 区域\n\n整个关系图可以分为以下几个区域：\n\n- **左边区域——架构函数区域**：这些函数并不针对某一特定的视频格式。\n- **右上方黄色区域——协议处理函数区域**：不同的协议（RTP，RTMP，FILE）会调用不同的协议处理函数。\n- **右边中间绿色区域——封装格式处理函数区域**：不同的封装格式（MKV，FLV，MPEG2TS，AVI）会调用不同的封装格式处理函数。\n- **右边下方蓝色区域——编解码函数区域**：不同的编码标准（HEVC，H.264，MPEG2）会调用不同的编解码函数。\n\n### 箭头线\n\n为了把调用关系表示的更明显，图中的箭头线也使用了不同的颜色：\n\n- 红色的箭头线：标志了编码的流程。\n\n- 其他颜色的箭头线：标志了函数之间的调用关系。其中：\n  - 调用 URLProtocol 结构体中的函数用**黄色箭头线**标识；\n  - 调用 AVOutputFormat 结构体中的函数用**绿色箭头线**标识；\n  - 调用 AVCodec 结构体中的函数用**蓝色箭头线**标识。\n\n### 函数所在的文件\n\n每个函数标识了它所在的文件路径。\n\n### 函数功能简述\n\n下面简单列出几个区域中函数之间的调用关系（函数之间的调用关系使用缩进的方式表现出来）。详细的函数分析可以参考相关的《FFmpeg源代码分析》系列文章。\n\n#### 左边区域（架构函数）\n\n**<font color=red>1. av_register_all()【函数简单分析】</font>**\n\n- **<font color=red>1) avcodec_register_all()</font>**\n  - **(a) REGISTER_HWACCEL()**\n  - **(b) REGISTER_ENCODER()**\n  - **(c) REGISTER_DECODER()**\n  - **(d) REGISTER_PARSER()**\n  - **(e) REGISTER_BSF()**\n\n- **2) REGISTER_MUXER()**\n\n- **3) REGISTER_DEMUXER()**\n\n- **4) REGISTER_PROTOCOL()**\n\n**<font color=red>2. avformat_alloc_output_context2()【函数简单分析】</font>**\n\n- **<font color=red>1) avformat_alloc_context()</font>**\n\n  - **(a) av_malloc(sizeof(AVFormatContext))**\n\n  - **(b) avformat_get_context_defaults()**\n    - **a) av_opt_set_defaults()**\n\n- **<font color=red>2) av_guess_format()</font>**\n  - **<font color=red>(a) av_oformat_next()</font>**\n  - **<font color=red>(b) av_match_name()</font>**\n  - **<font color=red>(c) av_match_ext()</font>**\n\n**<font color=red>3. avio_open2()【函数简单分析】</font>**\n\n- **1) ffurl_open()**\n  - **(a) ffurl_alloc()**\n    - **a) url_find_protocol()**\n    - **b) url_alloc_for_protocol()**\n  - **(b) ffurl_connect()**\n    - **<font color=#FFC000>a) URLProtocol->url_open()</font>**\n\n- **2) ffio_fdopen()**\n  - **(a) av_malloc(buffer_size)**\n  - **<font color=red>(b) avio_alloc_context()</font>**\n    - **a) av_mallocz(sizeof(AVIOContext))**\n    - **b) ffio_init_context()**\n\n**<font color=red>4. avformat_new_stream()【函数简单分析】</font>**\n\n- **1) av_mallocz(sizeof(AVStream))**\n\n- **<font color=red>2) avcodec_alloc_context3()</font>**\n  - **(a) av_malloc(sizeof(AVCodecContext))**\n  - **(b) avcodec_get_context_defaults3()**\n\n**<font color=red>5. avcodec_find_encoder()【函数简单分析】</font>**\n\n- **1) find_encdec()**\n\n**<font color=red>6. avcodec_open2()【函数简单分析】</font>**\n\n- **1) AVCodec->init()**\n\n**<font color=red>7. avformat_write_header()【函数简单分析】</font>**\n\n- **1) init_muxer()**\n\n- **<font color=#009900>2) AVOutputFormat->write_header()</font>**\n\n- **3) init_pts()**\n\n**<font color=red>8. avcodec_encode_video2()【函数简单分析】</font>**\n\n- **<font color=#3072C2>1) AVCodec->encode2()</font>**\n\n**<font color=red>9. av_write_frame()【函数简单分析】</font>**\n\n- **1) check_packet()**\n\n- **2) compute_pkt_fields2()**\n\n- **3) write_packet()**\n  - **<font color=#009900>(a) AVOutputFormat->write_packet()</font>**\n\n**<font color=red>10. av_write_trailer()【函数简单分析】</font>**\n\n- **1) write_packet()**\n\n- **<font color=#009900>2) AVOutputFormat->write_trailer()</font>**\n\n**<font color=red>11. avcodec_close()【函数简单分析】</font>**\n\n- **<font color=#3072C2>1) AVCodec->close()</font>**\n\n**<font color=red>12. avformat_free_context()【函数简单分析】</font>**\n\n- **1) ff_free_stream()**\n\n**<font color=red>13. avio_close()【函数简单分析】</font>**\n\n- **1) avio_flush()**\n  - **(a) flush_buffer()**\n- **2) ffurl_close()**\n  - **(a) ffurl_closep()**\n    - **<font color=#FFC000>a) URLProtocol->url_close()</font>**\n\n#### 右上区域（URLProtocol协议处理函数）\n\nURLProtocol结构体包含如下协议处理函数指针：\n\n- **<font color=#FFC000>url_open()：打开</font>**\n- **<font color=#FFC000>url_read()：读取</font>**\n- **<font color=#FFC000>url_write()：写入</font>**\n- **<font color=#FFC000>url_seek()：调整进度</font>**\n- **<font color=#FFC000>url_close()：关闭</font>**\n\n【例子】不同的协议对应着上述接口有不同的实现函数，举几个例子：\n\n**File协议（即文件）对应的URLProtocol结构体 `ff_file_protocol`：**\n\n```c\nurl_open() -> file_open() -> open()\nurl_read() -> file_read() -> read()\nurl_write() -> file_write() -> write()\nurl_seek() -> file_seek() -> lseek()\nurl_close() -> file_close() -> close()\n```\n\n**RTMP协议（libRTMP）对应的URLProtocol结构体 `ff_librtmp_protocol`：**\n\n```c\nurl_open() -> rtmp_open() -> RTMP_Init(), RTMP_SetupURL(), RTMP_Connect(), RTMP_ConnectStream()\nurl_read() -> rtmp_read() -> RTMP_Read()\nurl_write() -> rtmp_write() -> RTMP_Write()\nurl_seek() -> rtmp_read_seek() -> RTMP_SendSeek()\nurl_close() -> rtmp_close() -> RTMP_Close()\n```\n\n**UDP协议对应的URLProtocol结构体 `ff_udp_protocol`：**\n\n```c\nurl_open() -> udp_open()\nurl_read() -> udp_read()\nurl_write() -> udp_write()\nurl_seek() -> udp_close()\nurl_close() -> udp_close()\n```\n\n#### 右中区域（AVOutputFormat封装格式处理函数）\n\nAVOutputFormat包含如下封装格式处理函数指针：\n\n- **<font color=#009900>write_header()：写文件头</font>**\n- **<font color=#009900>write_packet()：写一帧数据</font>**\n- **<font color=#009900>write_trailer()：写文件尾</font>**\n\n【例子】不同的封装格式对应着上述接口有不同的实现函数，举几个例子：\n\n**FLV封装格式对应的AVOutputFormat结构体 `ff_flv_muxer`：**\n\n```c\nwrite_header() -> flv_write_header()\nwrite_packet() –> flv_write_packet()\nwrite_trailer() -> flv_write_trailer()\n```\n\n**MKV封装格式对应的AVOutputFormat结构体 `ff_matroska_muxer`：**\n\n```c\nwrite_header() -> mkv_write_header()\nwrite_packet() –> mkv_write_flush_packet()\nwrite_trailer() -> mkv_write_trailer()\n```\n\n**MPEG2TS封装格式对应的AVOutputFormat结构体 `ff_mpegts_muxer`：**\n\n```c\nwrite_header() -> mpegts_write_header()\nwrite_packet() –> mpegts_write_packet()\nwrite_trailer() -> mpegts_write_end()\n```\n\n**AVI封装格式对应的AVOutputFormat结构体 `ff_avi_muxer`：**\n\n```c\nwrite_header() -> avi_write_header()\nwrite_packet() –> avi_write_packet()\nwrite_trailer() -> avi_write_trailer()\n```\n\n#### 右下区域（AVCodec编解码函数）\n\nAVCodec包含如下编解码函数指针：\n\n- **<font color=#3072C2>init()：初始化</font>**\n- **<font color=#3072C2>encode2()：编码一帧数据</font>**\n- **<font color=#3072C2>close()：关闭</font>**\n\n【例子】不同的编解码器对应着上述接口有不同的实现函数，举几个例子：\n\n**HEVC编码器对应的AVCodec结构体 `ff_libx265_encoder`：**\n\n```c\ninit() -> libx265_encode_init() -> x265_param_alloc(), x265_param_default_preset(), x265_encoder_open()\nencode2() -> libx265_encode_frame() -> x265_encoder_encode()\nclose() -> libx265_encode_close() -> x265_param_free(), x265_encoder_close()\n```\n\n**H.264编码器对应的AVCodec结构体 `ff_libx264_encoder`：**\n\n```c\ninit() -> X264_init() -> x264_param_default(), x264_encoder_open(), x264_encoder_headers()\nencode2() -> X264_frame() -> x264_encoder_encode()\nclose() -> X264_close() -> x264_encoder_close()\n```\n\n**VP8编码器（libVPX）对应的AVCodec结构体 `ff_libvpx_vp8_encoder`：**\n\n```c\ninit() -> vpx_init() -> vpx_codec_enc_config_default()\nencode2() -> vp8_encode() -> vpx_codec_enc_init(), vpx_codec_encode()\nclose() -> vp8_free() -> vpx_codec_destroy()\n```\n\n**MPEG2编码器对应的AVCodec结构体 `ff_mpeg2video_encoder`：**\n\n```c\ninit() -> encode_init()\nencode2() -> ff_mpv_encode_picture()\nclose() -> ff_mpv_encode_end()\n```\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"FFMpeg实时美颜直播推流","url":"/2019-05-19/reference/FFmpeg/ffmpeg实时美颜推流/","content":"\n> [基于FFmpeg进行RTMP推流（二）](<https://www.jianshu.com/p/6b9ab2652147>)\n\n实战 - 基于ffmpeg，qt5，opencv视频课程\n\n## 1. 基础知识\n\n### 1.1 直播推流流程分析\n\n<!-- more -->\n\n<img src=\"/images/imageFFmpeg/直播.png\">\n\nrtmp 延时一般 1-3 秒\n\n<img src=\"/images/imageFFmpeg/推流流程.png\">\n\n### 1.2 直播 rtmp 协议分析\n\n<img src=\"/images/imageFFmpeg/协议.png\">\n\n<img src=\"/images/imageFFmpeg/传输协议.png\">\n\n<img src=\"/images/imageFFmpeg/RTMP.png\">\n\n<img src=\"/images/imageFFmpeg/RTMP优缺点.png\">\n\n<img src=\"/images/imageFFmpeg/RTMP协议类型.png\">\n\n<img src=\"/images/imageFFmpeg/握手.png\">\n\n<img src=\"/images/imageFFmpeg/推流.png\">\n\n## 2. 直播服务器讲解和配置\n\n<img src=\"/images/imageFFmpeg/流媒体服务器.png\">\n\n### 2.1 直播服务器介绍 crtmpserver 编译运行\n\n```shell\n$ apt-get install wget cmake\n$ apt-get install libssl-dev\n$ wget  https://codeload.github.com/j0sh/crtmpserver/zip/centosinit --no-check-certificate\n$ unzip centosinit\n$ cd builders/cmake\n$ cmake . \n$ make\n$ ./crtmpserver/crtmpserver ./crtmpserver/crtmpserver.lua\n\n#\n$ ffmpeg  -i test.flv  -f flv rtmp://192.168.1.44/live\n\n#ʹc rtmp://192.168.1.44/live\n#ʹplay ffplay rtmp://192.168.1.44/live  -fflags nobuffer\n```\n\n```shell\n# error\nCMake Error at cmake_find_modules/Find_openssl.cmake:99 (MESSAGE):\n  Looking for openssl headers - not found\nCall Stack (most recent call first):\n  CMakeLists.txt:46 (INCLUDE)\n$ cmake -DOPENSSL_ROOT_DIR=/usr/local/opt/openssl -DOPENSSL_LIBRARIES=/usr/local/opt/openssl/lib\n```\n\n### 2.2 下载 ffmpeg 工具推流并使用功能 vlc 拉流播放测试\n\n```shell\n$ wget https://nginx.org/download/nginx-1.16.0.tar.gz --no-check-certificate\n$ git clone https://github.com/arut/nginx-rtmp-module.git\n$ ./configure --add-module=/home/miaopei/workdir/test/ffmpet-test/nginx/nginx-rtmp-module \n$ make\n$ make install\n```\n\n<img src=\"/images/imageFFmpeg/ffmpeg工具推流测试.png\">\n\n```shell\n# nginx.conf 配置\nrtmp {\n    server {\n        listen 1935;\n        chunk_size 4096;\n        application live {\n            live on;\n        }\n    }\n}\n```\n\n```shell\n# 推流命令\n$ ffmpeg -i test.mp4 -c copy -f flv rtmp://192.168.2.76/live\n```\n\n```shell\n# 网页查看推流的状态\nserver {\n    listen 8080;\n    location /stat{\n        rtmp_stat all;\n        rtmp_stat_stylesheet stat.xsl;\n    }\n    location /stat.xsl{\n        root /home/miaopei/workdir/test/ffmpet-test/nginx/nginx-rtmp-module;\n    }\n}\n```\n\n```shell\nReload config:\n $ nginx -s reload\nReopen Logfile:\n $ nginx -s reopen\nStop process:\n $ nginx -s stop\nWaiting on exit process\n $ nginx -s quit\n```\n\n## 3. FFMpeg SDK 解封和推流\n\n### 3.1 ffmpeg SDK开发环境准备\n\n<img src=\"/images/imageFFmpeg/使用FFMpegSDK推流.png\">\n\n<img src=\"/images/imageFFmpeg/avformat_open_input.png\">\n\n<img src=\"/images/imageFFmpeg/AVFormatContext.png\">\n\n<img src=\"/images/imageFFmpeg/AVStream.png\">\n\n<img src=\"/images/imageFFmpeg/AVPacket.png\">\n\n<img src=\"/images/imageFFmpeg/GOP.png\">\n\n<img src=\"/images/imageFFmpeg/基于海康或大华相机推流.png\">\n\n## 4. OpencvSDK 基础\n\n> [Mac源码安装使用OpenCV](<https://blog.csdn.net/u010164190/article/details/79108608>)\n>\n> [在MacOS 10.13.2 下编译 OpenCV3.4.0 + OpenCV Contrib 3.4.0 成 Java 库](<https://blog.csdn.net/marksim/article/details/79146346>)\n>\n> [在MacOS上安装OpenCV 3.4(c++)](<https://www.jianshu.com/p/a36d41241ae8>)\n\n<details><summary>OpenCV 源码编译：</summary>\n\n```shell\n# 下载 OpenCV 3.4.0\n$ \n# 解压，进入到 opencv-3.4.0 目录下\n$ mkdir -p build/install\n$ cd build\n$ cmake -G \"Unix Makefiles\" -j8 -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_OSX_ARCHITECTURES=x86_64 -D CMAKE_INSTALL_PREFIX=/Users/miaopei/install/opencv/opencv-3.4.0/build/install ../\n$ make -j8\n$ make install\n# 配置环境变量\nPKG_CONFIG_PATH=$PKG_CONFIG_PATH:Users/miaopei/install/opencv/opencv-3.4.0/build/install/lib/pkgconfig\nexport PKG_CONFIG_PATH\nexport LD_LIBRARY_PATH=Users/miaopei/install/opencv/opencv-3.4.0/build/install/bin:SLD_LIBRARY_PATH\nexport PATH=${PATH}:Users/miaopei/install/opencv/opencv-3.4.0/build/install/lib\n# 测试demo,打印当前版本号\n#include <opencv2/core/utility.hpp>\n#include <iostream>\nint main(int argc, const char* argv[]){\n  std::cout << \"Welcome to OpenCV \" << CV_VERSION << std::endl; \n  return 0;\n}\n# Makefile\nCXX ?= g++\nCXXFLAGS += -c -Wall $(shell pkg-config --cflags opencv)\nLDFLAGS += $(shell pkg-config --libs --static opencv)\nall: test_version\nopencv_example: test_version.o; $(CXX) $< -o $@ $(LDFLAGS)\n%.o: %.cpp; $(CXX) $< -o $@ $(CXXFLAGS)\nclean: ; rm -f test_version.o test_version\n```\n\n</details>\n\n<img src=\"/images/imageFFmpeg/第一个例子显示图片.png\">\n\n<img src=\"/images/imageFFmpeg/waitKey.png\">\n\n### 4.1 VideoCapture打开摄像头接口讲解和源码分析\n\n<img src=\"/images/imageFFmpeg/打开摄像头接口说明和源码分析.png\">\n\n<img src=\"/images/imageFFmpeg/创建和清理mat空间.png\">\n\n<img src=\"/images/imageFFmpeg/图像存放方式-连续.png\">\n\n<img src=\"/images/imageFFmpeg/isContinuous.png\">\n\n<img src=\"/images/imageFFmpeg/直接地址访问连续空间.png\">\n\n<img src=\"/images/imageFFmpeg/读取一帧视频.png\">\n\n## 5. 视频采集编码推流和类封装\n\n<img src=\"/images/imageFFmpeg/基于opencv采集推流.png\">\n\n<img src=\"/images/imageFFmpeg/opencv采集rtsp解码.png\">\n\n<img src=\"/images/imageFFmpeg/sws_getCachedContext.png\">\n\n<img src=\"/images/imageFFmpeg/sws_scale.png\">\n\n<img src=\"/images/imageFFmpeg/avcodec_find_encoder.png\">\n\n<img src=\"/images/imageFFmpeg/avcodec_alloc_context3.png\">\n\n<img src=\"/images/imageFFmpeg/avcodec_alloc_context3-01.png\">\n\n<img src=\"/images/imageFFmpeg/gop-01.png\">\n\n<img src=\"/images/imageFFmpeg/avcodec_send_frame.png\">\n\n## 6. 音频录制编码推流和类封装\n\n<img src=\"/images/imageFFmpeg/音频.png\">\n\n<img src=\"/images/imageFFmpeg/样本类型planar.png\">\n\n<img src=\"/images/imageFFmpeg/大小端模式.png\">\n\n<img src=\"/images/imageFFmpeg/一帧数据量.png\">\n\nQT音频录制接口：\n\n<img src=\"/images/imageFFmpeg/QAudioFormat.png\">\n\n<img src=\"/images/imageFFmpeg/QAudioInput.png\">\n\n<img src=\"/images/imageFFmpeg/QIODevice.png\">\n\n## 7. 音视频同步编码推流处理\n\n<img src=\"/images/imageFFmpeg/视频录制接口封装.png\">\n\n<img src=\"/images/imageFFmpeg/音视频同步.png\">\n\n## 8. XRtmpStreamer 项目完成（界面和美颜）\n\n直播推流要求实时性，一秒钟25帧，做美颜的总耗时一定要低于40ms（每帧消耗40ms）\n\n现在视频推流一般都是1280 X 720\n\n手机端是基于GPU 第三方库做的计算\n\n美颜算法一般都是基于GPU做的\n\n<img src=\"/images/imageFFmpeg/使用opencv磨皮.png\">\n\n<img src=\"/images/imageFFmpeg/bilateralFilter双边滤波-边缘平滑.png\">\n\n<img src=\"/images/imageFFmpeg/自定义过滤器类XFilter.png\">\n\n<img src=\"/images/imageFFmpeg/XController.png\">\n\n<img src=\"/images/imageFFmpeg/类图.png\">\n\n**头文件尽量不用引用命名空间，因为不知道谁来调用，可能会出现问题**。\n\n**头文件中尽量不要引用第三方库文件，应为涉及到第三方库版本升级之类的，第三方头文件的引用应该在代码中引用**。\n\n## 9. 补充\n\n### 9.0 流媒体协议介绍（rtp/rtcp/rtsp/rtmp/mms/hls）\n\n**1. RTP**：\n\n> 参考文档 RFC3550/RFC3551\n\n(Real-time Transport Protocol) 是用于 Internet 上针对多媒体数据流的一种传输层协议。RTP 协议详细说明了在互联网上传递音频和视频的标准数据包格式。RTP 协议常用于流媒体系统（配合 RTCP协议），视频会议和一键通（Push to Talk）系统（配合 H.323 或 SIP），使它成为 IP 电话产业的技术基础。**RTP 协议和 RTP 控制协议 RTCP 一起使用**，而且它是建立在 **UDP** 协议上的。\n\n**RTP 本身并没有提供按时发送机制或其它服务质量（QoS）保证**，它依赖于低层服务去实现这一过程。 RTP 并不保证传送或防止无序传送，也不确定底层网络的可靠性。 RTP 实行有序传送， RTP 中的序列号允许接收方重组发送方的包序列，同时序列号也能用于决定适当的包位置，例如：在视频解码中，就不需要顺序解码。\n\n**RTP 由两个紧密链接部分组成**： \n\n- RTP ― 传送具有实时属性的数据；\n- RTP 控制协议（RTCP） ― 监控服务质量并传送正在进行的会话参与者的相关信息。\n\n**2. RTCP**\n\n实时传输控制协议（Real-time Transport Control Protocol 或 RTP Control Protocol 或简写 RTCP）是实时传输协议（RTP）的一个姐妹协议。RTCP 为 RTP 媒体流提供信道外（out-of-band）控制。**RTCP 本身并不传输数据，但和 RTP 一起协作将多媒体数据打包和发送**。RTCP 定期在流多媒体会话参加者之间传输控制数据。RTCP 的主要功能是为 RTP 所提供的服务质量（Quality of Service）提供反馈。\n\nRTCP 收集相关媒体连接的统计信息，例如：传输字节数，传输分组数，丢失分组数，jitter，单向和双向网络延迟等等。网络应用程序可以利用 RTCP 所提供的信息试图提高服务质量，比如限制信息流量或改用压缩比较小的编解码器。RTCP 本身不提供数据加密或身份认证。SRTCP 可以用于此类用途。\n\n**3. SRTP & SRTCP**\n\n> 参考文档 RFC3711\n\n安全实时传输协议（Secure Real-time Transport Protocol 或 SRTP）是在实时传输协议（Real-time Transport Protocol 或 RTP）基础上所定义的一个协议，**旨在为单播和多播应用程序中的实时传输协议的数据提供加密、消息认证、完整性保证和重放保护**。它是由 David Oran（思科）和 Rolf Blom（爱立信）开发的，并最早由 IETF 于 2004年3月作为 RFC3711 发布。\n\n由于实时传输协议和可以被用来控制实时传输协议的会话的实时传输控制协议（RTP Control Protocol 或 RTCP）有着紧密的联系，安全实时传输协议同样也有一个伴生协议，它被称为安全实时传输控制协议（Secure RTCP 或 SRTCP）；安全实时传输控制协议为实时传输控制协议提供类似的与安全有关的特性，就像安全实时传输协议为实时传输协议提供的那些一样。\n\n在使用实时传输协议或实时传输控制协议时，使不使用安全实时传输协议或安全实时传输控制协议是可选的；但即使使用了安全实时传输协议或安全实时传输控制协议，所有它们提供的特性（如加密和认证）也都是可选的，这些特性可以被独立地使用或禁用。唯一的例外是在使用安全实时传输控制协议时，必须要用到其消息认证特性。\n\n**4. RTSP**\n\n> 参考文档 RFC2326\n\n是由 Real Networks 和 Netscape 共同提出的。该协议定义了一对多应用程序如何有效地通过 IP 网络传送多媒体数据。RTSP 提供了一个可扩展框架，使实时数据，如音频与视频的受控、点播成为可能。数据源包括现场数据与存储在剪辑中的数据。**该协议目的在于控制多个数据发送连接，为选择发送通道，如UDP、多播UDP与TCP提供途径，并为选择基于RTP上发送机制提供方法**。\n\nRTSP（Real Time Streaming Protocol）是用来控制声音或影像的多媒体串流协议，并允许同时多个串流需求控制，传输时所用的网络通讯协定并不在其定义的范围内，服务器端可以自行选择使用 TCP 或 UDP来传送串流内容，它的语法和运作跟 HTTP 1.1 类似，**但并不特别强调时间同步，所以比较能容忍网络延迟**。而前面提到的允许同时多个串流需求控制（Multicast），除了可以降低服务器端的网络用量，更进而支持多方视讯会议（Video Conference）。 因为与 HTTP1.1 的运作方式相似，所以代理服务器《Proxy》的快取功能《Cache》也同样适用于 RTSP，并因 RTSP 具有重新导向功能，可视实际负载情况来转换提供服务的服务器，以避免过大的负载集中于同一服务器而造成延迟。\n\n**5. RTSP 和 RTP 的关系**\n\nRTP 不象 http 和 ftp 可完整的下载整个影视文件，它是以固定的数据率在网络上发送数据，客户端也是按照这种速度观看影视文件，当影视画面播放过后，就不可以再重复播放，除非重新向服务器端要求数据。\n\nRTSP 与 RTP 最大的区别在于：RTSP 是一种双向实时数据传输协议，它允许客户端向服务器端发送请求，如回放、快进、倒退等操作。当然，RTSP 可基于 RTP 来传送数据，还可以选择 TCP、UDP、组播 UDP 等通道来发送数据，具有很好的扩展性。它是一种类似与 http 协议的网络应用层协议。目前碰到的一个应用：服务器端实时采集、编码并发送两路视频，客户端接收并显示两路视频。由于客户端不必对视频数据做任何回放、倒退等操作，可直接采用 UDP + RTP + 组播实现。\n\n<img src=\"/images/imageFFmpeg/RTSP和RTP的关系.png\">\n\nRTP：实时传输协议（Real-time Transport Protocol） \n\n- RTP/RTCP 是实际传输数据的协议 \n\n- RTP 传输音频/视频数据，如果是 PLAY，Server 发送到 Client 端，如果是 RECORD，可以由Client 发送到 Server \n\n- 整个 RTP 协议由两个密切相关的部分组成：\n  - RTP数据协议\n  - RTP控制协议（即RTCP） \n\nRTSP：实时流协议（Real Time Streaming Protocol，RTSP） \n\n- RTSP 的请求主要有 DESCRIBE, SETUP, PLAY, PAUSE, TEARDOWN, OPTIONS 等，顾名思义可以知道起对话和控制作用 \n\n- RTSP 的对话过程中 SETUP 可以确定 RTP/RTCP 使用的端口，PLAY/PAUSE/TEARDOWN 可以开始或者停止 RTP 的发送，等等 \n\nRTCP： \n\n- RTCP 包括 Sender Report 和 Receiver Report，用来进行音频/视频的同步以及其他用途，是一种控制协议\n\n**6. SDP**\n\n会话描述协议（SDP）为会话通知、会话邀请和其它形式的多媒体会话初始化等目的提供了多媒体会话描述。\n\n会话目录用于协助多媒体会议的通告，并为会话参与者传送相关设置信息。SDP 即用于将这种信息传输到接收端。**SDP 完全是一种会话描述格式 ― 它不属于传输协议 ― 它只使用不同的适当的传输协议，包括会话通知协议（SAP）、会话初始协议（SIP）、实时流协议（RTSP）、MIME 扩展协议的电子邮件以及超文本传输协议（HTTP）**。\n\nSDP 的设计宗旨是通用性，它可以应用于大范围的网络环境和应用程序，而不仅仅局限于组播会话目录，**但 SDP 不支持会话内容或媒体编码的协商**。\n\n在因特网组播骨干网（Mbone）中，会话目录工具被用于通告多媒体会议，并为参与者传送会议地址和参与者所需的会议特定工具信息，这由 SDP 完成。SDP 连接好会话后，传送足够的信息给会话参与者。SDP 信息发送利用了会话通知协议（SAP），它周期性地组播通知数据包到已知组播地址和端口处。这些信息是 UDP 数据包，其中包含 SAP 协议头和文本有效载荷（text payload）。这里文本有效载荷指的是 SDP 会话描述。此外信息也可以通过电子邮件或 WWW （World Wide Web） 进行发送。\n\n**SDP 文本信息包括**：\n\n- 会话名称和意图；\n- 会话持续时间；\n- 构成会话的媒体；\n- 有关接收媒体的信息（地址等）。\n- 协议结构\n\n**SDP 信息是文本信息，采用 UTF-8 编 码中的 ISO 10646 字符集。SDP 会话描述如下：（标注 * 符号的表示可选字段）**：\n\n```shell\nv = （协议版本）\no = （所有者/创建者和会话标识符）\ns = （会话名称）\ni = * （会话信息）\nu = * （URI 描述）\ne = * （Email 地址）\np = * （电话号码）\nc = * （连接信息 ― 如果包含在所有媒体中，则不需要该字段）\nb = * （带宽信息）\n```\n\n一个或更多时间描述（如下所示）：\n\n```shell\nz = * （时间区域调整）\nk = * （加密密钥）\na = * （0 个或多个会话属性行）\n```\n\n0个或多个媒体描述（如下所示）\n\n时间描述\n\n```shell\nt = （会话活动时间）\nr = * （0或多次重复次数）\n```\n\n媒体描述\n\n```shell\nm = （媒体名称和传输地址）\ni = * （媒体标题）\nc = * （连接信息 — 如果包含在会话层则该字段可选）\nb = * （带宽信息）\nk = * （加密密钥）\na = * （0 个或多个会话属性行）\n```\n\n**7. RTMP/RTMPS**\n\nRTMP(Real Time Messaging Protocol) 实时消息传送协议是 Adobe Systems 公司为 Flash 播放器和服务器之间音频、视频和数据传输 开发的开放协议。\n\n它有三种变种：\n\n> 1) 工作在 TCP 之上的明文协议，使用端口1935；\n>\n> 2) RTMPT 封装在 HTTP 请求之中，可穿越防火墙；\n>\n> 3) RTMPS 类似 RTMPT，但使用的是 HTTPS 连接；\n\nRTMP 协议(Real Time Messaging Protocol)是被 Flash 用于对象, 视频, 音频的传输. **这个协议建立在 TCP 协议或者轮询 HTTP 协议之上**.\n\nRTMP 协议就像一个用来装数据包的容器, 这些数据既可以是 AMF 格式的数据,也可以是 FLV 中的视/音频数据. 一个单一的连接可以通过不同的通道传输多路网络流. 这些通道中的包都是按照固定大小的包传输的.\n\n**8. mms**\n\nMMS (Microsoft Media Server Protocol)，中文“微软媒体服务器协议”，用来访问并流式接收 Windows Media 服务器中 `.asf` 文件的一种协议。MMS 协议用于访问 Windows Media 发布点上的单播内容。MMS 是连接 Windows Media 单播服务的默认方法。若观众在 Windows Media Player 中键入一个 URL 以连接内容，而不是通过超级链接访问内容，则他们必须使用MMS 协议引用该流。MMS的预设埠（端口）是1755\n\n当使用 MMS 协议连接到发布点时，使用协议翻转以获得最佳连接。“协议翻转”始于试图通过 MMSU 连接客户端。 MMSU 是 MMS 协议结合 UDP 数据传送。如果 MMSU 连接不成功，则服务器试图使用 MMST。MMST 是 MMS 协议结合 TCP 数据传送。\n\n如果连接到编入索引的 `.asf` 文件，想要快进、后退、暂停、开始和停止流，则必须使用 MMS。不能用 UNC 路径快进或后退。若您从独立的 Windows Media Player 连接到发布点，则必须指定单播内容的 URL。若内容在主发布点点播发布，则 URL 由服务器名和 `.asf` 文件名组成。例如：`mms://windows_media_server/sample.asf`。其中 windows_media_server 是 Windows Media 服务器名，sample.asf 是您想要使之转化为流的 `.asf` 文件名。\n\n若您有实时内容要通过广播单播发布，则该 URL 由服务器名和发布点别名组成。例如：`mms://windows_media_server/LiveEvents`。这里 windows_media_server 是 Windows Media 服务器名，而 LiveEvents 是发布点名\n\n**9. HLS**\n\nHTTP Live Streaming（HLS）是苹果公司(Apple Inc.)实现的基于HTTP的流媒体传输协议，可实现流媒体的直播和点播，主要应用在 iOS 系统，为 iOS 设备（如iPhone、iPad）提供音视频直播和点播方案。HLS 点播，基本上就是常见的分段 HTTP 点播，不同在于，它的分段非常小。\n\n相对于常见的流媒体直播协议，例如 RTMP协议、RTSP协议、MMS协议等，HLS直播最大的不同在于，直播客户端获取到的，并不是一个完整的数据流。HLS 协议在服务器端将直播数据流存储为连续的、很短时长的媒体文件（MPEG-TS格式），而客户端则不断的下载并播放这些小文件，因为服务器端总是会将最新的直播数据生成新的小文件，这样客户端只要不停的按顺序播放从服务器获取到的文件，就实现了直播。由此可见，基本上可以认为，**HLS 是以点播的技术方式来实现直播**。由于数据通过 HTTP 协议传输，所以完全不用考虑防火墙或者代理的问题，而且分段文件的时长很短，客户端可以很快的选择和切换码率，以适应不同带宽条件下的播放。不过 HLS 的这种技术特点，决定了它的延迟一般总是会高于普通的流媒体直播协议。　\n\n根据以上的了解要实现 HTTP Live Streaming 直播，需要研究并实现以下技术关键点：\n\n- 采集视频源和音频源的数据\n- 对原始数据进行H264编码和AAC编码\n- 视频和音频数据封装为MPEG-TS包\n- HLS分段生成策略及m3u8索引文件\n- HTTP传输协议\n\n### 9.1 HLS，RTSP，RTMP的区别\n\n- HLS （ HTTP Live Streaming）苹果公司提出的流媒体协议，直接把流媒体切片成一段段，信息保存到m3u列表文件中，可以将不同速率的版本切成相应的片；播放器可以直接使用http协议请求流数据，可以在不同速率的版本间自由切换，实现无缝播放；省去使用其他协议的烦恼。缺点是延迟大小受切片大小影响，不适合直播，适合视频点播。\n\n- RTSP （Real-Time Stream Protocol）由Real Networks 和 Netscape共同提出的，基于文本的多媒体播放控制协议。RTSP定义流格式，流数据经由RTP传输；RTSP实时效果非常好，适合视频聊天，视频监控等方向。\n\n- RTMP（Real Time Message Protocol） 有 Adobe 公司提出，用来解决多媒体数据传输流的多路复用（Multiplexing）和分包（packetizing）的问题，优势在于低延迟，稳定性高，支持所有摄像头格式，浏览器加载 flash插件就可以直接播放。\n\n总结：HLS 延迟大，适合视频点播；RTSP虽然实时性最好，但是实现复杂，适合视频聊天和视频监控；RTMP强在浏览器支持好，加载flash插件后就能直接播放，所以非常火，相反在浏览器里播放rtsp就很困难了。\n\n### 9.2 RTSP、RTCP、RTP区别\n\n**1：RTSP实时流协议**\n\n作为一个应用层协议，RTSP提供了一个可供扩展的框架，它的意义在于使得实时流媒体数据的受控和点播变得可能。总的说来，RTSP是一个流媒体表示 协议，主要用来控制具有实时特性的数据发送，但它本身并不传输数据，而是必须依赖于下层传输协议所提供的某些服务。RTSP可以对流媒体提供诸如播放、暂 停、快进等操作，它负责定义具体的控制消息、操作方法、状态码等，此外还描述了与RTP间的交互操作（RFC2326）。\n\n**2：RTCP控制协议**\n\nRTCP控制协议需要与RTP数据协议一起配合使用，当应用程序启动一个RTP会话时将同时占用两个端口，分别供RTP和RTCP使用。RTP本身并 不能为按序传输数据包提供可靠的保证，也不提供流量控制和拥塞控制，这些都由RTCP来负责完成。通常RTCP会采用与RTP相同的分发机制，向会话中的 所有成员周期性地发送控制信息，应用程序通过接收这些数据，从中获取会话参与者的相关资料，以及网络状况、分组丢失概率等反馈信息，从而能够对服务质量进 行控制或者对网络状况进行诊断。\n\nRTCP协议的功能是通过不同的RTCP数据报来实现的，主要有如下几种类型：\n\n- SR：发送端报告，所谓发送端是指发出RTP数据报的应用程序或者终端，发送端同时也可以是接收端。(SERVER定时间发送给CLIENT)。\n\n- RR：接收端报告，所谓接收端是指仅接收但不发送RTP数据报的应用程序或者终端。(SERVER接收CLIENT端发送过来的响应)。\n\n- SDES：源描述，主要功能是作为会话成员有关标识信息的载体，如用户名、邮件地址、电话号码等，此外还具有向会话成员传达会话控制信息的功能。\n\n- BYE：通知离开，主要功能是指示某一个或者几个源不再有效，即通知会话中的其他成员自己将退出会话。\n\n- APP：由应用程序自己定义，解决了RTCP的扩展性问题，并且为协议的实现者提供了很大的灵活性。\n\n**3：RTP数据协议**\n\nRTP数据协议负责对流媒体数据进行封包并实现媒体流的实时传输，每一个RTP数据报都由头部（Header）和负载（Payload）两个部分组成，其中头部前12个字节的含义是固定的，而负载则可以是音频或者视频数据。\n\nRTP用到的地方就是 PLAY ，服务器往客户端传输数据用UDP协议，RTP是在传输数据的前面加了个12字节的头(描述信息)。\n\nRTP载荷封装设计本文的网络传输是基于IP协议，所以最大传输单元(MTU)最大为1500字节，在使用IP／UDP／RTP的协议层次结构的时候，这 其中包括至少20字节的IP头，8字节的UDP头，以及12字节的RTP头。这样，头信息至少要占用40个字节，那么RTP载荷的最大尺寸为1460字 节。以H264 为例，如果一帧数据大于1460，则需要分片打包，然后到接收端再拆包，组合成一帧数据，进行解码播放。\n\n### 9.3 RTSP、 RTMP、HTTP的共同点、区别\n\n共同点：\n\n- RTSP RTMP HTTP都是在应用应用层。\n- 理论上RTSP RTMPHTTP都可以做直播和点播，但一般做直播用RTSP RTMP，做点播用HTTP。做视频会议的时候原来用SIP协议，现在基本上被RTMP协议取代了。\n\n区别：\n\n- HTTP: 即超文本传送协议(ftp即文件传输协议)。\n\n  - HTTP:（Real Time Streaming Protocol），实时流传输协议。\n\n  - HTTP全称Routing Table Maintenance Protocol（路由选择表维护协议）。\n\n- HTTP将所有的数据作为文件做处理。http协议不是流媒体协议。\n\n  - RTMP 和 RTSP协议是流媒体协议。\n\n- RTMP协议是Adobe的私有协议,未完全公开，RTSP协议和HTTP协议是共有协议，并有专门机构做维护。\n\n- RTMP协议一般传输的是flv，f4v格式流，RTSP协议一般传输的是ts,mp4格式的流。HTTP没有特定的流。\n\n- RTSP传输一般需要2-3个通道，命令和数据通道分离，HTTP和RTMP一般在TCP一个通道上传输命令和数据。\n\n## 10. 代码\n\n<details><summary>FFmpeg SDK解封和推流</summary>\n\n```c++\n#include <iostream>\nusing namespace std;\n//引入头文件\nextern \"C\"\n{\n    #include \"libavformat/avformat.h\"\n    //引入时间\n    #include \"libavutil/time.h\"\n}\n//引入库\n#pragma comment(lib,\"avformat.lib\")\n//工具库，包括获取错误信息等\n#pragma comment(lib,\"avutil.lib\")\n//编解码的库\n#pragma comment(lib,\"avcodec.lib\")\n\nint avError(int errNum);\n\nstatic double r2d(AVRational r)\n{\n    return r.num == 0 || r.den == 0 ? 0. : (double)r.num / (double)r.den;\n}\nint main() {\n    int videoindex = -1;\n    //所有代码执行之前要调用av_register_all和avformat_network_init\n    //初始化所有的封装和解封装 flv mp4 mp3 mov。不包含编码和解码\n    av_register_all();\n\n    //初始化网络库\n    avformat_network_init();\n\n    //使用的相对路径，执行文件在bin目录下。test.mp4放到bin目录下即可\n    const char *inUrl = \"hs.mp4\";\n    //输出的地址\n    const char *outUrl = \"rtmp://192.166.11.13/live\";\n\n    //////////////////////////////////////////////////////////////////\n    //                   输入流处理部分\n    /////////////////////////////////////////////////////////////////\n    //打开文件，解封装 avformat_open_input\n    //AVFormatContext **ps  输入封装的上下文。包含所有的格式内容和所有的IO。如果是文件就是文件IO，网络就对应网络IO\n    //const char *url  路径\n    //AVInputFormt * fmt 封装器\n    //AVDictionary ** options 参数设置\n    AVFormatContext *ictx = NULL;\n\n    AVOutputFormat *ofmt = NULL;\n\n    //打开文件，解封文件头\n    int ret = avformat_open_input(&ictx, inUrl, 0, NULL);\n    if (ret < 0) {\n        return avError(ret);\n    }\n    cout << \"avformat_open_input success!\" << endl;\n    //获取音频视频的信息 .h264 flv 没有头信息\n    ret = avformat_find_stream_info(ictx, 0);\n    if (ret != 0) {\n        return avError(ret);\n    }\n    //打印视频视频信息\n    //0打印所有  inUrl 打印时候显示，\n    av_dump_format(ictx, 0, inUrl, 0);\n\n    //////////////////////////////////////////////////////////////////\n    //                   输出流处理部分\n    /////////////////////////////////////////////////////////////////\n    AVFormatContext * octx = NULL;\n    //如果是输入文件 flv可以不传，可以从文件中判断。如果是流则必须传\n    //创建输出上下文\n    ret = avformat_alloc_output_context2(&octx, NULL, \"flv\", outUrl);\n    if (ret < 0) {\n        return avError(ret);\n    }\n    cout << \"avformat_alloc_output_context2 success!\" << endl;\n\n    ofmt = octx->oformat;\n    cout << \"nb_streams  \" << ictx->nb_streams << endl;\n    int i;\n    for (i = 0; i < ictx->nb_streams; i++) {\n        //获取输入视频流\n        AVStream *in_stream = ictx->streams[i];\n        //为输出上下文添加音视频流（初始化一个音视频流容器）\n        AVStream *out_stream = avformat_new_stream(octx, in_stream->codec->codec);\n        if (!out_stream) {\n            printf(\"未能成功添加音视频流\\n\");\n            ret = AVERROR_UNKNOWN;\n        }\n\n        //将输入编解码器上下文信息 copy 给输出编解码器上下文\n        //ret = avcodec_copy_context(out_stream->codec, in_stream->codec);\n        ret = avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar);\n        //ret = avcodec_parameters_from_context(out_stream->codecpar, in_stream->codec);\n        //ret = avcodec_parameters_to_context(out_stream->codec, in_stream->codecpar);\n        if (ret < 0) {\n            printf(\"copy 编解码器上下文失败\\n\");\n        }\n        out_stream->codecpar->codec_tag = 0;\n\n        out_stream->codec->codec_tag = 0;\n        if (octx->oformat->flags & AVFMT_GLOBALHEADER) {\n            out_stream->codec->flags = out_stream->codec->flags | CODEC_FLAG_GLOBAL_HEADER;\n        }\n    }\n\n    //输入流数据的数量循环\n    for (i = 0; i < ictx->nb_streams; i++) {\n        if (ictx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO) {\n            videoindex = i;\n            break;\n        }\n    }\n    av_dump_format(octx, 0, outUrl, 1);\n\n    //////////////////////////////////////////////////////////////////\n    //                   准备推流\n    /////////////////////////////////////////////////////////////////\n    //打开IO\n    ret = avio_open(&octx->pb, outUrl, AVIO_FLAG_WRITE);\n    if (ret < 0) {\n        avError(ret);\n    }\n\n    //写入头部信息\n    ret = avformat_write_header(octx, 0);\n    if (ret < 0) {\n        avError(ret);\n    }\n    cout << \"avformat_write_header Success!\" << endl;\n    //推流每一帧数据\n    //int64_t pts  [ pts*(num/den)  第几秒显示]\n    //int64_t dts  解码时间 [P帧(相对于上一帧的变化) I帧(关键帧，完整的数据) B帧(上一帧和下一帧的变化)]  有了B帧压缩率更高。\n    //uint8_t *data    \n    //int size\n    //int stream_index\n    //int flag\n    AVPacket pkt;\n    //获取当前的时间戳  微妙\n    long long start_time = av_gettime();\n    long long frame_index = 0;\n    while (1) {\n        //输入输出视频流\n        AVStream *in_stream, *out_stream;\n        //获取解码前数据\n        ret = av_read_frame(ictx, &pkt);\n        if (ret < 0) {\n            break;\n        }\n\n        /*\n        PTS（Presentation Time Stamp）显示播放时间\n        DTS（Decoding Time Stamp）解码时间\n        */\n        //没有显示时间（比如未解码的 H.264 ）\n        if (pkt.pts == AV_NOPTS_VALUE) {\n            //AVRational time_base：时基。通过该值可以把PTS，DTS转化为真正的时间。\n            AVRational time_base1 = ictx->streams[videoindex]->time_base;\n\n            //计算两帧之间的时间\n            /*\n            r_frame_rate 基流帧速率  （不是太懂）\n            av_q2d 转化为double类型\n            */\n            int64_t calc_duration = (double)AV_TIME_BASE / av_q2d(ictx->streams[videoindex]->r_frame_rate);\n\n            //配置参数\n            pkt.pts = (double)(frame_index*calc_duration) / (double)(av_q2d(time_base1)*AV_TIME_BASE);\n            pkt.dts = pkt.pts;\n            pkt.duration = (double)calc_duration / (double)(av_q2d(time_base1)*AV_TIME_BASE);\n        }\n\n        //延时\n        if (pkt.stream_index == videoindex) {\n            AVRational time_base = ictx->streams[videoindex]->time_base;\n            AVRational time_base_q = { 1,AV_TIME_BASE };\n            //计算视频播放时间\n            int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);\n            //计算实际视频的播放时间\n            int64_t now_time = av_gettime() - start_time;\n\n            AVRational avr = ictx->streams[videoindex]->time_base;\n            cout << avr.num << \" \" << avr.den << \"  \"<<pkt.dts <<\"  \"<<pkt.pts<<\"   \"<< pts_time <<endl;\n            if (pts_time > now_time) {\n                //睡眠一段时间（目的是让当前视频记录的播放时间与实际时间同步）\n                av_usleep((unsigned int)(pts_time - now_time));\n            }\n        }\n\n        in_stream = ictx->streams[pkt.stream_index];\n        out_stream = octx->streams[pkt.stream_index];\n\n        //计算延时后，重新指定时间戳\n        pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base,(AVRounding) (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));\n        pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));\n        //到这一帧时候经历了多长时间\n        pkt.duration = (int)av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);\n        //字节流的位置，-1 表示不知道字节流位置\n        pkt.pos = -1;\n\n        if (pkt.stream_index == videoindex) {\n            printf(\"Send %8d video frames to output URL\\n\", frame_index);\n            frame_index++;\n        }\n\n        //向输出上下文发送（向地址推送）\n        ret = av_interleaved_write_frame(octx, &pkt);\n\n        if (ret < 0) {\n            printf(\"发送数据包出错\\n\");\n            break;\n        }\n\n        //释放\n        av_free_packet(&pkt);\n    }\n    return 0;\n}\n\nint avError(int errNum) {\n    char buf[1024];\n    //获取错误信息\n    av_strerror(errNum, buf, sizeof(buf));\n    cout << \" failed! \" << buf << endl;\n    return -1;\n}\n```\n\n</details>\n\n<details><summary>rtsp数据源rtmp推流 openCV磨皮</summary>\n\n```c++\n#include <opencv2/highgui.hpp>\n#include <iostream>\nextern \"C\"\n{\n    #include <libswscale/swscale.h>\n    #include <libavcodec/avcodec.h>\n    #include <libavformat/avformat.h>\n}\n#pragma comment(lib, \"swscale.lib\")\n#pragma comment(lib, \"avcodec.lib\")\n#pragma comment(lib, \"avutil.lib\")\n#pragma comment(lib, \"avformat.lib\")\n#pragma comment(lib,\"opencv_world320.lib\")\nusing namespace std;\nusing namespace cv;\nint main(int argc, char *argv[])\n{\n    //海康相机的rtsp url\n    char *inUrl = \"rtsp://test:test123456@192.168.1.64\";\n    //nginx-rtmp rtmp  直播服务器rtmp推流URL\n    char *outUrl = \"rtmp://192.168.1.44/live\";\n\n    //注册所有的编解码器\n    avcodec_register_all();\n\n    //注册所有的封装器\n    av_register_all();\n\n    //注册所有网络协议\n    avformat_network_init();\n\n    // opencv 接口\n    VideoCapture cam;\n    Mat frame;\n    namedWindow(\"video\");\n\n    //像素格式转换上下文\n    SwsContext *vsc = NULL;\n\n    //输出的数据结构\n    AVFrame *yuv = NULL;\n\n    //编码器上下文\n    AVCodecContext *vc = NULL;\n\n    //rtmp flv 封装器\n    AVFormatContext *ic = NULL;\n\n    try{  \n        /// 1 使用opencv打开rtsp相机\n        cam.open(inUrl);\n        if (!cam.isOpened()){\n            throw exception(\"cam open failed!\");\n        }\n        cout << inUrl << \" cam open success\" << endl;\n        int inWidth = cam.get(CAP_PROP_FRAME_WIDTH);\n        int inHeight = cam.get(CAP_PROP_FRAME_HEIGHT);\n        int fps = cam.get(CAP_PROP_FPS);\n\n        /// 2 初始化格式转换上下文\n        vsc = sws_getCachedContext(vsc,\n                 inWidth, inHeight, AV_PIX_FMT_BGR24,     // 源宽、高、像素格式\n                 inWidth, inHeight, AV_PIX_FMT_YUV420P,   // 目标宽、高、像素格式\n                 SWS_BICUBIC,  // 尺寸变化使用算法\n                 0, 0, 0\n                 );\n        if (!vsc){\n            throw exception(\"sws_getCachedContext failed!\");\n        }\n        \n        /// 3 初始化输出的数据结构\n        yuv = av_frame_alloc();\n        yuv->format = AV_PIX_FMT_YUV420P;\n        yuv->width = inWidth;\n        yuv->height = inHeight;\n        yuv->pts = 0;\n        \n        // 配yuv空间\n        int ret = av_frame_get_buffer(yuv, 32);\n        if (ret != 0){\n            char buf[1024] = { 0 };\n            av_strerror(ret, buf, sizeof(buf) - 1);\n            throw exception(buf);\n        }\n\n        /// 4 初始化编码上下文\n        // a 找到编码器\n        AVCodec *codec = avcodec_find_encoder(AV_CODEC_ID_H264);\n        if (!codec){\n            throw exception(\"Can`t find h264 encoder!\");\n        }\n        \n        // b 创建编码器上下文\n        vc = avcodec_alloc_context3(codec);\n        if (!vc){\n            throw exception(\"avcodec_alloc_context3 failed!\");\n        }\n        \n        // c 配置编码器参数\n        vc->flags |= AV_CODEC_FLAG_GLOBAL_HEADER;   // 全局参数\n        vc->codec_id = codec->id;\n        vc->thread_count = 8;\n\n        vc->bit_rate = 50 * 1024 * 8;    // 压缩后每秒视频的bit位大小 50kB\n        vc->width = inWidth;\n        vc->height = inHeight;\n        vc->time_base = { 1,fps };\n        vc->framerate = { fps,1 };\n\n        // 画面组的大小，多少帧一个关键帧\n        vc->gop_size = 50;\n        vc->max_b_frames = 0;\n        vc->pix_fmt = AV_PIX_FMT_YUV420P;\n        \n        // d 打开编码器上下文\n        ret = avcodec_open2(vc, 0, 0);\n        if (ret != 0){\n            char buf[1024] = { 0 };\n            av_strerror(ret, buf, sizeof(buf) - 1);\n            throw exception(buf);\n        }\n        cout << \"avcodec_open2 success!\" << endl;\n\n        /// 5 输出封装器和视频流配置\n        // a 创建输出封装器上下文\n        ret = avformat_alloc_output_context2(&ic, 0, \"flv\", outUrl);\n        if (ret != 0){\n            char buf[1024] = { 0 };\n            av_strerror(ret, buf, sizeof(buf) - 1);\n            throw exception(buf);\n        }\n        \n        // b 添加视频流\n        AVStream *vs = avformat_new_stream(ic, NULL);\n        if (!vs){\n            throw exception(\"avformat_new_stream failed\");\n        }\n        vs->codecpar->codec_tag = 0;\n        \n        // 从编码器复制参数\n        avcodec_parameters_from_context(vs->codecpar, vc);\n        av_dump_format(ic, 0, outUrl, 1);\n\n        /// 6 打开rtmp 的网络输出IO\n        ret = avio_open(&ic->pb, outUrl, AVIO_FLAG_WRITE);\n        if (ret != 0){\n            char buf[1024] = { 0 };\n            av_strerror(ret, buf, sizeof(buf) - 1);\n            throw exception(buf);\n        }\n\n        // 写入封装头\n        ret = avformat_write_header(ic, NULL);\n        if (ret != 0){\n            char buf[1024] = { 0 };\n            av_strerror(ret, buf, sizeof(buf) - 1);\n            throw exception(buf);\n        }\n\n        AVPacket pack;\n        memset(&pack, 0, sizeof(pack));\n        int vpts = 0;\n        for (;;){\n            /// 取rtsp视频帧，解码视频帧\n            if (!cam.grab()){\n                continue;\n            }\n            \n            /// yuv转换为rgb\n            if (!cam.retrieve(frame)){\n                continue;\n            }\n            //imshow(\"video\", frame);\n            //waitKey(1);\n\n            /// rgb to yuv\n            // 输入的数据结构\n            uint8_t *indata[AV_NUM_DATA_POINTERS] = { 0 };\n            //indata[0] bgrbgrbgr\n            //plane indata[0] bbbbb indata[1]ggggg indata[2]rrrrr \n            indata[0] = frame.data;\n            int insize[AV_NUM_DATA_POINTERS] = { 0 };\n            \n            // 一行（宽）数据的字节数\n            insize[0] = frame.cols * frame.elemSize();\n            int h = sws_scale(vsc, indata, insize, 0, frame.rows,  // 源数据\n                              yuv->data, yuv->linesize);\n            if (h <= 0){\n                continue;\n            }\n            //cout << h << \" \" << flush;\n            \n            /// h264\n            yuv->pts = vpts;\n            vpts++;\n            ret = avcodec_send_frame(vc, yuv);\n            if (ret != 0)\n                continue;\n\n            ret = avcodec_receive_packet(vc, &pack);\n            if (ret != 0 || pack.size > 0){\n                //cout << \"*\" << pack.size << flush;\n            }\n            else{\n                continue;\n            }\n            \n            // 推流\n            pack.pts = av_rescale_q(pack.pts, vc->time_base, vs->time_base);\n            pack.dts = av_rescale_q(pack.dts, vc->time_base, vs->time_base);\n            pack.duration = av_rescale_q(pack.duration, vc->time_base, vs->time_base);\n            ret = av_interleaved_write_frame(ic, &pack);\n            if (ret == 0){\n                cout << \"#\" << flush;\n            }\n        }\n    }\n    catch (exception &ex){\n        if (cam.isOpened())\n            cam.release();\n        \n        if (vsc){\n            sws_freeContext(vsc);\n            vsc = NULL;\n        }\n\n        if (vc){\n            avio_closep(&ic->pb);\n            avcodec_free_context(&vc);\n        }\n\n        cerr << ex.what() << endl;\n    }\n    getchar();\n    return 0;\n}\n```\n\n</details>\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"FFMPEG+SDL视频播放器","url":"/2019-05-18/reference/FFmpeg/FFmpeg+SDL视频播放器/","content":"\n> [GitHub](<https://github.com/HatsuneMikuV/FFmpeg_Leixiaohua>)\n>\n> [FFmpeg](<https://blog.csdn.net/leixiaohua1020/column/info/ffmpeg-devel/7>)\n>\n> [ffmpeg 源代码简单分析](<https://blog.csdn.net/leixiaohua1020/article/details/12677129>)\n\n# 100行代码实现最简单的基于FFMPEG+SDL的视频播放器\n\n> [simplest_ffmpeg_player](<https://github.com/leixiaohua1020/simplest_ffmpeg_player>)\n\n该播放器虽然简单，但是几乎包含了使用FFMPEG播放一个视频所有必备的API，并且使用SDL显示解码出来的视频。\n\n<!-- more -->\n\n并且支持流媒体等多种视频输入，处于简单考虑，没有音频部分，同时视频播放采用直接延时40ms的方式\n\n![播放器解码的流程用图](/images/imageFFmpeg/Thor/播放器解码的流程用图.png)\n\n![SDL1.x显示YUV图像的流程图](/images/imageFFmpeg/Thor/SDL显示YUV图像的流程图.png)\n\n![SDL2.0显示YUV的流程图](/images/imageFFmpeg/Thor/SDL2.0显示YUV的流程图.png)\n\n对比SDL1.2的流程图，发现变化还是很大的。几乎所有的API都发生了变化。但是函数和变量有一定的对应关系：\n\nSDL_SetVideoMode()————SDL_CreateWindow()\n\nSDL_Surface————SDL_Window\n\nSDL_CreateYUVOverlay()————SDL_CreateTexture()\n\nSDL_Overlay————SDL_Texture\n\n简单解释各个变量的作用：\n\n- SDL_Window就是使用SDL的时候弹出的那个窗口。在SDL1.x版本中，只可以创建一个一个窗口。在SDL2.0版本中，可以创建多个窗口。\n- SDL_Texture用于显示YUV数据。一个SDL_Texture对应一帧YUV数据。\n- SDL_Renderer用于渲染SDL_Texture至SDL_Window。\n- SDL_Rect用于确定SDL_Texture显示的位置。注意：一个SDL_Texture可以指定多个不同的\n- SDL_Rect，这样就可以在SDL_Window不同位置显示相同的内容（使用SDL_RenderCopy()函数）。\n\n它们的关系如下图所示：\n\n![](/images/imageFFmpeg/Thor/关系图.png)\n\n下图举了个例子，指定了4个SDL_Rect，可以实现4分屏的显示。\n\n![SDL_Rect四分屏显示](/images/imageFFmpeg/Thor/SDL_Rect四分屏显示.png)\n\n## simplest_ffmpeg_player（标准版）代码\n\n<details><summary>最基础的版本，学习的开始。</summary>\n\n```c\n\n/**\n * 最简单的基于FFmpeg的视频播放器 2\n * Simplest FFmpeg Player 2\n *\n * 本程序实现了视频文件的解码和显示(支持HEVC，H.264，MPEG2等)。\n * 是最简单的FFmpeg视频解码方面的教程。\n * 通过学习本例子可以了解FFmpeg的解码流程。\n * This software is a simplest video player based on FFmpeg.\n * Suitable for beginner of FFmpeg.\n *\n */\n\n#include <stdio.h>\n \n#define __STDC_CONSTANT_MACROS\n \n#ifdef _WIN32\n//Windows\nextern \"C\"\n{\n#include \"libavcodec/avcodec.h\"\n#include \"libavformat/avformat.h\"\n#include \"libswscale/swscale.h\"\n#include \"libavutil/imgutils.h\"\n#include \"SDL2/SDL.h\"\n};\n#else\n//Linux...\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n    \n#include <libavcodec/avcodec.h>\n#include <libavformat/avformat.h>\n#include <libswscale/swscale.h>\n#include <SDL2/SDL.h>\n#include <libavutil/imgutils.h>\n\n#ifdef __cplusplus\n};\n#endif\n#endif\n \n//Output YUV420P data as a file \n#define OUTPUT_YUV420P 0\n \nint main(int argc, char* argv[])\n{\n\tAVFormatContext\t*pFormatCtx; \t\t// 音视频格式化上下文\n\tint\t\t\t\ti, videoindex;\n\tAVCodecContext\t*pCodecCtx;\t\t\t// 音视频编解码器上下文\n\tAVCodec\t\t\t*pCodec;\t\t\t// 音视频编解码器\n\tAVFrame\t\t\t*pFrame, *pFrameYUV;\t// 音视频数据帧\n\tunsigned char \t*out_buffer;\n\tAVPacket \t\t*packet;\t\t\t// 音视频数据包\n\tint \t\t\ty_size;\n\tint \t\t\tret, got_picture;\n\tstruct SwsContext *img_convert_ctx; // 视频图像转换上下文\n \n\tchar filepath[]=\"bigbuckbunny_480x272.h265\";\n\t//SDL---------------------------\n\tint screen_w=0, screen_h=0;\n\tSDL_Window \t\t*screen; \n\tSDL_Renderer\t*sdlRenderer; \t\t// SDL渲染器\n\tSDL_Texture\t\t*sdlTexture;\t\t// SDL纹理\n\tSDL_Rect \t\tsdlRect;\t\t\t// 确定SDL_Texture显示的位置\n \n\tFILE *fp_yuv;\n \n\tav_register_all();\t\t\t\t\t// 注册所有的编解码器\n\tavformat_network_init();\n\tpFormatCtx = avformat_alloc_context();\n \n\tif(avformat_open_input(&pFormatCtx, filepath, NULL, NULL) != 0){\n\t\tprintf(\"Couldn't open input stream.\\n\");\n\t\treturn -1;\n\t}\n\tif(avformat_find_stream_info(pFormatCtx,NULL) < 0){\n\t\tprintf(\"Couldn't find stream information.\\n\");\n\t\treturn -1;\n\t}\n\tvideoindex = -1;\n\tfor(i=0; i<pFormatCtx->nb_streams; i++) \n\t\tif(pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){\n\t\t\tvideoindex = i;\n\t\t\tbreak;\n\t\t}\n\tif(videoindex == -1){\n\t\tprintf(\"Didn't find a video stream.\\n\");\n\t\treturn -1;\n\t}\n \n\tpCodecCtx = pFormatCtx->streams[videoindex]->codec;\n\tpCodec = avcodec_find_decoder(pCodecCtx->codec_id);\n\tif(pCodec == NULL){\n\t\tprintf(\"Codec not found.\\n\");\n\t\treturn -1;\n\t}\n\tif(avcodec_open2(pCodecCtx, pCodec,NULL) < 0){\n\t\tprintf(\"Could not open codec.\\n\");\n\t\treturn -1;\n\t}\n\t\n\tpFrame = av_frame_alloc();\n\tpFrameYUV = av_frame_alloc();\n\tout_buffer = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P,  pCodecCtx->width, pCodecCtx->height, 1));\n\tav_image_fill_arrays(pFrameYUV->data, pFrameYUV->linesize,out_buffer,\n\t\tAV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height, 1);\n\t\n\tpacket = (AVPacket *)av_malloc(sizeof(AVPacket));\n\t//Output Info-----------------------------\n\tprintf(\"--------------- File Information ----------------\\n\");\n\tav_dump_format(pFormatCtx, 0, filepath, 0);\n\tprintf(\"-------------------------------------------------\\n\");\n\timg_convert_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height, pCodecCtx->pix_fmt, \n\t\tpCodecCtx->width, pCodecCtx->height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); \n \n#if OUTPUT_YUV420P \n    fp_yuv=fopen(\"output.yuv\", \"wb+\");  \n#endif  \n\t\n\tif(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {  \n\t\tprintf( \"Could not initialize SDL - %s\\n\", SDL_GetError()); \n\t\treturn -1;\n\t} \n \n\tscreen_w = pCodecCtx->width;\n\tscreen_h = pCodecCtx->height;\n\t//SDL 2.0 Support for multiple windows\n\tscreen = SDL_CreateWindow(\"Simplest ffmpeg player's Window\", SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED,\n\t\tscreen_w, screen_h,\n\t\tSDL_WINDOW_OPENGL);\n \n\tif(!screen) {  \n\t\tprintf(\"SDL: could not create window - exiting:%s\\n\",SDL_GetError()); \n\t\treturn -1;\n\t}\n \n\tsdlRenderer = SDL_CreateRenderer(screen, -1, 0);  \n\t//IYUV: Y + U + V  (3 planes)\n\t//YV12: Y + V + U  (3 planes)\n\tsdlTexture = SDL_CreateTexture(sdlRenderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, pCodecCtx->width, pCodecCtx->height);  \n \n\tsdlRect.x = 0;\n\tsdlRect.y = 0;\n\tsdlRect.w = screen_w;\n\tsdlRect.h = screen_h;\n \n\t//SDL End----------------------\n\twhile(av_read_frame(pFormatCtx, packet)>=0){\n\t\tif(packet->stream_index == videoindex){\n\t\t\tret = avcodec_decode_video2(pCodecCtx, pFrame, &got_picture, packet);\n\t\t\tif(ret < 0){\n\t\t\t\tprintf(\"Decode Error.\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tif(got_picture){\n\t\t\t\tsws_scale(img_convert_ctx, (const unsigned char* const*)pFrame->data, pFrame->linesize, 0, pCodecCtx->height, \n\t\t\t\t\tpFrameYUV->data, pFrameYUV->linesize);\n\t\t\t\t\n#if OUTPUT_YUV420P\n\t\t\t\ty_size = pCodecCtx->width*pCodecCtx->height;  \n\t\t\t\tfwrite(pFrameYUV->data[0], 1, y_size, fp_yuv);    //Y \n\t\t\t\tfwrite(pFrameYUV->data[1], 1, y_size/4, fp_yuv);  //U\n\t\t\t\tfwrite(pFrameYUV->data[2], 1, y_size/4, fp_yuv);  //V\n#endif\n\t\t\t\t//SDL---------------------------\n#if 0\n\t\t\t\tSDL_UpdateTexture(sdlTexture, NULL, pFrameYUV->data[0], pFrameYUV->linesize[0]);  \n#else\n\t\t\t\tSDL_UpdateYUVTexture(sdlTexture, &sdlRect,\n\t\t\t\tpFrameYUV->data[0], pFrameYUV->linesize[0],\n\t\t\t\tpFrameYUV->data[1], pFrameYUV->linesize[1],\n\t\t\t\tpFrameYUV->data[2], pFrameYUV->linesize[2]);\n#endif\t\n\t\t\t\t\n\t\t\t\tSDL_RenderClear(sdlRenderer);  \n\t\t\t\tSDL_RenderCopy(sdlRenderer, sdlTexture, NULL, &sdlRect);  \n\t\t\t\tSDL_RenderPresent(sdlRenderer);  \n\t\t\t\t//SDL End-----------------------\n\t\t\t\t//Delay 40ms\n\t\t\t\tSDL_Delay(40);\n\t\t\t}\n\t\t}\n\t\tav_free_packet(packet);\n\t}\n\t//flush decoder\n\t//FIX: Flush Frames remained in Codec\n\twhile (1) {\n\t\tret = avcodec_decode_video2(pCodecCtx, pFrame, &got_picture, packet);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tif (!got_picture)\n\t\t\tbreak;\n\t\tsws_scale(img_convert_ctx, (const unsigned char* const*)pFrame->data, pFrame->linesize, 0, pCodecCtx->height, \n\t\t\tpFrameYUV->data, pFrameYUV->linesize);\n#if OUTPUT_YUV420P\n\t\tint y_size = pCodecCtx->width*pCodecCtx->height;  \n\t\tfwrite(pFrameYUV->data[0], 1, y_size, fp_yuv);    //Y \n\t\tfwrite(pFrameYUV->data[1], 1, y_size/4, fp_yuv);  //U\n\t\tfwrite(pFrameYUV->data[2], 1, y_size/4, fp_yuv);  //V\n#endif\n\t\t//SDL---------------------------\n\t\tSDL_UpdateTexture(sdlTexture, &sdlRect, pFrameYUV->data[0], pFrameYUV->linesize[0]);  \n\t\tSDL_RenderClear(sdlRenderer);  \n\t\tSDL_RenderCopy(sdlRenderer, sdlTexture, NULL, &sdlRect);  \n\t\tSDL_RenderPresent(sdlRenderer);  \n\t\t//SDL End-----------------------\n\t\t//Delay 40ms\n\t\tSDL_Delay(40);\n\t}\n \n\tsws_freeContext(img_convert_ctx);\n \n#if OUTPUT_YUV420P \n    fclose(fp_yuv);\n#endif \n \n\tSDL_Quit();\n \n\tav_frame_free(&pFrameYUV);\n\tav_frame_free(&pFrame);\n\tavcodec_close(pCodecCtx);\n\tavformat_close_input(&pFormatCtx);\n \n\treturn 0;\n}\n```\n\n</details>\n\n## simplest_ffmpeg_player_su（SU版）代码\n\n标准版的基础之上引入了 SDL 的 Event。\n\n效果如下：\n\n- SDL弹出的窗口可以移动了\n- 画面显示是严格的40ms一帧\n\n<details><summary>代码：</summary>\n\n```c\n\n/**\n * 最简单的基于FFmpeg的视频播放器2(SDL升级版)\n * Simplest FFmpeg Player 2(SDL Update)\n *\n * 本程序实现了视频文件的解码和显示(支持HEVC，H.264，MPEG2等)。\n * 是最简单的FFmpeg视频解码方面的教程。\n * 通过学习本例子可以了解FFmpeg的解码流程。\n * 本版本中使用SDL消息机制刷新视频画面。\n * This software is a simplest video player based on FFmpeg.\n * Suitable for beginner of FFmpeg.\n *\n * 备注:\n * 标准版在播放视频的时候，画面显示使用延时40ms的方式。这么做有两个后果：\n * （1）SDL弹出的窗口无法移动，一直显示是忙碌状态\n * （2）画面显示并不是严格的40ms一帧，因为还没有考虑解码的时间。\n * SU（SDL Update）版在视频解码的过程中，不再使用延时40ms的方式，而是创建了\n * 一个线程，每隔40ms发送一个自定义的消息，告知主函数进行解码显示。这样做之后：\n * （1）SDL弹出的窗口可以移动了\n * （2）画面显示是严格的40ms一帧\n */\n#include <stdio.h>\n \n#define __STDC_CONSTANT_MACROS\n \n#ifdef _WIN32\n//Windows\nextern \"C\"\n{\n#include \"libavcodec/avcodec.h\"\n#include \"libavformat/avformat.h\"\n#include \"libswscale/swscale.h\"\n#include \"libavutil/imgutils.h\"\n#include \"SDL2/SDL.h\"\n};\n#else\n//Linux...\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif \n#include <libavcodec/avcodec.h>\n#include <libavformat/avformat.h>\n#include <libswscale/swscale.h>\n#include <libavutil/imgutils.h>\n#include <SDL2/SDL.h>   \n#ifdef __cplusplus\n};\n#endif\n#endif\n \n//Refresh Event\n#define SFM_REFRESH_EVENT  (SDL_USEREVENT + 1)\n#define SFM_BREAK_EVENT  (SDL_USEREVENT + 2)\n \nint thread_exit = 0;\nint thread_pause = 0;\n \nint sfp_refresh_thread(void *opaque){\n\tthread_exit = 0;\n\tthread_pause = 0;\n \n\twhile (!thread_exit) {\n\t\tif(!thread_pause){\n\t\t\tSDL_Event event;\n\t\t\tevent.type = SFM_REFRESH_EVENT;\n\t\t\tSDL_PushEvent(&event);\n\t\t}\n\t\tSDL_Delay(40);\n\t}\n\tthread_exit = 0;\n\tthread_pause = 0;\n\t//Break\n\tSDL_Event event;\n\tevent.type = SFM_BREAK_EVENT;\n\tSDL_PushEvent(&event);\n \n\treturn 0;\n}\n \nint main(int argc, char* argv[])\n{\n\tAVFormatContext\t*pFormatCtx;\n\tint\t\t\t\ti, videoindex;\n\tAVCodecContext\t*pCodecCtx;\t\t\t// 音视频编解码器上下文\n\tAVCodec\t\t\t*pCodec;\n\tAVFrame\t\t\t*pFrame, *pFrameYUV;\n\tunsigned char \t*out_buffer;\n\tAVPacket \t\t*packet;\n\tint \t\t\tret, got_picture;\n \n\t//------------SDL----------------\n\tint screen_w, screen_h;\n\tSDL_Window \t\t*screen; \n\tSDL_Renderer\t*sdlRenderer;\n\tSDL_Texture\t\t*sdlTexture;\n\tSDL_Rect \t\tsdlRect;\n\tSDL_Thread \t\t*video_tid;\n\tSDL_Event \t\tevent;\n \n\tstruct SwsContext *img_convert_ctx;\n \n\t//char filepath[]=\"bigbuckbunny_480x272.h265\";\n\tchar filepath[]=\"Titanic.ts\";\n \n\tav_register_all();\t// 注册所有的编解码器\n\tavformat_network_init();\n\tpFormatCtx = avformat_alloc_context(); \t// 分配内存并设置默认值\n \n    // 该函数用于打开多媒体数据并且获得一些相关的信息,AVInputFormat的read_header()完成了视音频流对应的AVStream的创建\n\tif(avformat_open_input(&pFormatCtx, filepath, NULL, NULL) != 0){\n\t\tprintf(\"Couldn't open input stream.\\n\");\n\t\treturn -1;\n\t}\n    // 该函数可以读取一部分视音频数据并且获得一些相关的信息,该函数主要用于给每个媒体流（音频/视频）的AVStream结构体赋值\n\tif(avformat_find_stream_info(pFormatCtx, NULL) < 0){\n\t\tprintf(\"Couldn't find stream information.\\n\");\n\t\treturn -1;\n\t}\n\tvideoindex = -1;\n\tfor(i=0; i<pFormatCtx->nb_streams; i++) \n\t\tif(pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO){\n\t\t\tvideoindex = i;\n\t\t\tbreak;\n\t\t}\n\tif(videoindex == -1){\n\t\tprintf(\"Didn't find a video stream.\\n\");\n\t\treturn -1;\n\t}\n\tpCodecCtx = pFormatCtx->streams[videoindex]->codec; // 找到视频流编解码器\n\tpCodec = avcodec_find_decoder(pCodecCtx->codec_id); \n\tif(pCodec == NULL){\n\t\tprintf(\"Codec not found.\\n\");\n\t\treturn -1;\n\t}\n    // 该函数用于初始化一个视音频编解码器的AVCodecContext\n    //（1）为各种结构体分配内存（通过各种av_malloc()实现）。\n\t//（2）将输入的AVDictionary形式的选项设置到AVCodecContext。\n\t//（3）其他一些零零碎碎的检查，比如说检查编解码器是否处于“实验”阶段。\n\t//（4）如果是编码器，检查输入参数是否符合编码器的要求\n\t//（5）调用AVCodec的init()初始化具体的解码器。\n\tif(avcodec_open2(pCodecCtx, pCodec,NULL) < 0){\n\t\tprintf(\"Could not open codec.\\n\");\n\t\treturn -1;\n\t}\n\tpFrame = av_frame_alloc();\n\tpFrameYUV = av_frame_alloc();\n \n\tout_buffer = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height, 1));\n\tav_image_fill_arrays(pFrameYUV->data, pFrameYUV->linesize,\n                         out_buffer,AV_PIX_FMT_YUV420P, pCodecCtx->width, \n                         pCodecCtx->height, 1);\n \n\t//Output Info-----------------------------\n\tprintf(\"---------------- File Information ---------------\\n\");\n\tav_dump_format(pFormatCtx, 0, filepath, 0);\n\tprintf(\"-------------------------------------------------\\n\");\n\t\n    // 初始化一个SwsContext\n    // 参数说明：\n    // 源图像的宽, 源图像的高, 源图像的像素格式, \n    // 目标图像的宽, 目标图像的高, 目标图像的像素格式, 设定图像拉伸使用的算法\n\timg_convert_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height, \n                                     pCodecCtx->pix_fmt, pCodecCtx->width, \n                                     pCodecCtx->height, AV_PIX_FMT_YUV420P, \n                                     SWS_BICUBIC, NULL, NULL, NULL); \n\t\n\tif(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {  \n\t\tprintf( \"Could not initialize SDL - %s\\n\", SDL_GetError()); \n\t\treturn -1;\n\t} \n\t//SDL 2.0 Support for multiple windows\n\tscreen_w = pCodecCtx->width;\n\tscreen_h = pCodecCtx->height;\n\tscreen = SDL_CreateWindow(\"Simplest ffmpeg player's Window\", \n                              SDL_WINDOWPOS_UNDEFINED,SDL_WINDOWPOS_UNDEFINED,\n                              screen_w, screen_h, SDL_WINDOW_OPENGL);\n \n\tif(!screen) {  \n\t\tprintf(\"SDL: could not create window - exiting:%s\\n\",SDL_GetError()); \n\t\treturn -1;\n\t}\n\tsdlRenderer = SDL_CreateRenderer(screen, -1, 0);  \n\t//IYUV: Y + U + V  (3 planes)\n\t//YV12: Y + V + U  (3 planes)\n\tsdlTexture = SDL_CreateTexture(sdlRenderer, SDL_PIXELFORMAT_IYUV, \n                                   SDL_TEXTUREACCESS_STREAMING, \n                                   pCodecCtx->width, pCodecCtx->height);  \n \n\tsdlRect.x = 0;\n\tsdlRect.y = 0;\n\tsdlRect.w = screen_w;\n\tsdlRect.h = screen_h;\n \n\tpacket = (AVPacket *)av_malloc(sizeof(AVPacket));\n \n\tvideo_tid = SDL_CreateThread(sfp_refresh_thread, NULL, NULL);\n\t//------------SDL End------------\n\t//Event Loop\t\n\tfor (;;) {\n\t\t//Wait\n\t\tSDL_WaitEvent(&event);\n\t\tif(event.type == SFM_REFRESH_EVENT){\n\t\t\twhile(1){\n                // av_read_frame()的作用是读取码流中的音频若干帧或者视频一帧\n                // 例如，解码视频的时候，每解码一个视频帧，需要先调用 av_read_frame()获得一帧视频的压缩数据，然后才能对该数据进行解码（例如H.264中一帧压缩数据通常对应一个NAL）\n                // 参数说明：输入的AVFormatContext, 输出的AVPacket\n\t\t\t\tif(av_read_frame(pFormatCtx, packet)<0)\n\t\t\t\t\tthread_exit = 1;\n \n\t\t\t\tif(packet->stream_index == videoindex)\n\t\t\t\t\tbreak;\n\t\t\t}\n            // 作用是解码一帧视频数据,输入一个压缩编码的结构体AVPacket，输出一个解码后的结构体AVFrame\n            // avcodec_decode_video2()主要做了以下几个方面的工作：\n            //（1）对输入的字段进行了一系列的检查工作：例如宽高是否正确，输入是否为视频等等。\n\t\t\t//（2）通过ret = avctx->codec->decode(avctx, picture, got_picture_ptr,&tmp)这句代码，调用了相应AVCodec的decode()函数，完成了解码操作。\n\t\t\t//（3）对得到的AVFrame的一些字段进行了赋值，例如宽高、像素格式等等。\n\t\t\tret = avcodec_decode_video2(pCodecCtx, pFrame, &got_picture, packet);\n\t\t\tif(ret < 0){\n\t\t\t\tprintf(\"Decode Error.\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tif(got_picture){\n                // sws_scale()是用于转换像素的函数\n                // 参数说明：\n                // 1）转换格式的上下文。也就是 sws_getContext 函数返回的结果。\n                // 2）输入图像的每个颜色通道的数据指针\n                // 3）输入图像的每个颜色通道的跨度\n                // 4）参数int srcSliceY, int srcSliceH,定义在输入图像上处理区域，srcSliceY是起始位置，srcSliceH是处理多少行。\n                // 如果srcSliceY=0，srcSliceH=height，表示一次性处理完整个图像。\t\t\t\t\t// 这种设置是为了多线程并行，例如可以创建两个线程，第一个线程处理 [0, h/2-1]行，第二个线程处理 [h/2, h-1]行。并行处理加快速度。\n                // 5）参数uint8_t *const dst[], const int dstStride[]定义输出图像信息（输出的每个颜色通道数据指针，每个颜色通道行字节数）\n\t\t\t\tsws_scale(img_convert_ctx, \n                          (const unsigned char* const*)pFrame->data, \n                          pFrame->linesize, 0, pCodecCtx->height, \n                          pFrameYUV->data, pFrameYUV->linesize);\n\t\t\t\t//SDL---------------------------\n\t\t\t\tSDL_UpdateTexture( sdlTexture, NULL, pFrameYUV->data[0], pFrameYUV->linesize[0] );  \n\t\t\t\tSDL_RenderClear( sdlRenderer );  \n\t\t\t\t//SDL_RenderCopy( sdlRenderer, sdlTexture, &sdlRect, &sdlRect );  \n\t\t\t\tSDL_RenderCopy( sdlRenderer, sdlTexture, NULL, NULL);  \n\t\t\t\tSDL_RenderPresent( sdlRenderer );  \n\t\t\t\t//SDL End-----------------------\n\t\t\t}\n\t\t\tav_free_packet(packet);\n\t\t}else if(event.type == SDL_KEYDOWN){\n\t\t\t//Pause\n\t\t\tif(event.key.keysym.sym == SDLK_SPACE)\n\t\t\t\tthread_pause =! thread_pause;\n\t\t}else if(event.type == SDL_QUIT){\n\t\t\tthread_exit = 1;\n\t\t}else if(event.type == SFM_BREAK_EVENT){\n\t\t\tbreak;\n\t\t}\n\t}\n \n\tsws_freeContext(img_convert_ctx);\n \n\tSDL_Quit();\n\t//--------------\n\tav_frame_free(&pFrameYUV);\n\tav_frame_free(&pFrame);\n\tavcodec_close(pCodecCtx);\n\tavformat_close_input(&pFormatCtx);\n \n\treturn 0;\n}\n```\n\n</details>\n\n### av_read_packet()\n\n通过 `av_read_packet()`，读取一个包，需要说明的是此函数必须是包含整数帧的，不存在半帧的情况。\n\n以 ts 流为例，是读取一个完整的 PES 包（一个完整 pes 包包含若干视频或音频 es 包），读取完毕后，通过 `av_parser_parse2()` 分析出视频一帧（或音频若干帧），返回，下次进入循环的时候，如果上次的数据没有完全取完，则 `st = s->cur_st` ; 不会是NULL，即再此进入 `av_parser_parse2()` 流程，而不是下面的 `av_read_packet（）` 流程.\n\n这样就保证了，如果读取一次包含了 N 帧视频数据（以视频为例），则调用 `av_read_frame（）` N 次都不会去读数据，而是返回第一次读取的数据，直到全部解析完毕。\n\n函数调用结构图：\n\n![av_read_packet](/images/imageFFmpeg/Thor/av_read_packet.png)\n\n## FFmpeg 源码分析\n\n### av_register_all()\n\n- `av_register_all()` - ffmpeg注册复用器，编码器\n\n该函数在所有基于ffmpeg的应用程序中几乎都是第一个被调用的。只有调用了该函数，才能使用复用器，编码器等。\n\n```c\n// 可见解复用器注册都是用\nREGISTER_DEMUXER  (X,x)\n// 例如：\nREGISTER_DEMUXER  (AAC, aac)\n\n// 可见复用器注册都是用\nREGISTER_MUXER    (X,x))\n// 例如：\nREGISTER_MUXER    (ADTS, adts)\n\n// 既有解复用器又有复用器的话，可以用\nREGISTER_MUXDEMUX (X,x));\n// 例如：\nREGISTER_MUXDEMUX (AC3, ac3);\n```\n\n看一下宏的定义，这里以**解复用器**为例：\n\n```c\n#define REGISTER_DEMUXER(X,x) { \\\n    extern AVInputFormat ff_##x##_demuxer; \\\n    if(CONFIG_##X##_DEMUXER) av_register_input_format(&ff_##x##_demuxer); }\n\n/*\n * 注意：define 里面的##可能不太常见，它的含义就是拼接两个字符串，比如\n * #define Conn(x,y) x##y\n * 那么\n * int n = Conn(123,456);  结果就是 n = 123456;\n */\n```\n\n我们以 `REGISTER_DEMUXER  (AAC, aac)` 为例，则它等效于\n\n```c\nextern AVInputFormat ff_aac_demuxer; \nif(CONFIG_AAC_DEMUXER) av_register_input_format(&ff_aac_demuxer);\n```\n\n从上面这段代码我们可以看出，真正注册的函数是 `av_register_input_format(&ff_aac_demuxer)`，那我就看看这个和函数的作用，查看一下 `av_register_input_format()` 的代码：\n\n```c\nvoid av_register_input_format(AVInputFormat *format)\n{\n    AVInputFormat **p;\n    p = &first_iformat;\n    while (*p != NULL) p = &(*p)->next;\n    *p = format;\n    format->next = NULL;\n}\n```\n\n这段代码是比较容易理解的，首先先提一点，first_iformat 是个什么东东呢？其实它是 Input Format 链表的头部地址，是一个全局静态变量，定义如下：\n\n```c\n/** head of registered input format linked list */\nstatic AVInputFormat *first_iformat = NULL;\n```\n\n由此我们可以分析出 `av_register_input_format()` 的含义，一句话概括就是：\n\n- 遍历链表并把当前的 Input Format 加到链表的尾部。\n\n至此 `REGISTER_DEMUXER  (X, x)` 分析完毕。\n\n同理，**复用器**道理是一样的，只是注册函数改为 `av_register_output_format()`；\n\n**既有解复用器又有复用器**的话，有一个宏定义：\n\n```c\n#define REGISTER_MUXDEMUX(X,x)  REGISTER_MUXER(X,x); REGISTER_DEMUXER(X,x)\n```\n\n可见是分别注册了复用器和解复用器。\n\n此外还有网络协议的注册，注册函数为 `ffurl_register_protocol()`，在此不再详述。\n\n<details><summary>下面贴出它的源代码（allformats.c）</summary>\n\n```c\n#include \"avformat.h\"\n#include \"rtp.h\"\n#include \"rdt.h\"\n#include \"url.h\"\n// 定义的宏？宏的速度会快一点？注册 AVOutputFormat\n// define中，#用来把参数转换成字符串，##则用来连接前后两个参数，把它们变成一个字符串。\n// 感觉有点像JAva中的EL，可以随意拼接字符串\n#define REGISTER_MUXER(X,x) { \\\n    extern AVOutputFormat ff_##x##_muxer; \\\n    if(CONFIG_##X##_MUXER) av_register_output_format(&ff_##x##_muxer); }\n// 定义的宏？宏的速度会快一点？注册AVInputFormat\n#define REGISTER_DEMUXER(X,x) { \\\n    extern AVInputFormat ff_##x##_demuxer; \\\n    if(CONFIG_##X##_DEMUXER) av_register_input_format(&ff_##x##_demuxer); }\n// 注册函数 av_register_input_format\n \n// 定义的宏？宏的速度会快一点？两个一起注册！\n#define REGISTER_MUXDEMUX(X,x)  REGISTER_MUXER(X,x); REGISTER_DEMUXER(X,x)\n// 定义的宏？宏的速度会快一点？注册URLProtocol\n// extern URLProtocol ff_##x##_protocol;\n// 在 librtmp 中，对应的就是 ff_rtmp_protocol\n// 这样就把 librtmp 整合起来了\n// 由此可见 URLProtocol 的名字是固定的\n#define REGISTER_PROTOCOL(X,x) { \\\n    extern URLProtocol ff_##x##_protocol; \\\n    if(CONFIG_##X##_PROTOCOL) ffurl_register_protocol(&ff_##x##_protocol, sizeof(ff_##x##_protocol)); }\n// 注册函数 ffurl_register_protocol\nvoid av_register_all(void)\n{\n    static int initialized;\n \n    if (initialized)\n        return;\n    initialized = 1;\n    // 注册所有的 codec\n    avcodec_register_all();\n    // 注册所有的 MUXER（复用器和解复用器）\n    /* (de)muxers */\n    REGISTER_MUXER    (A64, a64);\n    REGISTER_DEMUXER  (AAC, aac);\n    REGISTER_MUXDEMUX (AC3, ac3);\n    REGISTER_DEMUXER  (ACT, act);\n    REGISTER_DEMUXER  (ADF, adf);\n    REGISTER_MUXER    (ADTS, adts);\n    REGISTER_MUXDEMUX (ADX, adx);\n    REGISTER_DEMUXER  (AEA, aea);\n    REGISTER_MUXDEMUX (AIFF, aiff);\n    REGISTER_MUXDEMUX (AMR, amr);\n    REGISTER_DEMUXER  (ANM, anm);\n    REGISTER_DEMUXER  (APC, apc);\n    REGISTER_DEMUXER  (APE, ape);\n    REGISTER_DEMUXER  (APPLEHTTP, applehttp);\n    REGISTER_MUXDEMUX (ASF, asf);\n    REGISTER_MUXDEMUX (ASS, ass);\n    REGISTER_MUXER    (ASF_STREAM, asf_stream);\n    REGISTER_MUXDEMUX (AU, au);\n    REGISTER_MUXDEMUX (AVI, avi);\n    REGISTER_DEMUXER  (AVISYNTH, avisynth);\n    REGISTER_MUXER    (AVM2, avm2);\n    REGISTER_DEMUXER  (AVS, avs);\n    REGISTER_DEMUXER  (BETHSOFTVID, bethsoftvid);\n    REGISTER_DEMUXER  (BFI, bfi);\n    REGISTER_DEMUXER  (BINTEXT, bintext);\n    REGISTER_DEMUXER  (BINK, bink);\n    REGISTER_MUXDEMUX (BIT, bit);\n    REGISTER_DEMUXER  (BMV, bmv);\n    REGISTER_DEMUXER  (C93, c93);\n    REGISTER_MUXDEMUX (CAF, caf);\n    REGISTER_MUXDEMUX (CAVSVIDEO, cavsvideo);\n    REGISTER_DEMUXER  (CDG, cdg);\n    REGISTER_MUXER    (CRC, crc);\n    REGISTER_MUXDEMUX (DAUD, daud);\n    REGISTER_DEMUXER  (DFA, dfa);\n    REGISTER_MUXDEMUX (DIRAC, dirac);\n    REGISTER_MUXDEMUX (DNXHD, dnxhd);\n    REGISTER_DEMUXER  (DSICIN, dsicin);\n    REGISTER_MUXDEMUX (DTS, dts);\n    REGISTER_MUXDEMUX (DV, dv);\n    REGISTER_DEMUXER  (DXA, dxa);\n    REGISTER_DEMUXER  (EA, ea);\n    REGISTER_DEMUXER  (EA_CDATA, ea_cdata);\n    REGISTER_MUXDEMUX (EAC3, eac3);\n    REGISTER_MUXDEMUX (FFM, ffm);\n    REGISTER_MUXDEMUX (FFMETADATA, ffmetadata);\n    REGISTER_MUXDEMUX (FILMSTRIP, filmstrip);\n    REGISTER_MUXDEMUX (FLAC, flac);\n    REGISTER_DEMUXER  (FLIC, flic);\n    REGISTER_MUXDEMUX (FLV, flv);\n    REGISTER_DEMUXER  (FOURXM, fourxm);\n    REGISTER_MUXER    (FRAMECRC, framecrc);\n    REGISTER_MUXER    (FRAMEMD5, framemd5);\n    REGISTER_MUXDEMUX (G722, g722);\n    REGISTER_MUXDEMUX (G723_1, g723_1);\n    REGISTER_DEMUXER  (G729, g729);\n    REGISTER_MUXER    (GIF, gif);\n    REGISTER_DEMUXER  (GSM, gsm);\n    REGISTER_MUXDEMUX (GXF, gxf);\n    REGISTER_MUXDEMUX (H261, h261);\n    REGISTER_MUXDEMUX (H263, h263);\n    REGISTER_MUXDEMUX (H264, h264);\n    REGISTER_DEMUXER  (ICO, ico);\n    REGISTER_DEMUXER  (IDCIN, idcin);\n    REGISTER_DEMUXER  (IDF, idf);\n    REGISTER_DEMUXER  (IFF, iff);\n    REGISTER_MUXDEMUX (IMAGE2, image2);\n    REGISTER_MUXDEMUX (IMAGE2PIPE, image2pipe);\n    REGISTER_DEMUXER  (INGENIENT, ingenient);\n    REGISTER_DEMUXER  (IPMOVIE, ipmovie);\n    REGISTER_MUXER    (IPOD, ipod);\n    REGISTER_MUXER    (ISMV, ismv);\n    REGISTER_DEMUXER  (ISS, iss);\n    REGISTER_DEMUXER  (IV8, iv8);\n    REGISTER_MUXDEMUX (IVF, ivf);\n    REGISTER_DEMUXER  (JV, jv);\n    REGISTER_MUXDEMUX (LATM, latm);\n    REGISTER_DEMUXER  (LMLM4, lmlm4);\n    REGISTER_DEMUXER  (LOAS, loas);\n    REGISTER_DEMUXER  (LXF, lxf);\n    REGISTER_MUXDEMUX (M4V, m4v);\n    REGISTER_MUXER    (MD5, md5);\n    REGISTER_MUXDEMUX (MATROSKA, matroska);\n    REGISTER_MUXER    (MATROSKA_AUDIO, matroska_audio);\n    REGISTER_MUXDEMUX (MICRODVD, microdvd);\n    REGISTER_MUXDEMUX (MJPEG, mjpeg);\n    REGISTER_MUXDEMUX (MLP, mlp);\n    REGISTER_DEMUXER  (MM, mm);\n    REGISTER_MUXDEMUX (MMF, mmf);\n    REGISTER_MUXDEMUX (MOV, mov);\n    REGISTER_MUXER    (MP2, mp2);\n    REGISTER_MUXDEMUX (MP3, mp3);\n    REGISTER_MUXER    (MP4, mp4);\n    REGISTER_DEMUXER  (MPC, mpc);\n    REGISTER_DEMUXER  (MPC8, mpc8);\n    REGISTER_MUXER    (MPEG1SYSTEM, mpeg1system);\n    REGISTER_MUXER    (MPEG1VCD, mpeg1vcd);\n    REGISTER_MUXER    (MPEG1VIDEO, mpeg1video);\n    REGISTER_MUXER    (MPEG2DVD, mpeg2dvd);\n    REGISTER_MUXER    (MPEG2SVCD, mpeg2svcd);\n    REGISTER_MUXER    (MPEG2VIDEO, mpeg2video);\n    REGISTER_MUXER    (MPEG2VOB, mpeg2vob);\n    REGISTER_DEMUXER  (MPEGPS, mpegps);\n    REGISTER_MUXDEMUX (MPEGTS, mpegts);\n    REGISTER_DEMUXER  (MPEGTSRAW, mpegtsraw);\n    REGISTER_DEMUXER  (MPEGVIDEO, mpegvideo);\n    REGISTER_MUXER    (MPJPEG, mpjpeg);\n    REGISTER_DEMUXER  (MSNWC_TCP, msnwc_tcp);\n    REGISTER_DEMUXER  (MTV, mtv);\n    REGISTER_DEMUXER  (MVI, mvi);\n    REGISTER_MUXDEMUX (MXF, mxf);\n    REGISTER_MUXER    (MXF_D10, mxf_d10);\n    REGISTER_DEMUXER  (MXG, mxg);\n    REGISTER_DEMUXER  (NC, nc);\n    REGISTER_DEMUXER  (NSV, nsv);\n    REGISTER_MUXER    (NULL, null);\n    REGISTER_MUXDEMUX (NUT, nut);\n    REGISTER_DEMUXER  (NUV, nuv);\n    REGISTER_MUXDEMUX (OGG, ogg);\n    REGISTER_MUXDEMUX (OMA, oma);\n    REGISTER_MUXDEMUX (PCM_ALAW,  pcm_alaw);\n    REGISTER_MUXDEMUX (PCM_MULAW, pcm_mulaw);\n    REGISTER_MUXDEMUX (PCM_F64BE, pcm_f64be);\n    REGISTER_MUXDEMUX (PCM_F64LE, pcm_f64le);\n    REGISTER_MUXDEMUX (PCM_F32BE, pcm_f32be);\n    REGISTER_MUXDEMUX (PCM_F32LE, pcm_f32le);\n    REGISTER_MUXDEMUX (PCM_S32BE, pcm_s32be);\n    REGISTER_MUXDEMUX (PCM_S32LE, pcm_s32le);\n    REGISTER_MUXDEMUX (PCM_S24BE, pcm_s24be);\n    REGISTER_MUXDEMUX (PCM_S24LE, pcm_s24le);\n    REGISTER_MUXDEMUX (PCM_S16BE, pcm_s16be);\n    REGISTER_MUXDEMUX (PCM_S16LE, pcm_s16le);\n    REGISTER_MUXDEMUX (PCM_S8,    pcm_s8);\n    REGISTER_MUXDEMUX (PCM_U32BE, pcm_u32be);\n    REGISTER_MUXDEMUX (PCM_U32LE, pcm_u32le);\n    REGISTER_MUXDEMUX (PCM_U24BE, pcm_u24be);\n    REGISTER_MUXDEMUX (PCM_U24LE, pcm_u24le);\n    REGISTER_MUXDEMUX (PCM_U16BE, pcm_u16be);\n    REGISTER_MUXDEMUX (PCM_U16LE, pcm_u16le);\n    REGISTER_MUXDEMUX (PCM_U8,    pcm_u8);\n    REGISTER_DEMUXER  (PMP, pmp);\n    REGISTER_MUXER    (PSP, psp);\n    REGISTER_DEMUXER  (PVA, pva);\n    REGISTER_DEMUXER  (QCP, qcp);\n    REGISTER_DEMUXER  (R3D, r3d);\n    REGISTER_MUXDEMUX (RAWVIDEO, rawvideo);\n    REGISTER_DEMUXER  (RL2, rl2);\n    REGISTER_MUXDEMUX (RM, rm);\n    REGISTER_MUXDEMUX (ROQ, roq);\n    REGISTER_DEMUXER  (RPL, rpl);\n    REGISTER_MUXDEMUX (RSO, rso);\n    REGISTER_MUXDEMUX (RTP, rtp);\n    REGISTER_MUXDEMUX (RTSP, rtsp);\n    REGISTER_MUXDEMUX (SAP, sap);\n    REGISTER_DEMUXER  (SBG, sbg);\n    REGISTER_DEMUXER  (SDP, sdp);\n#if CONFIG_RTPDEC\n    av_register_rtp_dynamic_payload_handlers();\n    av_register_rdt_dynamic_payload_handlers();\n#endif\n    REGISTER_DEMUXER  (SEGAFILM, segafilm);\n    REGISTER_MUXER    (SEGMENT, segment);\n    REGISTER_DEMUXER  (SHORTEN, shorten);\n    REGISTER_DEMUXER  (SIFF, siff);\n    REGISTER_DEMUXER  (SMACKER, smacker);\n    REGISTER_MUXDEMUX (SMJPEG, smjpeg);\n    REGISTER_DEMUXER  (SOL, sol);\n    REGISTER_MUXDEMUX (SOX, sox);\n    REGISTER_MUXDEMUX (SPDIF, spdif);\n    REGISTER_MUXDEMUX (SRT, srt);\n    REGISTER_DEMUXER  (STR, str);\n    REGISTER_MUXDEMUX (SWF, swf);\n    REGISTER_MUXER    (TG2, tg2);\n    REGISTER_MUXER    (TGP, tgp);\n    REGISTER_DEMUXER  (THP, thp);\n    REGISTER_DEMUXER  (TIERTEXSEQ, tiertexseq);\n    REGISTER_MUXER    (MKVTIMESTAMP_V2, mkvtimestamp_v2);\n    REGISTER_DEMUXER  (TMV, tmv);\n    REGISTER_MUXDEMUX (TRUEHD, truehd);\n    REGISTER_DEMUXER  (TTA, tta);\n    REGISTER_DEMUXER  (TXD, txd);\n    REGISTER_DEMUXER  (TTY, tty);\n    REGISTER_DEMUXER  (VC1, vc1);\n    REGISTER_MUXDEMUX (VC1T, vc1t);\n    REGISTER_DEMUXER  (VMD, vmd);\n    REGISTER_MUXDEMUX (VOC, voc);\n    REGISTER_DEMUXER  (VQF, vqf);\n    REGISTER_DEMUXER  (W64, w64);\n    REGISTER_MUXDEMUX (WAV, wav);\n    REGISTER_DEMUXER  (WC3, wc3);\n    REGISTER_MUXER    (WEBM, webm);\n    REGISTER_DEMUXER  (WSAUD, wsaud);\n    REGISTER_DEMUXER  (WSVQA, wsvqa);\n    REGISTER_MUXDEMUX (WTV, wtv);\n    REGISTER_DEMUXER  (WV, wv);\n    REGISTER_DEMUXER  (XA, xa);\n    REGISTER_DEMUXER  (XBIN, xbin);\n    REGISTER_DEMUXER  (XMV, xmv);\n    REGISTER_DEMUXER  (XWMA, xwma);\n    REGISTER_DEMUXER  (YOP, yop);\n    REGISTER_MUXDEMUX (YUV4MPEGPIPE, yuv4mpegpipe);\n \n    /* external libraries */\n#if CONFIG_LIBMODPLUG\n    REGISTER_DEMUXER  (LIBMODPLUG, libmodplug);\n#endif\n    REGISTER_MUXDEMUX (LIBNUT, libnut);\n    // 注册所有的 Protocol（位于 DEMUXER 之前（我的理解~~））\n    // 文件也是一种Protocol\n    /* protocols */\n    REGISTER_PROTOCOL (APPLEHTTP, applehttp);\n    REGISTER_PROTOCOL (CACHE, cache);\n    REGISTER_PROTOCOL (CONCAT, concat);\n    REGISTER_PROTOCOL (CRYPTO, crypto);\n    REGISTER_PROTOCOL (FILE, file);\n    REGISTER_PROTOCOL (GOPHER, gopher);\n    REGISTER_PROTOCOL (HTTP, http);\n    REGISTER_PROTOCOL (HTTPPROXY, httpproxy);\n    REGISTER_PROTOCOL (HTTPS, https);\n    REGISTER_PROTOCOL (MMSH, mmsh);\n    REGISTER_PROTOCOL (MMST, mmst);\n    REGISTER_PROTOCOL (MD5,  md5);\n    REGISTER_PROTOCOL (PIPE, pipe);\n    REGISTER_PROTOCOL (RTMP, rtmp);\n// 如果包含了 LibRTMP\n#if CONFIG_LIBRTMP\n    REGISTER_PROTOCOL (RTMP, rtmpt);\n    REGISTER_PROTOCOL (RTMP, rtmpe);\n    REGISTER_PROTOCOL (RTMP, rtmpte);\n    REGISTER_PROTOCOL (RTMP, rtmps);\n#endif\n    REGISTER_PROTOCOL (RTP, rtp);\n    REGISTER_PROTOCOL (TCP, tcp);\n    REGISTER_PROTOCOL (TLS, tls);\n    REGISTER_PROTOCOL (UDP, udp);\n}\n```\n\n</details>\n\n整个代码没太多可说的，首先确定是不是已经初始化过了（initialized），如果没有，就调用 `avcodec_register_all()` 注册编解码器（这个先不分析），然后就是注册，注册，注册...直到完成所有注册。\n\nPS：曾经研究过一阵子 RTMP 协议，以及对应的开源工程 librtmp。在这里发现有一点值得注意，ffmpeg自带了 RTMP 协议的支持，只有使用 `rtmpt://, rtmpe://, rtmpte://` 等的时候才会使用 librtmp 库。\n\n函数调用关系图如下图所示。`av_register_all()` 调用了 `avcodec_register_all()` 。 `avcodec_register_all()` 注册了和编解码器有关的组件：硬件加速器，解码器，编码器，Parser，Bitstream Filter。`av_register_all()` 除了调用 `avcodec_register_all()` 之外，还注册了复用器，解复用器，协议处理器。\n\n![av_register_all 函数调用关系图](/images/imageFFmpeg/Thor/av_register_all.png)\n\n下面附上复用器，解复用器，协议处理器的代码。\n\n注册复用器的函数是 `av_register_output_format()`。\n\n```c\nvoid av_register_output_format(AVOutputFormat *format)\n{\n    AVOutputFormat **p;\n    p = &first_oformat;\n    while (*p != NULL) p = &(*p)->next;\n    *p = format;\n    format->next = NULL;\n}\n```\n\n注册解复用器的函数是 `av_register_input_format()`。\n\n```c\nvoid av_register_input_format(AVInputFormat *format)\n{\n    AVInputFormat **p;\n    p = &first_iformat;\n    while (*p != NULL) p = &(*p)->next;\n    *p = format;\n    format->next = NULL;\n}\n```\n\n注册协议处理器的函数是 `ffurl_register_protocol()` 。\n\n```c\nint ffurl_register_protocol(URLProtocol *protocol)\n{\n    URLProtocol **p;\n    p = &first_protocol;\n    while (*p)\n        p = &(*p)->next;\n    *p = protocol;\n    protocol->next = NULL;\n    return 0;\n}\n```\n\n### avcodec_register_all()\n\nffmpeg注册编解码器等的函数 `avcodec_register_all()`（注意不是 `av_register_all()`，那是注册所有东西的）。该函数在所有基于ffmpeg的应用程序中几乎都是第一个被调用的。只有调用了该函数，才能使用编解码器等。\n\n其实注册编解码器和注册复用器解复用器道理是差不多的，重复的内容不再多说。\n\n```c\n// 编码器的注册是：\nREGISTER_ENCODER (X,x);\t\tREGISTER_ENCODER (LJPEG, ljpeg);\n\n// 解码器的注册是：\nREGISTER_DECODER (X,x);\t\tREGISTER_DECODER (H264, h264);\n\n// 既包含编码器有包含解码器的注册是：\nREGISTER_ENCDEC  (X,x);\t\tREGISTER_ENCDEC  (BMP, bmp);\n\n// 此外还有几种注册：\n// Parser：\nREGISTER_PARSER  (X,x);\t\tREGISTER_PARSER  (H264, h264);\n\n// BSF（bitstream filters，比特流滤镜，有一个常用：h264_mp4toannexb）：\nREGISTER_BSF     (X,x);\t\tREGISTER_BSF (H264_MP4TOANNEXB, h264_mp4toannexb);\n\n// HWACCEL（hardware accelerators，硬件加速器）：\nREGISTER_HWACCEL (X,x);\t\tREGISTER_HWACCEL (H264_DXVA2, h264_dxva2);\n```\n\n我们来看一下宏的定义，这里以编解码器为例：\n\n```c\n#define REGISTER_ENCODER(X,x) { \\\n          extern AVCodec ff_##x##_encoder; \\\n          if(CONFIG_##X##_ENCODER)  avcodec_register(&ff_##x##_encoder); }\n#define REGISTER_DECODER(X,x) { \\\n          extern AVCodec ff_##x##_decoder; \\\n          if(CONFIG_##X##_DECODER)  avcodec_register(&ff_##x##_decoder); }\n#define REGISTER_ENCDEC(X,x)  REGISTER_ENCODER(X,x); REGISTER_DECODER(X,x)\n```\n\n在这里，我发现其实编码器和解码器用的注册函数都是一样的：`avcodec_register()`\n\n以 `REGISTER_DECODER (H264, h264)` 为例，就是等效于\n\n```c\nextern AVCodec ff_h264_decoder; \nif(CONFIG_H264_DECODER)  avcodec_register(&ff_h264_decoder);\n```\n\n下面看一下 `avcodec_register()` 的源代码：\n\n```c\n//注册所有的AVCodec\nvoid avcodec_register(AVCodec *codec)\n{\n    AVCodec **p;\n    //初始化\n    avcodec_init();\n    //从第一个开始\n    p = &first_avcodec;\n    while (*p != NULL) p = &(*p)->next;\n    *p = codec;\n    codec->next = NULL;\n \n    if (codec->init_static_data)\n        codec->init_static_data(codec);\n}\n```\n\n这段代码是比较容易理解的。首先先提一点，first_avcdec 是就是 AVCodec 链表的头部地址，是一个全局静态变量，定义如下：\n\n```c\n/* encoder management */\nstatic AVCodec *first_avcodec = NULL;\n```\n\n由此我们可以分析出avcodec_register()的含义，一句话概括就是：遍历链表并把当前的AVCodec加到链表的尾部。\n同理，**Parser，BSF（bitstream filters，比特流滤镜），HWACCEL（hardware accelerators，硬件加速器）**的注册方式都是类似的。不再详述。\n\n<details><summary>下面贴出它的原代码：</summary>\n\n```c\n#include \"avcodec.h\"\n// 硬件加速\n#define REGISTER_HWACCEL(X,x) { \\\n          extern AVHWAccel ff_##x##_hwaccel; \\\n          if(CONFIG_##X##_HWACCEL) av_register_hwaccel(&ff_##x##_hwaccel); }\n \n#define REGISTER_ENCODER(X,x) { \\\n          extern AVCodec ff_##x##_encoder; \\\n          if(CONFIG_##X##_ENCODER)  avcodec_register(&ff_##x##_encoder); }\n// 定义的宏？宏的速度会快一点？注册AVCodec\n// extern AVCodec ff_##x##_decoder;\n// 注意：extern 表明全局唯一\n// 在h264中，对应的就是 ff_h264_decoder\n// 由此可见 AVCodecParser 的名字是固定的\n#define REGISTER_DECODER(X,x) { \\\n          extern AVCodec ff_##x##_decoder; \\\n          if(CONFIG_##X##_DECODER)  avcodec_register(&ff_##x##_decoder); }\n#define REGISTER_ENCDEC(X,x)  REGISTER_ENCODER(X,x); REGISTER_DECODER(X,x)\n// 定义的宏？宏的速度会快一点？注册AVCodecParser\n// extern AVCodecParser ff_##x##_parser;\n// 在h264中，对应的就是ff_h264_parser\n// 由此可见AVCodecParser的名字是固定的\n#define REGISTER_PARSER(X,x) { \\\n          extern AVCodecParser ff_##x##_parser; \\\n          if(CONFIG_##X##_PARSER)  av_register_codec_parser(&ff_##x##_parser); }\n#define REGISTER_BSF(X,x) { \\\n          extern AVBitStreamFilter ff_##x##_bsf; \\\n          if(CONFIG_##X##_BSF)     av_register_bitstream_filter(&ff_##x##_bsf); }\n \nvoid avcodec_register_all(void)\n{\n    static int initialized;\n \n    if (initialized)\n        return;\n    initialized = 1;\n \n    /* hardware accelerators */\n    REGISTER_HWACCEL (H263_VAAPI, h263_vaapi);\n    REGISTER_HWACCEL (H264_DXVA2, h264_dxva2);\n    REGISTER_HWACCEL (H264_VAAPI, h264_vaapi);\n    REGISTER_HWACCEL (H264_VDA, h264_vda);\n    REGISTER_HWACCEL (MPEG1_VDPAU, mpeg1_vdpau);\n    REGISTER_HWACCEL (MPEG2_DXVA2, mpeg2_dxva2);\n    REGISTER_HWACCEL (MPEG2_VAAPI, mpeg2_vaapi);\n    REGISTER_HWACCEL (MPEG2_VDPAU, mpeg2_vdpau);\n    REGISTER_HWACCEL (MPEG4_VAAPI, mpeg4_vaapi);\n    REGISTER_HWACCEL (VC1_DXVA2, vc1_dxva2);\n    REGISTER_HWACCEL (VC1_VAAPI, vc1_vaapi);\n    REGISTER_HWACCEL (WMV3_DXVA2, wmv3_dxva2);\n    REGISTER_HWACCEL (WMV3_VAAPI, wmv3_vaapi);\n \n    /* video codecs */\n    REGISTER_ENCODER (A64MULTI, a64multi);\n    REGISTER_ENCODER (A64MULTI5, a64multi5);\n    REGISTER_DECODER (AASC, aasc);\n    REGISTER_ENCDEC  (AMV, amv);\n    REGISTER_DECODER (ANM, anm);\n    REGISTER_DECODER (ANSI, ansi);\n    REGISTER_ENCDEC  (ASV1, asv1);\n    REGISTER_ENCDEC  (ASV2, asv2);\n    REGISTER_DECODER (AURA, aura);\n    REGISTER_DECODER (AURA2, aura2);\n    REGISTER_ENCDEC  (AVRP, avrp);\n    REGISTER_DECODER (AVS, avs);\n    REGISTER_DECODER (BETHSOFTVID, bethsoftvid);\n    REGISTER_DECODER (BFI, bfi);\n    REGISTER_DECODER (BINK, bink);\n    REGISTER_ENCDEC  (BMP, bmp);\n    REGISTER_DECODER (BMV_VIDEO, bmv_video);\n    REGISTER_DECODER (C93, c93);\n    REGISTER_DECODER (CAVS, cavs);\n    REGISTER_DECODER (CDGRAPHICS, cdgraphics);\n    REGISTER_DECODER (CINEPAK, cinepak);\n    REGISTER_ENCDEC  (CLJR, cljr);\n    REGISTER_DECODER (CSCD, cscd);\n    REGISTER_DECODER (CYUV, cyuv);\n    REGISTER_DECODER (DFA, dfa);\n    REGISTER_DECODER (DIRAC, dirac);\n    REGISTER_ENCDEC  (DNXHD, dnxhd);\n    REGISTER_ENCDEC  (DPX, dpx);\n    REGISTER_DECODER (DSICINVIDEO, dsicinvideo);\n    REGISTER_ENCDEC  (DVVIDEO, dvvideo);\n    REGISTER_DECODER (DXA, dxa);\n    REGISTER_DECODER (DXTORY, dxtory);\n    REGISTER_DECODER (EACMV, eacmv);\n    REGISTER_DECODER (EAMAD, eamad);\n    REGISTER_DECODER (EATGQ, eatgq);\n    REGISTER_DECODER (EATGV, eatgv);\n    REGISTER_DECODER (EATQI, eatqi);\n    REGISTER_DECODER (EIGHTBPS, eightbps);\n    REGISTER_DECODER (EIGHTSVX_EXP, eightsvx_exp);\n    REGISTER_DECODER (EIGHTSVX_FIB, eightsvx_fib);\n    REGISTER_DECODER (ESCAPE124, escape124);\n    REGISTER_DECODER (ESCAPE130, escape130);\n    REGISTER_ENCDEC  (FFV1, ffv1);\n    REGISTER_ENCDEC  (FFVHUFF, ffvhuff);\n    REGISTER_ENCDEC  (FLASHSV, flashsv);\n    REGISTER_ENCDEC  (FLASHSV2, flashsv2);\n    REGISTER_DECODER (FLIC, flic);\n    REGISTER_ENCDEC  (FLV, flv);\n    REGISTER_DECODER (FOURXM, fourxm);\n    REGISTER_DECODER (FRAPS, fraps);\n    REGISTER_DECODER (FRWU, frwu);\n    REGISTER_ENCDEC  (GIF, gif);\n    REGISTER_ENCDEC  (H261, h261);\n    REGISTER_ENCDEC  (H263, h263);\n    REGISTER_DECODER (H263I, h263i);\n    REGISTER_ENCODER (H263P, h263p);\n    REGISTER_DECODER (H264, h264);\n    REGISTER_DECODER (H264_CRYSTALHD, h264_crystalhd);\n    REGISTER_DECODER (H264_VDPAU, h264_vdpau);\n    REGISTER_ENCDEC  (HUFFYUV, huffyuv);\n    REGISTER_DECODER (IDCIN, idcin);\n    REGISTER_DECODER (IFF_BYTERUN1, iff_byterun1);\n    REGISTER_DECODER (IFF_ILBM, iff_ilbm);\n    REGISTER_DECODER (INDEO2, indeo2);\n    REGISTER_DECODER (INDEO3, indeo3);\n    REGISTER_DECODER (INDEO4, indeo4);\n    REGISTER_DECODER (INDEO5, indeo5);\n    REGISTER_DECODER (INTERPLAY_VIDEO, interplay_video);\n    REGISTER_ENCDEC  (JPEG2000, jpeg2000);\n    REGISTER_ENCDEC  (JPEGLS, jpegls);\n    REGISTER_DECODER (JV, jv);\n    REGISTER_DECODER (KGV1, kgv1);\n    REGISTER_DECODER (KMVC, kmvc);\n    REGISTER_DECODER (LAGARITH, lagarith);\n    REGISTER_ENCODER (LJPEG, ljpeg);\n    REGISTER_DECODER (LOCO, loco);\n    REGISTER_DECODER (MDEC, mdec);\n    REGISTER_DECODER (MIMIC, mimic);\n    REGISTER_ENCDEC  (MJPEG, mjpeg);\n    REGISTER_DECODER (MJPEGB, mjpegb);\n    REGISTER_DECODER (MMVIDEO, mmvideo);\n    REGISTER_DECODER (MOTIONPIXELS, motionpixels);\n    REGISTER_DECODER (MPEG_XVMC, mpeg_xvmc);\n    REGISTER_ENCDEC  (MPEG1VIDEO, mpeg1video);\n    REGISTER_ENCDEC  (MPEG2VIDEO, mpeg2video);\n    REGISTER_ENCDEC  (MPEG4, mpeg4);\n    REGISTER_DECODER (MPEG4_CRYSTALHD, mpeg4_crystalhd);\n    REGISTER_DECODER (MPEG4_VDPAU, mpeg4_vdpau);\n    REGISTER_DECODER (MPEGVIDEO, mpegvideo);\n    REGISTER_DECODER (MPEG_VDPAU, mpeg_vdpau);\n    REGISTER_DECODER (MPEG1_VDPAU, mpeg1_vdpau);\n    REGISTER_DECODER (MPEG2_CRYSTALHD, mpeg2_crystalhd);\n    REGISTER_DECODER (MSMPEG4_CRYSTALHD, msmpeg4_crystalhd);\n    REGISTER_DECODER (MSMPEG4V1, msmpeg4v1);\n    REGISTER_ENCDEC  (MSMPEG4V2, msmpeg4v2);\n    REGISTER_ENCDEC  (MSMPEG4V3, msmpeg4v3);\n    REGISTER_DECODER (MSRLE, msrle);\n    REGISTER_ENCDEC  (MSVIDEO1, msvideo1);\n    REGISTER_DECODER (MSZH, mszh);\n    REGISTER_DECODER (MXPEG, mxpeg);\n    REGISTER_DECODER (NUV, nuv);\n    REGISTER_ENCDEC  (PAM, pam);\n    REGISTER_ENCDEC  (PBM, pbm);\n    REGISTER_ENCDEC  (PCX, pcx);\n    REGISTER_ENCDEC  (PGM, pgm);\n    REGISTER_ENCDEC  (PGMYUV, pgmyuv);\n    REGISTER_DECODER (PICTOR, pictor);\n    REGISTER_ENCDEC  (PNG, png);\n    REGISTER_ENCDEC  (PPM, ppm);\n    REGISTER_ENCDEC  (PRORES, prores);\n    REGISTER_DECODER (PRORES_LGPL, prores_lgpl);\n    REGISTER_DECODER (PTX, ptx);\n    REGISTER_DECODER (QDRAW, qdraw);\n    REGISTER_DECODER (QPEG, qpeg);\n    REGISTER_ENCDEC  (QTRLE, qtrle);\n    REGISTER_ENCDEC  (R10K,  r10k);\n    REGISTER_ENCDEC  (R210,  r210);\n    REGISTER_ENCDEC  (RAWVIDEO, rawvideo);\n    REGISTER_DECODER (RL2, rl2);\n    REGISTER_ENCDEC  (ROQ, roq);\n    REGISTER_DECODER (RPZA, rpza);\n    REGISTER_ENCDEC  (RV10, rv10);\n    REGISTER_ENCDEC  (RV20, rv20);\n    REGISTER_DECODER (RV30, rv30);\n    REGISTER_DECODER (RV40, rv40);\n    REGISTER_DECODER (S302M, s302m);\n    REGISTER_ENCDEC  (SGI, sgi);\n    REGISTER_DECODER (SMACKER, smacker);\n    REGISTER_DECODER (SMC, smc);\n    REGISTER_ENCDEC  (SNOW, snow);\n    REGISTER_DECODER (SP5X, sp5x);\n    REGISTER_DECODER (SUNRAST, sunrast);\n    REGISTER_ENCDEC  (SVQ1, svq1);\n    REGISTER_DECODER (SVQ3, svq3);\n    REGISTER_ENCDEC  (TARGA, targa);\n    REGISTER_DECODER (THEORA, theora);\n    REGISTER_DECODER (THP, thp);\n    REGISTER_DECODER (TIERTEXSEQVIDEO, tiertexseqvideo);\n    REGISTER_ENCDEC  (TIFF, tiff);\n    REGISTER_DECODER (TMV, tmv);\n    REGISTER_DECODER (TRUEMOTION1, truemotion1);\n    REGISTER_DECODER (TRUEMOTION2, truemotion2);\n    REGISTER_DECODER (TSCC, tscc);\n    REGISTER_DECODER (TXD, txd);\n    REGISTER_DECODER (ULTI, ulti);\n    REGISTER_DECODER (UTVIDEO, utvideo);\n    REGISTER_ENCDEC  (V210,  v210);\n    REGISTER_DECODER (V210X, v210x);\n    REGISTER_ENCDEC  (V308, v308);\n    REGISTER_ENCDEC  (V410, v410);\n    REGISTER_DECODER (VB, vb);\n    REGISTER_DECODER (VBLE, vble);\n    REGISTER_DECODER (VC1, vc1);\n    REGISTER_DECODER (VC1_CRYSTALHD, vc1_crystalhd);\n    REGISTER_DECODER (VC1_VDPAU, vc1_vdpau);\n    REGISTER_DECODER (VC1IMAGE, vc1image);\n    REGISTER_DECODER (VCR1, vcr1);\n    REGISTER_DECODER (VMDVIDEO, vmdvideo);\n    REGISTER_DECODER (VMNC, vmnc);\n    REGISTER_DECODER (VP3, vp3);\n    REGISTER_DECODER (VP5, vp5);\n    REGISTER_DECODER (VP6, vp6);\n    REGISTER_DECODER (VP6A, vp6a);\n    REGISTER_DECODER (VP6F, vp6f);\n    REGISTER_DECODER (VP8, vp8);\n    REGISTER_DECODER (VQA, vqa);\n    REGISTER_ENCDEC  (WMV1, wmv1);\n    REGISTER_ENCDEC  (WMV2, wmv2);\n    REGISTER_DECODER (WMV3, wmv3);\n    REGISTER_DECODER (WMV3_CRYSTALHD, wmv3_crystalhd);\n    REGISTER_DECODER (WMV3_VDPAU, wmv3_vdpau);\n    REGISTER_DECODER (WMV3IMAGE, wmv3image);\n    REGISTER_DECODER (WNV1, wnv1);\n    REGISTER_DECODER (XAN_WC3, xan_wc3);\n    REGISTER_DECODER (XAN_WC4, xan_wc4);\n    REGISTER_DECODER (XL, xl);\n    REGISTER_ENCDEC  (XWD, xwd);\n    REGISTER_ENCDEC  (Y41P, y41p);\n    REGISTER_DECODER (YOP, yop);\n    REGISTER_ENCDEC  (YUV4, yuv4);\n    REGISTER_ENCDEC  (ZLIB, zlib);\n    REGISTER_ENCDEC  (ZMBV, zmbv);\n \n    /* audio codecs */\n    REGISTER_ENCDEC  (AAC, aac);\n    REGISTER_DECODER (AAC_LATM, aac_latm);\n    REGISTER_ENCDEC  (AC3, ac3);\n    REGISTER_ENCODER (AC3_FIXED, ac3_fixed);\n    REGISTER_ENCDEC  (ALAC, alac);\n    REGISTER_DECODER (ALS, als);\n    REGISTER_DECODER (AMRNB, amrnb);\n    REGISTER_DECODER (AMRWB, amrwb);\n    REGISTER_DECODER (APE, ape);\n    REGISTER_DECODER (ATRAC1, atrac1);\n    REGISTER_DECODER (ATRAC3, atrac3);\n    REGISTER_DECODER (BINKAUDIO_DCT, binkaudio_dct);\n    REGISTER_DECODER (BINKAUDIO_RDFT, binkaudio_rdft);\n    REGISTER_DECODER (BMV_AUDIO, bmv_audio);\n    REGISTER_DECODER (COOK, cook);\n    REGISTER_ENCDEC  (DCA, dca);\n    REGISTER_DECODER (DSICINAUDIO, dsicinaudio);\n    REGISTER_ENCDEC  (EAC3, eac3);\n    REGISTER_DECODER (FFWAVESYNTH, ffwavesynth);\n    REGISTER_ENCDEC  (FLAC, flac);\n    REGISTER_ENCDEC  (G723_1, g723_1);\n    REGISTER_DECODER (G729, g729);\n    REGISTER_DECODER (GSM, gsm);\n    REGISTER_DECODER (GSM_MS, gsm_ms);\n    REGISTER_DECODER (IMC, imc);\n    REGISTER_DECODER (MACE3, mace3);\n    REGISTER_DECODER (MACE6, mace6);\n    REGISTER_DECODER (MLP, mlp);\n    REGISTER_DECODER (MP1, mp1);\n    REGISTER_DECODER (MP1FLOAT, mp1float);\n    REGISTER_ENCDEC  (MP2, mp2);\n    REGISTER_DECODER (MP2FLOAT, mp2float);\n    REGISTER_DECODER (MP3, mp3);\n    REGISTER_DECODER (MP3FLOAT, mp3float);\n    REGISTER_DECODER (MP3ADU, mp3adu);\n    REGISTER_DECODER (MP3ADUFLOAT, mp3adufloat);\n    REGISTER_DECODER (MP3ON4, mp3on4);\n    REGISTER_DECODER (MP3ON4FLOAT, mp3on4float);\n    REGISTER_DECODER (MPC7, mpc7);\n    REGISTER_DECODER (MPC8, mpc8);\n    REGISTER_ENCDEC  (NELLYMOSER, nellymoser);\n    REGISTER_DECODER (QCELP, qcelp);\n    REGISTER_DECODER (QDM2, qdm2);\n    REGISTER_ENCDEC  (RA_144, ra_144);\n    REGISTER_DECODER (RA_288, ra_288);\n    REGISTER_DECODER (SHORTEN, shorten);\n    REGISTER_DECODER (SIPR, sipr);\n    REGISTER_DECODER (SMACKAUD, smackaud);\n    REGISTER_ENCDEC  (SONIC, sonic);\n    REGISTER_ENCODER (SONIC_LS, sonic_ls);\n    REGISTER_DECODER (TRUEHD, truehd);\n    REGISTER_DECODER (TRUESPEECH, truespeech);\n    REGISTER_DECODER (TTA, tta);\n    REGISTER_DECODER (TWINVQ, twinvq);\n    REGISTER_DECODER (VMDAUDIO, vmdaudio);\n    REGISTER_ENCDEC  (VORBIS, vorbis);\n    REGISTER_DECODER (WAVPACK, wavpack);\n    REGISTER_DECODER (WMALOSSLESS, wmalossless);\n    REGISTER_DECODER (WMAPRO, wmapro);\n    REGISTER_ENCDEC  (WMAV1, wmav1);\n    REGISTER_ENCDEC  (WMAV2, wmav2);\n    REGISTER_DECODER (WMAVOICE, wmavoice);\n    REGISTER_DECODER (WS_SND1, ws_snd1);\n \n    /* PCM codecs */\n    REGISTER_ENCDEC  (PCM_ALAW, pcm_alaw);\n    REGISTER_DECODER (PCM_BLURAY, pcm_bluray);\n    REGISTER_DECODER (PCM_DVD, pcm_dvd);\n    REGISTER_ENCDEC  (PCM_F32BE, pcm_f32be);\n    REGISTER_ENCDEC  (PCM_F32LE, pcm_f32le);\n    REGISTER_ENCDEC  (PCM_F64BE, pcm_f64be);\n    REGISTER_ENCDEC  (PCM_F64LE, pcm_f64le);\n    REGISTER_DECODER (PCM_LXF, pcm_lxf);\n    REGISTER_ENCDEC  (PCM_MULAW, pcm_mulaw);\n    REGISTER_ENCDEC  (PCM_S8, pcm_s8);\n    REGISTER_DECODER (PCM_S8_PLANAR, pcm_s8_planar);\n    REGISTER_ENCDEC  (PCM_S16BE, pcm_s16be);\n    REGISTER_ENCDEC  (PCM_S16LE, pcm_s16le);\n    REGISTER_DECODER (PCM_S16LE_PLANAR, pcm_s16le_planar);\n    REGISTER_ENCDEC  (PCM_S24BE, pcm_s24be);\n    REGISTER_ENCDEC  (PCM_S24DAUD, pcm_s24daud);\n    REGISTER_ENCDEC  (PCM_S24LE, pcm_s24le);\n    REGISTER_ENCDEC  (PCM_S32BE, pcm_s32be);\n    REGISTER_ENCDEC  (PCM_S32LE, pcm_s32le);\n    REGISTER_ENCDEC  (PCM_U8, pcm_u8);\n    REGISTER_ENCDEC  (PCM_U16BE, pcm_u16be);\n    REGISTER_ENCDEC  (PCM_U16LE, pcm_u16le);\n    REGISTER_ENCDEC  (PCM_U24BE, pcm_u24be);\n    REGISTER_ENCDEC  (PCM_U24LE, pcm_u24le);\n    REGISTER_ENCDEC  (PCM_U32BE, pcm_u32be);\n    REGISTER_ENCDEC  (PCM_U32LE, pcm_u32le);\n    REGISTER_DECODER (PCM_ZORK , pcm_zork);\n \n    /* DPCM codecs */\n    REGISTER_DECODER (INTERPLAY_DPCM, interplay_dpcm);\n    REGISTER_ENCDEC  (ROQ_DPCM, roq_dpcm);\n    REGISTER_DECODER (SOL_DPCM, sol_dpcm);\n    REGISTER_DECODER (XAN_DPCM, xan_dpcm);\n \n    /* ADPCM codecs */\n    REGISTER_DECODER (ADPCM_4XM, adpcm_4xm);\n    REGISTER_ENCDEC  (ADPCM_ADX, adpcm_adx);\n    REGISTER_DECODER (ADPCM_CT, adpcm_ct);\n    REGISTER_DECODER (ADPCM_EA, adpcm_ea);\n    REGISTER_DECODER (ADPCM_EA_MAXIS_XA, adpcm_ea_maxis_xa);\n    REGISTER_DECODER (ADPCM_EA_R1, adpcm_ea_r1);\n    REGISTER_DECODER (ADPCM_EA_R2, adpcm_ea_r2);\n    REGISTER_DECODER (ADPCM_EA_R3, adpcm_ea_r3);\n    REGISTER_DECODER (ADPCM_EA_XAS, adpcm_ea_xas);\n    REGISTER_ENCDEC  (ADPCM_G722, adpcm_g722);\n    REGISTER_ENCDEC  (ADPCM_G726, adpcm_g726);\n    REGISTER_DECODER (ADPCM_IMA_AMV, adpcm_ima_amv);\n    REGISTER_DECODER (ADPCM_IMA_APC, adpcm_ima_apc);\n    REGISTER_DECODER (ADPCM_IMA_DK3, adpcm_ima_dk3);\n    REGISTER_DECODER (ADPCM_IMA_DK4, adpcm_ima_dk4);\n    REGISTER_DECODER (ADPCM_IMA_EA_EACS, adpcm_ima_ea_eacs);\n    REGISTER_DECODER (ADPCM_IMA_EA_SEAD, adpcm_ima_ea_sead);\n    REGISTER_DECODER (ADPCM_IMA_ISS, adpcm_ima_iss);\n    REGISTER_ENCDEC  (ADPCM_IMA_QT, adpcm_ima_qt);\n    REGISTER_DECODER (ADPCM_IMA_SMJPEG, adpcm_ima_smjpeg);\n    REGISTER_ENCDEC  (ADPCM_IMA_WAV, adpcm_ima_wav);\n    REGISTER_DECODER (ADPCM_IMA_WS, adpcm_ima_ws);\n    REGISTER_ENCDEC  (ADPCM_MS, adpcm_ms);\n    REGISTER_DECODER (ADPCM_SBPRO_2, adpcm_sbpro_2);\n    REGISTER_DECODER (ADPCM_SBPRO_3, adpcm_sbpro_3);\n    REGISTER_DECODER (ADPCM_SBPRO_4, adpcm_sbpro_4);\n    REGISTER_ENCDEC  (ADPCM_SWF, adpcm_swf);\n    REGISTER_DECODER (ADPCM_THP, adpcm_thp);\n    REGISTER_DECODER (ADPCM_XA, adpcm_xa);\n    REGISTER_ENCDEC  (ADPCM_YAMAHA, adpcm_yamaha);\n \n    /* subtitles */\n    REGISTER_ENCDEC  (ASS, ass);\n    REGISTER_ENCDEC  (DVBSUB, dvbsub);\n    REGISTER_ENCDEC  (DVDSUB, dvdsub);\n    REGISTER_DECODER (PGSSUB, pgssub);\n    REGISTER_ENCDEC  (SRT, srt);\n    REGISTER_ENCDEC  (XSUB, xsub);\n \n    /* external libraries */\n    REGISTER_ENCODER (LIBAACPLUS, libaacplus);\n    REGISTER_DECODER (LIBCELT, libcelt);\n    REGISTER_ENCDEC  (LIBDIRAC, libdirac);\n    REGISTER_ENCODER (LIBFAAC, libfaac);\n    REGISTER_ENCDEC  (LIBGSM, libgsm);\n    REGISTER_ENCDEC  (LIBGSM_MS, libgsm_ms);\n    REGISTER_ENCODER (LIBMP3LAME, libmp3lame);\n    REGISTER_ENCDEC  (LIBOPENCORE_AMRNB, libopencore_amrnb);\n    REGISTER_DECODER (LIBOPENCORE_AMRWB, libopencore_amrwb);\n    REGISTER_ENCDEC (LIBOPENJPEG, libopenjpeg);\n    REGISTER_ENCDEC  (LIBSCHROEDINGER, libschroedinger);\n    REGISTER_ENCDEC  (LIBSPEEX, libspeex);\n    REGISTER_DECODER (LIBSTAGEFRIGHT_H264, libstagefright_h264);\n    REGISTER_ENCODER (LIBTHEORA, libtheora);\n    REGISTER_DECODER (LIBUTVIDEO, libutvideo);\n    REGISTER_ENCODER (LIBVO_AACENC, libvo_aacenc);\n    REGISTER_ENCODER (LIBVO_AMRWBENC, libvo_amrwbenc);\n    REGISTER_ENCODER (LIBVORBIS, libvorbis);\n    REGISTER_ENCDEC  (LIBVPX, libvpx);\n    REGISTER_ENCODER (LIBX264, libx264);\n    REGISTER_ENCODER (LIBX264RGB, libx264rgb);\n    REGISTER_ENCODER (LIBXAVS, libxavs);\n    REGISTER_ENCODER (LIBXVID, libxvid);\n \n    /* text */\n    REGISTER_DECODER (BINTEXT, bintext);\n    REGISTER_DECODER  (XBIN, xbin);\n    REGISTER_DECODER  (IDF, idf);\n \n    /* parsers */\n    REGISTER_PARSER  (AAC, aac);\n    REGISTER_PARSER  (AAC_LATM, aac_latm);\n    REGISTER_PARSER  (AC3, ac3);\n    REGISTER_PARSER  (ADX, adx);\n    REGISTER_PARSER  (CAVSVIDEO, cavsvideo);\n    REGISTER_PARSER  (DCA, dca);\n    REGISTER_PARSER  (DIRAC, dirac);\n    REGISTER_PARSER  (DNXHD, dnxhd);\n    REGISTER_PARSER  (DVBSUB, dvbsub);\n    REGISTER_PARSER  (DVDSUB, dvdsub);\n    REGISTER_PARSER  (FLAC, flac);\n    REGISTER_PARSER  (GSM, gsm);\n    REGISTER_PARSER  (H261, h261);\n    REGISTER_PARSER  (H263, h263);\n    REGISTER_PARSER  (H264, h264);\n    REGISTER_PARSER  (MJPEG, mjpeg);\n    REGISTER_PARSER  (MLP, mlp);\n    REGISTER_PARSER  (MPEG4VIDEO, mpeg4video);\n    REGISTER_PARSER  (MPEGAUDIO, mpegaudio);\n    REGISTER_PARSER  (MPEGVIDEO, mpegvideo);\n    REGISTER_PARSER  (PNM, pnm);\n    REGISTER_PARSER  (RV30, rv30);\n    REGISTER_PARSER  (RV40, rv40);\n    REGISTER_PARSER  (VC1, vc1);\n    REGISTER_PARSER  (VP3, vp3);\n    REGISTER_PARSER  (VP8, vp8);\n \n    /* bitstream filters */\n    REGISTER_BSF     (AAC_ADTSTOASC, aac_adtstoasc);\n    REGISTER_BSF     (CHOMP, chomp);\n    REGISTER_BSF     (DUMP_EXTRADATA, dump_extradata);\n    REGISTER_BSF     (H264_MP4TOANNEXB, h264_mp4toannexb);\n    REGISTER_BSF     (IMX_DUMP_HEADER, imx_dump_header);\n    REGISTER_BSF     (MJPEG2JPEG, mjpeg2jpeg);\n    REGISTER_BSF     (MJPEGA_DUMP_HEADER, mjpega_dump_header);\n    REGISTER_BSF     (MP3_HEADER_COMPRESS, mp3_header_compress);\n    REGISTER_BSF     (MP3_HEADER_DECOMPRESS, mp3_header_decompress);\n    REGISTER_BSF     (MOV2TEXTSUB, mov2textsub);\n    REGISTER_BSF     (NOISE, noise);\n    REGISTER_BSF     (REMOVE_EXTRADATA, remove_extradata);\n    REGISTER_BSF     (TEXT2MOVSUB, text2movsub);\n}\n```\n\n</details>\n\n整个代码的过程就是首先确定是不是已经初始化过了（initialized），如果没有，就注册，注册，注册...直到完成所有注册。\n\n函数的调用关系图如下图所示。`av_register_all()` 调用了 `avcodec_register_all()`。因此如果调用过 `av_register_all() ` 的话就不需要再调用 `avcodec_register_all()` 了。\n\n![av_register_all 函数调用关系图](/images/imageFFmpeg/Thor/av_register_all.png)\n\n下面附上硬件加速器，编码器/解码器，parser，Bitstream Filter的注册代码。\n\n硬件加速器注册函数是 `av_register_hwaccel()`。\n\n```c\nvoid av_register_hwaccel(AVHWAccel *hwaccel)\n{\n    AVHWAccel **p = last_hwaccel;\n    hwaccel->next = NULL;\n    while(*p || avpriv_atomic_ptr_cas((void * volatile *)p, NULL, hwaccel))\n        p = &(*p)->next;\n    last_hwaccel = &hwaccel->next;\n}\n```\n\n编解码器注册函数是 `avcodec_register()`。\n\n```c\nav_cold void avcodec_register(AVCodec *codec)\n{\n    AVCodec **p;\n    avcodec_init();\n    p = last_avcodec;\n    codec->next = NULL;\n \n    while(*p || avpriv_atomic_ptr_cas((void * volatile *)p, NULL, codec))\n        p = &(*p)->next;\n    last_avcodec = &codec->next;\n \n    if (codec->init_static_data)\n        codec->init_static_data(codec);\n}\n```\n\nparser注册函数是 `av_register_codec_parser()`。\n\n```c\nvoid av_register_codec_parser(AVCodecParser *parser)\n{\n    do {\n        parser->next = av_first_parser;\n    } while (parser->next != avpriv_atomic_ptr_cas((void * volatile *)&av_first_parser, parser->next, parser));\n}\n```\n\nBitstream Filter注册函数是 `av_register_bitstream_filter()`。\n\n```c\nvoid av_register_bitstream_filter(AVBitStreamFilter *bsf)\n{\n    do {\n        bsf->next = first_bitstream_filter;\n    } while(bsf->next != avpriv_atomic_ptr_cas((void * volatile *)&first_bitstream_filter, bsf->next, bsf));\n}\n```\n\n后两个函数中的 `avpriv_atomic_ptr_cas()` 定义如下。\n\n```c\nvoid *avpriv_atomic_ptr_cas(void * volatile *ptr, void *oldval, void *newval)\n{\n    if (*ptr == oldval) {\n        *ptr = newval;\n        return oldval;\n    }\n    return *ptr;\n}\n```\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"WebRTC（三）","url":"/2019-05-17/reference/webrtc/webrtc-03/","content":"\n## WebRTC 环境搭建\n\n### 简单的 https server 服务搭建\n\n<!-- more -->\n\n```shell\n# 二进制安装\n$ apt/brew/yum install nodejs\n$ apt/brew/yum install npm\n$ apt-cache search xxx \t\t# 查看源上相关软件版本信息\n\n# 源码安装\n# 下载 Nodejs 源码 http://nodejs.cn/download/\n$ wget -c https://npm.taobao.org/mirrors/node/v10.16.0/node-v10.16.0.tar.gz\n# 生成 Makefile\n$ ./configure --prefix=/usr/local/nodejs\n$ make -j 4 && sudo make install\n```\n\n```js\n# server.js\n'use strict'\n\nvar https = require('https');\nvar fs = require('fs');\n\nvar options = {\n    key  : fs.readFileSync('./cert/server.key'),\n    cert : fs.readFileSync('./cert/server.pem')\n}\n\nvar app = https.createServer(options, function(req, res){\n    res.writeHead(200, {'Content-Type':'text/plain'});\n    res.end('Hello Mr.Miaow!\\n');\n}).listen(443, '0.0.0.0');\n```\n\n启动 server\n\n```shell\n$ node server.js\n$ nohub node server.js &\n$ forever start server.js  \t# nmp install forever -g\n```\n\n### 真正的Web服务\n\n- 引用 express 模块\n- 引入 serve-index 模块\n- 指定发布目录\n\n```shell\n$ npm install express serve-index\n```\n\n<details><summary>web server 服务</summary>\n\n```js\n'use strict'\n\nvar http = require('http');\nvar https = require('https');\nvar fs = require('fs');\n\nvar express = require('express');\nvar serverIndex = require('serve-index');\n\nvar socketIo = require('socket.io');\nvar log4js = require('log4js');\n\nvar USERCOUNT = 3;\n\nlog4js.configure({\n    appenders: {\n        file: {\n            type: 'file',\n            filename: 'app.log',\n            layout: {\n                type: 'pattern',\n                pattern: '%r %p - %m',\n            }\n        }\n    },\n    categories: {\n        default: {\n            appenders: ['file'],\n            level: 'debug'\n        }\n    }\n});\n\nvar logger = log4js.getLogger();\n\nvar app = express();\napp.use(serverIndex('./public'));\napp.use(express.static('./public'));\n\nvar http_server = http.createServer(app);\nhttp_server.listen(80, '0.0.0.0');\n\nvar options = {\n    key  : fs.readFileSync('./cert/server.key'),\n    cert : fs.readFileSync('./cert/server.pem')\n}\n\nvar https_server = https.createServer(options, app);\n\n// bind socket.io with https_server\nvar io = socketIo.listen(https_server);\nvar sockio = socketIo.listen(http_server);\n\n// connection\nio.sockets.on('connection', (socket)=>{\n    logger.log('Socket.io connection ...');\n\n    socket.on('message', (room, data)=>{\n        socket.to(room).emit('message', room, data); //除自己之外\n    });\n\n    // 该函数应该加锁\n    socket.on('join', (room)=>{\n        socket.join(room);\n\n        var myRoom = io.sockets.adapter.rooms[room];\n        var users = (myRoom) ? Object.keys(myRoom.sockets).length : 0;\n\n        logger.debug('The number of user in room is:' + users);\n\n        // 在这里可以控制进入房间的人数,现在一个房间最多 2个人\n        // 为了便于客户端控制，如果是多人的话，应该将目前房间里\n        // 人的个数当做数据下发下去。\n        if(users < USERCOUNT) {\n            socket.emit('joined', room, socket.id);  // 谁来了发给谁\n            if (users > 1) {\n                socket.to(room).emit('otherjoin', room, socket.id);//除自己之外\n            }\n        }else {\n            socket.leave(room);\n            socket.emit('full', room, socket.id);\n        }\n        //socket.emit('joined', room, socket.id); // 发给自己\n        //socket.to(room).emit('joined', room, socket.id); // 发给除自己之外的房间内的所有人\n        //io.in(room).emit('joined', room, socket.id); // 发给房间内所有人\n        //socket.broadcast.emit('joined', room, socket.id); // 发给除自己之外，这个节点上的所有人\n    });\n\n    socket.on('leave', (room)=>{\n        var myRoom = io.sockets.adapter.rooms[room];\n        var users = (myRoom) ? Object.keys(myRoom.sockets).length : 0;\n        // users - 1\n        logger.debug('The number of user in room is:' + (users-1));\n\n        socket.leave(room);\n        socket.to(room).emit('bye', room, socket.id); //房间内所有人,除自己外\n        socket.emit('leaved', room, socket.id); // 给自己发leaved\n\n        //socket.to(room).emit('leaved', room, socket.id); // 除自己之外\n        //io.in(room).emit('leaved', room, socket.id); // 房间内所有人\n        //socket.broadcast.emit('leaved', room, socket.id); // 除自己，全部站点\n    })\n})\n\nhttps_server.listen(443, '0.0.0.0');\n```\n\n</details>\n\n<details><summary>项目目录结构图</summary>\n\n```shell\n$ tree -I \"node_modules|WebRTCAndroid\"     \n.\n├── app.log\n├── cert\n│   ├── server.key\n│   └── server.pem\n├── package-lock.json\n├── public\n│   ├── bandwidth\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── js\n│   │   │   └── main.js\n│   │   └── room.html\n│   ├── chat\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── index.html\n│   │   └── js\n│   │       ├── main.js\n│   │       └── third_party\n│   │           └── graph.js\n│   ├── chatroom\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── index.html\n│   │   └── js\n│   │       └── client.js\n│   ├── device\n│   │   ├── index.html\n│   │   └── js\n│   │       └── client.js\n│   ├── getstats\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── js\n│   │   │   ├── main.js\n│   │   │   └── third_party\n│   │   │       └── graph.js\n│   │   └── room.html\n│   ├── mediaDisplay\n│   │   ├── index.html\n│   │   └── js\n│   │       └── client.js\n│   ├── mediastream\n│   │   ├── index.html\n│   │   └── js\n│   │       └── client.js\n│   ├── mediastream_bak\n│   │   ├── index.html\n│   │   └── js\n│   │       └── client.js\n│   ├── only_audio\n│   │   ├── index.html\n│   │   └── js\n│   │       └── client.js\n│   ├── peerConnection\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── index.html\n│   │   └── js\n│   │       └── main.js\n│   ├── realyPeerConnection\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── js\n│   │   │   └── main.js\n│   │   └── room.html\n│   ├── sendfile\n│   │   ├── css\n│   │   │   └── main.css\n│   │   ├── index.html\n│   │   └── js\n│   │       ├── main_bw.js\n│   │       └── third_party\n│   │           └── graph.js\n│   └── testCreateOffer\n│       ├── index.html\n│       └── js\n│           └── main.js\n├── server-bak-01.js\n└── server.js\n\n38 directories, 42 files\n```\n\n</details>\n\n### PeerConnection\n\n<details><summary>room.html</summary>\n\n```html\n<html>\n    <head>\n        <title>WebRTC PeerConnection</title>\n        <link href=\"./css/main.css\" rel=\"stylesheet\" />\n    </head>\n\n    <body>\n        <div>\n            <div>\n                <button id=\"connserver\">Connect Sig Server</button>\n                <button id=\"leave\">Leave</button>\n            </div>\n\n            <div>\n                <input id=\"shareDesk\" type=\"checkbox\"/><label for=\"shareDesk\">Share Desktop</label>\n            </div>\n\n            <div id=\"preview\">\n                <div>\n                    <h2>Local:</h2>\n                    <video id=\"localvideo\" autoplay playsinline muted></video>\n\n                    <h2>Offer SDP:</h2>\n                    <textarea id=\"offer\"></textarea>\n                </div>\n                <div>\n                    <h2>Remote:</h2>\n                    <video id=\"remotevideo\" autoplay playsinline></video>\n\n                    <h2>Answer SDP:</h2>\n                    <textarea id=\"answer\"></textarea>\n                </div>\n            </div>\n        </div>\n\n        <script src=\"https://cdn.bootcss.com/socket.io/2.2.0/socket.io.js\"></script>\n        <script src=\"https://webrtc.github.io/adapter/adapter-latest.js\"></script>\n        <script src=\"./js/main.js\"></script>\n    </body>\n</html>\n```\n\n</details>\n\n<details><summary>main.css</summary>\n\n```css\n/*\n *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.\n *\n *  Use of this source code is governed by a BSD-style license\n *  that can be found in the LICENSE file in the root of the source\n *  tree.\n */\n\nbutton {\n  margin: 0 20px 25px 0;\n  vertical-align: top;\n  width: 134px;\n}\n\ntextarea {\n  color: #444;\n  font-size: 0.9em;\n  font-weight: 300;\n  height: 20.0em;\n  padding: 5px;\n  width: calc(100% - 10px);\n}\n\ndiv#getUserMedia {\n  padding: 0 0 8px 0;\n}\n\ndiv.input {\n  display: inline-block;\n  margin: 0 4px 0 0;\n  vertical-align: top;\n  width: 310px;\n}\n\ndiv.input > div {\n  margin: 0 0 20px 0;\n  vertical-align: top;\n}\n\ndiv.output {\n  background-color: #eee;\n  display: inline-block;\n  font-family: 'Inconsolata', 'Courier New', monospace;\n  font-size: 0.9em;\n  padding: 10px 10px 10px 25px;\n  position: relative;\n  top: 10px;\n  white-space: pre;\n  width: 270px;\n}\n\ndiv#preview {\n  border-bottom: 1px solid #eee;\n  margin: 0 0 1em 0;\n  padding: 0 0 0.5em 0;\n}\n\ndiv#preview > div {\n  display: inline-block;\n  vertical-align: top;\n  width: calc(50% - 12px);\n}\n\nsection#statistics div {\n  display: inline-block;\n  font-family: 'Inconsolata', 'Courier New', monospace;\n  vertical-align: top;\n  width: 308px;\n}\n\nsection#statistics div#senderStats {\n  margin: 0 20px 0 0;\n}\n\nsection#constraints > div {\n  margin: 0 0 20px 0;\n}\n\nh2 {\n  margin: 0 0 1em 0;\n}\n\nsection#constraints label {\n  display: inline-block;\n  width: 156px;\n}\n\nsection {\n  margin: 0 0 20px 0;\n  padding: 0 0 15px 0;\n}\n\nvideo {\n  background: #222;\n  margin: 0 0 0 0;\n  --width: 100%;\n  width: var(--width);\n  height: 225px;\n}\n\n@media screen and (max-width: 720px) {\n  button {\n    font-weight: 500;\n    height: 56px;\n    line-height: 1.3em;\n    width: 90px;\n  }\n  div#getUserMedia {\n    padding: 0 0 40px 0;\n  }\n  section#statistics div {\n    width: calc(50% - 14px);\n  }\n}\n```\n\n</details>\n\n<details><summary>main.js</summary>\n\n```js\n'use strict'\n\nvar localVideo = document.querySelector('video#localvideo');\nvar remoteVideo = document.querySelector('video#remotevideo');\n\nvar btnConn = document.querySelector('button#connserver');\nvar btnLeave = document.querySelector('button#leave');\n\nvar offer = document.querySelector('textarea#offer');\nvar answer = document.querySelector('textarea#answer');\n\nvar shareDeskBox  = document.querySelector('input#shareDesk');\n\nvar pcConfig = {\n    'iceServers': [{\n        'urls': 'turn:stun.al.learningrtc.cn:3478',\n        'credential': \"mypasswd\",\n        'username': \"garrylea\"\n    }]\n};\n\nvar localStream = null;\nvar remoteStream = null;\n\nvar roomid = '123123';\nvar socket = null;\n\nvar offerdesc = null;\nvar state = 'init';\n\nvar pc = null;\n\n// 以下代码是从网上找的\n//================================================================================\n//如果返回的是false说明当前操作系统是手机端，如果返回的是true则说明当前的操作系统是电脑端\nfunction IsPC() {\n    var userAgentInfo = navigator.userAgent;\n    var Agents = [\"Android\", \"iPhone\",\"SymbianOS\", \"Windows Phone\",\"iPad\", \"iPod\"];\n    var flag = true;\n\n    for (var v = 0; v < Agents.length; v++) {\n        if (userAgentInfo.indexOf(Agents[v]) > 0) {\n            flag = false;\n            break;\n        }\n    }\n    return flag;\n}\n\n//如果返回true 则说明是Android  false是ios\nfunction is_android() {\n    var u = navigator.userAgent, app = navigator.appVersion;\n    var isAndroid = u.indexOf('Android') > -1 || u.indexOf('Linux') > -1; //g\n    var isIOS = !!u.match(/\\(i[^;]+;( U;)? CPU.+Mac OS X/); //ios终端\n    if (isAndroid) {\n        //这个是安卓操作系统\n        return true;\n    }\n\n    if (isIOS) {\n        //这个是ios操作系统\n        return false;\n    }\n}\n\n//获取url参数\nfunction getQueryVariable(variable)\n{\n    var query = window.location.search.substring(1);\n    var vars = query.split(\"&\");\n    for (var i=0;i<vars.length;i++) {\n        var pair = vars[i].split(\"=\");\n        if(pair[0] == variable){return pair[1];}\n    }\n    return(false);\n}\n//=======================================================================\n\nfunction getRemoteStream(e){\n    remoteStream = e.streams[0];\n    remoteVideo.srcObject = e.streams[0];\n}\n\nfunction sendMessage(roomid, data)\n{\n    console.log('Send p2p message: roomid=' + roomid + ' data=' + data);\n    if(!socket) {\n        console.log('SendMessage is error: socket is null');\n    }\n    socket.emit('message', roomid, data);\n}\n\nfunction createPeerConnection(){\n    //如果是多人的话，在这里要创建一个新的连接.\n    //新创建好的要放到一个map表中。\n    //key=userid, value=peerconnection\n    console.log('Create RTCPeerConnection ...');\n    if(!pc) {\n        pc = new RTCPeerConnection(pcConfig);\n        pc.onicecandidate = (e)=> {\t\t// 监听 candidate 事件\n            if(e.candidate) {\n                console.log('Find an new candidate:', e.candidate);\n                sendMessage(roomid, {\n                    type: 'candidate',\n                    label: e.candidate.sdpMLineIndex,\n                    id: e.candidate.sdpMid,\n                    candidate: e.candidate.candidate\n                });\n            } else {\n                console.log('This is the end candidate');\n            }\n        }\n        pc.ontrack = getRemoteStream;  // 监听 轨 事件\n    } else {\n        console.log('The pc have be created!');\n    }\n\n    //if(localStream) {\n    //    localStream.getTracks().forEach((track)=>{\n    //        pc.addTrack(track);\n    //    });\n    //}\n    return;\n}\n\n//绑定永远与 peerconnection在一起，\n//所以没必要再单独做成一个函数\nfunction bindTracks(){\n    console.log('Bind tracks into RTCPeerConnection!');\n    if( pc === null || pc === undefined) {\n        console.error('pc is null or undefined!');\n        return;\n    }\n\n    if(localStream === null || localStream === undefined) {\n        console.error('localstream is null or undefined!');\n        return;\n    }\n\n    //add all track into peer connection\n    localStream.getTracks().forEach((track)=>{\n        pc.addTrack(track, localStream);\n    });\n}\n\nfunction getOffer(desc){\n    pc.setLocalDescription(desc);\n    offer.value = desc.sdp;\n    offerdesc = desc;\n    sendMessage(roomid, offerdesc);\n}\n\nfunction handleOfferError(err) {\n    console.error('Failed to get Offer!', err);\n}\n\nfunction call(){\n    console.log('call ...');\n    if(state === 'joined_conn') {\n        if(pc) {\n            // 控制接受远端视频和音频参数配置\n            var options = {  // 还有两个参数 1. icerestart 2. 静音检测  \n                offerToRecieveVideo: 1,\n                offerToRecieveAudio: 1\n            }\n            pc.createOffer(options)\n                .then(getOffer).catch(handleOfferError);\n        }\n    }\n}\n\nfunction closeLocalMedia(){\n    if(localStream && localStream.getTracks()) {\n        localStream.getTracks().forEach((track)=>{\n            track.stop();\n        });\n    }\n    localStream = null;\n}\n\nfunction hangup(){\n    console.log('Close RTCPeerConnection ...');\n    if(pc) {\n        offerdesc = null;\n        pc.close();\n        pc = null;\n    }\n}\n\nfunction getAnswer(desc){\n    pc.setLocalDescription(desc);  // 通知本地手机candidate\n    answer.value = desc.sdp;\n    sendMessage(roomid, desc);\n}\n\nfunction handleAnswerError(err){\n    console.error('Failed to get Answer!', err);\n}\n\nfunction conn(){\n    socket = io.connect();  // 与信令服务器进行连接\n\t// 注册 接收服务端的 消息函数\n    socket.on('joined', (roomid, id)=> {  // id -> 用户id\n        btnConn.disabled = true;\n        btnLeave.disabled = false;\n        state = 'joined';\n        createPeerConnection();\n        bindTracks();\n        console.log('Receive joined message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('otherjoin', (roomid, id)=> {\n        if(state === 'joined_unbind') {\n            createPeerConnection();\n            bindTracks();\n        }\n        state = 'joined_conn';\n        // 媒体协商\n        call();\n        console.log('Receive otherjoin message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('full', (roomid, id)=> {\n        state = 'leaved';\n        hangup();\n        //socket.disconnect();\n        closeLocalMedia();\n        btnConn.disabled = false;\n        btnLeave.disabled = true;\n        console.log('Receive full message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n        alert('The room is full!');\n    });\n\n    socket.on('leaved', (roomid, id)=> {\n        state = 'leaved';\n        socket.disconnect();  // 关闭连接\n        btnConn.disabled = false;\n        btnLeave.disabled = true;\n        console.log('Receive leaved message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('bye', (roomid, id)=> {\n        //state = 'created';\n        //当是多人通话时，应该带上当前房间的用户数\n        //如果当前房间用户不小于 2, 则不用修改状态\n        //并且，关闭的应该是对应用户的peerconnection\n        //在客户端应该维护一张peerconnection表，它是\n        //一个key:value的格式，key=userid, value=peerconnection\n\n        state = 'joined_unbind';\n        hangup();\n        offer.value = '';\n        answer.value = '';\n        console.log('Receive bye message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('disconnect', (socket) => {\n        if(!(state === 'leaved')){\n            hangup();\n            closeLocalMedia();\n        }\n        state = 'leaved';\n        console.log('Receive disconnect message! roomid=' + roomid);\n    });\n\t// 端对端的消息\n    socket.on('message', (roomid, data)=> {\n        // 媒体协商\n        if(data === null || data === undefined){\n            console.error('The message is invalid!');\n            return;\n        }\n\n        if(data.hasOwnProperty('type') && data.type === 'offer') {\n            offer.value = data.sdp;\n            pc.setRemoteDescription(new RTCSessionDescription(data));  // data 发送前是一个对象，发送过来的时候已经变成了一个文本，所以这儿要转换\n            pc.createAnswer()\n                .then(getAnswer).catch(handleAnswerError);\n        } else if(data.hasOwnProperty('type') && data.type == 'answer'){\n            answer.value = data.sdp;\n            pc.setRemoteDescription(new RTCSessionDescription(data));\n        } else if (data.hasOwnProperty('type') && data.type === 'candidate'){\n            var candidate = new RTCIceCandidate({\n                sdpMLineIndex: data.label,\n                candidate: data.candidate\n            });\n            pc.addIceCandidate(candidate); // 将 candidate 添加到本端\n        } else {\n            console.error('The message is invalid!', data);\n        }\n\n        console.log('Receive client message: roomid=' + roomid + ' data=' + data);\n    });\n\n    //roomid = getQueryVariable('room');\n    socket.emit('join', roomid);  // 发送消息，加入 roomid 这个房间\n    return;\n}\n\nfunction getMediaStream(stream){\n    if(localStream){\n        stream.getAudioTracks().forEach((track)=>{\n            localStream.addTrack(track);\n            stream.removeTrack(track);\n        });\n    }else{\n        localStream = stream;\n    }\n    localVideo.srcObject = localStream;  // 本地视频在视频标签中显示\n\n    //这个函数的位置特别重要，\n    //一定要放到getMediaStream之后再调用\n    //否则就会出现绑定失败的情况\n    //setup connection\n    conn();  // 信令功能实现\n}\n\nfunction handleError(err){\n    console.error('Failed to get Media Stream: ', err.name);\n}\n\nfunction getDeskStream(stream){\n    localStream = stream;\n}\n\nfunction handleShareDeskError(err){\n    console.error('Failed to get Media Stream!', err);\n}\n\nfunction shareDesk(){\n    if(IsPC()){\n        navigator.mediaDevices.getDisplayMedia({video: true})\n            .then(getDeskStream).catch(handleShareDeskError);\n        return true;\n    }\n    return false;\n}\n\nfunction start(){\n    if(!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n        console.error('The getUserMedia is not supported!');\n        return;\n    } else {\n        var constraints;\n        if(shareDeskBox.checked && shareDesk()) {\n            constraints = {\n                video: false,\n                audio:  {\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true\n                }\n            }\n        } else {\n            constraints = {\n                video: true,\n                audio:  {\n                    echoCancellation: true,  // 回音消除\n                    noiseSuppression: true,  // 降噪？\n                    autoGainControl: true\t // \n                }\n            }\n        }\n        navigator.mediaDevices.getUserMedia(constraints)\n            .then(getMediaStream).catch(handleError);\n    }\n}\n\nfunction connSignalServer(){\n    // 开启本地视频\n    start();\n    return true;\n}\n\nfunction closePeerConnection(){\n    console.log('Close RTCPeerConnection ...');\n    if(pc) {\n        pc.close();\n        pc = null;\n    }\n}\n\nfunction leave(){\n    if(socket) {\n        socket.emit('leave', roomid);\n    }\n    hangup();\n    closeLocalMedia();\n\n    offer.value = '';\n    answer.value = '';\n\n    btnConn.disabled = false;\n    btnLeave.disabled = true;\n}\n\nbtnConn.onclick = connSignalServer;\nbtnLeave.onclick = leave;\n```\n\n</details>\n\n## Chat\n\n<details><summary>main.css</summary>\n\n```css\n/*\n *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.\n *\n *  Use of this source code is governed by a BSD-style license\n *  that can be found in the LICENSE file in the root of the source\n *  tree.\n */\n\nbutton {\n  margin: 10px 20px 25px 0;\n  vertical-align: top;\n  width: 134px;\n}\n\ntable {\n  margin: 200px (50% - 100) 0 0; \n}\n\ntextarea {\n  color: #444;\n  font-size: 0.9em;\n  font-weight: 300;\n  height: 20.0em;\n  padding: 5px;\n  width: calc(100% - 10px);\n}\n\ndiv#getUserMedia {\n  padding: 0 0 8px 0;\n}\n\ndiv.input {\n  display: inline-block;\n  margin: 0 4px 0 0;\n  vertical-align: top;\n  width: 310px;\n}\n\ndiv.input > div {\n  margin: 0 0 20px 0;\n  vertical-align: top;\n}\n\ndiv.output {\n  background-color: #eee;\n  display: inline-block;\n  font-family: 'Inconsolata', 'Courier New', monospace;\n  font-size: 0.9em;\n  padding: 10px 10px 10px 25px;\n  position: relative;\n  top: 10px;\n  white-space: pre;\n  width: 270px;\n}\n\ndiv.label {\n        display: inline-block;\n        font-weight: 400;\n        width: 120px;\n}\n\ndiv.graph-container {\n  background-color: #ccc;\n  float: left;\n  margin: 0.2em;\n  width: calc(50%-1em);\n}\n\ndiv#preview {\n  border-bottom: 1px solid #eee;\n  margin: 0 0 1em 0;\n  padding: 0 0 0.5em 0;\n}\n\ndiv#preview > div {\n  display: inline-block;\n  vertical-align: top;\n  width: calc(50% - 12px);\n}\n\nsection#statistics div {\n  display: inline-block;\n  font-family: 'Inconsolata', 'Courier New', monospace;\n  vertical-align: top;\n  width: 308px;\n}\n\nsection#statistics div#senderStats {\n  margin: 0 20px 0 0;\n}\n\nsection#constraints > div {\n  margin: 0 0 20px 0;\n}\n\nh2 {\n  margin: 0 0 1em 0;\n}\n\nsection#constraints label {\n  display: inline-block;\n  width: 156px;\n}\n\nsection {\n  margin: 0 0 20px 0;\n  padding: 0 0 15px 0;\n}\n\nvideo {\n  background: #222;\n  margin: 0 0 0 0;\n  --width: 100%;\n  width: var(--width);\n  height: 225px;\n}\n\n@media screen and (max-width: 720px) {\n  button {\n    font-weight: 500;\n    height: 56px;\n    line-height: 1.3em;\n    width: 90px;\n  }\n\n  div#getUserMedia {\n    padding: 0 0 40px 0;\n  }\n\n  section#statistics div {\n    width: calc(50% - 14px);\n  }\n\n}\n```\n\n</details>\n\n<details><summary>Chat.html</summary>\n\n```html\n<html>\n    <head>\n        <title>WebRTC PeerConnection</title>\n        <link href=\"./css/main.css\" rel=\"stylesheet\" />\n    </head>\n\n    <body>\n        <div>\n            <div>\n                <button id=\"connserver\">Connect Sig Server</button>\n                <button id=\"leave\">Leave</button>\n            </div>\n\n            <div>\n                <label>BandWidth:</label>\n                <select id=\"bandwidth\" disabled>\n                    <option value=\"unlimited\" selected>unlimited</option>\n                    <option value=\"2000\">2000</option>\n                    <option value=\"1000\">1000</option>\n                    <option value=\"500\">500</option>\n                    <option value=\"250\">250</option>\n                    <option value=\"125\">125</option>\n                </select>\n                kbps\n            </div>\n\n            <div>\n                <input id=\"shareDesk\" type=\"checkbox\"/><label for=\"shareDesk\">Share Desktop</label>\n            </div>\n\n            <div id=\"preview\">\n                <div>\n                    <h2>Local:</h2>\n                    <video id=\"localvideo\" autoplay playsinline muted></video>\n\n                    <h2>Remote:</h2>\n                    <video id=\"remotevideo\" autoplay playsinline></video>\n                </div>\n\n                <div>\n                    <h2>Chat:</h2>\n                    <textarea id=\"chat\" disabled></textarea>\n                    <textarea id=\"sendtxt\" disabled></textarea>\n                    <button id=\"send\">Send</button>\n                </div>\n            </div>\n\n            <div id=\"preview\">\n                <div class=\"graph-container\" id=\"bitrateGraph\">\n                    <div>Bitrate</div>\n                    <canvas id=\"bitrateCanvas\"></canvas>\n                </div>\n                <div class=\"graph-container\" id=\"packetGraph\">\n                    <div>Packets sent per second</div>\n                    <canvas id=\"packetCanvas\"></canvas>\n                </div>\n            </div>\n        </div>\n\n        <script src=\"js/third_party/graph.js\"></script>\n        <script src=\"https://cdn.bootcss.com/socket.io/2.2.0/socket.io.js\"></script>\n        <script src=\"https://webrtc.github.io/adapter/adapter-latest.js\"></script>\n        <script src=\"./js/Chat.js\"></script>\n    </body>\n</html>\n```\n\n</details>\n\n<details><summary>Chat.js</summary>\n\n```js\n'use strict'\n\nvar localVideo = document.querySelector('video#localvideo');\nvar remoteVideo = document.querySelector('video#remotevideo');\n\nvar btnConn = document.querySelector('button#connserver');\nvar btnLeave = document.querySelector('button#leave');\n\nvar optBW = document.querySelector('select#bandwidth');\n\nvar shareDeskBox  = document.querySelector('input#shareDesk');\n\nvar chat = document.querySelector('textarea#chat');\nvar sendTxt = document.querySelector('textarea#sendtxt');\nvar btnSend = document.querySelector('button#send');\n\nvar pcConfig = {\n    'iceServers': [{\n        'urls': 'turn:stun.al.learningrtc.cn:3478',\n        'credential': \"mypasswd\",\n        'username': \"garrylea\"\n    }]\n};\n\nvar bitrateGraph;\nvar bitrateSeries;\n\nvar packetGraph;\nvar packetSeries;\n\nvar lastReportResult;\n\nvar localStream = null;\nvar remoteStream = null;\n\nvar roomid = '123123';\nvar socket = null;\n\nvar offerdesc = null;\nvar state = 'init';\n\nvar pc = null;\nvar dc = null;\n\n// 以下代码是从网上找的\n//=========================================================================================\n\n//如果返回的是false说明当前操作系统是手机端，如果返回的是true则说明当前的操作系统是电脑端\nfunction IsPC() {\n    var userAgentInfo = navigator.userAgent;\n    var Agents = [\"Android\", \"iPhone\",\"SymbianOS\", \"Windows Phone\",\"iPad\", \"iPod\"];\n    var flag = true;\n\n    for (var v = 0; v < Agents.length; v++) {\n        if (userAgentInfo.indexOf(Agents[v]) > 0) {\n            flag = false;\n            break;\n        }\n    }\n\n    return flag;\n}\n\n//如果返回true 则说明是Android  false是ios\nfunction is_android() {\n    var u = navigator.userAgent, app = navigator.appVersion;\n    var isAndroid = u.indexOf('Android') > -1 || u.indexOf('Linux') > -1; //g\n    var isIOS = !!u.match(/\\(i[^;]+;( U;)? CPU.+Mac OS X/); //ios终端\n    if (isAndroid) {\n        //这个是安卓操作系统\n        return true;\n    }\n\n    if (isIOS) {\n        //这个是ios操作系统\n        return false;\n    }\n}\n\n//获取url参数\nfunction getQueryVariable(variable){\n    var query = window.location.search.substring(1);\n    var vars = query.split(\"&\");\n    for (var i=0;i<vars.length;i++) {\n        var pair = vars[i].split(\"=\");\n        if(pair[0] == variable){return pair[1];}\n    }\n    return(false);\n}\n//=======================================================================\n\nfunction sendMessage(roomid, data){\n    console.log('Send p2p message: roomid=' + roomid + ' data=' + data);\n    if(!socket) {\n        console.log('SendMessage is error: socket is null');\n    }\n    socket.emit('message', roomid, data);\n}\n\nfunction getOffer(desc){\n    pc.setLocalDescription(desc);\n    offerdesc = desc;\n    sendMessage(roomid, offerdesc);\n}\n\nfunction handleOfferError(err) {\n    console.error('Failed to get Offer!', err);\n}\n\nfunction getAnswer(desc){\n    pc.setLocalDescription(desc);\n    optBW.disabled = false;\n    sendMessage(roomid, desc);\n}\n\nfunction handleAnswerError(err){\n    console.error('Failed to get Answer!', err);\n}\n\nfunction call(){\n    console.log('call ...');\n    if(state === 'joined_conn') {\n        if(pc) {\n            var options = {\n                offerToRecieveVideo: 1,\n                offerToRecieveAudio: 1\n            }\n            pc.createOffer(options)\n                .then(getOffer).catch(handleOfferError);\n        }\n    }\n}\n\nfunction connSignalServer(){\n    // 开启本地视频\n    start();\n    return true;\n}\n\nfunction receivemsg(e){\n    var msg = e.data;\n    if(msg) {\n        chat.value += '->' + msg + '\\r\\n';\n    } else {\n        console.error('Received msg is null!');\n    }\n}\n\nfunction dataChannelStateChange() {\n    var readyState = dc.readyState;\n    console.log('Send channel state is: ' + readyState);\n    if (readyState === 'open') {\n        sendTxt.disabled = false;\n        send.disabled = false;\n    } else {\n        sendTxt.disabled = true;\n        send.disabled = true;\n    }\n}\n\nfunction conn(){\n    socket = io.connect();\n\n    socket.on('joined', (roomid, id) => {\n        state = 'joined'\n\n        //如果是多人的话，第一个人不该在这里创建peerConnection\n        //都等到收到一个otherjoin时再创建\n        //所以，在这个消息里应该带当前房间的用户数\n        //\n        //create conn and bind media track\n        createPeerConnection();\n        bindTracks();\n\n        btnConn.disabled = true;\n        btnLeave.disabled = false;\n        console.log('receive joined message, state=', state);\n    });\n\n    socket.on('otherjoin', (roomid, id) => {\n        //如果是多人的话，每上来一个人都要创建一个新的 peerConnection\n        //\n        if(state === 'joined_unbind'){\n            createPeerConnection();\n            bindTracks();\n        }\n\n        //create data channel for transporting non-audio/video data\n        dc = pc.createDataChannel('chatchannel');\n        dc.onmessage = receivemsg;\n        dc.onopen = dataChannelStateChange;\n        dc.onclose = dataChannelStateChange;\n\n        state = 'joined_conn';\n\n        // 媒体协商\n        call();\n        \n        console.log('Receive otherjoin message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('full', (roomid, id)=> {\n        state = 'leaved';\n        hangup();\n        closeLocalMedia();\n\n        btnConn.disabled = false;\n        btnLeave.disabled = true;\n        console.log('Receive full message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n        alert('The room is full!');\n    });\n\n    socket.on('leaved', (roomid, id)=> {\n        state = 'leaved';\n        socket.disconnect();\n\n        btnConn.disabled = false;\n        btnLeave.disabled = true;\n        console.log('Receive leaved message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('bye', (roomid, id)=> {\n        //state = 'created';\n        //当是多人通话时，应该带上当前房间的用户数\n        //如果当前房间用户不小于 2, 则不用修改状态\n        //并且，关闭的应该是对应用户的peerconnection\n        //在客户端应该维护一张peerconnection表，它是\n        //一个key:value的格式，key=userid, value=peerconnection\n        state = 'joined_unbind';\n\n        hangup();\n        console.log('Receive bye message: roomid=' + roomid + ' userid=' + id + ' state=' + state);\n    });\n\n    socket.on('disconnect', (socket) => {\n        if(!(state === 'leaved')){\n            hangup();\n            closeLocalMedia();\n        }\n        state = 'leaved';\n        console.log('Receive disconnect message! roomid=' + roomid);\n    });\n\n    socket.on('message', (roomid, data)=> {\n        // 媒体协商\n        if(data === null || data === undefined){\n            console.error('The message is invalid!');\n            return;\n        }\n        if(data.hasOwnProperty('type') && data.type === 'offer') {\n            pc.setRemoteDescription(new RTCSessionDescription(data));\n            pc.createAnswer()\n                .then(getAnswer).catch(handleAnswerError);\n        } else if(data.hasOwnProperty('type') && data.type == 'answer'){\n            optBW.disabled = false;\n            pc.setRemoteDescription(new RTCSessionDescription(data));\n        } else if (data.hasOwnProperty('type') && data.type === 'candidate'){\n            var candidate = new RTCIceCandidate({\n                sdpMLineIndex: data.label,\n                candidate: data.candidate\n            });\n            pc.addIceCandidate(candidate);\n        } else {\n            console.error('The message is invalid!', data);\n        }\n        console.log('Receive client message: roomid=' + roomid + ' data=' + data);\n    });\n\n    //roomid = getQueryVariable('room');\n    socket.emit('join', roomid);\n\n    return;\n}\n\nfunction getMediaStream(stream){\n    if(localStream){\n        stream.getAudioTracks().forEach((track)=>{\n            localStream.addTrack(track);\n            stream.removeTrack(track);\n        });\n    }else{\n        localStream = stream;\n    }\n\n    localVideo.srcObject = localStream;\n\n    //这个函数的位置特别重要，\n    //一定要放到getMediaStream之后再调用\n    //否则就会出现绑定失败的情况\n    //\n    //setup connection\n    conn();\n\n    bitrateSeries = new TimelineDataSeries();\n    bitrateGraph = new TimelineGraphView('bitrateGraph', 'bitrateCanvas');\n    bitrateGraph.updateEndDate();\n\n    packetSeries = new TimelineDataSeries();\n    packetGraph = new TimelineGraphView('packetGraph', 'packetCanvas');\n    packetGraph.updateEndDate();\n}\n\nfunction handleError(err){\n    console.error('Failed to get Media Stream: ', err.name);\n}\n\nfunction getDeskStream(stream){\n    localStream = stream;\n}\n\nfunction handleShareDeskError(err){\n    console.error('Failed to get Media Stream!', err);\n}\n\nfunction shareDesk(){\n    if(IsPC()){\n        navigator.mediaDevices.getDisplayMedia({video: true})\n            .then(getDeskStream).catch(handleShareDeskError);\n        return true;\n    }\n    return false;\n}\n\nfunction start(){\n    if(!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n        console.error('The getUserMedia is not supported!');\n        return;\n    } else {\n        var constraints;\n        if(shareDeskBox.checked && shareDesk()) {\n            constraints = {\n                video: false,\n                audio:  {\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true\n                }\n            }\n        } else {\n            constraints = {\n                video: true,\n                audio:  {\n                    echoCancellation: true,\n                    noiseSuppression: true,\n                    autoGainControl: true\n                }\n            }\n        }\n\n        navigator.mediaDevices.getUserMedia(constraints)\n            .then(getMediaStream).catch(handleError);\n    }\n}\n\n\nfunction closeLocalMedia(){\n    if(localStream && localStream.getTracks()) {\n        localStream.getTracks().forEach((track)=>{\n            track.stop();\n        });\n    }\n    localStream = null;\n}\n\nfunction leave(){\n    if(socket) {\n        socket.emit('leave', '123123');\n    }\n    hangup();\n    closeLocalMedia();\n\n    btnConn.disabled = false;\n    btnLeave.disabled = true;\n}\n\nfunction getRemoteStream(e){\n    remoteStream = e.streams[0];\n    remoteVideo.srcObject = e.streams[0];\n}\n\nfunction createPeerConnection()\n{\n    //如果是多人的话，在这里要创建一个新的连接.\n    //新创建好的要放到一个map表中。\n    //key=userid, value=peerconnection\n    console.log('Create RTCPeerConnection ...');\n    if(!pc) {\n        pc = new RTCPeerConnection(pcConfig);\n\n        pc.onicecandidate = (e)=> {\n            if(e.candidate) {\n                console.log('Find an new candidate:', e.candidate);\n\n                sendMessage(roomid, {\n                    type: 'candidate',\n                    label: event.candidate.sdpMLineIndex,\n                    id: event.candidate.sdpMid,\n                    candidate: event.candidate.candidate\n                });\n            } else {\n                console.log('This is the end candidate');\n            }\n        }\n\n        pc.ondatachannel = e=> {\n            if(!dc){\n                dc = e.channel;\n                dc.onmessage = receivemsg; \n                dc.onopen = dataChannelStateChange;\n                dc.onclose = dataChannelStateChange;\n            }\n        }\n\n        pc.ontrack = getRemoteStream;\n    } else {\n        console.log('The pc have be created!');\n    }\n\n    //if(localStream) {\n    //    localStream.getTracks().forEach((track)=>{\n    //        pc.addTrack(track);\n    //    });\n    //}\n    return;\n}\n\n//绑定永远与 peerconnection在一起，\n//所以没必要再单独做成一个函数\nfunction bindTracks(){\n    console.log('Bind tracks into RTCPeerConnection!');\n    if( pc === null || pc === undefined) {\n        console.error('pc is null or undefined!');\n        return;\n    }\n\n    if(localStream === null || localStream === undefined) {\n        console.error('localstream is null or undefined!');\n        return;\n    }\n\n    //add all track into peer connection\n    localStream.getTracks().forEach((track)=>{\n        pc.addTrack(track, localStream);\n    });\n}\n\nfunction hangup(){\n    console.log('Close RTCPeerConnection ...');\n    if(pc) {\n        offerdesc = null;\n        pc.close();\n        pc = null;\n    }\n}\n\nfunction closePeerConnection(){\n    console.log('Close RTCPeerConnection ...');\n    if(pc) {\n        pc.close();\n        pc = null;\n    }\n}\n\nfunction change_bw(){\n    bandwidth.disabled = true;\n    var bw = optBW.options[optBW.selectedIndex].value;\n\n    var vsender = null;\n    var senders = pc.getSenders();\n\n    senders.forEach( sender => {\n        if(sender && sender.track.kind === 'video') {\n            vsender = sender;\n        } \n    });\n\n    var parameters = vsender.getParameters();\n    if(!parameters.encodings) {\n        return;\n    }\n\n    if(bw === 'unlimited') {\n        return;\n    } \n\n    parameters.encodings[0].maxBitrate = bw * 1000;\n\n    vsender.setParameters(parameters)\n        .then(()=>{\n            bandwidth.disabled = false;\n            console.log('Successed to set parameters!');\n        })\n        .catch(err =>{\n            console.error(err);\n        });\n\n    return;\n}\n\n// query getStats every second\n// 设置定时器，每秒处理一次，因为计算包的流量和发送包的次数都是1秒为单位的\nwindow.setInterval(() => {\n    if (!pc) {\n        return;\n    }\n\n    var vsender = null;\n    var senders = pc.getSenders();\n\n    senders.forEach( sender => {\n        if(sender && sender.track.kind === 'video') {\n            vsender = sender;\n        } \n    });\n\n    //const vsender = pc.getSenders()[0];\n    if (!vsender) {\n        return;\n    }\n\n    vsender.getStats()\n        .then(reports => {\n        \treports.forEach(report => {\n                let bytes;\n                let packets;\n                if (report.type === 'outbound-rtp') {\n                    if (report.isRemote) {\n                        return;\n                    }\n                    const curTs = report.timestamp;\n                    bytes = report.bytesSent;\n                    packets = report.packetsSent;\n                    if (lastReportResult && lastReportResult.has(report.id)) {\n                        // calculate bitrate\n                        const bitrate = 8 * (bytes - lastReportResult.get(report.id).bytesSent) /\n                              (curTs - lastReportResult.get(report.id).timestamp);\n\n                        // append to chart\n                        bitrateSeries.addPoint(curTs, bitrate);\n                        bitrateGraph.setDataSeries([bitrateSeries]);\n                        bitrateGraph.updateEndDate();\n\n                        // calculate number of packets and append to chart\n                        packetSeries.addPoint(curTs, packets - lastReportResult.get(report.id).packetsSent);\n                        packetGraph.setDataSeries([packetSeries]);\n                        packetGraph.updateEndDate();\n                    }\n                }\n        \t});\n        \tlastReportResult = reports;\n    \t})\n        .catch(err =>{\n            console.log(err);\n        });\n}, 1000);\n\nfunction sendText(){\n    var data = sendTxt.value;\n    if(data) {\n        dc.send(data);\n    }\n\n    sendTxt.value = '';\n    chat.value += '<-' + data + '\\r\\n';\n}\n\nbtnConn.onclick = connSignalServer;\nbtnLeave.onclick = leave;\noptBW.onchange = change_bw;\n\nbtnSend.onclick = sendText;\n```\n\n</details>\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"WebRTC（二）","url":"/2019-05-15/reference/webrtc/webrtc-02/","content":"\n## 端对端1V1传输基本流程\n\n### 媒体能力协商过程\n\nWebRTC 端对端连接：\n\n**RTCPeerConnection**：\n\n- 基本格式\n\n  ```js\n  pc = new RTCPeerConnection([configuration]);\n  ```\n\n<!-- more -->\n\n**RTCPeerConnection 方法分类**：\n\n- 媒体协商\n- Stream/Track\n- 传输相关方法\n- 统计相关方法\n\n<img src=\"/images/imageWebRTC/媒体协商过程.png\">\n\n<img src=\"/images/imageWebRTC/协商状态变化.png\">\n\n**媒体协商方法**：\n\n- createOffer\n- createAnswer\n- setLocakDescription\n- setRemoteDescription\n\n**createOffer**：\n\n- 基本格式\n\n  ```js\n  aPromise = myPeerConnection.createOffer([options]);\n  ```\n\n**createAnswer**：\n\n- 基本格式\n\n  ```js\n  aPromise = myPeerConnection.createAnswer([options]);\n  ```\n\n**setLocakDescription**：\n\n- 基本格式\n\n  ```js\n  aPromise = myPc.setLocalDescription(sessionDescription);\n  ```\n\n**setRemoteDescription**：\n\n- 基本格式\n\n  ```js\n  aPromise = myPc.setRemoteDescription(sessionDescription);\n  ```\n\n**Track 方法**：\n\n- addTrack\n- removeTrack\n\n**addTrack**：\n\n- 基本格式\n\n  ```js\n  rtpSender = myPc.addTrack(track, stream...);\n  ```\n\n- Parameters\n\n  <img src=\"/images/imageWebRTC/addTrackParameters.png\">\n\n**removeTrack**：\n\n- 基本格式\n\n  ```js\n  myPc.remoteTrack(rtpSender);\n  ```\n\n**重要事件**：\n\n- onnegotiationneeded  - 协商的时候触发这个事件\n- onicecandidate - 当收到 ICE 候选者的时候触发这个事件\n\n### 1:1 连接的基本流程\n\n<img src=\"/images/imageWebRTC/端对端连接的基本流程.png\">\n\n**A 与 B 通信，大的方向分为三部分**：\n\n- 媒体协商部分\n- ICE 候选者的交换、连接、检测部分\n- 媒体数据流的通信部分\n\n### 【实战】WebRTC 视频传输\n\nTODO\n\n### 【实战】显示通讯双方的 SDP 内容\n\nTODO\n\n## WebRTC核心之SDP详解\n\n### 【协议规范】SDP 规范\n\n**SDP 规范**：\n\n- 会话层\n- 媒体层\n\n可以把会话层看做树根，媒体层看成树干。\n\n**会话层**：\n\n- 会话的名称与目的\n- 会话的存活时间\n- 会话中包含多个媒体信息\n\n**SDP 媒体信息**：\n\n- 媒体格式\n- 传输协议\n- 传输 IP 和 端口\n- 媒体负载类型\n\n**SDP 格式**：\n\n- 由多个 `<type>=<value>` 组成\n- 一个会话级描述\n- 多个媒体级描述\n\n**SDP 结构**：\n\n- Session Description\n- Time Description\n- Media Description\n\n<img src=\"/images/imageWebRTC/SessionDescription.png\">\n\n<img src=\"/images/imageWebRTC/TimeDescription.png\">\n\n<img src=\"/images/imageWebRTC/MediaDescription.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-01.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-02.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-03.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-04.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-05.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-06.png\">\n\n<img src=\"/images/imageWebRTC/字段含义-07.png\">\n\n### 【协议规范】WebRTC 中的 SDP\n\n<img src=\"/images/imageWebRTC/WebRTC中的SDP.png\">\n\n### 【详解】WebRTC 中 Offer_Answer SDP\n\n<details><summary>SDP报文内容</summary>\n\n```shell\nv=0\no=- 2584450093346841581 2 IN IP4 127.0.0.1\ns=-\nt=0 0\na=group:BUNDLE audio video data\na=msid-semantic: WMS 616cfbb1-33a3-4d8c-8275-a199d6005549\nm=audio 9 UDP/TLS/RTP/SAVPF 111 103 104 9 0 8 106 105 13 110 112 113 126\nc=IN IP4 0.0.0.0 \na=rtcp:9 IN IP4 0.0.0.0\na=ice-ufrag:sXJ3\na=ice-pwd:yEclOTrLg1gEubBFefOqtmyV\na=fingerprint:sha-256 22:14:B5:AF:66:12:C7:C7:8D:EF:4B:DE:40:25:ED:5D:8F:17:54:DD:88:33:C0:13:2E:FD:1A:FA:7E:7A:1B:79\na=setup:actpass\na=mid:audio\na=extmap:1 urn:ietf:params:rtp-hdrext:ssrc-audio-level\na=sendrecv\na=rtcp-mux\na=rtpmap:111 opus/48000/2\na=rtcp-fb:111 transport-cc\na=fmtp:111 minptime=10;useinbandfec=1\na=rtpmap:103 ISAC/16000\na=rtpmap:104 ISAC/32000\na=rtpmap:9 G722/8000\na=rtpmap:0 PCMU/8000\na=rtpmap:8 PCMA/8000\na=rtpmap:106 CN/32000\na=rtpmap:105 CN/16000\na=rtpmap:13 CN/8000\na=rtpmap:110 telephone-event/48000\na=rtpmap:112 telephone-event/32000\na=rtpmap:113 telephone-event/16000\na=rtpmap:126 telephone-event/8000\na=ssrc:120276603 cname:iSkJ2vn5cYYubTve\na=ssrc:120276603 msid:616cfbb1-33a3-4d8c-8275-a199d6005549 1da3d329-7399-4fe9-b20f-69606bebd363\na=ssrc:120276603 mslabel:616cfbb1-33a3-4d8c-8275-a199d6005549\na=ssrc:120276603 label:1da3d329-7399-4fe9-b20f-69606bebd363\nm=video 9 UDP/TLS/RTP/SAVPF 96 98 100 102 127 97 99 101 125\nc=IN IP4 0.0.0.0\na=rtcp:9 IN IP4 0.0.0.0\na=ice-ufrag:sXJ3\na=ice-pwd:yEclOTrLg1gEubBFefOqtmyV\na=fingerprint:sha-256 22:14:B5:AF:66:12:C7:C7:8D:EF:4B:DE:40:25:ED:5D:8F:17:54:DD:88:33:C0:13:2E:FD:1A:FA:7E:7A:1B:79\na=setup:actpass\na=mid:video\na=extmap:2 urn:ietf:params:rtp-hdrext:toffset\na=extmap:3 http://www.webrtc.org/experiments/rtp-hdrext/abs-send-time\na=extmap:4 urn:3gpp:video-orientation\na=extmap:5 http://www.ietf.org/id/draft-holmer-rmcat-transport-wide-cc-extensions-01\na=extmap:6 http://www.webrtc.org/experiments/rtp-hdrext/playout-delay\na=sendrecv\na=rtcp-mux\na=rtcp-rsize\na=rtpmap:96 VP8/90000\na=rtcp-fb:96 ccm fir\na=rtcp-fb:96 nack\na=rtcp-fb:96 nack pli\na=rtcp-fb:96 goog-remb\na=rtcp-fb:96 transport-cc\na=rtpmap:97 rtx/90000\na=fmtp:97 apt=96\na=rtpmap:98 VP9/90000\na=rtcp-fb:98 ccm fir\na=rtcp-fb:98 nack\na=rtcp-fb:98 nack pli\na=rtcp-fb:98 goog-remb\na=rtcp-fb:98 transport-cc\na=rtpmap:100 H264/90000\na=rtcp-fb:100 ccm fir\na=rtcp-fb:100 nack\na=rtcp-fb:100 nack pli\na=rtcp-fb:100 goog-remb\na=rtcp-fb:100 transport-cc\na=fmtp:100 level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=42e01f\na=rtpmap:102 red/90000\na=rtpmap:127 ulpfec/90000\na=rtpmap:97 rtx/90000\na=fmtp:97 apt=96\na=rtpmap:99 rtx/90000\na=fmtp:99 apt=98\na=rtpmap:101 rtx/90000\na=fmtp:101 apt=100\na=rtpmap:125 rtx/90000\na=fmtp:125 apt=102\na=ssrc-group:FID 2580761338 611523443\na=ssrc:2580761338 cname:iSkJ2vn5cYYubTve\na=ssrc:2580761338 msid:616cfbb1-33a3-4d8c-8275-a199d6005549 bf270496-a23e-47b5-b901-ef23096cd961\na=ssrc:2580761338 mslabel:616cfbb1-33a3-4d8c-8275-a199d6005549\na=ssrc:2580761338 label:bf270496-a23e-47b5-b901-ef23096cd961\na=ssrc:611523443 cname:iSkJ2vn5cYYubTve\na=ssrc:611523443 msid:616cfbb1-33a3-4d8c-8275-a199d6005549 bf270496-a23e-47b5-b901-ef23096cd961\na=ssrc:611523443 mslabel:616cfbb1-33a3-4d8c-8275-a199d6005549\na=ssrc:611523443 label:bf270496-a23e-47b5-b901-ef23096cd961\nm=application 9 DTLS/SCTP 5000\nc=IN IP4 0.0.0.0\na=ice-ufrag:sXJ3\na=ice-pwd:yEclOTrLg1gEubBFefOqtmyV\na=fingerprint:sha-256 22:14:B5:AF:66:12:C7:C7:8D:EF:4B:DE:40:25:ED:5D:8F:17:54:DD:88:33:C0:13:2E:FD:1A:FA:7E:7A:1B:79\na=setup:actpass\na=mid:data\na=sctpmap:5000 webrtc-datachannel 1024\n```\n\n</details>\n\n## 实现1V1音视频实时互动直播系统\n\n### STUN/TURN 服务器搭建\n\n> [webrtc进阶-信令篇-之三：信令、stun、turn、ice](<https://blog.csdn.net/fireroll/article/details/50780863>)\n>\n> [webRTC+coturn穿透服务器的安装与搭建](<https://blog.csdn.net/lamb7758/article/details/77045735>)\n>\n> [WebRTC的信令服务器Collider和Turn服务器搭建](<https://my.oschina.net/andywang1988/blog/848645>)\n>\n> [AppRTC(WebRTC)服务器搭建](<https://www.jianshu.com/p/a19441034f17>)\n\ncoTurn Download Address：<https://github.com/coturn/coturn>\n\nICE 测试地址：<https://webrtc.github.io/samples>\n\ncoturn 编译安装\n\n```shell\n$ git clone https://github.com/coturn/coturn \n$ cd coturn \n$ ./configure \n$ make \n$ make install\n```\n\n安装sqlite\n\n```shell\n$ sudo atp-get install sqlite\n```\n\n生成认证用户\n\n```shell\n$ turnadmin -A –u 用户名 -r beijing -p 密码 \n$ turnadmin -a –u 用户名 -r beijing -p 密码 \n# A 是添加管理员\n```\n\n然后生成md5码\n\n```shell\n$ turnadmin -k –u 用户名 -r beijing -p 密码\n```\n\n生成证书\n\n```shell\n$ openssl req -x509 -newkey rsa:2048 -keyout /etc/turn_server_pkey.pem -out /etc/turn_server_cert.pem -days 99999 -nodes \n# 一路回车就好\n```\n\n创建配置文件\n\n```shell\n$ vi /usr/local/etc/turnserver.conf \nlistening-device=eth1 \nrelay-device=eth1 \nlistening-port=3478 \nlistening-ip=YOU_IP \nlistening-ip=YOU_IP2 \n# (stun 需要两个公网ip，只有一个公网ip只能作文turn服务器) \ntls-listening-port=5349 \nlt-cred-mech \nmin-port=59000 \nmax-port=65000 \nrealm=beijing \nno-loopback-peers \nno-multicast-peers \nmobility \nno-cli \ncert=/etc/turn_server_cert.pem \npkey=/etc/turn_server_pkey.pem \nfingerprint \nstale-nonce=600\n```\n\n启动 coturn\n\n```shell\n# 启动 turn server\n$ turnserver -c /usr/local/coturn/etc/turnserver.conf\n```\n\n<img src=\"/images/imageWebRTC/STUNTURN服务器选型.png\">\n\n<img src=\"/images/imageWebRTC/coTurn服务器搭建与部署.png\">\n\n<img src=\"/images/imageWebRTC/coTurn服务器配置.png\">\n\n<img src=\"/images/imageWebRTC/测试turn服务.png\">\n\n### 【参数介绍】再论 RTCPeerConnection\n\n<img src=\"/images/imageWebRTC/RTCPeerConnection-01.png\">\n\n<img src=\"/images/imageWebRTC/Configurations-01.png\">\n\n<img src=\"/images/imageWebRTC/Configurations-02.png\">\n\n<img src=\"/images/imageWebRTC/Configurations-03.png\">\n\n<img src=\"/images/imageWebRTC/Configurations-04.png\">\n\n<img src=\"/images/imageWebRTC/addIceCandidate.png\">\n\n### 直播系统中的信令及其逻辑关系\n\n【实战】真正的音视频传输\n\n**客户端信令消息**：\n\n- join 加入房间\n- leave 离开房间\n- message 端到端消息\n\n**端到端信令消息**：\n\n- Offer 消息\n- Answer 消息\n- Candidate 消息\n\n**服务端信令消息**：\n\n- joined 已加入房间\n- otherjoin 其它用户加入房间\n- full 房间人数已满\n- leaved 已离开房间\n- bye 对方离开房间\n\n<img src=\"/images/imageWebRTC/直播系统消息处理流程.png\">\n\n### 实现 1：1 音视频实时互动信令服务器\n\n信令服务器改造\n\nTODO\n\n### 再论CreateOffer\n\n<img src=\"/images/imageWebRTC/createOffer.png\">\n\n**CreateOffer 实战**：\n\n- 接收远端音频\n- 接收远端视频\n- 静音检测\n- ICE restart\n\n### WebRTC 客户端状态机及处理逻辑\n\n直播客户端的实现：\n\n<img src=\"/images/imageWebRTC/客户端状态机.png\">\n\n<img src=\"/images/imageWebRTC/客户端流程图.png\">\n\n<img src=\"/images/imageWebRTC/客户端流程图-01.png\">\n\n<img src=\"/images/imageWebRTC/端对端连接的基本流程.png\">\n\n### WebRTC 客户端的实现\n\n<img src=\"/images/imageWebRTC/注意要点.png\">\n\n### 共享远程桌面\n\n<img src=\"/images/imageWebRTC/getDisplayMedia-01.png\">\n\n<img src=\"/images/imageWebRTC/需要注意的点.png\">\n\n## WebRTC核心之RTP媒体控制与数据统计\n\n### RTPPReceiver 发送器\n\nRTP Media 里边两个重要的类：Receiver 和 Sender\n\n<img src=\"/images/imageWebRTC/Receiver和Sender.png\">\n\nReceiver 和 Sender 共有的三个属性\n\n<img src=\"/images/imageWebRTC/RTCRtpSender属性.png\">\n\n<img src=\"/images/imageWebRTC/RTCRtpReceiver.png\">\n\n- getParameters  - 编解码器相关参数\n- getSynchronizationSources - 获取共享源（同步源），每一个媒体流都一个唯一的共享源\n- getContributingSources - 贡献来源，最主要用于混音和混频的情况\n- getStats - 获取统计信息\n- getCapabilities - 获取协商后的媒体能力\n\n### RTPSender 发送器\n\n<img src=\"/images/imageWebRTC/RTCRtpSender.png\">\n\n<img src=\"/images/imageWebRTC/RTPMedia.png\">\n\nRTCRtpTransceiver 可以同时处理 sender 和 receiver\n\n<img src=\"/images/imageWebRTC/RTCRtpTransceiver.png\">\n\n### 传输速率的控制\n\n<img src=\"/images/imageWebRTC/RTPMedia-01.png\">\n\n<img src=\"/images/imageWebRTC/chromeWebRTC-internals.png\">\n\nchrome WebRTC 状态查询地址：<chrome://webrtc-internals>\n\n### 【实战】WebRTC统计信息\n\nTODO\n\n## WebRTC非音视频数据传输\n\n### 传输非音视频数据基础知识\n\n<img src=\"/images/imageWebRTC/createDataChannel.png\">\n\n<img src=\"/images/imageWebRTC/option-01.png\">\n\n- ordered - udp包不保证包是有序的（传输非音视频数据的时候包是否是按顺序到达的），webrtc传输音视频的时候使用的是udp，webrtc在upd之上做了一层协议可以保证消息是按顺序到达（底层如果包是乱序的对包进行排序）\n- maxPacketLifeTime/maxRetransmits - 包存活时间（包最大的存活时间/最大的传输次数），这两个参数是二选一，不能同时使用\n\n<img src=\"/images/imageWebRTC/option-02.png\">\n\n- negotiated - 协商，在创建datachannel的时候可以进行协商\n\n<img src=\"/images/imageWebRTC/使用Options.png\">\n\n<img src=\"/images/imageWebRTC/DataChannel事件.png\">\n\n- onmessage - 当对方有数据过来的时候触发\n- onopen - 当创建好 datachannel 的时候触发\n\n<img src=\"/images/imageWebRTC/创建RTCDataChannel.png\">\n\n<img src=\"/images/imageWebRTC/非音视频数据传输方式.png\">\n\n- SCTP - stream control transport 流控\n- configurable - 可配置的\n\n- Reliability：可靠性\n- Delivery：可达性\n- Transmission：传输方式\n- Flow control：流控\n- Congestion control：拥塞控制\n\n### 端到端文本聊天\n\nTODO\n\n### 文件实时传输\n\n<img src=\"/images/imageWebRTC/知识点.png\">\n\n## WebRTC实时数据传输网络协议详解\n\n> [浅析TCP字节流与UDP数据报的区别](<https://blog.csdn.net/oshirdey/article/details/38467391>)\n\n**tcp是流式传输， 这是什么意思？** \n\n假设A给B通过TCP发了200字节， 然后又发了300字节， 此时B调用recv（设置预期接受1000个字节）， 那么请问B实际接受到多少字节？  根据我们之前讲得tcp粘包特性，可知， B端调用一次recv, 接受到的是500字节。\n\n所谓流式传输， 说白了， 就是管道中的水， 第一次给你发了200斤的水， 第二次给你发了300斤的水， 然后你在对端取的时候， 这200斤和300斤的水， 已经粘在一起了， 无法直接分割， 没有界限了。\n\n**udp是数据报传输，  什么意思？**  \n\n假设A给B通过udp发了200字节， 然后又发了300字节， 此时B调用recvfrom（设置预期接受1000个字节）， 那么请问B实际接受到多少字节？   写了个程序测了一下， 发现B调用recvfrom接收到的是200自己， 另外的300字节必须再次调用recvfrom来接收。\n\n所谓的数据报传输， 说白了， 就有消息和消息之间有天然的分割， 对端接收的时候， 不会出现粘包。 发10次， 就需要10次来接收。\n\n### 【协议规范】RTP-SRTP协议头详解\n\n<img src=\"/images/imageWebRTC/协议栈.png\">\n\n<img src=\"/images/imageWebRTC/传输协议.png\">\n\n- DTLS - 证书检测，加密算法协商\n\n<img src=\"/images/imageWebRTC/RTP协议.png\">\n\n- contributing source - 贡献源，CC - 表示贡献者一共有多少个（最多可以表示16个贡献者）\n\n<img src=\"/images/imageWebRTC/RTP字段说明.png\">\n\n> [RTP报文头中的SSRC和CSRC](<https://blog.csdn.net/zhushentian/article/details/79804742>)\n>\n> [RTP/RTCP协议详解](https://www.cnblogs.com/foxmin/archive/2012/03/13/2393349.html)\n>\n> [RTP 有效负载(载荷)类型，RTP Payload Type](https://www.cnblogs.com/x_wukong/p/6391611.html)\n\n- 最大的 RTP 包包含的字节是1400多字节，压缩后的 H264 帧也能达到 1M 多。一般帧 封包 后的最后一个包 就是 M 位\n- timestamp - 同一个帧的所有封包的 timestamp 是相同的，并且 seq number 是连续的。H264 内部有封包的起始位和结束位，根据这些标识就可以将多个封包组成一个完整的帧\n- SSRC - SSRC的作用就是贡献者，视频和音频的SSRC是完全不相同的。同一个视频的SSRC有可能发生变化（产生冲突会发生变化，因为SSRC是随机数）\n- CSRC - 贡献者\n\n### 【协议规范】RTCP 中的 SR 与 RR 报文\n\n<img src=\"/images/imageWebRTC/RTCP包.png\">\n\n<img src=\"/images/imageWebRTC/RTCPPayloadType.png\">\n\n- SDES - 中最重要的一个字段是 cname\n- FR - 请求关键帧\n- necho - 发现丢包重传\n\n<img src=\"/images/imageWebRTC/RTCPHeader.png\">\n\n<img src=\"/images/imageWebRTC/RTCPHeader说明.png\">\n\n<img src=\"/images/imageWebRTC/RTCPSenderReport.png\">\n\n<img src=\"/images/imageWebRTC/SenderInfomationBlock.png\">\n\n<img src=\"/images/imageWebRTC/SenderInfo说明.png\">\n\n- NTP - 不同源之间的同步，比如音频和视频之间的同步\n\n<img src=\"/images/imageWebRTC/ReportBlock.png\">\n\n<img src=\"/images/imageWebRTC/ReceiveReportBlock.png\">\n\n<img src=\"/images/imageWebRTC/RTCPReceiverReport.png\">\n\n<img src=\"/images/imageWebRTC/RTCPSR-RR发送时机.png\">\n\n<img src=\"/images/imageWebRTC/RTCPSDES.png\">\n\n<img src=\"/images/imageWebRTC/SDESitem.png\">\n\n<img src=\"/images/imageWebRTC/SDES说明.png\">\n\n- CNAME - 对于webrtc来说这个字段基本上是不用SDES这个消息，因为在SDP中就有CNAME的描述，除非音频源或者视频源发生了中断（or中转）会重新生成SSRC，然后再进行重新绑定\n\n<img src=\"/images/imageWebRTC/RTCPBYE.png\">\n\n<img src=\"/images/imageWebRTC/RTCPAPP.png\">\n\n<img src=\"/images/imageWebRTC/RTCPAPP字段说明.png\">\n\n### 【协议规范】DTSL\n\n<img src=\"/images/imageWebRTC/DTLS.png\">\n\n<img src=\"/images/imageWebRTC/SRTP.png\">\n\n### wireshark 分析 rtp-rtcp 包\n\nTODO\n\n## Android端与浏览器互通\n\n### Android 与浏览器互通\n\n<img src=\"/images/imageWebRTC/主要内容.png\">\n\n<img src=\"/images/imageWebRTC/需要权限.png\">\n\n<img src=\"/images/imageWebRTC/Android权限管理.png\">\n\n<img src=\"/images/imageWebRTC/引入库.png\">\n\n<img src=\"/images/imageWebRTC/信令处理.png\">\n\n<img src=\"/images/imageWebRTC/AndroidSocketio.png\">\n\n<img src=\"/images/imageWebRTC/socketio接收消息.png\">\n\n### WebRTCNative 开发逻辑\n\n<img src=\"/images/imageWebRTC/结构图.png\">\n\n<img src=\"/images/imageWebRTC/呼叫端时序图.png\">\n\n<img src=\"/images/imageWebRTC/被叫端时序图.png\">\n\n<img src=\"/images/imageWebRTC/关闭时序图.png\">\n\n<img src=\"/images/imageWebRTC/webrtc处理流程.png\">\n\n<img src=\"/images/imageWebRTC/重要类-01.png\">\n\n<img src=\"/images/imageWebRTC/重要类-02.png\">\n\n<img src=\"/images/imageWebRTC/两个观察者.png\">\n\n### 实战-权限申请-库的引入与界面\n\n<img src=\"/images/imageWebRTC/权限库界面.png\">\n\n### 实战-通过 socket.io 实现信令收发\n\n<img src=\"/images/imageWebRTC/收发信令.png\">\n\n### 实战-Android 与浏览器互通\n\n创建 PeerConnection：\n\n- 音视频数据采集\n- 创建 PeerConnection\n\n媒体能力协商：\n\n- 协商媒体能力\n- Candidate 连通\n- 视频渲染\n\n## iOS端与浏览器互通\n\n### IOS权限获取\n\n<img src=\"/images/imageWebRTC/主要内容-01.png\">\n\n<img src=\"/images/imageWebRTC/主要内容-02.png\">\n\n### IOS引入WebRTC库\n\n<img src=\"/images/imageWebRTC/引入WebRTC库的方式.png\">\n\n<img src=\"/images/imageWebRTC/引入WebRTC库.png\">\n\n<img src=\"/images/imageWebRTC/Podfile.png\">\n\n### IOS端SocketIO的使用\n\n<img src=\"/images/imageWebRTC/socketio的使用.png\">\n\n<img src=\"/images/imageWebRTC/连接服务器.png\">\n\n<img src=\"/images/imageWebRTC/发送消息.png\">\n\n<img src=\"/images/imageWebRTC/注册侦听消息.png\">\n\n### IOS界面布局\n\nTODO\n\n### IOS本地视频采集与展示\n\nTODO\n\n### IOS端RTCPeerConnection\n\nTODO\n\n### IOS媒体协商\n\n<img src=\"/images/imageWebRTC/媒体协商.png\">\n\n<img src=\"/images/imageWebRTC/信令时序图.png\">\n\n### IOS远端视频渲染\n\n<img src=\"/images/imageWebRTC/RTCPeerConnection委托.png\">\n\n## 课程总结\n\n<img src=\"/images/imageWebRTC/小结.png\">\n\n<img src=\"/images/imageWebRTC/信令服务器.png\">\n\n<img src=\"/images/imageWebRTC/JS客户端实现.png\">\n\n<img src=\"/images/imageWebRTC/JS客户端实现-01.png\">\n\n<img src=\"/images/imageWebRTC/进阶.png\">\n\n<img src=\"/images/imageWebRTC/进阶-01.png\">\n\n<img src=\"/images/imageWebRTC/行业痛点.png\">\n\n## 即时通信（IM）和实时通信（RTC）的区别\n\n即时通信（IM=Instant messaging）和实时通信（rtc=Real-time communication）都是一套网络通信系统，其本质都是对信息进行转发。其最大的不同点，是对信息传递的时间规定。二者的区别可以从以下几个方面：\n\n### 场景\n\n-  **即时通信**\n\n  常见场景包括文字聊天、语音消息发送、文件传输、音视频播放等。**通俗的说，就是发短信**。\n\n- **实时通信**\n\n  场景包括语音、视频电话会议、网络电话等。**通俗的说，就是打电话**。\n\n### 要求\n\n- **即时通讯**\n\n  主要要求可靠，考核送达率。要是你发一条短信，结果丢了，对方没收到！你再也不相信短信了吧。\n\n- **实时通信**\n\n  主要要求低延时和接通率。\n\n  - **低延时**：你打一通电话，每说一句话，对方得几秒钟才有回应，这电话你也讲不下去了吧。\n  - **接通率**：你打电话，你这边听到接通了，实际上对方的手机毫无反应，这实际上就没接通。\n\n### 技术环节\n\n- **即时通信**\n\n  消息发送和确认，【消息接入端、服务端消息逻辑处理，服务端消息缓存和存储，转发，服务端用户状态管理，心跳机制，消息发送端】、消息接收和确认。\n\n- **实时通信**\n\n  技术环节：采集、前处理、编码、【服务端接入、转发、服务端接入】、解码、播放和渲染。 \n\n这些技术环节重合的部分是：**信息转发**。\n\n### 传输协议\n\n公共互联网上，最常用的通信协议有TCP、UDP。\n\n- **TCP**：Transmission Control Protocol，传输控制协议是基于连接的协议，也就是说，在正式收发数据前，必须和对方建立可靠的连接。**有延迟不可控的特点**。\n- **UDP**：User Data Protocol，用户数据报协议，是与TCP相对应的协议。它是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发送过去。 **存在丢包、抖动、延迟的特征**。\n\n即时通信系统为了保证连接的可靠性，最常用的是TCP协议或者类TCP连接协议。这类协议的特点是追求连接的可靠性，而造成了延迟的不可控性，超过2秒的延迟响应是常态，甚至几十分钟的延迟响应，而电信级的实时通信标准是400ms，而基于互联网的实时通信需要另辟蹊径，开创出新的传输解决方案。发短信，延迟几秒钟送达，对使用者影响不大。\n\n实时通信，一般采用 UDP 作为基础传输协议。在设计低延时的实时通信服务时，UDP 表现要比 TCP 好得多。这是因为实时通信中，低时延比可靠性更重要。打电话，几秒的延迟是不能忍受的。\n\nTCP协议封装了消息的重传机制，在丢包的情况下，采用TCP协议的应用程序几乎无法优化这个重传机制，来达到低时延的效果。特别是在移动互联网络中，超过30%丢包时，TCP 的延时可以到几十分钟, 超过 50%丢包时，甚至很容易断开。 在同样丢包30%的链路上，UDP还可以传输数据，TCP就无法进行实时通信了。\n\n### 成本\n\n成本涉及到的环节有：**服务端接入、存储和转发**。\n\n二者成本会产生差异的环节有：\n\n- 从服务端接入方式来看，即时通信采用TCP协议来保证可靠性，可能会建立多个连接，相比无连接的UDP传输方式，这是一种昂贵的传输方式。实时通信可以基于UDP协议，与服务端建立灵活的、快速的接入机制。\n- 存储方面，实时通信在服务端是实时转发，不会在服务端存储数据，而即时消息系统一般会将缓存转为存储数据，包括富媒体数据，会占用大量的存储空间，产生更多的存储成本。\n- 从成本上来看，传输同样信息量的数据，基于TCP的即时通信方式，更侧重于可靠性，会优先采用多线机房的传输方式，成本比较高；\n- 而基于UDP的实时通信方式，会优先选取最优路径进行传输数据，并可以动态调整传输路径，这样能够高效的利用带宽，提高传输效率，降低成本。\n\n### 可用的解决方案\n\n- **即时通信**：XMPP，MQTT\n- **实时通信**：WebRTC、Tokbox\n\n> 免费的 im 与 rtc 示例：<https://github.com/starrtc/android-demo>\n\n## Reference\n\n> [JavaScript 是如何工作的:WebRTC 和对等网络的机制！](<https://juejin.im/post/5c511219e51d45522c3066c9>)\n>\n> [深入理解WebRTC](https://segmentfault.com/a/1190000011403597)\n>\n> [WebRTC架构简介](https://blog.csdn.net/fishmai/article/details/69681595)\n>\n> [HTTPS证书生成原理和部署细节](<https://www.barretlee.com/blog/2015/10/05/how-to-build-a-https-server/>)\n>\n> [SSL证书生成流程](<https://www.jianshu.com/p/9091ebd439a0>)\n\n> [Gradle官网](<https://gradle.org/install/>)\n>\n> [Gradle 包](<http://tools.android-studio.org/index.php/9-tools/109-android-tools-download>)\n>\n> [Mac下AndroidStudio中手动配置Gradle](<https://www.jianshu.com/p/36e569c1bb12>)\n\n> [WebRTC的拥塞控制和带宽策略](<https://mp.weixin.qq.com/s?__biz=MzU1NTEzOTM5Mw==&mid=2247485966&idx=1&sn=113920ae7c4f908f4576fbcc1484781a&chksm=fbd9a220ccae2b36aa8796f02f37da4e19748a42635a5f8ad72554dd05ab97fe094e28227905&mpshare=1&scene=1&srcid=05292tFBxW7U2DRJaaSvqwG2#rd>)\n>\n> [小议WebRTC拥塞控制算法：GCC介绍](http://yunxin.163.com/blog/video18-0905/)\n>\n> [***详读webrtc的视频统计信息之延迟、抖动与丢包](<https://www.servercoder.com/2019/04/24/webrtc-statistics/>)\n\n### socket.io 原理详解\n\n> [socket.io 原理详解](<https://blog.csdn.net/u013243347/article/details/86669988>)\n\n### 音视频数据采集与控制\n\n> [WebRTC音频引擎实现分析](<https://www.jianshu.com/p/5a8a91cd84ef>)\n>\n> [WebRTC手记之本地音频采集](https://www.cnblogs.com/fangkm/p/4374668.html)\n>\n> [WebRTC音视频同步分析](https://blog.csdn.net/lincaig/article/details/81209895#%E5%9B%9B%E3%80%81WebRTC%E9%9F%B3%E8%A7%86%E9%A2%91%E5%90%8C%E6%AD%A5%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90)\n>\n> [webrtc数据接收、解码、渲染等接口调用流程图](<https://blog.csdn.net/lincaig/article/details/86500656>)\n>\n> [WebRTC视频采集和编码过程](<https://blog.csdn.net/lincaig/article/details/79587417>)\n>\n> [H264中4x4、8x8和16x16尺寸对应场景](<https://blog.csdn.net/lincaig/article/details/84594764>)\n\n### NAT 穿越\n\n> [ICE协议下NAT穿越的实现（STUN&TURN）](<https://www.jianshu.com/p/84e8c78ca61d>)\n>\n> [WebRTC中NAT穿透浅析](https://www.cnblogs.com/xiaofengfengzhang/p/6590687.html)\n\n### ICE 原理\n\n> [（转）NAT与NAT穿越学习总结--ICE过程讲的不错](https://www.cnblogs.com/wangle1001986/p/6083871.html)\n>\n> [ICE 原理学习](<https://blog.csdn.net/voipmaker/article/details/8453702>)\n\n### 媒体协商过程\n\n> [完整WebRTC技术及应用概要](<http://www.ctiforum.com/news/guandian/549857.html>)\n>\n> [WebRTC SDP协议](<https://blog.csdn.net/qq_21358401/article/details/79341031>)\n>\n> [WebRTC 中的 SDP 协议](<https://www.jianshu.com/p/026c7ef271cb>)\n\n### 网络处理与流程\n\n> [WEBRTC 视频接收原理及流程](<https://blog.csdn.net/doitsjz/article/details/52022642>)\n>\n> [webrtc(11) 数据接收——总流程](<https://blog.csdn.net/NB_vol_1/article/details/82119450>)\n>\n> [WebRTC 是如何进行通信的，WebRCT 的三种网络结构 | 野狗 WebRTC 专栏](<https://blog.wilddog.com/?p=2196>)\n>\n> [WebRTC 音视频引擎研究 -- 整体架构分析](<http://ngudream.com/2016/01/18/webrtc-structure/>)\n\n### 如何实现并规模高并发\n\n> [即构自研WebRTC网关服务器架构实践](<https://juejin.im/post/5b568f7a6fb9a04fad3a1b9f>)\n>\n> [WebRTC、标准测试、大前端技术、实时网络质量……RTC2018帮你“划重点”！](<https://new.qq.com/omn/20180909/20180909A10XFQ.html>)\n\n### 如何更好的利用网络\n\n> [WebRTC 点对点直播](<https://cloud.tencent.com/developer/article/1004661>)\n\n### 回音消除与降噪\n\n> [WebRTC音频降噪使用](<https://blog.csdn.net/tanningzhong/article/details/88664338>)\n>\n> [webrtc--AudioProcessing-- 音频降噪的处理过程](<http://www.voidcn.com/article/p-wcltdqbt-bck.html>)\n>\n> [PCM音频处理——使用WebRTC音频降噪模块与其他方式的对比](<https://www.jianshu.com/p/77a363960711>)\n>\n> [音频降噪在 58 直播中的研究与实现](<https://www.infoq.cn/article/QOp4IOao_DJJ6eNsIOXp>)\n>\n> [单独编译和使用webrtc音频回声消除模块(附完整源码+测试音频文件)](<https://www.cnblogs.com/mod109/p/5469799.html>)\n>\n> [借助Webrtc音频采集、降噪、回音消除、静音检测、编解码、播放等功能，剥离Webrtc自带的RTP传输协议，使用私有的协议进行传输。从而实现自己的实时语音聊天功能。](<https://gitee.com/icedbeer/Webrtc_Audio>)\n>\n> [webrtc学习2-音频预处理模块](http://www.cclk.cc/2017/08/14/webrtc/webrtc%E5%AD%A6%E4%B9%A02-%E9%9F%B3%E9%A2%91%E9%A2%84%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97/)\n\n### OpenCV\n\nTODO\n\n### OpenGL视频渲染\n\nTODO\n\nWebRTC的分层协议图：\n\n<img src=\"/images/imageWebRTC/webrtc分层协议图.png\">\n\n信令，会话和协议：\n\n<img src=\"/images/imageWebRTC/信令会话协议.png\">\n\n## 问题解决里程\n\nnode 启动 server 报错：\n\n```shell\nevents.js:141\n      throw er; // Unhandled 'error' event\n      ^\n\nError: listen EACCES 0.0.0.0:443\n    at Object.exports._errnoException (util.js:870:11)\n    at exports._exceptionWithHostPort (util.js:893:20)\n    at Server._listen2 (net.js:1224:19)\n    at listen (net.js:1273:10)\n    at net.js:1382:9\n    at nextTickCallbackWith3Args (node.js:452:9)\n    at process._tickCallback (node.js:358:17)\n    at Function.Module.runMain (module.js:444:11)\n    at startup (node.js:136:18)\n    at node.js:966:3\n[Solve]$ sudo setcap 'cap_net_bind_service=+ep' $(readlink -f $(which node))\n```\n\n查询端口是否别占用：\n\n```shell\n# Linux\n$ netstat -ntpl | grep 443\n# MacOS\n# lsof 通过list open file命令可以查看到当前打开文件，在linux中所有事物都是以文件形式存在，包括网络连接及硬件设备。\n$ sudo lsof -i:80  // sudo lsof -i tcp:80\n# -i 参数表示网络链接，:80 指明端口号，该命令会同时列出 PID\n$ sudo lsof -i -P | grep -i \"listen\"\n# 查看所有进程监听的端口\n```\n\n## 补充\n\n![1V1互动直播通讯过程](/images/imageWebRTC/supplement/1V1互动直播通讯过程.png)\n\n![知识框架介绍](/images/imageWebRTC/supplement/TIM-01.jpg)\n\n![知识框架介绍](/images/imageWebRTC/supplement/TIM-02.jpg)\n\n![WebRTC-ICE Candidate Exchange](/images/imageWebRTC/supplement/WebRTC-ICECandidateExchange.svg)\n\n![WebRTC-Signaling Diagram](/images/imageWebRTC/supplement/WebRTC-SignalingDiagram.svg)\n\n<details><summary>turnserver.conf</summary>\n\n```shell\n# RFC5766-TURN-SERVER configuration file\n# RFC5766-TURN-SERVER配置文件\n#\n# Boolean values note: where boolean value is supposed to be used,\n# you can use '0', 'off', 'no', 'false', 'f' as 'false,\n# and you can use '1', 'on', 'yes', 'true', 't' as 'true'\n# If the value is missed, then it means 'true'.\n#\n# 布尔值注意: 布尔值应该被使用,\n# 您可以使用'0', 'off', 'no', 'false', 'f' 相当于 'false,\n# 还有你可以用'1', 'on', 'yes', 'true', 't' 相当于 'true'\n# 如果没有值，相当于'true'.\n#\n\n# Listener interface device (optional, Linux only).\n# NOT RECOMMENDED.\n#\n# 侦听器接口设备(仅可选,Linux)。\n# 不推荐。\n#\nlistening-device=eth0\n\n# TURN listener port for UDP and TCP (Default: 3478).\n# Note: actually, TLS & DTLS sessions can connect to the\n# \"plain\" TCP & UDP port(s), too - if allowed by configuration.\n#\n# TURN为UDP和TCP的侦听器端口(默认: 3478)。\n# 注:实际上,TLS和DTLS会话可以连接到\"清晰的\"TCP和UDP端口,——如果允许配置。\n#\n#listening-port=3478\nlistening-port=3478\n\n# TURN listener port for TLS (Default: 5349).\n# Note: actually, \"plain\" TCP & UDP sessions can connect to the TLS & DTLS\n# port(s), too - if allowed by configuration. The TURN server\n# \"automatically\" recognizes the type of traffic. Actually, two listening\n# endpoints (the \"plain\" one and the \"tls\" one) are equivalent in terms of\n# functionality; but we keep both endpoints to satisfy the RFC 5766 specs.\n# For secure TCP connections, we currently support SSL version 3 and\n# TLS version 1.0, 1.1 and 1.2. SSL2 \"encapculation mode\" is also supported.\n# For secure UDP connections, we support DTLS version 1.\n#\n# TURN为TLS的侦听器端口(默认: 5349)。\n# 注意:事实上,\"清晰的\"TCP和UDP会话可以连接到TLS和DTLS端口，如果允许配置。\n# TURN服务器\"自动\"识别传输类型。实际上,两个监听终端点(\"清晰的\"端和\"TLS\"端)是\n# 对等的功能;但我们保持两个端点来满足RFC 5766规范。\n# 对于安全的TCP连接,我们目前支持SSL的3个版本，是TLS 1.0版本,1.1版本和1.2版本。\n# SSL2还支持\"encapculation模式\"。对于安全的UDP连接,我们支持DTLS版本1。\n#\n#tls-listening-port=5349\ntls-listening-port=5349\n\n# Alternative listening port for UDP and TCP listeners;\n# default (or zero) value means \"listening port plus one\".\n# This is needed for RFC 5780 support\n# (STUN extension specs, NAT behavior discovery). The TURN Server\n# supports RFC 5780 only if it is started with more than one\n# listening IP address of the same family (IPv4 or IPv6).\n# RFC 5780 is supported only by UDP protocol, other protocols\n# are listening to that endpoint only for \"symmetry\".\n#\n# 选择UDP和TCP监听器监听端口;\n# 默认(或者0)是表示监听的端口加1.\n# 这是必须的，为了RFC 5780的支持(STUN的扩展规范, NAT后端的发现)。\n# TURN服务器支持RFC 5780只有启动与多个监听同一族的IP地址(IPv4或IPv6).\n# RFC 5780只有UDP协议,支持其他协议是监听\"对称\"型端口的。\n#\n#alt-listening-port=0\n                                   \n# Alternative listening port for TLS and DTLS protocols.\n# Default (or zero) value means \"TLS listening port plus one\".\n#\n# 选择监听端口TLS和DTLS协议。\n# 默认(或者0)是表示TLS监听的端口加1.\n#\n#alt-tls-listening-port=0\n    \n# Listener IP address of relay server. Multiple listeners can be specified.\n# If no IP(s) specified in the config file or in the command line options,\n# then all IPv4 and IPv6 system IPs will be used for listening.\n#\n# 侦听器中继服务器的IP地址。可以指定多个侦听器。\n# 如果没有在配置文件或者命令选项中指定监听的IP，\n# 那么所有的IPv4和IPv6所有的IP将被监听\n#\nlistening-ip=172.26.7.151\n\n# Auxiliary STUN/TURN server listening endpoint.\n# Aux servers have almost full TURN and STUN functionality.\n# The (minor) limitations are:\n# 1) Auxiliary servers do not have alternative ports and\n# they do not support STUN RFC 5780 functionality (CHANGE REQUEST).\n# 2) Auxiliary servers also are never returning ALTERNATIVE-SERVER reply.\n# Valid formats are 1.2.3.4:5555 for IPv4 and [1:2::3:4]:5555 for IPv6.\n# There may be multiple aux-server options, each will be used for listening\n# to client requests.\n#\n# 辅助STUN/TURN服务器监听端口。\n# 辅助服务器几乎有齐TURN和STUN功能\n# (一些)局限性:\n# 1) 辅助服务器没有替代的端口并且他们不支持STUN RFC 5780功能(变更请求)。\n# 2) 辅助服务器也不会返回ALTERNATIVE-SERVER回复。\n# 有效格式，IPv4的1.2.3.4:5555 和IPv6的[1:2::3:4]:5555。\n# 可能会有多个aux-server选项,每个将用于监听客户端请求。\n#\n#aux-server=172.17.19.110:33478\n#aux-server=[2607:f0d0:1002:51::4]:33478\n\n# (recommended for older Linuxes only)\n# Automatically balance UDP traffic over auxiliary servers (if configured).\n# The load balancing is using the ALTERNATE-SERVER mechanism.\n# The TURN client must support 300 ALTERNATE-SERVER response for this\n# functionality.\n#\n# (仅推荐老的Linuxes)\n# 在辅助服务器自动均衡UDP流量(如果配置)。\n# 使用ALTERNATE-SERVER的负载均衡机制。\n# TURN客户端必须支持300个ALTERNATE-SERVER响应。\n#\n#udp-self-balance\n\n# Relay interface device for relay sockets (optional, Linux only).\n# NOT RECOMMENDED.\n#\n# 终极接口设备为中继套接字(可选, 仅Linux).\n# 不推荐。\n#\n#relay-device=eth1\n\n# Relay address (the local IP address that will be used to relay the\n# packets to the peer).\n# Multiple relay addresses may be used.\n# The same IP(s) can be used as both listening IP(s) and relay IP(s).\n# If no relay IP(s) specified, then the turnserver will apply the default\n# policy: it will decide itself which relay addresses to be used, and it\n# will always be using the client socket IP address as the relay IP address\n# of the TURN session (if the requested relay address family is the same\n# as the family of the client socket).\n#\n# 中继地址(本地IP地址将用于传递数据包的给每个端)\n# 可以使用多个中继地址。\n# 相同的IP可以用作监听IP和继电器IP。\n# 如果没有指定中继IP，那么turnserver将应用默认策略：它将自行决定使用那个中继\n# 地址，并且它总是会使用客户端套接字的IP地址作为中继的IP地址在TURN会话中(如果\n# 请求的中继地址族解决同族的客户端套接字)。\n#\n#relay-ip=172.17.19.105\n#relay-ip=2607:f0d0:1002:51::5\nrelay-ip=172.26.7.151\n\n# For Amazon EC2 users:#\n# TURN Server public/private address mapping, if the server is behind NAT.\n# In that situation, if a -X is used in form \"-X <ip>\" then that ip will be reported\n# as relay IP address of all allocations. This scenario works only in a simple case\n# when one single relay address is be used, and no RFC5780 functionality is required.\n# That single relay address must be mapped by NAT to the 'external' IP.\n# The \"external-ip\" value, if not empty, is returned in XOR-RELAYED-ADDRESS field.\n# For that 'external' IP, NAT must forward ports directly (relayed port 12345\n# must be always mapped to the same 'external' port 12345).\n# In more complex case when more than one IP address is involved,\n# that option must be used several times, each entry must\n# have form \"-X <public-ip/private-ip>\", to map all involved addresses.\n# RFC5780 NAT discovery STUN functionality will work correctly,\n# if the addresses are mapped properly, even when the TURN server itself\n# is behind A NAT.\n# By default, this value is empty, and no address mapping is used.\n#\n# Amazon EC2用户:\n# TURN服务器公开/私有的地址映射，假如服务器是在NAT后端。\n# 在这种情况下,如果一个表单中\"-X <ip>\"使用一个-X，然后该ip将被作为中继ip地址来使用。\n# 这种情况只适用于一个简单的例子,当一个中继的地址是被使用,和没有RFC5780功能是必需的。\n# 单个中继地址必须通过NAT映射到外部的IP。\n# 外部的IP值，假如不为空，通过XOR-RELAYED-ADDRESS字段返回。\n# 外部的IP,NAT必须直接转发端口(转发端口12345，必须总是映射到相同的外部端口12345)。\n# 在更复杂的情况下,当涉及到多个IP地址,这个选项必须使用几次,每个条目必须形\n# 成\"-X <public-ip/private-ip>\",将所有涉及到的地址。\n# RFC5780 NAT发现STUN功能正常工作，如果正确的地址映射,即使TURN服务器本身是\n# 在一个NAT后。\n# 默认，该值为空，并且没有使用地址映射。\n#\n#external-ip=60.70.80.91\n#\n#OR:\n#\n#external-ip=60.70.80.91/172.17.19.101\n#external-ip=60.70.80.92/172.17.19.102\nexternal-ip=47.92.119.215\n\n\n# Number of relay threads to handle the established connections\n# (in addition to authentication thread and the listener thread).\n# If set to 0 then application runs relay process in a single thread,\n# in the same thread with the listener process (the authentication thread will\n# still be a separate thread).\n# In the older systems (Linux kernel before 3.9),\n# the number of UDP threads is always one thread per network listening endpoint -\n# including the auxiliary endpoints - unless 0 (zero) or 1 (one) value is set.\n#\n# 数量的中继线程处理建立连接(除了验证线程和侦听器线程)。\n# 如果设置为0,那么应用程序中继进程在一个线程中运行,在同一\n# 个线程中监听处理(身份验证线程仍然是一个单独的线程)。\n# 在旧系统(3.9 Linux内核之前),数量的UDP线程总是一个线程监听一个网络端点,包括辅助端点——除非设置0或1值。\n#\n#relay-threads=0\nrelay-threads=10\n\n# Lower and upper bounds of the UDP relay endpoints:\n# (default values are 49152 and 65535)\n#\n# UDP中继端点的上下边界:\n# (默认是49152至65535)\n#\nmin-port=49152\nmax-port=65535\n    \n# Uncomment to run TURN server in 'normal' 'moderate' verbose mode.\n# By default the verbose mode is off.\n#\n# 取消TURN服务器运行'normal' 'moderate'详细模式。\n# 默认情况下,详细模式是关闭的。\n#\n#verbose\n    \n# Uncomment to run TURN server in 'extra' verbose mode.\n# This mode is very annoying and produces lots of output.\n# Not recommended under any normal circumstances.\n#\n# 取消TURN服务器运行'extra'详细模式。\n# 这种模式是非常恼人的,产生大量的输出。\n# 在任何正常情况下不建议。\n#\n#Verbose\n\n# Uncomment to use fingerprints in the TURN messages.\n# By default the fingerprints are off.\n#\n# 取消在TURN消息中使用指纹。\n# 默认情况下,指纹是关闭的。\n#\n#fingerprint\n\n# Uncomment to use long-term credential mechanism.\n# By default no credentials mechanism is used (any user allowed).\n# This option can be used with either flat file user database or\n# PostgreSQL DB or MySQL DB or Redis DB for user keys storage.\n#\n# 取消使用长期证书机制。\n# 默认情况下不使用凭证机制(允许任何用户)。\n# 这个选项可能使用用户数据文件或PostgreSQL或MySQL或Redis来存储用户密钥。\n#\n#lt-cred-mech\nlt-cred-mech\n\n# Uncomment to use short-term credential mechanism.\n# By default no credentials mechanism is used (any user allowed).\n# For short-term credential mechanism you have to use PostgreSQL or\n# MySQL or Redis database for user password storage.\n#\n# 取消使用短期证书机制。\n# 默认情况下不使用凭证机制(允许任何用户)。\n# 短期证书机制必须使用PostgreSQL或MySQL或Redis数据库来存储用户密码。\n#\n#st-cred-mech\n\n# This option is opposite to lt-cred-mech or st-cred-mech.\n# (TURN Server with no-auth option allows anonymous access).\n# If neither option is defined, and no users are defined,\n# then no-auth is default. If at least one user is defined,\n# in this file or in command line or in usersdb file, then\n# lt-cred-mech is default.\n#\n# 这个选项是lt-cred-mech或st-cred-mech相反。\n# (TURN服务器no-auth选项允许匿名访问)。\n# 如果没有选项定义,没有用户定义,那么no-auth默认。\n# 如果至少定义有一个用户,在这个文件中或在命令行或usersdb文件,\n# 那么lt-cred-mech默认。\n#\n#no-auth\n\n# TURN REST API flag.\n# Flag that sets a special authorization option that is based upon authentication secret.\n# This feature can be used with the long-term authentication mechanism, only.\n# This feature purpose is to support \"TURN Server REST API\", see\n# \"TURN REST API\" link in the project's page\n# http://code.google.com/p/rfc5766-turn-server/.\n# This option is used with timestamp:\n# usercombo -> \"timestamp:userid\"\n# turn user -> usercombo\n# turn password -> base64(hmac(secret key, usercombo))\n# This allows TURN credentials to be accounted for a specific user id.\n# If you don't have a suitable id, the timestamp alone can be used.\n# This option is just turning on secret-based authentication.\n# The actual value of the secret is defined either by option static-auth-secret,\n# or can be found in the turn_secret table in the database (see below).\n#\n# TURN REST API标志。\n# 标志是设置一个特殊的授权选项,是基于身份验证的私密。\n# 这个功能可以用于长期验证机制。\n# 这个功能的目的是支持\"TURN Server REST API\",看到\"TURN Server REST API\"项目的页面的链接\n# http://code.google.com/p/rfc5766-turn-server/。\n# 这个选项是使用时间戳:\n# usercombo -> \"timestamp:userid\"\n# turn user -> usercombo\n# turn password -> base64(hmac(secret key, usercombo))\n# 这允许TURN凭证占用一个特定的用户id。\n# 如果你没有一个合适的id,可以使用单独的时间戳。\n# 这个选项只是打开基于私密的身份验证。\n# 实际值定义的私密就是通过选择static-auth-secret,或可以在数据库中找到turn_secret表(见下文)。\n#\nuse-auth-secret\n\n# 'Static' authentication secret value (a string) for TURN REST API only.\n# If not set, then the turn server\n# will try to use the 'dynamic' value in turn_secret table\n# in user database (if present). The database-stored  value can be changed on-the-fly\n# by a separate program, so this is why that other mode is 'dynamic'.\n#\n# TURN REST API的'Static'身份验证的私密值(字符串)\n# 如果没有设置，那么turn服务器将尝试使用'dynamic'值在用户数据库的turn_secret表(如果存在)。\n# 数据库存储的值可以随时改变，通过单独的程序，所以这就是'dynamic'模式。\n#\nstatic-auth-secret=ling \n\n# 'Static' user accounts for long term credentials mechanism, only.\n# This option cannot be used with TURN REST API or with short-term credentials\n# mechanism.\n# 'Static' user accounts are NOT dynamically checked by the turnserver process,\n# so that they can NOT be changed while the turnserver is running.\n#\n# 'Static'用户长期占凭证机制。\n# 这个选项不能用于TURN REST API或短期凭证机制。\n# 'Static'用户帐户不是turnserver程序动态检查,所以他们不能改变在turnserver运行时。\n#\n#user=username1:key1\n#user=username2:key2\n# OR:\n#user=username1:password1\n#user=username2:password2\n#\n# Keys must be generated by turnadmin utility. The key value depends\n# on user name, realm, and password:\n#\n# 钥匙必须由turnadmin实用程序生成。键值取决于用户名称、领域和密码:\n#\n# Example:\n# 例子,使用以下命令:\n#\n# $ turnadmin -k -u ninefingers -r north.gov -p youhavetoberealistic\n#\n# Output: 0xbc807ee29df3c9ffa736523fb2c4e8ee\n# 输出是: 0xbc807ee29df3c9ffa736523fb2c4e8ee\n#\n# ('0x' in the beginning of the key is what differentiates the key from\n# password. If it has 0x then it is a key, otherwise it is a password).\n# ('0x'开始的关键是区分从密码的关键。如果它有0x,那么它是一个关键,否则这是一个密码)。\n#\n# The corresponding user account entry in the config file will be:\n# 相应的配置文件中的用户帐户条目将:\n#\n#user=ninefingers:0xbc807ee29df3c9ffa736523fb2c4e8ee\n# Or, equivalently, with open clear password (less secure):\n#或者是这样，明文密码(不安全的):\nuser=ling:0xc93be9d24fb0eab034f539c855528a26\n#\nuser=ling:ling1234\n\n# 'Dynamic' user accounts database file name.\n# Only users for long-term mechanism can be stored in a flat file,\n# short-term mechanism will not work with option, the short-term\n# mechanism required PostgreSQL or MySQL or Redis database.\n# 'Dynamic' long-term user accounts are dynamically checked by the turnserver process,\n# so that they can be changed while the turnserver is running.\n# Default file name is turnuserdb.conf.\n#\n# 'Dynamic'用户帐户数据库文件名。\n# 只有用户长期机制可以存储在一个文件,短期机制不会处理选项,短期机制需要PostgreSQL或MySQL或\n# Redis数据库。\n# 'Dynamic'的长期用户帐户在turnserver程序中动态检查的,这样他们可以改变的在turnserver运行时。\n# 默认文件名是turnuserdb.conf.\n#\n#userdb=/usr/local/etc/turnuserdb.conf\nuserdb=/etc/turnuserdb.conf\n\n# PostgreSQL database connection string in the case that we are using PostgreSQL\n# as the user database.\n# This database can be used for long-term and short-term credential mechanisms\n# and it can store the secret value for secret-based timed authentication in TURN RESP API.\n# See http://www.postgresql.org/docs/8.4/static/libpq-connect.html for 8.x PostgreSQL\n# versions connection string format, see\n# http://www.postgresql.org/docs/9.2/static/libpq-connect.html#LIBPQ-CONNSTRING\n# for 9.x and newer connection string formats.\n#\n# PostgreSQL数据库连接字符串,使用PostgreSQL作为用户数据库。\n# 该数据库可用于长期和短期证书机制,它可以存储的私密值，为基于私密身份验证的在TURN RESP API中。\n# 8.x PostgreSQL版本请参见http://www.postgresql.org/docs/8.4/static/libpq-connect.html的连接字符串\n# 格式,9.x和更新的请参阅http://www.postgresql.org/docs/9.2/static/libpq-connect.html LIBPQ-CONNSTRING\n# 的连接字符串格式。\n#\n#psql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> connect_timeout=30\"\n\n# MySQL database connection string in the case that we are using MySQL\n# as the user database.\n# This database can be used for long-term and short-term credential mechanisms\n# and it can store the secret value for secret-based timed authentication in TURN RESP API.\n# Use string format as below (space separated parameters, all optional):\n#\n# MySQL数据库连接字符串,使用MySQL作为用户数据库。\n# 该数据库可用于长期和短期证书机制,它可以存储的私密值，为基于私密身份验证的在TURN RESP API中。\n# 使用字符串格式如下(空间分离参数,所有可选):\n#\n#mysql-userdb=\"host=<host> dbname=<database-name> user=<database-user> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Redis database connection string in the case that we are using Redis\n# as the user database.\n# This database can be used for long-term and short-term credential mechanisms\n# and it can store the secret value for secret-based timed authentication in TURN RESP API.\n# Use string format as below (space separated parameters, all optional):\n#\n# Redis数据库连接字符串,使用Redis作为用户数据库。\n# 该数据库可用于长期和短期证书机制,它可以存储的私密值，为基于私密身份验证的在TURN RESP API中。\n# 使用字符串格式如下(空间分离参数,所有可选):\n#\n#redis-userdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Redis status and statistics database connection string, if used (default - empty, no Redis stats DB used).\n# This database keeps allocations status information, and it can be also used for publishing\n# and delivering traffic and allocation event notifications.\n# The connection string has the same parameters as redis-userdb connection string.\n# Use string format as below (space separated parameters, all optional):\n#\n# Redis状态和统计数据库连接字符串,如果使用(默认空,没有Redis统计数据库使用)。\n# 这个数据库保持分配状态信息,它也可以用于发布和交付传输和分配事件通知。\n# 连接字符串有相同的参数作为redis-userdb连接字符串。\n# 使用字符串格式如下(空间分离参数,所有可选):\n#\n#redis-statsdb=\"ip=<ip-address> dbname=<database-number> password=<database-user-password> port=<port> connect_timeout=<seconds>\"\n\n# Realm for long-term credentials mechanism and for TURN REST API.\n#\n# TURN REST API的长期凭证机制范围。\n#\n#realm=mycompany.org\n\n# Per-user allocation quota.\n# default value is 0 (no quota, unlimited number of sessions per user).\n#\n# 每个用户分配配额。\n# 默认值为0(没有配额,每个用户无限数量的会话)。\n#\n#user-quota=0\n\n# Total allocation quota.\n# default value is 0 (no quota).\n#\n# 总分配配额。\n# 默认值为0(无配额)。\n#\n#total-quota=0\n\n# Max bytes-per-second bandwidth a TURN session is allowed to handle\n# (input and output network streams are treated separately). Anything above\n# that limit will be dropped or temporary suppressed (within\n# the available buffer limits).\n#\n# TURN会话允许最大的传输占用带宽(输入和输出网络流分别处理)。\n# 高于限制将被删除或暂时抑制(在可用的缓冲区范围内)。\n#\n#max-bps=0\nmax-bps=1024\n\n# Uncomment if no UDP client listener is desired.\n# By default UDP client listener is always started.\n#\n# 如果没有UDP客户端监听器需要取消。\n# 默认情况下UDP客户端监听器总是启动。\n#\n#no-udp\n\n# Uncomment if no TCP client listener is desired.\n# By default TCP client listener is always started.\n#\n# 如果没有TCPP客户端监听器需要取消。\n# 默认情况下TCPP客户端监听器总是启动。\n#\n#no-tcp\n\n# Uncomment if no TLS client listener is desired.\n# By default TLS client listener is always started.\n#\n# 如果没有TLS客户端监听器需要取消。\n# 默认情况下TLS客户端监听器总是启动。\n#\n#no-tls\n\n# Uncomment if no DTLS client listener is desired.\n# By default DTLS client listener is always started.\n#\n# 如果没有DTLS客户端监听器需要取消。\n# 默认情况下DTLS客户端监听器总是启动。\n#\n#no-dtls\n\n# Uncomment if no UDP relay endpoints are allowed.\n# By default UDP relay endpoints are enabled (like in RFC 5766).\n#\n# 如果不允许UDP中继端点需要取消。\n# 默认情况下启用UDP继电器端点(如在RFC 5766)。\n#\n#no-udp-relay\n\n# Uncomment if no TCP relay endpoints are allowed.\n# By default TCP relay endpoints are enabled (like in RFC 6062).\n#\n# 如果不允许TCP中继端点需要取消。\n# 默认情况下启用TCP继电器端点(如在RFC 5766)。\n#\n#no-tcp-relay\n\n# Uncomment if extra security is desired,\n# with nonce value having limited lifetime (600 secs).\n# By default, the nonce value is unique for a session,\n# but it has unlimited lifetime. With this option,\n# the nonce lifetime is limited to 600 seconds, after that\n# the client will get 438 error and will have to re-authenticate itself.\n#\n# 取消如果需要额外的安全,现时已有有限的生命周期(600秒)。\n# 默认情况下,一个会话的唯一临界值,但它一般拥有无限的生命周期。这个选项,临界值\n# 仅限于600秒,之后,客户端将得到438错误,将不得不重新认证。\n#\nstale-nonce\n\n# Certificate file.\n# Use an absolute path or path relative to the\n# configuration file.\n#\n# 证书文件。\n# 使用绝对路径或路径相对于配置文件。\n#\ncert=/etc/cacert.pem\n\n# Private key file.\n# Use an absolute path or path relative to the\n# configuration file.\n# Use PEM file format.\n#\n# 私钥文件。\n# 使用绝对路径或路径相对于配置文件。使用PEM文件格式。\n#\npkey=/etc/cakey.pem\n\n# Private key file password, if it is in encoded format.\n# This option has no default value.\n#\n# 私有密钥文件密码,如果是在编码格式。\n# 这个选项没有默认值。\n#\n#pkey-pwd=...\n\n# Allowed OpenSSL cipher list for TLS/DTLS connections.\n# Default value is \"DEFAULT\".\n#\n# 允许OpenSSL的密码列表为TLS/DTLS连接。\n# 默认值是\"DEFAULT\"\n#\n#cipher-list=\"DEFAULT\"\n\n# CA file in OpenSSL format.\n# Forces TURN server to verify the client SSL certificates.\n# By default it is not set: there is no default value and the client\n# certificate is not checked.\n#\n# 在OpenSSL格式的CA文件。\n# 强制TURN服务器验证客户端SSL证书。\n# 默认情况下它没有设置:没有默认值,不检查的客户端证书。\n#\n# Example:\n#CA-file=/etc/ca11111.crt\n\n# Curve name for EC ciphers, if supported by OpenSSL library (TLS and DTLS).\n# The default value is prime256v1.\n#\n# 曲线名称的EC密码,如果由OpenSSL库支持(TLS和DTLS)。\n# 默认值是prime256v1。\n#\n#ec-curve-name=prime256v1\n\n# Use 566 bits predefined DH TLS key. Default size of the key is 1066.\n#\n# 使用566位预定义DH TLS键。默认键大小是1066\n#\n#dh566\n\n# Use 2066 bits predefined DH TLS key. Default size of the key is 1066.\n#\n# 使用2066位预定义DH TLS键。默认键大小是1066\n#\n#dh2066\n\n# Use custom DH TLS key, stored in PEM format in the file.\n# Flags --dh566 and --dh2066 are ignored when the DH key is taken from a file.\n#\n# 使用惯例的DH TLS键，使用PEM格式存储在文件里\n# 当DH键从文件里加载，将忽略标志--dh566和--dh2066\n#\n#dh-file=<DH-PEM-file-name>\n\n# Flag to prevent stdout log messages.\n# By default, all log messages are going to both stdout and to\n# the configured log file. With this option everything will be\n# going to the configured log only (unless the log file itself is stdout).\n#\n# 标志防止输出日志信息\n# 默认情况下,所有日志消息将输出到配置的日志文件。采用这一选项都将只配置日志\n# (除非日志文件本身是输出的)。\n#\n#no-stdout-log\n\n# Option to set the log file name.\n# By default, the turnserver tries to open a log file in\n# /var/log, /var/tmp, /tmp and current directories directories\n# (which open operation succeeds first that file will be used).\n# With this option you can set the definite log file name.\n# The special names are \"stdout\" and \"-\" - they will force everything\n# to the stdout. Also, the \"syslog\" name will force everything to\n# the system log (syslog).\n# In the runtime, the logfile can be reset with the SIGHUP signal\n# to the turnserver process.\n#\n# 设置日志文件\n# 默认情况下,turnserver尝试一个日志文件在/var/log,/var/tmp,/tmp和\n# 当前目录(那个文件先打开成功,文件将被使用)。\n# 采用这一选项可以设置明确的日志文件名。\n# 特殊的名字是\"stdout\"和\"-\"——他们将强制所有的输出。同时,\"syslog\"名称将强制所有的系统日志(syslog)。\n# 在运行时,日志文件可以重置通过SIGHUP信号在turnserver程序中。\n#\n#log-file=/var/tmp/turn.log\n\n# Option to redirect all log output into system log (syslog).\n#\n# 选择重定向所有日志输出到系统日志(syslog)。\n#\n#syslog\n\n# This flag means that no log file rollover will be used, and the log file\n# name will be constructed as-is, without PID and date appendage.\n#\n# 这个标志意味着没有日志文件将使用翻转,并按原样将创建日志文件名称,没有PID和日期的附加。\n#\n#simple-log\n\n# Option to set the \"redirection\" mode. The value of this option\n# will be the address of the alternate server for UDP & TCP service in form of\n# <ip>[:<port>]. The server will send this value in the attribute\n# ALTERNATE-SERVER, with error 300, on ALLOCATE request, to the client.\n# Client will receive only values with the same address family\n# as the client network endpoint address family.\n# See RFC 5389 and RFC 5766 for ALTERNATE-SERVER functionality description.\n# The client must use the obtained value for subsequent TURN communications.\n# If more than one --alternate-server options are provided, then the functionality\n# can be more accurately described as \"load-balancing\" than a mere \"redirection\".\n# If the port number is omitted, then the default port\n# number 3478 for the UDP/TCP protocols will be used.\n# Colon ( characters in IPv6 addresses may conflict with the syntax of\n# the option. To alleviate this conflict, literal IPv6 addresses are enclosed\n# in square brackets in such resource identifiers, for example:\n# [2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478 .\n# Multiple alternate servers can be set. They will be used in the\n# round-robin manner. All servers in the pool are considered of equal weight and\n# the load will be distributed equally. For example, if we have 4 alternate servers,\n# then each server will receive 25% of ALLOCATE requests. A alternate TURN server\n# address can be used more than one time with the alternate-server option, so this\n# can emulate \"weighting\" of the servers.\n#\n# 选项设置\"redirection\"模式。这个选项的值将备用服务器的地址UDP和TCP服务形式的<ip>[:<port>]。\n# 服务器将发送这个值属性ALTERNATE-SERVER,错误300,在ALLOCATE请求,客户端。\n# 客户端将只接收和自己相同的地址族的客户端的值。查看RFC 5389和RFC 5766为ALTERNATE-SERVER的功能描述。\n# 客户端必须使用获得的值为随后的TURN通信。如果不止一个——alternate-server选项提供,那么功能可以更准确\n# 地描述为\"load-balancing\",而不仅仅是一个\"redirection\"。如果端口号省略,那么为UDP/TCP协议，使用默认端\n# 口号是3478。冒号(在IPv6地址字符可能与选项的语法冲突。缓解这种冲突,文字IPv6地址包含在方括号在这种\n# 资源标识符,例如[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478 。\n# 可以设置多个备用服务器。他们将用于循环的方式。所有服务器池中被认为是平等的重量和载荷将平均分配的原则。\n# 例如,如果我们有4个备用服务器,每个服务器将获得25%的分配请求。备用TURN服务器地址可以使用超过一次\n# alternate-server选项,所以这可以效仿的\"weighting\"服务器。\n#\n# Examples:\n#alternate-server=1.2.3.4:5678\n#alternate-server=11.22.33.44:56789\n#alternate-server=5.6.7.8\n#alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n              \n# Option to set alternative server for TLS & DTLS services in form of\n# <ip>:<port>. If the port number is omitted, then the default port\n# number 5349 for the TLS/DTLS protocols will be used. See the previous\n# option for the functionality description.\n#\n# 选项设置替代服务器TLS和DTLS服务形式的<ip>:<port>。\n# 如果省略的端口号,那么默认端口号5349将使用TLS/DTLS协议。看到前面选择的功能描述。\n#\n# Examples:\n#tls-alternate-server=1.2.3.4:5678\n#tls-alternate-server=11.22.33.44:56789\n#tls-alternate-server=[2001:db8:85a3:8d3:1319:8a2e:370:7348]:3478\n\n# Option to suppress TURN functionality, only STUN requests will be processed.\n# Run as STUN server only, all TURN requests will be ignored.\n# By default, this option is NOT set.\n#\n# 选择抑制TURN功能,只有STUN的请求将被处理。\n# 作为STUN服务器,所有TURN请求将被忽略。\n# 默认情况下,没有设置这个选项。\n#\n#stun-only\n\n# Option to suppress STUN functionality, only TURN requests will be processed.\n# Run as TURN server only, all STUN requests will be ignored.\n# By default, this option is NOT set.\n#\n# 选择抑制STUN功能,只有TURN的请求将被处理。\n# 作为TURN服务器,所有STUN请求将被忽略。\n# 默认情况下,没有设置这个选项。\n#\n#no-stun\n\n# This is the timestamp/username separator symbol (character) in TURN REST API.\n# The default value is ':'.\n#\n# 这是时间戳/用户名分离器符号(字符)在TURN REST API。\n# 默认是使用':'\n#\n# rest-api-separator=:    \n\n# Flag that can be used to disallow peers on the loopback addresses (127.x.x.x and ::1).\n# This is an extra security measure.\n#\n# 标记用于不接受的端在环回地址(127.x.x.x 和 ::1)。\n# 这是一个额外的安全措施。\n#\n#no-loopback-peers\n172.26.7.151\n# Flag that can be used to disallow peers on well-known broadcast addresses (224.0.0.0 and above, and FFXX:*).\n# This is an extra security measure.\n#\n# 标记用于不接受的端在广播地址(224.0.0.0和以上的，和FFXX:*)。\n# 这是一个额外的安全措施。\n#\n#no-multicast-peers\n\n# Option to set the max time, in seconds, allowed for full allocation establishment.\n# Default is 60 seconds.\n#\n# 选项设置的最大时间,以秒为单位,允许完整的分配。\n# 默认60秒\n#\n#max-allocate-timeout=60\n\n# Option to allow or ban specific ip addresses or ranges of ip addresses.\n# If an ip address is specified as both allowed and denied, then the ip address is\n# considered to be allowed. This is useful when you wish to ban a range of ip\n# addresses, except for a few specific ips within that range.\n# This can be used when you do not want users of the turn server to be able to access\n# machines reachable by the turn server, but would otherwise be unreachable from the\n# internet (e.g. when the turn server is sitting behind a NAT)\n#\n# 选择允许或禁止特定的ip地址或ip地址范围。\n# 如果指定一个ip地址允许和拒绝,那么ip地址被认为是允许的。这是有用的,当你希望禁止一个范\n# 围的ip地址,除了一些特定的ip范围内。\n# 这可以使用当你不希望turn服务器的用户能够访问机器通过turn服务器,但可能是另一方面从互联\n# 网上不能到达(例如,当turn服务器是在一个NAT后)\n#\n# Examples:\n# denied-peer-ip=83.166.64.0-83.166.95.255\n# allowed-peer-ip=83.166.68.45\n\n# File name to store the pid of the process.\n# Default is /var/run/turnserver.pid (if superuser account is used) or\n# /var/tmp/turnserver.pid .\n#\n# 存储进程pid的文件名。\n# 默认是/var/run/turnserver.pid(超级用户使用)或者是/var/tmp/turnserver.pid\n#\n#pidfile=\"/var/run/turnserver.pid\"\npidfile=\"/var/tmp/turnserver.pid\"\n\n# Require authentication of the STUN Binding request.\n# By default, the clients are allowed anonymous access to the STUN Binding functionality.\n#\n# 需要STUN绑定请求的身份验证。\n# 默认情况下,客户允许匿名访问STUN绑定功能。\n#\n#secure-stun\n\n# Require SHA256 digest function to be used for the message integrity.\n# By default, the server uses SHA1 (as per TURN standard specs).\n# With this option, the server\n# always requires the stronger SHA256 function. The client application\n# must support SHA256 hash function if this option is used. If the server obtains\n# a message from the client with a weaker (SHA1) hash function then the\n# server returns error code 426.\n#\n# 需要SHA256采摘功能用于消息的完整性。\n# 默认情况下,服务器使用SHA1(按标准规格)。\n# 采用这一选项,服务器总是需要更强的SHA256功能。客户端应用程序必须支持SHA256散列函数\n# 如果使用这个选项。如果服务器获得消息从客户端较弱(SHA1)散列函数那么服务器返回错误代码426。\n#\n#sha256\n\n# Mobility with ICE (MICE) specs support.\n#\n# 移动的ICE(MICE)的规范支持。\n#\n#mobility\n\n# User name to run the process. After the initialization, the turnserver process\n# will make an attempt to change the current user ID to that user.\n#\n# 用户名运行程序。初始化后,turnserver程序将试图改变当前用户的用户ID。\n#\n#proc-user=<user-name>\n\n# Group name to run the process. After the initialization, the turnserver process\n# will make an attempt to change the current group ID to that group.\n#\n# 组名运行程序。初始化后,turnserver程序将试图改变当前组的组ID。\n#\n#proc-group=<group-name>\n\n# Turn OFF the CLI support.\n# By default it is always ON.\n# See also options cli-ip and cli-port.\n#\n# 关掉CLI的支持。\n# 默认情况下它总是ON。\n# 参阅选项cli-ip和cli-port。\n#\n#no-cli\n\n#Local system IP address to be used for CLI server endpoint. Default value\n# is 127.0.0.1.\n#\n# 本地系统的IP地址将用于CLI服务器端点。默认值是127.0.0.1。\n#\n#cli-ip=127.0.0.1\n\n# CLI server port. Default is 5766.\n#\n# CLI服务器端口。默认是5766。\n#\n#cli-port=5766\n\n# CLI access password. Default is empty (no password).\n#\n# CLI访问密码。默认是空的(没有密码)。\n#\ncli-password=logen\n\n# Server relay. NON-STANDARD AND DANGEROUS OPTION.\n# Only for those applications when we want to run\n# server applications on the relay endpoints.\n# This option eliminates the IP permissions check on\n# the packets incoming to the relay endpoints.\n#\n# 中继服务器。NON-STANDARD和DANGEROUS的选择。\n# 只对这些应用程序时,我们想在中继服务器上运行服务器应用程序端点。\n# 这个选项可以消除IP权限检查传递的数据包传入的端点。\n#\n#server-relay\n\n# Maximum number of output sessions in ps CLI command.\n# This value can be changed on-the-fly in CLI. The default value is 256.\n#\n# 最大数量的输出会议在ps CLI命令。\n# 这个值可以动态改变在CLI。默认值是256。\n#\n#cli-max-output-sessions\n\n# Set network engine type for the process (for internal purposes).\n#\n# 设置网络引擎类型(用于内部目的)的过程。\n#\n#ne=[1|2|3]\n\n# Do not allow an SSL/TLS version of protocol\n#\n# 不允许一个SSL/TLS版本的协议\n#\n#no-sslv2\n#no-sslv3\n#no-tlsv1\n#no-tlsv1_1\n#no-tlsv1_2%  \n```\n\n</details>\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"WebRTC（一）","url":"/2019-05-14/reference/webrtc/webrtc-01/","content":"\n> [WebRTC API](<https://developer.mozilla.org/zh-CN/docs/Web/API/WebRTC_API>)\n>\n> [Node.js v10.15.3 文档](<http://nodejs.cn/api/>)\n>\n> [廖雪峰 - nodejs](<https://www.liaoxuefeng.com/wiki/1022910821149312/1023025235359040>)\n>\n> [Webrtc笔记-获取源码](<https://www.jianshu.com/p/310c0d133c3c>)\n>\n> [WebRTC音频引擎实现分析](<https://www.jianshu.com/p/5a8a91cd84ef>)\n>\n> [实时通信RTC技术栈之：视频编解码](<http://www.52im.net/thread-1034-1-1.html>)\n>\n> [开源实时音视频技术WebRTC中RTP/RTCP数据传输协议的应用](<http://www.52im.net/thread-589-1-1.html>)\n>\n> [WebRTC项目源码在国内的镜像](<https://gitee.com/ibaoger/webrtc>)\n\n## WebRTC 介绍\n\n<!-- more -->\n\n- Google 开源\n- 跨平台\n- 用于浏览器\n- 实时传输\n  - 100ms 延迟 通话质量非常好\n  - 200ms 延迟 通话质量比较优质\n  - 500ms 延迟 可以接受\n  - 超过1s 非常迟滞\n- 音视频引擎\n\nWebRTC 应用：\n\n<img src=\"/images/imageWebRTC/webrtc应用.png\">\n\nWebRTC 愿景：\n\n<img src=\"/images/imageWebRTC/webrtc愿景.png\">\n\n学习 WebRTC 的难点：\n\n<img src=\"/images/imageWebRTC/学习WebRTC的难点.png\">\n\n学习路线：\n\n<img src=\"/images/imageWebRTC/学习路线.png\">\n\n学习内容：\n\n<img src=\"/images/imageWebRTC/学习内容.png\">\n\n学习收获：\n\n<img src=\"/images/imageWebRTC/学习收获.png\">\n\nWebRTC能做啥：\n\n<img src=\"/images/imageWebRTC/WebRTC能做啥.png\">\n\n能学到什么：\n\n<img src=\"/images/imageWebRTC/能学到什么.png\">\n\ngoogle webrtc 示例：https://appr.tc/\n\n## WebRTC 原理与架构\n\nWebRTC 整体架构：\n\n<img src=\"/images/imageWebRTC/webrtc架构.png\">\n\n\n\nWebRTC 的目录结构图：\n\n<img src=\"/images/imageWebRTC/WebRTC目录结构-01.png\">\n\n<img src=\"/images/imageWebRTC/WebRTC目录结构-02.png\">\n\n<img src=\"/images/imageWebRTC/WebRTCModules目录-01.png\">\n\n<img src=\"/images/imageWebRTC/WebRTCModules目录-02.png\">\n\nWebRTC 两个基本概念：轨与流\n\n- Track\n- MediaStream\n\nWebRTC重要类：\n\n- MediaStream\n- RTCPeerConnection\n- RTCDataChannel\n\nPeerConnection调用过程：\n\n<img src=\"/images/imageWebRTC/PeerConnection调用过程.png\">\n\n调用时序图：\n\n<img src=\"/images/imageWebRTC/调用时序图.png\">\n\n![PeerConnection连接建立流程图](/images/imageWebRTC/PeerConnection连接建立流程图.png)\n\n对于上图中描述的PeerConnection建立的完整流程进行以下说明（上图是以ClientA主动向ClientB发起连接为例）：\n\n- 首先 ClientA 和 ClientB 均通过双向通信方式如 WebSocket 连接到 Signaling Server 上；\n- ClientA 在本地首先通 GetMedia 访问本地的 media 接口和数据，并创建 PeerConnection 对象，调用其 AddStream 方法把本地的 Media 添加到 PeerConnection 对象中。**对于 ClientA 而言，既可以在与 Signaling Server 建立连接之前就创建并初始化 PeerConnection 如阶段 1，也可以在建立 Signaling Server 连接之后创建并初始化 PeerConnection 如阶段 2；ClientB 既可以在上图的1阶段也可以在 2 阶段做同样的事情，访问自己的本地接口并创建自己的 PeerConnection 对象**。\n- 通信由 ClientA 发起，所以 ClientA 调用 PeerConnection 的 CreateOffer 接口创建自己的 SDP offer，然后把这个 SDP Offer 信息通过 Signaling Server 通道中转发给 ClientB；\n- ClientB 收到 Signaling Server 中转过来的 ClientA 的 SDP 信息也就是 offer 后，调用 CreateAnswer 创建自己的 SDP 信息也就是 answer，然后把这个 answer 同样通过 Signaling server 转发给 ClientA；\n- ClientA 收到转发的 answer 消息以后，两个 peers 就做好了建立连接并获取对方 media streaming 的准备；\n- ClientA 通过自己 PeerConnection 创建时传递的参数等待来自于 ICE server 的通信，获取自己的 candidate，当 candidate available 的时候会自动回掉 PeerConnection 的 OnIceCandidate；\n- ClientA 通过 Signling Server 发送自己的 Candidate 给 ClientB，ClientB 依据同样的逻辑把自己的 Candidate 通过 Signaling Server 中转发给 ClientA；\n- 至此 ClientA 和 ClientB 均已经接收到对方的 Candidate，通过 PeerConnection 建立连接。至此 P2P 通道建立。\n\n> [WebRTC之PeerConnection的建立过程](https://www.cnblogs.com/cther/p/myPeerConnection.html)\n>\n> [WebRTC系列（3）：PeerConnection通信建立流程](<https://www.jianshu.com/p/43957ee18f1a>)\n\n## Web服务器原理与Nodejs搭建\n\n> [node.js基本工作原理及流程](<https://blog.csdn.net/xiangzhihong8/article/details/53954600>)\n>\n> [Nodejs的运行原理-架构篇](https://www.cnblogs.com/peiyu1988/p/8192066.html)\n>\n> [Node.js 原理简介](https://www.cnblogs.com/bingooo/p/6720540.html)\n>\n> [NodeJS 事件循环（第一部分）- 事件循环机制概述](<https://zhuanlan.zhihu.com/p/37427130>)\n\nWeb服务器选型：\n\n- Nodejs\n- Nginx\n- Apache\n\nWeb服务工作原理：\n\n<img src=\"/images/imageWebRTC/web服务工作原理.png\">\n\nNodejs工作原理：\n\n<img src=\"/images/imageWebRTC/Nodejs工作原理.png\">\n\nJavaScript解析：\n\n<img src=\"/images/imageWebRTC/JavaScript解析.png\">\n\nNodejs 事件处理：\n\n<img src=\"/images/imageWebRTC/Nodejs事件处理.png\">\n\n两个V8引擎：\n\n<img src=\"/images/imageWebRTC/两个V8引擎.png\">\n\n最简单的http服务：\n\n- **require** 引入http模块\n- 创建http服务\n- 侦听端口\n\n启动Nodejs服务：\n\n- node app.js\n- nohub node app.js\n- forever start app.js\n- pm2 start app.js\n\nHttps基本原理：\n\n<img src=\"/images/imageWebRTC/https基本原理.png\">\n\nNodejs 搭建 https 服务：\n\n- 生成 HTTPS证书\n- 引入 HTTPS模块\n- 指定证书位置，并创建 HTTPS 服务\n\n真正的Web服务：\n\n- 引用 express 模块\n- 引入 server-index 模块\n- 指定发布目录\n\n## JavaScript 必备知识回顾\n\n基础知识：\n\n- 变量与类型\n- 基本运算\n- `if/else`\n- for循环\n- 函数\n- 日志打印\n\n变量与类型：\n\n<img src=\"/images/imageWebRTC/变量与类型.png\">\n\n基本运算：\n\n<img src=\"/images/imageWebRTC/基本运算.png\">\n\n<img src=\"/images/imageWebRTC/ifelse.png\">\n\n<img src=\"/images/imageWebRTC/for循环.png\">\n\n<img src=\"/images/imageWebRTC/函数.png\">\n\n## WebRTC设备管理\n\nenumerateDevices：\n\n<img src=\"/images/imageWebRTC/enumerateDevices.png\">\n\nJavaScript中的Promise：\n\n<img src=\"/images/imageWebRTC/JavaScript中的Promise.png\">\n\n## WebRTC音视频数据采集\n\n音视频采集API：\n\n<img src=\"/images/imageWebRTC/音视频采集API.png\">\n\ngetUserMedia的不同实现：\n\n<img src=\"/images/imageWebRTC/getUserMedia的不同实现.png\">\n\n适配置不同浏览器的方法：\n\n<img src=\"/images/imageWebRTC/适配置不同浏览器的方法.png\">\n\n`https://webrtc.github.io/adapter/adapter-latest.js`\n\nWebRTC音视频采集约束：\n\n视频约束详解：\n\n- width\n\n- height\n\n  宽高比例：`4:3`  `16:9`\n\n- aspectRatio\n\n- frameRate\n\n- facingMode\n\n  - user - 前置摄像头\n  - environment - 后置摄像头\n  - left - 前置左侧摄像头\n  - right - 前置右侧摄像头\n\n- resizeMode\n\n音频约束详解：\n\n- volume - `范围 0 - 1.0`\n- sampleRate\n- sampleSize -  一般16位\n- echoCancellation - 回音消除\n- autoGainControl - 是否在原有声音基础上增加音量\n- noiseSuppression - 降噪\n- latency - 延迟大小\n- channelCount - 声道  乐器一般是双声道\n- deviceID - 作用是多个设备切换\n- groupID \n\nWebRTC约束例子：\n\n```json\n{\n    audio: true,\n    video: {\n        width: {\n            min: 300,\n            max: 640,\n        },\n        height: {\n            min: 300,\n            max: 480,\n        },\n        frameRate: {\n            min: 15,\n            max: 30,\n        }\n    }\n}\n```\n\n浏览器视频特效：\n\n- CSS filter，`-webkit-filter/filter`\n- 如何将 video 与 filter 关联\n- OpenGL/Metal/...\n\n支持的特效种类：\n\n<img src=\"/images/imageWebRTC/支持的特效种类.png\">\n\n保存图片是实现滤镜效果，可以对 canvas.data 进行数据修改。\n\nMediaStream API 获取视频约束：\n\n<img src=\"/images/imageWebRTC/MediaStream.png\">\n\n<img src=\"/images/imageWebRTC/MediaStream事件.png\">\n\n## WebRTC音视频录制实战\n\n### WebRTC录制基本知识\n\nMediaRecoder类：\n\n<img src=\"/images/imageWebRTC/MediaRecoder.png\">\n\n<img src=\"/images/imageWebRTC/MediaRecorder参数.png\">\n\n<img src=\"/images/imageWebRTC/MediaRecorderAPI-01.png\">\n\n<img src=\"/images/imageWebRTC/MediaRecorderAPI-02.png\">\n\n<img src=\"/images/imageWebRTC/MediaRecorder事件.png\">\n\n<img src=\"/images/imageWebRTC/JavaScript几种存储数据的方式.png\">\n\n### WebRTC 捕获桌面\n\n<img src=\"/images/imageWebRTC/getDisplayMedia.png\">\n\n捕获桌面需要设置Chrome，具体 操作：<chrome://flags/#enable-experimental-web-platform-features>\n\n- Experimental Web Platform features 设置为 enable\n\n## WebRTC信令服务器实现\n\n如果没有信令服务器WebRTC之间是不能通信的。\n\n两个client之间通信必须有两个信息通过信令服务器的：\n\n- 媒体信息， SDP\n- 网络信息\n- 具体的业务\n\n<img src=\"/images/imageWebRTC/信令服务器的作用.png\">\n\n<img src=\"/images/imageWebRTC/为什么要使用socketio.png\">\n\n<img src=\"/images/imageWebRTC/socketio工作原理.png\">\n\nSocket.IO 发送消息：\n\n- 给本次连接发送消息\n\n  ```js\n  socket.emit()\n  ```\n\n- 给某个房间内所有人发送消息\n\n  ```js\n  io.in(room).emit()\n  ```\n\n- 除本链接外，给某个房间内所有人发送消息\n\n  ```js\n  socket.to(room).emit()\n  ```\n\n- 除本链接外，给所有人发送消息\n\n  ```js\n  socket.broadcast.emit()\n  ```\n\nSocket.IO 客户端处理消息：\n\n- 发送 action 命令\n\n  ```js\n  S: socket.emit('action');\n  C: socket.on('action', function(){...});\n  ```\n\n- 发送了一个 action 命令，还有 data 数据\n\n  ```js\n  S: socket.emit('action', data);\n  C: socket.on('action', function(data){...});\n  ```\n\n- 发送了 action 命令，还有两个数据\n\n  ```js\n  S: socket.emit('action', arg1, arg2);\n  C: socket.on('action', function(arg1, arg2){...});\n  ```\n\n- 发送了一个 action 命令，在 emit 方法中包含回调函数\n\n  ```js\n  S: socket.emit('action', data, function(arg1, arg2){...};\n  C: socket.on('action', function(data, fn){fn('a', 'b');});\n  ```\n\n### [实战] 通过 socket.io 实现信令服务器\n\n改造服务端的基本流程：\n\n- 安装 socket.io\n- 引入 socket.io\n- 处理 connection 消息\n\n## WebRTC网络基础补充：P2P/STUN/TRUN/ICE知识\n\n> [P2P通信原理与实现](<https://zhuanlan.zhihu.com/p/26796476>)\n\n### WebRTC 网络传输基本知识\n\nWebRTC 传输基本知识：\n\n- NAT（Network Address Translator）\n- STUN（Simple Traversal of UDP Through NAT）\n- TURN（Travelsal Using Relays around NAT）\n- ICE（Interactive Connectivity Establishment）\n\n<img src=\"/images/imageWebRTC/NAT.png\">\n\nNAT 产生的原因：\n\n- 由于IPv4的地址不够\n- 处于网络安全的原因\n\nNAT 的种类：\n\n- 完全锥型 NAT（Full Cone NAT）\n- 地址限制锥型 NAT（Address Restricted Cone NAT）\n- 端口限制锥型 NAT（Port Restricted Cone NAT）\n- 对称型 NAT（Symmetric NAT）\n\n### NAT 打洞原理\n\n<img src=\"/images/imageWebRTC/完全锥型NAT.png\">\n\n<img src=\"/images/imageWebRTC/地址限制锥型NAT.png\">\n\n<img src=\"/images/imageWebRTC/端口限制锥型NAT.png\">\n\n<img src=\"/images/imageWebRTC/对称型NAT.png\">\n\nNAT 穿越原理：\n\n- C1，C2 向 STUN 发消息\n- 交换公网 IP 及 端口\n- C1 -> C2，C2 -> C1，甚至是端口猜测\n\n<img src=\"/images/imageWebRTC/NAT穿越组合.png\">\n\n### NAT 类型检测\n\n<img src=\"/images/imageWebRTC/NAT类型判断.png\">\n\n公网 IP：\n\n<img src=\"/images/imageWebRTC/NAT类型检测-01.png\">\n\n如果 Client 收到的 IP 和第一次发出去的 IP 是不一样的，则是对称型 NAT，如果是一样的需要进一步判断：\n\n<img src=\"/images/imageWebRTC/NAT类型检测-02.png\">\n\nClient 通过 Port2 发送消息到 STUN Port1，STUN Server 通过 Port2 给 Client 回消息，如果 Client 能收到消息，则说明是 IP 限制型的；如果不能收到，则说明是端口限制型的：\n\n<img src=\"/images/imageWebRTC/NAT类型检测-03.png\">\n\n### 【协议规范】STUN 协议一\n\nSTUN 介绍：\n\n- STUN 存在的目的就是进行 NAT 穿越\n- STUN 是典型的客户端 / 服务器模式。客户端发送请求，服务端进行响应\n\nRFC STUN 规范：\n\n- **RFC3489/STUN**\n\n  SImple Traversal of UDP Trough NAT\n\n- **RFC5389/STUN** — 包含UDP和TCP\n\n  Session Traversal Utilities for NAT\n\nSTUN 协议：\n\n- 包括 20 字节的 STUN header\n- Body 中可以有 0 个或多个 Attribute\n\nSTUN header（RFC3489）：\n\n- 其中 2 个字节（16bit）类型\n- 2 个字节（16bit）消息长度，不包括消息头\n- 16 个字节（128bit）事物ID，请求与响应事物 ID 相同\n\nSTUN header（RFC5389）格式：\n\n<img src=\"/images/imageWebRTC/STUNHeader格式.png\">\n\n<img src=\"/images/imageWebRTC/STUNMessageType.png\">\n\nM 代表请求值，C 代表分类：\n\n<img src=\"/images/imageWebRTC/STUNMessageType-01.png\">\n\n<img src=\"/images/imageWebRTC/C0C1.png\">\n\nRFC5389 把私密类型去掉了：\n\n<img src=\"/images/imageWebRTC/STUN消息类型.png\">\n\n### 【协议规范】STUN 协议二\n\nInter 机子都是小端模式：\n\n<img src=\"/images/imageWebRTC/大小端模式.png\">\n\n<img src=\"/images/imageWebRTC/STUNMessageType-02.png\">\n\n<img src=\"/images/imageWebRTC/TransactionID.png\">\n\n<img src=\"/images/imageWebRTC/STUNMessageBody.png\">\n\n<img src=\"/images/imageWebRTC/TLV.png\">\n\n<img src=\"/images/imageWebRTC/RFC3489定义的属性.png\">\n\n<img src=\"/images/imageWebRTC/Attribute的使用.png\">\n\n### 【协议规范】TURN 协议\n\nTURN 介绍：\n\n- 其目的是解决对称 NAT 无法穿越的问题\n- 其建立在 STUN 之上，消息格式使用 STUN 格式消息\n- TURN Client 要求服务端分配一个公共 IP 和 Port 用于接受 或 发送数据\n\n<img src=\"/images/imageWebRTC/TURN例子.png\">\n\n<img src=\"/images/imageWebRTC/TURN使用的传输协议.png\">\n\n<img src=\"/images/imageWebRTC/TURNAllocate.png\">\n\nTURN 发送机制：\n\n- Send 和 Data\n- Channel\n\n<img src=\"/images/imageWebRTC/TURNSendAndData.png\">\n\n<img src=\"/images/imageWebRTC/TURNChannel.png\">\n\n<img src=\"/images/imageWebRTC/TURN的使用.png\">\n\n### 【协议规范】ICE 框架\n\n<img src=\"/images/imageWebRTC/ICE.png\">\n\n<img src=\"/images/imageWebRTC/ICECandidate.png\">\n\nCandidate 类型：\n\n- 主机候选者\n- 反射侯选者\n- 中继候选者\n\nICE 具体做些什么：\n\n- 收集 Candidate\n- 对 Candidate Pair 排序\n- 连通性检查\n\n<img src=\"/images/imageWebRTC/Candidate关系图.png\">\n\n收集 Candidate：\n\n- Host Candidate：本机所有 IP 和指定端口\n- Reflexive Candidate：STUN/TURN\n- Relay Candidate：TURN\n\n什么是 SDP：\n\n- **SDP（Session Description Protocol）** 它只是一种信息格式的描述标准，本身不属于传输协议，但是可以被其他传输协议用来交换必要的信息。\n\n<img src=\"/images/imageWebRTC/SDP例子.png\">\n\n形成 Candidate Pair：\n\n- 一方收集到所有候选者后，通过信令传给对方\n- 同样，另一方收到候选者后，也做收集工作\n- 当双方拿到全部列表后，将侯选者形成匹配对儿\n\n连通性检查：\n\n- 对侯选者进行优先级排序\n- 对每个侯选对进行发送检查\n- 对每个侯选对进行接收检查\n\n<img src=\"/images/imageWebRTC/连通性过程.png\">\n\n### 网络协议分析方法 tcpdump 与 wireshark讲解\n\n常用工具：\n\n- Linux 服务端用 tcpdump\n- 其它端 WireShark\n\n<img src=\"/images/imageWebRTC/tcpdump.png\">\n\n### 网络协议分析方法 tcpdump 与 wireshark 实战\n\nvim 打开二进制数据：\n\n```shell\n：%！xxd\n```\n\nWireShark 中的逻辑运算：\n\n- 与：and 或 &&\n- 或：or 或 ||\n- 非：not 或 ！\n\nWireShark 中判断语句：\n\n- 等于：eq 或 ==\n- 小于：lt 或 <\n- 大于：gt 或 >\n- 小于等于：le 或 <=\n- 大于等于：ge 或 >=\n- 不等于：ne 或 !=\n\nWireShark 按协议过滤：\n\n- stun\n- tcp\n- udp\n\n模拟STUN数据可以使用这个网站中的工具：<https://webrtc.github.io/samples>\n\nWireshark 按 IP 过滤：\n\n```\nip.dst == 192.168.1.2\nip.src == 192.168.1.2\nip.addr == 192.168.1.2\n```\n\nWireShark 按 port 过滤：\n\n```\ntcp.port == 8080\nudp.port == 3478\nudp.dstport == 3478\nudp.srcport == 3478\n```\n\nWireShark 过滤长度：\n\n```\nudp.length < 30\ntcp.length < 30\nhttp.content_length < 30\n```\n\nWireShark 过滤内容：\n\nTODO\n\n","tags":["WebRTC"],"categories":["WebRTC"]},{"title":"PM计算题笔记","url":"/2019-05-10/ispm_note_1/","content":"\n#### 成本/进度管理\n```\n# 公式\nPV-计划成本 AC-实际成本 EV-挣值 SV-进度偏差  CV-成本偏差 CPI-成本执行指数 SPI-进度执行指数\nSV = EV-PV\nCV = EV-AC\nCPI = EV/AC\nSPI = EV/PV\nEV = sum(月进度*成本)\n\n- CV>0 或者 CPI<1 代表成本超支 反之 成本节省\n- SV>0 或者 SPI<1 代表进度落后 反之 进度超前\n\n# 非典型偏差公式\nBAC-全部预算成本 EAC-现实际成本 ETC-剩余完工估算成本 EAC-完工实际成本\nETC = BAC - EV\nEAC = ETC + AC\n\n```","tags":["项目管理","笔记"],"categories":["PM"]},{"title":"FFmpeg命令大全","url":"/2019-05-04/reference/FFmpeg/FFmpeg命令大全/","content":"\n## 1. 前言\n\nFFMPEG 是特别强大的专门用于处理音视频的开源库。你既可以使用它的 API 对音视频进行处理，也可以使用它提供的工具，如 ffmpeg, ffplay, ffprobe，来编辑你的音视频文件。\n\n本文将简要介绍一下 FFMPEG 库的基本目录结构及其功能，然后详细介绍一下我们在日常工作中，如何使用 ffmpeg 提供的工具来处理音视频文件。\n\n<!-- more -->\n\n## 2. FFMPEG 目录及作用\n\n- libavcodec： 提供了一系列编码器的实现。\n- libavformat： 实现在流协议，容器格式及其本IO访问。\n- libavutil： 包括了hash器，解码器和各类工具函数。\n- libavfilter： 提供了各种音视频过滤器。\n- libavdevice： 提供了访问捕获设备和回放设备的接口。\n- libswresample： 实现了混音和重采样。\n- libswscale： 实现了色彩转换和缩放工能。\n\n## 3. FFMPEG 基本概念\n\n在讲解 FFMPEG 命令之前，我们先要介绍一些音视频格式的基要概念。\n\n- 音／视频流\n\n  在音视频领域，我们把一路音／视频称为一路**流**。如我们小时候经常使用VCD看港片，在里边可以选择粤语或国语声音，其实就是CD视频文件中存放了两路音频流，用户可以选择其中一路进行播放。\n\n- 容器\n\n  我们一般把 MP4､ FLV、MOV 等文件格式称之为**容器**。也就是在这些常用格式文件中，可以存放多路音视频文件。以 MP4 为例，就可以存放一路视频流，多路音频流，多路字幕流。\n\n- channel\n\n  channel 是音频中的概念，称之为声道。在一路音频流中，可以有单声道，双声道或立体声。\n\n## 4. FFMPEG 命令\n\n我们按使用目的可以将 FFMPEG 命令分成以下几类：\n\n- 基本信息查询命令\n- 录制\n- 分解 / 复用\n- 处理原始数据\n- 滤镜\n- 切割与合并\n- 图／视互转\n- 直播相关\n\n除了 FFMPEG 的基本信息查询命令外，其它命令都按下图所示的流程处理音视频。\n\n<img src=\"/images/imageFFmpeg/音视频处理流程.png\">\n\n然后将编码的数据包传送给解码器（除非为数据流选择了流拷贝，请参阅进一步描述）。 解码器产生未压缩的帧（原始视频/ PCM音频/ ...），可以通过滤波进一步处理（见下一节）。 在过滤之后，帧被传递到编码器，编码器并输出编码的数据包。 最后，这些传递给复用器，将编码的数据包写入输出文件。\n\n默认情况下，ffmpeg只包含输入文件中每种类型（视频，音频，字幕）的一个流，并将其添加到每个输出文件中。 它根据以下标准挑选每一个的“最佳”：对于视频，它是具有最高分辨率的流，对于音频，它是具有最多channel的流，对于字幕，是第一个字幕流。 在相同类型的几个流相等的情况下，选择具有最低索引的流。\n\n您可以通过使用 `-vn / -an / -sn / -dn` 选项来禁用某些默认设置。 要进行全面的手动控制，请使用 `-map`选项，该选项禁用刚描述的默认设置。\n\n下面我们就来详细介绍一下这些命令。\n\n## 5. 基本信息查询命令\n\nFFMPEG 可以使用下面的参数进行基本信息查询。例如，想查询一下现在使用的 FFMPEG 都支持哪些 filter，就可以用 `ffmpeg -filters` 来查询。详细参数说明如下：\n\n| **参数**     | **说明**                           |\n| ------------ | ---------------------------------- |\n| -version     | 显示版本。                         |\n| -formats     | 显示可用的格式（包括设备）。       |\n| -demuxers    | 显示可用的demuxers。               |\n| -muxers      | 显示可用的muxers。                 |\n| -devices     | 显示可用的设备。                   |\n| -codecs      | 显示libavcodec已知的所有编解码器。 |\n| -decoders    | 显示可用的解码器。                 |\n| -encoders    | 显示所有可用的编码器。             |\n| -bsfs        | 显示可用的比特流filter。           |\n| -protocols   | 显示可用的协议。                   |\n| -filters     | 显示可用的libavfilter过滤器。      |\n| -pix_fmts    | 显示可用的像素格式。               |\n| -sample_fmts | 显示可用的采样格式。               |\n| -layouts     | 显示channel名称和标准channel布局。 |\n| -colors      | 显示识别的颜色名称。               |\n\n接下来介绍的是 FFMPEG 处理音视频时使用的命令格式与参数。\n\n## 6. 命令基本格式及参数\n\n下面是 FFMPEG 的基本命令格式：\n\n```shell\n$ ffmpeg [global_options] {[input_file_options] -i input_url} ...\n                         {[output_file_options] output_url} ...\n```\n\nffmpeg 通过 `-i` 选项读取输任意数量的输入“文件”（可以是常规文件，管道，网络流，抓取设备等），并写入任意数量的输出“文件”。\n\n原则上，每个输入 / 输出“文件”都可以包含任意数量的不同类型的视频流（视频 / 音频 / 字幕 / 附件 / 数据）。 **流的数量和 / 或类型是由容器格式来限制**。 选择从哪个输入进入到哪个输出将自动完成或使用 `-map` 选项。\n\n要引用选项中的输入文件，您必须使用它们的索引（从 0 开始）。 例如。 第一个输入文件是0，第二个输入文件是1，等等。类似地，文件内的流被它们的索引引用。 **例如： 2：3 是指第三个输入文件中的第四个流**。\n\n上面就是 FFMPEG 处理音视频的常用命令，下面是一些常用参数：\n\n### 6.1 主要参数\n\n| **参数**                                                     | **说明**                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| -f fmt（输入/输出）                                          | 强制输入或输出文件格式。 格式通常是自动检测输入文件，并从输出文件的文件扩展名中猜测出来，所以在大多数情况下这个选项是不需要的。 |\n| -i url（输入）                                               | 输入文件的网址                                               |\n| -y（全局参数）                                               | 覆盖输出文件而不询问。                                       |\n| -n（全局参数）                                               | 不要覆盖输出文件，如果指定的输出文件已经存在，请立即退出。   |\n| -c [：stream_specifier] codec（输入/输出，每个流）           | 选择一个编码器（当在输出文件之前使用）或解码器（当在输入文件之前使用时）用于一个或多个流。codec 是解码器/编码器的名称或 copy（仅输出）以指示该流不被重新编码。如：`ffmpeg -i INPUT -map 0 -c:v libx264 -c:a copy OUTPUT` |\n| -codec [：stream_specifier]编解码器（输入/输出，每个流）     | 同 -c                                                        |\n| -t duration（输入/输出）                                     | 当用作输入选项（在-i之前）时，限制从输入文件读取的数据的持续时间。当用作输出选项时（在输出url之前），在持续时间到达持续时间之后停止输出。 |\n| -ss位置（输入/输出）                                         | 当用作输入选项时（在-i之前），在这个输入文件中寻找位置。 请注意，在大多数格式中，不可能精确搜索，因此ffmpeg将在位置之前寻找最近的搜索点。 当转码和-accurate_seek被启用时（默认），搜索点和位置之间的这个额外的分段将被解码和丢弃。 当进行流式复制或使用-noaccurate_seek时，它将被保留。当用作输出选项（在输出url之前）时，解码但丢弃输入，直到时间戳到达位置。 |\n| -frames [：stream_specifier] framecount（output，per-stream） | 停止在帧计数帧之后写入流。                                   |\n| -filter [：stream_specifier] filtergraph（output，per-stream） | 创建由filtergraph指定的过滤器图，并使用它来过滤流。filtergraph是应用于流的filtergraph的描述，并且必须具有相同类型的流的单个输入和单个输出。在过滤器图形中，输入与标签中的标签相关联，标签中的输出与标签相关联。有关filtergraph语法的更多信息，请参阅ffmpeg-filters手册。 |\n\n### 6.2 视频参数\n\n| **参数**                                            | **说明**                                                     |\n| --------------------------------------------------- | ------------------------------------------------------------ |\n| -vframes num（输出）                                | 设置要输出的视频帧的数量。对于-frames：v，这是一个过时的别名，您应该使用它。 |\n| -r [：stream_specifier] fps（输入/输出，每个流）    | 设置帧率（Hz值，分数或缩写）。作为输入选项，忽略存储在文件中的任何时间戳，根据速率生成新的时间戳。这与用于-framerate选项不同（它在FFmpeg的旧版本中使用的是相同的）。如果有疑问，请使用-framerate而不是输入选项-r。作为输出选项，复制或丢弃输入帧以实现恒定输出帧频fps。 |\n| -s [：stream_specifier]大小（输入/输出，每个流）    | 设置窗口大小。作为输入选项，这是video_size专用选项的快捷方式，由某些分帧器识别，其帧尺寸未被存储在文件中。作为输出选项，这会将缩放视频过滤器插入到相应过滤器图形的末尾。请直接使用比例过滤器将其插入到开头或其他地方。格式是'wxh'（默认 - 与源相同）。 |\n| -aspect [：stream_specifier] 宽高比（输出，每个流） | 设置方面指定的视频显示宽高比。aspect可以是浮点数字符串，也可以是num：den形式的字符串，其中num和den是宽高比的分子和分母。例如“4：3”，“16：9”，“1.3333”和“1.7777”是有效的参数值。如果与-vcodec副本一起使用，则会影响存储在容器级别的宽高比，但不会影响存储在编码帧中的宽高比（如果存在）。 |\n| -vn（输出）                                         | 禁用视频录制。                                               |\n| -vcodec编解码器（输出）                             | 设置视频编解码器。这是 `-codec：v` 的别名。                  |\n| -vf filtergraph（输出）                             | 创建由filtergraph指定的过滤器图，并使用它来过滤流。          |\n\n### 6.3 音频参数\n\n| **参数**                                                    | **说明**                                                     |\n| ----------------------------------------------------------- | ------------------------------------------------------------ |\n| -aframes（输出）                                            | 设置要输出的音频帧的数量。这是 `-frames：a` 的一个过时的别名。 |\n| -ar [：stream_specifier] freq（输入/输出，每个流）          | 设置音频采样频率。对于输出流，它默认设置为相应输入流的频率。对于输入流，此选项仅适用于音频捕获设备和原始分路器，并映射到相应的分路器选件。 |\n| -ac [：stream_specifier]通道（输入/输出，每个流）           | 设置音频通道的数量。对于输出流，它默认设置为输入音频通道的数量。对于输入流，此选项仅适用于音频捕获设备和原始分路器，并映射到相应的分路器选件。 |\n| -an（输出）                                                 | 禁用录音。                                                   |\n| -acodec编解码器（输入/输出）                                | 设置音频编解码器。这是-codec的别名：a。                      |\n| -sample_fmt [：stream_specifier] sample_fmt（输出，每个流） | 设置音频采样格式。使用-sample_fmts获取支持的样本格式列表。   |\n| -af filtergraph（输出）                                     | 创建由filtergraph指定的过滤器图，并使用它来过滤流。          |\n\n了解了这些基本信息后，接下来我们看看 FFMPEG 具体都能干些什么吧。\n\n## 7. 录制\n\n首先通过下面的命令查看一下 mac 上都有哪些设备。\n\n```shell\n$ ffmpeg -f avfoundation -list_devices true -i \"\"\n```\n\n**录屏**\n\n```shell\n$ ffmpeg -f avfoundation -i 1 -r 30 out.yuv\n```\n\n- -f 指定使用 avfoundation 采集数据。\n\n- -i 指定从哪儿采集数据，它是一个文件索引号。在我的MAC上，1代表桌面（可以通过上面的命令查询设备索引号）。\n\n- -r 指定帧率。按ffmpeg官方文档说-r与-framerate作用相同，但实际测试时发现不同。-framerate 用于限制输入，而 -r 用于限制输出。\n\n注意：桌面的输入对帧率没有要求，所以不用限制桌面的帧率。其实限制了也没用。\n\n**录屏+声音**\n\n```shell\n$ ffmpeg  -f avfoundation -i 1:0  -r 29.97 -c:v libx264 -crf 0 -c:a libfdk_aac -profile:a aac_he_v2 -b:a 32k  out.flv\n```\n\n- -i 1:0 冒号前面的 \"1\" 代表的屏幕索引号。冒号后面的\"0\"代表的声音索相号。\n\n- -c:v 与参数 -vcodec 一样，表示视频编码器。c 是 codec 的缩写，v 是video的缩写。\n\n- -crf 是 x264 的参数。 0 表式无损压缩。\n\n- -c:a 与参数 -acodec 一样，表示音频编码器。\n\n- -profile 是 fdk_aac 的参数。 aac_he_v2 表式使用 AAC_HE v2 压缩数据。\n\n- -b:a 指定音频码率。 b 是 bitrate的缩写, a是 audio的缩与。\n\n**录视频**\n\n```shell\n$ ffmpeg -framerate 30 -f avfoundation -i 0 out.mp4\n```\n\n- -framerate 限制视频的采集帧率。这个必须要根据提示要求进行设置，如果不设置就会报错。\n\n- -f 指定使用 avfoundation 采集数据。\n\n- -i 指定视频设备的索引号。\n\n**视频+音频**\n\n```shell\n$ ffmpeg -framerate 30 -f avfoundation -i 0:0 out.mp4 \n```\n\n**录音**\n\n```shell\n$ ffmpeg -f avfoundation -i :0 out.wav\n```\n\n**录制音频裸数据**\n\n```shell\n$ ffmpeg  -f avfoundation -i :0 -ar 44100 -f s16le out.pcm\n```\n\n## 8. 分解与复用\n\n流拷贝是通过将 copy 参数提供给-codec选项来选择流的模式。它使得ffmpeg省略了指定流的解码和编码步骤，所以它只能进行多路分解和多路复用。 这对于更改容器格式或修改容器级元数据很有用。 在这种情况下，上图将简化为：\n\n<img src=\"/images/imageFFmpeg/分解与复用.png\">\n\n由于没有解码或编码，速度非常快，没有质量损失。 但是，由于许多因素，在某些情况下可能无法正常工作。 应用过滤器显然也是不可能的，因为过滤器处理未压缩的数据。\n\n**抽取音频流**\n\n```shell\n$ ffmpeg -i input.mp4 -acodec copy -vn out.aac\n```\n\n- acodec: 指定音频编码器，copy 指明只拷贝，不做编解码。\n\n- vn: v 代表视频，n 代表 no 也就是无视频的意思。\n\n**抽取视频流**\n\n```shell\n$ ffmpeg -i input.mp4 -vcodec copy -an out.h264\n```\n\n- vcodec: 指定视频编码器，copy 指明只拷贝，不做编解码。\n\n- an: a 代表视频，n 代表 no 也就是无音频的意思。\n\n**转格式**\n\n```shell\n$ ffmpeg -i out.mp4 -vcodec copy -acodec copy out.flv\n```\n\n上面的命令表式的是音频、视频都直接 copy，只是将 mp4 的封装格式转成了 flv。\n\n**音视频合并**\n\n```shell\n$ ffmpeg -i out.h264 -i out.aac -vcodec copy -acodec copy out.mp4\n```\n\n## 9. **处理原始数据**\n\n**提取YUV数据**\n\n```shell\n$ ffmpeg -i input.mp4 -an -c:v rawvideo -pixel_format yuv420p out.yuv\n$ ffplay -s wxh out.yuv\n```\n\n- -c:v rawvideo 指定将视频转成原始数据\n\n- -pixel_format yuv420p 指定转换格式为 yuv420p\n\n**YUV 转 H264**\n\n```shell\n$ ffmpeg -f rawvideo -pix_fmt yuv420p -s 320x240 -r 30 -i out.yuv -c:v libx264 -f rawvideo out.h264\n```\n\n**提取 PCM 数据**\n\n```shell\n$ ffmpeg -i out.mp4 -vn -ar 44100 -ac 2 -f s16le out.pcm\n$ ffplay -ar 44100 -ac 2 -f s16le -i out.pcm\n```\n\n**PCM 转 WAV**\n\n```shell\n$ ffmpeg -f s16be -ar 8000 -ac 2 -acodec pcm_s16be -i input.raw output.wav\n```\n\n## 10. **滤镜**\n\n在编码之前，ffmpeg 可以使用 libavfilter 库中的过滤器处理原始音频和视频帧。 几个链式过滤器形成一个过滤器图形。 ffmpeg 区分两种类型的过滤器图形：简单和复杂。\n\n### 10.1 简单滤镜\n\n简单的过滤器图是那些只有一个输入和输出，都是相同的类型。 在上面的图中，它们可以通过在解码和编码之间插入一个额外的步骤来表示：\n\n<img src=\"/images/imageFFmpeg/简单滤镜.png\">\n\n简单的 filtergraphs 配置了 per-stream-filter 选项（分别为视频和音频使用 `-vf` 和 `-af` 别名）。 一个简单的视频 filtergraph 可以看起来像这样的例子：\n\n<img src=\"/images/imageFFmpeg/简单滤镜-01.png\">\n\n请注意，某些滤镜会更改帧属性，但不会改变帧内容。 例如。 上例中的 fps 过滤器会改变帧数，但不会触及帧内容。 另一个例子是 setpts 过滤器，它只设置时间戳，否则不改变帧。\n\n### 10.2 复杂滤镜\n\n复杂的过滤器图是那些不能简单描述为应用于一个流的线性处理链的过滤器图。 例如，当图形有多个输入和/或输出，或者当输出流类型与输入不同时，就是这种情况。 他们可以用下图来表示：\n\n<img src=\"/images/imageFFmpeg/复杂滤镜.png\">\n\n复杂的过滤器图使用 `-filter_complex` 选项进行配置。 请注意，此选项是全局性的，因为复杂的过滤器图形本质上不能与单个流或文件明确关联。\n\n`-lavfi` 选项等同于 `-filter_complex`。\n\n一个复杂的过滤器图的一个简单的例子是覆盖过滤器，它有两个视频输入和一个视频输出，包含一个视频叠加在另一个上面。 它的音频对应是 amix 滤波器。\n\n**添加水印**\n\n```shell\n$ ffmpeg -i out.mp4  -vf \"movie=logo.png,scale=64:48[watermask];[in][watermask] overlay=30:10 [out]\" water.mp4\n```\n\n- -vf 中的 movie 指定 logo 位置。scale 指定 logo 大小。overlay 指定 logo 摆放的位置。\n\n**删除水印**\n\n先通过 ffplay 找到要删除 LOGO 的位置\n\n```shell\n$ ffplay -i test.flv -vf delogo=x=806:y=20:w=70:h=80:show=1\n```\n\n使用 delogo 滤镜删除 LOGO\n\n```shell\n$ ffmpeg -i test.flv -vf delogo=x=806:y=20:w=70:h=80 output.flv\n```\n\n**视频缩小一倍**\n\n```shell\n$ ffmpeg -i out.mp4 -vf scale=iw/2:-1 scale.mp4\n```\n\n- -vf scale 指定使用简单过滤器 scale，`iw/2:-1` 中的 iw 指定按整型取视频的宽度。 -1 表示高度随宽度一起变化。\n\n**视频裁剪**\n\n```shell\n$ ffmpeg -i VR.mov  -vf crop=in_w-200:in_h-200 -c:v libx264 -c:a copy -video_size 1280x720 vr_new.mp4\n```\n\ncrop 格式：`crop=out_w:out_h:x:y`\n\n- out_w: 输出的宽度。可以使用 in_w 表式输入视频的宽度。\n\n- out_h: 输出的高度。可以使用 in_h 表式输入视频的高度。\n\n- x : X坐标\n\n- y : Y坐标\n\n如果 x 和 y 设置为 0, 说明从左上角开始裁剪。如果不写是从中心点裁剪。\n\n**倍速播放**\n\n```shell\n$ ffmpeg -i out.mp4 -filter_complex \"[0:v]setpts=0.5*PTS[v];[0:a]atempo=2.0[a]\" -map \"[v]\" -map \"[a]\" speed2.0.mp4\n```\n\n- -filter_complex 复杂滤镜，`[0:v]` 表示第一个（文件索引号是 0）文件的视频作为输入。`setpts=0.5*PTS` 表示每帧视频的 pts 时间戳都乘 0.5 ，也就是差少一半。`[v]` 表示输出的别名。音频同理就不详述了。\n\n- map 可用于处理复杂输出，如可以将指定的多路流输出到一个输出文件，也可以指定输出到多个文件。\"[v]\" 复杂滤镜输出的别名作为输出文件的一路流。上面 map的用法是将复杂滤镜输出的视频和音频输出到指定文件中。\n\n**对称视频**\n\n```shell\n$ ffmpeg  -i out.mp4 -filter_complex \"[0:v]pad=w=2*iw[a];[0:v]hflip[b];[a][b]overlay=x=w\" duicheng.mp4\n```\n\n- hflip 水平翻转\n\n如果要修改为垂直翻转可以用 vflip。\n\n**画中画**\n\n```shell\n$ ffmpeg -i out.mp4 -i out1.mp4 -filter_complex \"[1:v]scale=w=176:h=144:force_original_aspect_ratio=decrease[ckout];[0:v][ckout]overlay=x=W-w-10:y=0[out]\" -map \"[out]\" -movflags faststart new.mp4\n```\n\n**录制画中画**\n\n```shell\n$ ffmpeg  -f avfoundation -i \"1\" -framerate 30 -f avfoundation -i \"0:0\" \n-r 30 -c:v libx264 -preset ultrafast \n-c:a libfdk_aac -profile:a aac_he_v2 -ar 44100 -ac 2 \n-filter_complex \"[1:v]scale=w=176:h=144:force_original_aspect_ratio=decrease[a];[0:v][a]overlay=x=W-w-10:y=0[out]\" \n-map \"[out]\" -movflags faststart -map 1:a b.mp4\n```\n\n**多路视频拼接**\n\n```shell\n$ ffmpeg  -f avfoundation -i \"1\" -framerate 30 -f avfoundation   -i \"0:0\" -r 30 -c:v libx264 -preset ultrafast -c:a libfdk_aac -profile:a aac_he_v2 -ar 44100 -ac 2 -filter_complex \"[0:v]scale=320:240[a];[a]pad=640:240[b];[b][1:v]overlay=320:0[out]\" -map \"[out]\" -movflags faststart  -map 1:a  c.mp4\n```\n\n## 11. **音视频的拼接与裁剪**\n\n**裁剪**\n\n```shell\n$ ffmpeg -i out.mp4 -ss 00:00:00 -t 10 out1.mp4\n```\n\n- -ss 指定裁剪的开始时间，精确到秒\n\n- -t 被裁剪后的时长。\n\n**合并**\n\n首先创建一个 inputs.txt 文件，文件内容如下：\n\n```shell\n$ file '1.flv'\n$ file '2.flv'\n$ file '3.flv'\n```\n\n然后执行下面的命令：\n\n```shell\n$ ffmpeg -f concat -i inputs.txt -c copy output.flv\n```\n\n**hls切片**\n\n```shell\n$ ffmpeg -i out.mp4 -c:v libx264 -c:a libfdk_aac -strict -2 -f hls  out.m3u8\n```\n\n- -strict -2 指明音频使有AAC。\n\n- -f hls 转成 m3u8 格式。\n\n## 12. 视频图片互转\n\n**视频转 JPEG**\n\n```shell\n$ ffmpeg -i test.flv -r 1 -f image2 image-%3d.jpeg\n```\n\n**视频转 gif**\n\n```shell\n$ ffmpeg -i out.mp4 -ss 00:00:00 -t 10 out.gif\n```\n\n**图片转视频**\n\n```shell\n$ ffmpeg  -f image2 -i image-%3d.jpeg images.mp4\n```\n\n## 13. **直播相关**\n\n**推流**\n\n```shell\n$ ffmpeg -re -i out.mp4 -c copy -f flv rtmp://server/live/streamName\n```\n\n**拉流保存**\n\n```shell\n$ ffmpeg -i rtmp://server/live/streamName -c copy dump.flv\n```\n\n**转流**\n\n```shell\n$ ffmpeg -i rtmp://server/live/originalStream -c:a copy -c:v copy -f flv rtmp://server/live/h264Stream\n```\n\n**实时推流**\n\n```shell\n$ ffmpeg -framerate 15 -f avfoundation -i \"1\" -s 1280x720 -c:v libx264  -f  flv rtmp://localhost:1935/live/room\n```\n\n## 14. **ffplay**\n\n**播放 YUV 数据**\n\n```shell\n$ ffplay -pix_fmt nv12 -s 192x144 1.yuv\n```\n\n**播放 YUV 中的 Y 平面**\n\n```shell\n$ ffplay -pix_fmt nv21 -s 640x480 -vf extractplanes='y' 1.yuv\n```\n\n\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"分辨率/码率/帧率/带宽的概念","url":"/2019-05-03/webrtc_note_2/","content":"\n### 分辨率\n\n> 【分辨率】(px) = VideoWidth * VideoHeight\n\n分辨率就是我们常说的600x400分辨率、1920x1080分辨率，分辨率影响视频图像的大小，与视频图像大小成正比：视频分辨率越高，图像越大，对应的视频文件本身大小也会越大。\n\n下面引用一下百度百科的解释：\n\n> 显示分辨率（屏幕分辨率）是屏幕图像的精密度，是指显示器所能显示的像素有多少。由于屏幕上的点、线和面都是由像素组成的，显示器可显示的像素越多，画面就越精细，同样的屏幕区域内能显示的信息也越多，所以分辨率是个非常重要的性能指标之一。可以把整个图像想象成是一个大型的棋盘，而分辨率的表示方式就是所有经线和纬线交叉点的数目。显示分辨率一定的情况下，显示屏越小图像越清晰，反之，显示屏大小固定时，显示分辨率越高图像越清晰。\n\n* * * * *\n\n### [](https://ouchunrun.github.io/2018/10/25/%E5%88%86%E8%BE%A8%E7%8E%87%E3%80%81%E5%B8%A7%E7%8E%87%E3%80%81%E7%A0%81%E7%8E%87%E3%80%81%E5%B8%A6%E5%AE%BD%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3/#%E7%A0%81%E7%8E%87 \"码率\")码率\n\n> 【码率】(kbps)=【文件大小】(bit) /【时间】(秒)\n\n码率是指视频文件在单位时间内使用的数据流量，也叫码流率。码率越大，说明单位时间内取样率越大，数据流精度就越高，这样表现出来的的效果就是：视频画面更清晰画质更高。\n\n-   比如被压缩的1080P的视频，假设它的长度为100分钟,大小为1GB。\\\n    【时间】(秒) = 100X60S=6000s\\\n    【文件大小】(bit) = 1GB=1024MB= 1024X1024KB=1024X1024X1024Byte=1024X1024X1024X8bit=8589934592bit\\\n    【码率】 = 8589934592 / 6000 = 1.4Mbit/s\\\n    也就是说这个视频的大概码率是1.4Mbit/s\n\n* * * * *\n\n### [](https://ouchunrun.github.io/2018/10/25/%E5%88%86%E8%BE%A8%E7%8E%87%E3%80%81%E5%B8%A7%E7%8E%87%E3%80%81%E7%A0%81%E7%8E%87%E3%80%81%E5%B8%A6%E5%AE%BD%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3/#%E5%B8%A7%E7%8E%87 \"帧率\")帧率\n\n帧率：FPS（每秒钟要多少帧画面）\n\n由于人类眼睛的特殊生理结构，如果所看画面之帧率高于24的时候，就会认为是连贯的，此现象称之为视觉暂留\n\n视频帧率影响的是画面流畅感，也就是说视频帧率超高，表现出来的效果就是：画面越显得流畅。你也可以这样理解，假设1秒只显1帧，那么一段视频看起来，就是有很明显的卡顿感，不流畅不连惯。当然视频帧率越高，意味着画面越多，也就相应的，这个视频文件的大小也会随之增加，占用存储空间也就增大了。帧率就是在1秒钟时间里传输的图片的帧数，也可以理解为图形处理器每秒钟能够刷新几次。\n\n所以说要想得到非常流畅的画面感，帧率当然是越高越好，但是超过60帧每秒估计人类也可能感知不出效果了。当然，在实际情况中我们还要考虑服务器整体性能。\n\n* * * * *\n\n### [](https://ouchunrun.github.io/2018/10/25/%E5%88%86%E8%BE%A8%E7%8E%87%E3%80%81%E5%B8%A7%E7%8E%87%E3%80%81%E7%A0%81%E7%8E%87%E3%80%81%E5%B8%A6%E5%AE%BD%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3/#%E5%B8%A6%E5%AE%BD \"带宽\")带宽\n\n网络带宽是指在单位时间（一般指的是1秒钟）内能传输的数据量。网络和高速公路类似，带宽越大，就类似高速公路的车道越多，其通行能力越强。网络带宽作为衡量网络特征的一个重要指标。\n\n> 传输带宽也指的是数据传输的速率。对于流媒体的播放，影响最大的属性就是传输带宽。如果带宽过低，使得数据传输下载的速度小于视频流播放的数率，那么在视频的播放将会经常出现停顿和缓冲，极大的影响了客户观看的流畅性；而为了保证视频观看的流畅性，在低带宽的条件下，只能选择低品质、低码流的视频进行传输，这样又会影响到客户的光看效果。所以，一个良好的传输带宽环境是客户活动高品质的流媒体体验的重要保证。\n\n### [](https://ouchunrun.github.io/2018/10/25/%E5%88%86%E8%BE%A8%E7%8E%87%E3%80%81%E5%B8%A7%E7%8E%87%E3%80%81%E7%A0%81%E7%8E%87%E3%80%81%E5%B8%A6%E5%AE%BD%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3/#%E6%B8%85%E6%99%B0%E5%BA%A6 \"清晰度\")清晰度\n\n在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰。\\\n在分辨率一定的情况下，码率与清晰度成正比关系，码率越高，图像越清晰；码率越低，图像越不清晰。\n\n* * * * *\n\n### [](https://ouchunrun.github.io/2018/10/25/%E5%88%86%E8%BE%A8%E7%8E%87%E3%80%81%E5%B8%A7%E7%8E%87%E3%80%81%E7%A0%81%E7%8E%87%E3%80%81%E5%B8%A6%E5%AE%BD%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%90%86%E8%A7%A3/#%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE%E8%BF%87%E7%A8%8B \"视频播放过程\")视频播放过程\n\n视频播放器播放一个互联网上的视频文件，需要经过以下几个步骤：解协议，解封装，解码视音频，视音频同步。如果播放本地文件则不需要解协议，为以下几个步骤：解封装，解码视音频，视音频同步。他们的过程如图所示。\n\n![9.png](https://i.loli.net/2018/10/25/5bd1b99f50d62.png)","tags":["视频流"],"categories":["WebRTC"]},{"title":"音视频核心技术","url":"/2019-04-30/reference/FFmpeg/音视频核心技术/","content":"\n> [雷神 FFmpeg源代码结构图 - 解码](<https://blog.csdn.net/leixiaohua1020/article/details/44220151>)\n>\n> [雷神 GitHub](<https://github.com/HatsuneMikuV/FFmpeg_Leixiaohua>)\n>\n> [雷神 FFmpeg blog](<https://blog.csdn.net/leixiaohua1020/column/info/ffmpeg-devel/7>)\n>\n> [ffmpeg filter过滤器 基础实例及全面解析](<https://blog.csdn.net/newchenxf/article/details/51364105>)\n\n<!-- more -->\n\n<img src=\"/images/imageFFmpeg/FFmpeg音视频架构-01.png\">\n\n<img src=\"/images/imageFFmpeg/FFmpeg音视频架构-02.png\">\n\n## 1. 学习大纲\n\n**FFmpeg 常用命令**：\n\n- 视频录制命令\n- 多媒体文件的分解/复用命令\n- 裁剪与合并命令\n- 图片/视频互转命令\n- 直播相关命令\n- 各种滤镜命令\n\n**FFmpeg 基本开发**：\n\n- C 语言回顾\n- FFmpeg 核心概念与常用结构体\n- 实战 - 多媒体文件的分解与复用\n- 实战 - 多媒体格式的互转\n- 实战 - 从 MP4 裁剪一段视频\n- 作业 - 实现一个简单的小咖秀\n\n**音视频编解码实战**：\n\n- 实战 - H264 解码\n- 实战 - H264 编码\n- 实战 - 音频 AAC 解码\n- 实战 - 音频 AAC 编码\n- 实战 - 视频转图片\n\n**音视频渲染实战**：\n\n- SDL 事件处理\n- SDL 视频文理渲染\n- SDL 音频渲染\n- 实战1 - 实现 YUV 视频播放\n- 实战2 - YUV 视频倍数播放\n- 实战3 - 实现 PCM 播放器\n\n**FFmpeg 开发播放器核心功能**：\n\n- 实战 - 实现 MP4 文件的视频播放\n- 实战 - 实现 MP4 文件的音频播放\n- 实战 - 实现一个初级播放器\n- 实战 - 音视频同步\n- 实战 - 实现播放器内核\n\n**Android 中实战 FFmpeg**：\n\n- 编译 Android 端可以使用的 FFmpeg\n- Java 与 C 语言相互调用\n- 实战 - Android 调用 FFmpeg\n\n**学习建议**：\n\n- 牢牢抓住音视频的处理机制，了解其本质\n- 勤加练习，熟能生巧\n- 待着问题去学习，事半功倍\n\n**音视频的广泛应用**：\n\n- 直播类：音视频会议、教育直播、娱乐/游戏直播\n- 短视频：抖音、快手、小咖秀\n- 网络视频：优酷、腾讯视频、爱奇艺等\n- 音视频通话：微信、QQ、Skype等\n- 视频监控\n- 人工智能：人脸识别，智能音箱等，更关注算法\n\n**播放器架构**：\n\n<img src=\"/images/imageFFmpeg/播放器架构.png\">\n\n**渲染流程**：\n\n<img src=\"/images/imageFFmpeg/渲染流程.png\">\n\n**FFmpeg 都能做啥**：\n\n- FFmpeg 是一个非常优秀的多媒体框架\n- FFmpeg 可以运行在 Linux、Mac、Windows 等平台上\n- 能够解码、编码、转码、复用、解复用、过滤音视频数据\n\n**FFmpeg 下载与安装**：\n\n<details><summary>FFMpeg 下载与安装</summary> \n\n```shell\n$ git clone https://git.ffmpeg.org/ffmpeg.git\n$ config -- help\n$ make && make install\n```\n\n</details>\n\n## 2. FFmpeg 常用命令实战\n\n我们按使用目的可以将 FFMPEG 命令分成以下几类：\n\n- 基本信息查询命令\n- 录制\n- 分解 / 复用\n- 处理原始数据\n- 滤镜\n- 切割与合并\n- 图／视互转\n- 直播相关\n\n除了 FFMPEG 的基本信息查询命令外，其它命令都按下图所示的流程处理音视频。\n\n<img src=\"/images/imageFFmpeg/FFmpeg处理音视频流程.png\">\n\n<img src=\"/images/imageFFmpeg/FFmpeg基本信息查询命令.png\">\n\n<img src=\"/images/imageFFmpeg/FFmpeg录屏命令.png\">\n\n```shell\n$ ffplay -s 2560x1600 -pix_fmt uyvy422 out.yuv\n```\n\n<img src=\"/images/imageFFmpeg/分解与复用-01.png\">\n\n<img src=\"/images/imageFFmpeg/多媒体格式转换.png\">\n\n## 3. 初级开发内容\n\n- FFmpeg 日志的使用及目录的操作\n- 介绍 FFmpeg 的基本概念及常用的结构体\n- 对复用/解复用及流程操作的各种实践\n\nFFmpeg 代码结构：\n\n- libavcodec： 提供了一系列编码器的实现。\n- libavformat： 实现在流协议，容器格式及其本IO访问。\n- libavutil： 包括了hash器，解码器和各类工具函数。\n- libavfilter： 提供了各种音视频过滤器。\n- libavdevice： 提供了访问捕获设备和回放设备的接口。\n- libswresample： 实现了混音和重采样。\n- libswscale： 实现了色彩转换和缩放工能。\n\n### 3.1 FFmpeg 日志系统\n\n```c++\n#include <libavutil/log.h>\n\nav_log_set_level(AV_LOG_DEBUG)\n    \nav_log(NULL, AV_LOG_INFO, \"...%s\\n\", op)\n```\n\n- AV_LOG_ERROR\n- AV_LOG_WARNING\n- AV_LOG_INFO\n\n<details><summary>FFmpeg日志系统使用</summary>\n\n```c\n#include <stdio.h>\n#include <libavutil/log.h>\n\nint main(int argc, char *argv[])\n{\n    av_log_set_level(AV_LOG_DEBUG);\n\n    av_log(NULL, AV_LOG_INFO, \"hello world: %s!\\n\", \"aaa\");\n\n    return 0;\n}\n```\n\n</details>\n\n### 3.2 FFmpeg 文件与目录操作\n\n文件的删除与重命名：\n\n```c++\n#include <libavformat/avformat.h>\n\navpriv_io_delete()\n    \navpriv_io_move(src, dst)\n```\n\n<details><summary>FFmpeg文件与目录操作</summary>\n\n```c\n#include <stdio.h>\n#include <libavutil/log.h>\n#include <libavformat/avformat.h>\n\nint main(int argc, char *argv[])\n{\n    int ret;\n    ret = avpriv_io_delete(\"./mytestfile.txt\");\n    if(ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Failed to delete file mytestfile.txt\\n\");\n        return -1\n    }\n    \n    ret = avpriv_io_move(\"111.txt\", \"222.txt\");\n    if(ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Filed to rename\\n\");\n        return -1;\n    } \n\n    return 0;\n}\n```\n\n```shell\n$ clang -g -o ffmpeg_del ffmpeg_file.c `pkg-config --libs libavformat`\n\n# pkg-config --libs libavformat 指令可以搜索libavformat库所在路径\n\n$ pkg-config --libs libavformat\n-L/usr/local/ffmpeg/lib -lavformat\n```\n\n</details>\n\n### 3.3 FFmpeg 操作目录重要函数\n\n```c\navio_open_dir()\navio_read_dir()\navio_close_dir()\n```\n\n操作目录重要结构体：\n\n- AVIODirContext\n\n  操作目录的上下文\n\n- AVIODirEntry\n\n  目录项。用于存放文件名，文件大小等信息\n\n<details><summary>FFmpeg操作目录</summary>\n\n```c\n#include <stdio.h>\n#include <libavutil/log.h>\n#include <libavformat/avformat.h>\n\nint main(int argc, char *argv[])\n{\n    av_log_set_level(AV_LOG_INFO);\n\n    int ret;\n    AVIODirContext *ctx = NULL;\n    AVIODirEntry *entry = NULL;\n\n    ret = avio_open_dir(&ctx, \"./\", NULL);\n    if (ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Cant open dir:%s\\n\", av_err2str(ret));\n        return -1;\n    }\n    while(1) {\n        ret = avio_read_dir(ctx, &entry);\n        if (ret < 0) {\n            av_log(NULL, AV_LOG_ERROR, \"Cant read dir: %s\\n\", av_err2str(ret));\n            goto __fail;\n        }\n        if (!entry) {\n            break;\n        }\n\n        av_log(NULL, AV_LOG_INFO, \"%l2\"PRId64\" %s\\n\",\n               entry->size,\n               entry->name);\n\n        avio_free_directory_entry(&entry);\n    }\n__fail:\n    avio_close_dir(&ctx);\n    return 0;\n}\n```\n\n```shell\n$ clang -g -o list ffmpeg_list.c `pkg-config --libs libavformat libavutil`\n```\n\n</details>\n\n### 3.4 多媒体文件的基本概念\n\n- 多媒体文件其实是个容器\n- 在容器里有很多流（Stream/Track)\n- 每种流是由不同的编码器编码的\n- 从流中读出的数据称为包\n- 在一个包中包含着一个或多个帧\n\n几个重要的结构体：\n\n- AVFormatContext\n- AVStream\n- AVPacket\n\nFFmpeg 操作流数据的基本步骤：\n\n解复用 —> 获取流 —> 读取数据包 —>  释放资源\n\n### 3.5 [实战] 打印音/视频信息\n\n```c\nav_register_all()\navformat_open_input() / avformat_close_input()\nav_dump_format()\n```\n\n<details><summary>[实战] 打印音/视频信息</summary>\n\n```c\n#include <stdio.h>\n#include <libavutil/log.h>\n#include <libavformat/avformat.h>\n\nint main(int argc, char *argv[])\n{\n    int ret;\n    av_log_set_level(AV_LOG_INFO);\n\n    AVFormatContext *fmt_ctx = NULL;\n\n    av_register_all();\n\n    ret = avformat_open_input(&fmt_ctx, \"./test.mp4\", NULL, NULL);\n    if (ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Can't open file: %s\\n\", av_err2str(ret));\n        return -1;\n    }\n\n    av_dump_format(fmt_ctx, 0, \"./test.mp4\", 0);\n\n    avformat_close_input(&fmt_ctx);\n\n    return 0;\n}\n```\n\n</details>\n\n### 3.6 [实战] 抽取音频数据\n\n```c\nav_init_packet()\nav_find_best_stream()\nav_read_frame() / av_packet_unref()\n```\n\n<details><summary>[实战] 抽取音频数据</summary>\n\n```c\n#include <stdio.h>\n#include <libavutil/log.h>\n#include <libavformat/avformat.h>\n\nint main(int argc, char *argv[])\n{\n    int ret;\n    int len;\n    int audio_index;\n\n    char *src = NULL;\n    char *dst = NULL;\n\n    av_log_set_level(AV_LOG_INFO);\n\n    AVPacket pkt;\n    AVFormatContext *fmt_ctx = NULL;\n\n    av_register_all();\n\n    // 1. read two params form console\n    if (argc < 3) {\n        av_log(NULL, AV_LOG_ERROR, \"eg: %s in_file out_file\\n\", argv[0]);\n        return -1;\n    }\n    src = argv[1];\n    dst = argv[2];\n    if (!src || !dst) {\n        av_log(NULL, AV_LOG_ERROR, \"src or dst is null\\n\");\n        return -1;\n    }\n\n    ret = avformat_open_input(&fmt_ctx, src, NULL, NULL);\n    if (ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Can't open file: %s\\n\", av_err2str(ret));\n        return -1;\n    }\n\n    FILE *dst_fd = fopen(dst, \"wb\");\n    if (dst_fd) {\n        av_log(NULL, AV_LOG_ERROR, \"Can't open out file!\\n\");\n        avformat_close_input(&fmt_ctx);\n        return -1;\n    }\n    av_dump_format(fmt_ctx, 0, src, 0);\n\n    // 2. get stream\n    ret = av_find_best_stream(fmt_ctx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0);\n    if (ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Can't find the best stream!\\n\");\n        avformat_close_input(&fmt_ctx);\n        fclose(dst_fd);\n        return -1;\n    }\n\n    audio_index = ret;\n    av_init_packet(&pkt);\n    while(av_read_frame(fmt_ctx, &pkt) >= 0) {\n        if (pkt.stream_index == audio_index) {\n            // 3. write audio data to aac file.\n            len = fwrite(pkt.data, 1, pkt.size, dst_fd);\n            if (len != pkt.size) {\n                av_log(NULL, AV_LOG_WARNING, \"warning, length of data is not equal size of pkt!\\n\");\n            }\n        }\n        av_packet_unref(&pkt);\n    }\n\n    avformat_close_input(&fmt_ctx);\n    if (dst_fd) {\n        fclose(dst_fd);\n    }\n\n    return 0;\n}\n```\n\n```shell\n$ lang -g -o extra_audio extra_audio.c `pkg-config --libs libavutil libavformat`\n$ ./extra_audio test.mp4 killer.aa\n```\n\n</details>\n\n### 3.7 [实战] 抽取视频数据\n\n- Start code\n- SPS/PPS\n- codec -> extradata\n\n### 3.8 [实战] 将 MP4 转成 FLV 格式\n\n```c\navformat_alloc_output_context2() / avformat_free_context();\navformat_new_stream();\navcodec_parameters_copy();\navformat_write_header();\nav_write_frame() / av_interleaved_write_frame();\nav_write_trailer()\n```\n\n### 3.9 [实战] 从 MP4 截取一段视频\n\n```c\nav_seek_frame()\n```\n\n<details><summary>从 MP4 截取一段视频代码:</summary>\n\n```c++\n#include <stdlib.h>\n#include <libavutil/timestamp.h>\n#include <libavformat/avformat.h>\n\nstatic void log_packet(const AVFormatContext *fmt_ctx, const AVPacket *pkt, const char *tag)\n{\n    AVRational *time_base = &fmt_ctx->streams[pkt->stream_index]->time_base;\n\n    printf(\"%s: pts:%s pts_time:%s dts:%s dts_time:%s duration:%s duration_time:%s stream_index:%d\\n\",\n           tag,\n           av_ts2str(pkt->pts), av_ts2timestr(pkt->pts, time_base),\n           av_ts2str(pkt->dts), av_ts2timestr(pkt->dts, time_base),\n           av_ts2str(pkt->duration), av_ts2timestr(pkt->duration, time_base),\n           pkt->stream_index);\n}\n\nint cut_video(double from_seconds, double end_seconds, const char* in_filename, const char* out_filename) {\n    AVOutputFormat *ofmt = NULL;\n    AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;\n    AVPacket pkt;\n    int ret, i;\n\n    av_register_all();\n    \n    if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {\n        fprintf(stderr, \"Could not open input file '%s'\", in_filename);\n        goto end;\n    }\n    \n    if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {\n        fprintf(stderr, \"Failed to retrieve input stream information\");\n        goto end;\n    }\n    \n    av_dump_format(ifmt_ctx, 0, in_filename, 0);\n    \n    avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, out_filename);\n    if (!ofmt_ctx) {\n        fprintf(stderr, \"Could not create output context\\n\");\n        ret = AVERROR_UNKNOWN;\n        goto end;\n    }\n    \n    ofmt = ofmt_ctx->oformat;\n    \n    for (i = 0; i < ifmt_ctx->nb_streams; i++) {\n        AVStream *in_stream = ifmt_ctx->streams[i];\n        AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream->codec->codec);\n        if (!out_stream) {\n            fprintf(stderr, \"Failed allocating output stream\\n\");\n            ret = AVERROR_UNKNOWN;\n            goto end;\n        }\n    \n        ret = avcodec_copy_context(out_stream->codec, in_stream->codec);\n        if (ret < 0) {\n            fprintf(stderr, \"Failed to copy context from input to output stream codec context\\n\");\n            goto end;\n        }\n        out_stream->codec->codec_tag = 0;\n        if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER)\n            out_stream->codec->flags |= AV_CODEC_FLAG_GLOBAL_HEADER;\n    }\n    av_dump_format(ofmt_ctx, 0, out_filename, 1);\n    \n    if (!(ofmt->flags & AVFMT_NOFILE)) {\n        ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);\n        if (ret < 0) {\n            fprintf(stderr, \"Could not open output file '%s'\", out_filename);\n            goto end;\n        }\n    }\n    \n    ret = avformat_write_header(ofmt_ctx, NULL);\n    if (ret < 0) {\n        fprintf(stderr, \"Error occurred when opening output file\\n\");\n        goto end;\n    }\n    \n    //    int indexs[8] = {0};\n\n\n    //    int64_t start_from = 8*AV_TIME_BASE;\n    ret = av_seek_frame(ifmt_ctx, -1, from_seconds*AV_TIME_BASE, AVSEEK_FLAG_ANY);\n    if (ret < 0) {\n        fprintf(stderr, \"Error seek\\n\");\n        goto end;\n    }\n    \n    int64_t *dts_start_from = malloc(sizeof(int64_t) * ifmt_ctx->nb_streams);\n    memset(dts_start_from, 0, sizeof(int64_t) * ifmt_ctx->nb_streams);\n    int64_t *pts_start_from = malloc(sizeof(int64_t) * ifmt_ctx->nb_streams);\n    memset(pts_start_from, 0, sizeof(int64_t) * ifmt_ctx->nb_streams);\n    \n    while (1) {\n        AVStream *in_stream, *out_stream;\n    \n        ret = av_read_frame(ifmt_ctx, &pkt);\n        if (ret < 0)\n            break;\n    \n        in_stream  = ifmt_ctx->streams[pkt.stream_index];\n        out_stream = ofmt_ctx->streams[pkt.stream_index];\n    \n        log_packet(ifmt_ctx, &pkt, \"in\");\n    \n        if (av_q2d(in_stream->time_base) * pkt.pts > end_seconds) {\n            av_free_packet(&pkt);\n            break;\n        }\n    \n        if (dts_start_from[pkt.stream_index] == 0) {\n            dts_start_from[pkt.stream_index] = pkt.dts;\n            printf(\"dts_start_from: %s\\n\", av_ts2str(dts_start_from[pkt.stream_index]));\n        }\n        if (pts_start_from[pkt.stream_index] == 0) {\n            pts_start_from[pkt.stream_index] = pkt.pts;\n            printf(\"pts_start_from: %s\\n\", av_ts2str(pts_start_from[pkt.stream_index]));\n        }\n    \n        /* copy packet */\n        pkt.pts = av_rescale_q_rnd(pkt.pts - pts_start_from[pkt.stream_index], in_stream->time_base, out_stream->time_base, AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX);\n        pkt.dts = av_rescale_q_rnd(pkt.dts - dts_start_from[pkt.stream_index], in_stream->time_base, out_stream->time_base, AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX);\n        if (pkt.pts < 0) {\n            pkt.pts = 0;\n        }\n        if (pkt.dts < 0) {\n            pkt.dts = 0;\n        }\n        pkt.duration = (int)av_rescale_q((int64_t)pkt.duration, in_stream->time_base, out_stream->time_base);\n        pkt.pos = -1;\n        log_packet(ofmt_ctx, &pkt, \"out\");\n        printf(\"\\n\");\n    \n        ret = av_interleaved_write_frame(ofmt_ctx, &pkt);\n        if (ret < 0) {\n            fprintf(stderr, \"Error muxing packet\\n\");\n            break;\n        }\n        av_free_packet(&pkt);\n    }\n    free(dts_start_from);\n    free(pts_start_from);\n    \n    av_write_trailer(ofmt_ctx);\n    \nend:\n    avformat_close_input(&ifmt_ctx);\n    \n    /* close output */\n    if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))\n        avio_closep(&ofmt_ctx->pb);\n    avformat_free_context(ofmt_ctx);\n    \n    if (ret < 0 && ret != AVERROR_EOF) {\n        fprintf(stderr, \"Error occurred: %s\\n\", av_err2str(ret));\n        return 1;\n    }\n    \n    return 0;\n}\n\nint main(int argc, char *argv[]){\n    if(argc < 5){\n        fprintf(stderr, \"Usage: \\\n                command startime, endtime, srcfile, outfile\");\n        return -1;\n    }\n\n    double startime = atoi(argv[1]);\n    double endtime = atoi(argv[2]);\n    cut_video(startime, endtime, argv[3], argv[4]);\n    \n    return 0;\n}\n```\n\n</details>\n\n### 3.10 [实战] 一个简单的小咖秀\n\n- 将两个媒体文件中分别抽取音频与视频轨\n- 将音频与视频轨合并成一个新文件\n- 对音频与视频轨进行裁剪\n\n ## 4. FFmpeg 中级开发内容\n\n- FFmpeg H264 解码\n- FFmpeg H264 编码\n- FFmpeg AAC 解码\n- FFmpeg AAC 编码\n\n### 4.1 FFmpeg H264 解码\n\n```c\n#include <libavcodec/avcodec.h>\n```\n\n常用数据结构：\n\n- AVCodec 编码器结构体\n- AVCodecContext 编码器上下文\n- AVFrame 解码后的帧\n\n 结构体内存的分配与释放：\n\n```c\nav_frame_alloc / av_frame_free();\navcodec_alloc_context3();\navcodec_free_context();\n```\n\n解码步骤：\n\n- 查找解码器（avcodec_find_decoder）\n- 打开解码器（avcodec_open2）\n- 解码（avcodec_decode_video2）\n\n### 4.2 FFmpeg H264 编码\n\nH264编码流程：\n\n- 查找编码器（avcodec_find_encoder_by_name）\n- 设置参数，打开编码器（avcondec_open2）\n- 编码（avcondec_encode_video2）\n\n### 4.3 视频转图片\n\nTODO\n\n### 4.4 FFmpeg AAC 编码\n\n- 编码流程与视频相同\n- 编码函数 avcodec_encodec_audio2\n\n## 5. SDL 介绍\n\n> [SDL 官网]([http://www.libsdl.org](http://www.libsdl.org/))\n\n- SDL（Simple DirectMedia Layer） 是一套[开放源代码](https://zh.wikipedia.org/wiki/%E9%96%8B%E6%94%BE%E5%8E%9F%E5%A7%8B%E7%A2%BC)的[跨平台](https://zh.wikipedia.org/wiki/%E8%B7%A8%E5%B9%B3%E5%8F%B0)[多媒体](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%AA%92%E9%AB%94)开发[库](https://zh.wikipedia.org/wiki/%E5%87%BD%E5%BC%8F%E5%BA%AB)\n- 由 C 语言实现的跨平台的媒体开源库\n- 多用于开发游戏、模拟器、媒体播放器等多媒体应用领域\n\n语法与子系统：\n\nSDL将功能分成下列数个子系统（subsystem）：\n\n- **Video（图像）**—图像控制以及线程（thread）和事件管理（event）。\n- **Audio（声音）**—声音控制\n- **Joystick（摇杆）**—游戏摇杆控制\n- **CD-ROM（光盘驱动器）**—光盘媒体控制\n- **Window Management（视窗管理）**－与视窗程序设计集成\n- **Event（事件驱动）**－处理事件驱动\n\n以下是一支用C语言写成、非常简单的SDL示例：\n\n```c\n// Headers\n#include \"SDL.h\"\n\n// Main function\nint main(int argc, char* argv[])\n{\n    // Initialize SDL\n    if(SDL_Init(SDL_INIT_EVERYTHING) == -1)\n        return(1);\n\n    // Delay 2 seconds\n    SDL_Delay(2000);\n\n    // Quit SDL\n    SDL_Quit();\n\n    // Return\n    return 0;\n}\n```\n\n上述程序会加载所有SDL子系统（出错则退出程序），然后暂停两秒，最后关闭SDL并退出程序。\n\n### 5.1 SDL 编译与安装\n\n- 下载 SDL 源码\n- 生成Makefile configure --prefix=/usr/local\n- 安装 sudo make -j 8 && make install\n\n### 5.2 使用 SDL 基本步骤\n\n- 添加头文件 #include <SDL.h>\n- 初始化 SDL\n- 退出 SDL\n\nSDL 渲染窗口：\n\n```c\nSDL_Init() / SDL_Quit();\nSDL_CreateWindow() / SDL_DestoryWindow();\nSDL_CreateRender();  // 创建渲染器\n```\n\n```shell\n$ clang -g -o first_sdl first_sdl.c `pkg-config --libs sdl2`\n```\n\nSDL 渲染窗口：\n\n```c\nSDL_CreateRender() / SDL_DestoryRenderer();\nSDL_RenderClear();\nSDL_RenderPresent();\n```\n\n### 5.3 SDL 事件基本原理\n\n- SDL 将所有的事件都存放在一个队列中\n- 所有对事件的操作，其实就是队列的操作\n\nSDL 事件种类：\n\n- SDL_WindowEvent：窗口事件\n- SDL_KeyboardEvent：键盘事件\n- SDL_MouseMotionEvent：鼠标事件\n- 自定义事件\n\nSDL 事件处理：\n\n```c\nSDL_PollEvent(); // 轮询检测\nSDL_WaitEvent(); // 常用的方式\nSDL_WaitEventTimeout();\n```\n\n### 5.4 文理渲染\n\nSDL 渲染基本原理：\n\n<img src=\"/images/imageFFmpeg/SDL渲染基本原理.png\">\n\nSDL 文理相关 API：\n\n```c\nSDL_CreateTexture();\n- format: YUV, RGB\n- access: Texture 类型， Target， Stream\n\nSDL_DestroyTexture();\n```\n\nSDL 渲染相关 API：\n\n```c\nSDL_SetRenderTarget();\nSDL_RenderClear();\nSDL_RenderCopy();\nSDL_RenderPresent();\n```\n\n### 5.5 [实战] YUV 视频播放器\n\n创建线程：\n\n```c\nSDL_CreateThread();\n- fn: 线程执行函数\n- name: 线程名\n- data: 执行函数参数\n```\n\nSDL 更新文理：\n\n```c \nSDL_UpdateTexture();\nSDL_UpdateYUVTexture();\n```\n\n### 5.6 SDL 播放音频\n\n播放音频基本流程：\n\n<img src=\"/images/imageFFmpeg/播放音频基本流程.png\">\n\n播放音频的基本原则：\n\n- 声卡向你要数据而不是你主动推给声卡\n- 数据的多少由音频参数决定的\n\nSDL 音频 API：\n\n```c\nSDL_OpenAudio() / SDL_CloseAudio();\nSDL_PauseAudio();\nSDL_MixAudio();\n```\n\n### 5.7 实现 PCM 播放器\n\nTODO\n\n## 6. 最简单的播放器\n\n- 该播放器只实现视频播放\n- 将 FFmpeg 与 SDL 结合到一起\n- 通过 FFmpeg 解码视频数据\n- 通过 SDL 进行渲染\n\n```shell\n$ clang -g -o player2 player2.c `pkg-config --cflags --libs sdl2 libavformat libavutil libswscale libavcodec libswresample`\n```\n\n最简单的播放器之二：\n\n- 可以同时播放音频与视频\n- 使用队列存放音频包\n\n### 6.1 多线程与锁\n\n为什么要用多线程：\n\n- 多线程的好处\n- 多线程带来的问题\n\n线程的互斥与同步：\n\n- 互斥\n\n- 同步\n\n  大的任务分为很多小任务通过信号协调\n\n锁与信号量：\n\n- 锁的种类\n- 通过信号进行同步\n\n锁的中种类：\n\n- 读写锁\n- 自旋锁\n- 可重入锁\n\nSDL 线程的创建：\n\n```c\nSDL_CreateThread();\nSDL_WaitThread();\n```\n\nSDL 锁：\n\n```c\nSDL_CreateMutex() / SDL_DestroyMutex();  // 创建互斥量\nSDL_LockMutex() / SDL_UnlockMutex();\t // 锁互斥量于解锁互斥量\n```\n\nSDL 条件变量：\n\n```c\nSDL_CreateCond() / SDL_DestroyCond();\nSDL_CondWait() / SDL_CondSignal();\n```\n\n### 6.2 锁与条件变量的使用\n\nTODO\n\n### 6.3 播放器线程模型\n\n<img src=\"/images/imageFFmpeg/播放器线程模型.png\">\n\n### 6.4 线程的退出机制\n\n- 主线程接收到退出事件\n- 解复用线程在循环分流时对 quit 进行判断\n- 视频解码线程从视频流队列中取包时对 quit 进行判断\n- 音视解码从音频流队列中取包时对 quit 进行判断\n- 音视循环解码时对 quit 进行判断\n- 在收到信号变量消息时对 quit 进行判断\n\n### 6.5 音视频同步\n\n时间戳：\n\n- PTS：Presentation timestamp  渲染时间戳\n- DTS：Decoding timestamp 解码时间戳\n- I（intra）/ B（bidirectional）/ P（predicted）帧\n\n时间戳顺序：\n\n- 实际帧顺序：I B B P\n- 存放帧顺序：I P B B\n- 解码时间戳：1 4 2 3\n- 展示时间戳：1 2 3 4\n\n由于有了 B 帧之后，它打乱了 PTS 时间戳，所以加了 DTS 解码时间戳。在大多数没有 B 帧的情况下 PTS 和 DTS 是一致的。\n\n从哪儿获得 PTS：\n\n- AVPacket 中的 PTS\n- AVFrame 中的 PTS\n- av_frame_get_best_effort_timestamp()\n\n时间基：\n\n- tbr：帧率\n- tbn：time base of stream 流的时间基\n- tbc：time base of codec 解码的时间基\n\n计算当前帧的 PTS：\n\n- `PTS = PTS * av_q2d(video_stream->time_base)`\n- `av_q2d(AVRotional a){ return a.num / (double)a.den; }`\n\n计算下一帧的 PTS：\n\n- video_clock：预测的下一帧视频的 PTS\n- frame_delay：1/tbr\n- audio_clock：音频当前播放的时间戳\n\n音视频同步的时候需要计算 audio_clock 和 video_clock，看视屏时间是在音频时间之前还是在音频时间之后，如果是在音频时间之前就立即播放，如果在音频时间之后需要 delay 一段时间播放（delay的时间计算：audio_clock  - video_clock）\n\n音视频同步方式：\n\n- 视频同步到音频\n- 音频同步到视频\n- 音频和视频都同步到系统时钟  \n\n视频播放的基本思路：\n\n- 一般的做法，展示第一帧视频帧后，获得要显示的下一个视频帧的 PTS，然后设置一个定时器，当定时器超时时后，刷新新的视屏帧，如此反复操作。\n\n<details><summary>最简单的播放器：</summary>\n\n```c++\n#include <stdio.h>\n#include <assert.h>\n#include <math.h>\n\n#include <SDL.h>\n\n#include <libavcodec/avcodec.h>\n#include <libavformat/avformat.h>\n#include <libswscale/swscale.h>\n#include <libswresample/swresample.h>\n\n// compatibility with newer API\n#if LIBAVCODEC_VERSION_INT < AV_VERSION_INT(55,28,1)\n#define av_frame_alloc avcodec_alloc_frame\n#define av_frame_free avcodec_free_frame\n#endif\n\n#define SDL_AUDIO_BUFFER_SIZE 1024\n#define MAX_AUDIO_FRAME_SIZE 192000 //channels(2) * data_size(2) * sample_rate(48000)\n\n#define MAX_AUDIOQ_SIZE (5 * 16 * 1024)\n#define MAX_VIDEOQ_SIZE (5 * 256 * 1024)\n\n#define AV_SYNC_THRESHOLD 0.01\n#define AV_NOSYNC_THRESHOLD 10.0\n\n#define SAMPLE_CORRECTION_PERCENT_MAX 10\n#define AUDIO_DIFF_AVG_NB 20\n\n#define FF_REFRESH_EVENT (SDL_USEREVENT)\n#define FF_QUIT_EVENT (SDL_USEREVENT + 1)\n\n#define VIDEO_PICTURE_QUEUE_SIZE 1\n#define DEFAULT_AV_SYNC_TYPE AV_SYNC_AUDIO_MASTER //AV_SYNC_VIDEO_MASTER\n\ntypedef struct PacketQueue {\n\tAVPacketList *first_pkt, *last_pkt;\n    int nb_packets;\n    int size;\n    SDL_mutex *mutex;\n    SDL_cond *cond;\n} PacketQueue;\n\ntypedef struct VideoPicture {\n    AVPicture *bmp;\n    int width, height; /* source height & width */\n    int allocated;\n    double pts;\n} VideoPicture;\n\ntypedef struct VideoState {\n    //multi-media file\n    char            filename[1024];\n    AVFormatContext *pFormatCtx;\n    int             videoStream, audioStream;\n\n    //sync\n    int             av_sync_type;\n    double          external_clock; /* external clock base */\n    int64_t         external_clock_time;\n\n    double          audio_diff_cum; /* used for AV difference average computation */\n    double          audio_diff_avg_coef;\n    double          audio_diff_threshold;\n    int             audio_diff_avg_count;\n\n    double          audio_clock;\n    double          frame_timer;\n    double          frame_last_pts;\n    double          frame_last_delay;\n\n    double          video_clock; ///<pts of last decoded frame / predicted pts of next decoded frame\n    double          video_current_pts; ///<current displayed pts (different from video_clock if frame fifos are used)\n    int64_t         video_current_pts_time;  ///<time (av_gettime) at which we updated video_current_pts - used to have running video pts\n\n    //audio\n    AVStream        *audio_st;\n    AVCodecContext  *audio_ctx;\n    PacketQueue     audioq;\n    uint8_t         audio_buf[(MAX_AUDIO_FRAME_SIZE * 3) / 2];\n    unsigned int    audio_buf_size;\n    unsigned int    audio_buf_index;\n    AVFrame         audio_frame;\n    AVPacket        audio_pkt;\n    uint8_t         *audio_pkt_data;\n    int             audio_pkt_size;\n    int             audio_hw_buf_size;\n\n    //video\n    AVStream        *video_st;\n    AVCodecContext  *video_ctx;\n    PacketQueue     videoq;\n    struct SwsContext *video_sws_ctx;\n    struct SwrContext *audio_swr_ctx;\n\n    VideoPicture    pictq[VIDEO_PICTURE_QUEUE_SIZE];\n    int             pictq_size, pictq_rindex, pictq_windex;\n    SDL_mutex       *pictq_mutex;\n    SDL_cond        *pictq_cond;\n\n    SDL_Thread      *parse_tid;\n    SDL_Thread      *video_tid;\n\n    int             quit;\n} VideoState;\n\nSDL_mutex    *text_mutex;\nSDL_Window   *win = NULL;\nSDL_Renderer *renderer;\nSDL_Texture  *texture;\n\nenum {\n    AV_SYNC_AUDIO_MASTER,\n    AV_SYNC_VIDEO_MASTER,\n    AV_SYNC_EXTERNAL_MASTER,\n};\n\nFILE *yuvfd = NULL;\nFILE *audiofd = NULL;\n\n/* Since we only have one decoding thread, the Big Struct\n   can be global in case we need it. */\nVideoState *global_video_state;\n\nvoid packet_queue_init(PacketQueue *q) {\n    memset(q, 0, sizeof(PacketQueue));\n    q->mutex = SDL_CreateMutex();\n    q->cond = SDL_CreateCond();\n}\n\nint packet_queue_put(PacketQueue *q, AVPacket *pkt) {\n    AVPacketList *pkt1;\n    if(av_dup_packet(pkt) < 0) {\n        return -1;\n    }\n    pkt1 = av_malloc(sizeof(AVPacketList));\n    if (!pkt1)\n        return -1;\n    pkt1->pkt = *pkt;\n    pkt1->next = NULL;\n\n    SDL_LockMutex(q->mutex);\n    \n    if (!q->last_pkt)\n        q->first_pkt = pkt1;\n    else\n        q->last_pkt->next = pkt1;\n    q->last_pkt = pkt1;\n    q->nb_packets++;\n    q->size += pkt1->pkt.size;\n    \n    SDL_CondSignal(q->cond);\n    SDL_UnlockMutex(q->mutex);\n    return 0;\n}\n\nint packet_queue_get(PacketQueue *q, AVPacket *pkt, int block)\n{\n    AVPacketList *pkt1;\n    int ret;\n    \n    SDL_LockMutex(q->mutex);\n\n    for(;;) {  \n        if(global_video_state->quit) {\n            ret = -1;\n            break;\n        }\n\n        pkt1 = q->first_pkt;\n        if (pkt1) {\n            q->first_pkt = pkt1->next;\n            if (!q->first_pkt)\n                q->last_pkt = NULL;\n            q->nb_packets--;\n            q->size -= pkt1->pkt.size;\n            *pkt = pkt1->pkt;\n            av_free(pkt1);\n            ret = 1;\n            break;\n        } else if (!block) {\n            ret = 0;\n            break;\n        } else {\n            SDL_CondWait(q->cond, q->mutex);\n        }\n    }\n    SDL_UnlockMutex(q->mutex);\n    return ret;\n}\n\ndouble get_audio_clock(VideoState *is) {\n    double pts;\n    int hw_buf_size, bytes_per_sec, n;\n\n    pts = is->audio_clock; /* maintained in the audio thread */\n    hw_buf_size = is->audio_buf_size - is->audio_buf_index;\n    bytes_per_sec = 0;\n    n = is->audio_ctx->channels * 2;\n    if(is->audio_st) {\n        bytes_per_sec = is->audio_ctx->sample_rate * n;\n    }\n    if(bytes_per_sec) {\n        pts -= (double)hw_buf_size / bytes_per_sec;\n    }\n    return pts;\n}\n\ndouble get_video_clock(VideoState *is) {\n    double delta;\n    delta = (av_gettime() - is->video_current_pts_time) / 1000000.0;\n    return is->video_current_pts + delta;\n}\n\ndouble get_external_clock(VideoState *is) {\n    return av_gettime() / 1000000.0;\n}\n\ndouble get_master_clock(VideoState *is) {\n    if(is->av_sync_type == AV_SYNC_VIDEO_MASTER) {\n        return get_video_clock(is);\n    } else if(is->av_sync_type == AV_SYNC_AUDIO_MASTER) {\n        return get_audio_clock(is);\n    } else {\n        return get_external_clock(is);\n    }\n}\n\n/* Add or subtract samples to get a better sync, return new\n   audio buffer size */\nint synchronize_audio(VideoState *is, short *samples,\n                 int samples_size, double pts) {\n    int n;\n    double ref_clock;\n\n    n = 2 * is->audio_ctx->channels;\n\n    if(is->av_sync_type != AV_SYNC_AUDIO_MASTER) {\n        double diff, avg_diff;\n        int wanted_size, min_size, max_size /*, nb_samples */;\n\n        ref_clock = get_master_clock(is);\n        diff = get_audio_clock(is) - ref_clock;\n\n        if(diff < AV_NOSYNC_THRESHOLD) {\n            // accumulate the diffs\n            is->audio_diff_cum = diff + is->audio_diff_avg_coef\n                * is->audio_diff_cum;\n            if(is->audio_diff_avg_count < AUDIO_DIFF_AVG_NB) {\n                is->audio_diff_avg_count++;\n            } else {\n                avg_diff = is->audio_diff_cum * (1.0 - is->audio_diff_avg_coef);\n                if(fabs(avg_diff) >= is->audio_diff_threshold) {\n                    wanted_size = samples_size + ((int)(diff * is->audio_ctx->sample_rate) * n);\n                    min_size = samples_size * ((100 - SAMPLE_CORRECTION_PERCENT_MAX) / 100);\n                    max_size = samples_size * ((100 + SAMPLE_CORRECTION_PERCENT_MAX) / 100);\n                    if(wanted_size < min_size) {\n                        wanted_size = min_size;\n                    } else if (wanted_size > max_size) {\n                        wanted_size = max_size;\n                    }\n                    if(wanted_size < samples_size) {\n                        /* remove samples */\n                        samples_size = wanted_size;\n                    } else if(wanted_size > samples_size) {\n                        uint8_t *samples_end, *q;\n                        int nb;\n\n                        /* add samples by copying final sample*/\n                        nb = (samples_size - wanted_size);\n                        samples_end = (uint8_t *)samples + samples_size - n;\n                        q = samples_end + n;\n                        while(nb > 0) {\n                            memcpy(q, samples_end, n);\n                            q += n;\n                            nb -= n;\n                        }\n                        samples_size = wanted_size;\n                    }\n                }\n            }\n        } else {\n            /* difference is TOO big; reset diff stuff */\n            is->audio_diff_avg_count = 0;\n            is->audio_diff_cum = 0;\n        }\n    }\n    return samples_size;\n}\n\nint audio_decode_frame(VideoState *is, uint8_t *audio_buf, \n               int buf_size, double *pts_ptr) {\n    int len1, data_size = 0;\n    AVPacket *pkt = &is->audio_pkt;\n    double pts;\n    int n;\n\n    for(;;) {\n        while(is->audio_pkt_size > 0) {\n            int got_frame = 0;\n            len1 = avcodec_decode_audio4(is->audio_ctx, &is->audio_frame, &got_frame, pkt);\n            if(len1 < 0) {\n                /* if error, skip frame */\n                is->audio_pkt_size = 0;\n                break;\n            }\n            data_size = 0;\n            if(got_frame) {\n                /*\n\t\t\t\tdata_size = av_samples_get_buffer_size(NULL, \n\t\t\t\t\t       is->audio_ctx->channels,\n\t\t\t\t\t       is->audio_frame.nb_samples,\n\t\t\t\t\t       is->audio_ctx->sample_fmt,\n\t\t\t\t\t       1);\n        \t\t*/\n                data_size = 2 * is->audio_frame.nb_samples * 2;\n                assert(data_size <= buf_size);\n\n                swr_convert(is->audio_swr_ctx,\n                            &audio_buf,\n                            MAX_AUDIO_FRAME_SIZE*3/2,\n                            (const uint8_t **)is->audio_frame.data,\n                            is->audio_frame.nb_samples);\n\n                fwrite(audio_buf, 1, data_size, audiofd);\n                //memcpy(audio_buf, is->audio_frame.data[0], data_size);\n            }\n            is->audio_pkt_data += len1;\n            is->audio_pkt_size -= len1;\n            if(data_size <= 0) {\n                /* No data yet, get more frames */\n                continue;\n            }\n            pts = is->audio_clock;\n            *pts_ptr = pts;\n            n = 2 * is->audio_ctx->channels;\n            is->audio_clock += (double)data_size /\n                (double)(n * is->audio_ctx->sample_rate);\n            /* We have data, return it and come back for more later */\n            return data_size;\n        }\n        if(pkt->data)\n            av_free_packet(pkt);\n\n        if(is->quit) {\n            return -1;\n        }\n        /* next packet */\n        if(packet_queue_get(&is->audioq, pkt, 1) < 0) {\n            return -1;\n        }\n        is->audio_pkt_data = pkt->data;\n        is->audio_pkt_size = pkt->size;\n        /* if update, update the audio clock w/pts */\n        if(pkt->pts != AV_NOPTS_VALUE) {\n            is->audio_clock = av_q2d(is->audio_st->time_base)*pkt->pts;\n        }\n    }\n}\n\nvoid audio_callback(void *userdata, Uint8 *stream, int len) {\n    VideoState *is = (VideoState *)userdata;\n    int len1, audio_size;\n    double pts;\n\n    SDL_memset(stream, 0, len);\n\n    while(len > 0) {\n        if(is->audio_buf_index >= is->audio_buf_size) {\n            /* We have already sent all our data; get more */\n            audio_size = audio_decode_frame(is, is->audio_buf, sizeof(is->audio_buf), &pts);\n            if(audio_size < 0) {\n                /* If error, output silence */\n                is->audio_buf_size = 1024 * 2 * 2;\n                memset(is->audio_buf, 0, is->audio_buf_size);\n            } else {\n                audio_size = synchronize_audio(is, (int16_t *)is->audio_buf, audio_size, pts);\n                is->audio_buf_size = audio_size;\n            }\n            is->audio_buf_index = 0;\n        }\n        len1 = is->audio_buf_size - is->audio_buf_index;\n        if(len1 > len)\n            len1 = len;\n        SDL_MixAudio(stream,(uint8_t *)is->audio_buf + is->audio_buf_index, len1, SDL_MIX_MAXVOLUME);\n        //memcpy(stream, (uint8_t *)is->audio_buf + is->audio_buf_index, len1);\n        len -= len1;\n        stream += len1;\n        is->audio_buf_index += len1;\n    }\n}\n\nstatic Uint32 sdl_refresh_timer_cb(Uint32 interval, void *opaque) {\n    SDL_Event event;\n    event.type = FF_REFRESH_EVENT;\n    event.user.data1 = opaque;\n    SDL_PushEvent(&event);\n    return 0; /* 0 means stop timer */\n}\n\n/* schedule a video refresh in 'delay' ms */\nstatic void schedule_refresh(VideoState *is, int delay) {\n    SDL_AddTimer(delay, sdl_refresh_timer_cb, is);\n}\n\nvoid video_display(VideoState *is) {\n    SDL_Rect rect;\n    VideoPicture *vp;\n    float aspect_ratio;\n    int w, h, x, y;\n    int i;\n\n    vp = &is->pictq[is->pictq_rindex];\n    if(vp->bmp) {\n\n        SDL_UpdateYUVTexture(texture, NULL, \n                             vp->bmp->data[0], vp->bmp->linesize[0],\n                             vp->bmp->data[1], vp->bmp->linesize[1],\n                             vp->bmp->data[2], vp->bmp->linesize[2]);\n\n        rect.x = 0;\n        rect.y = 0;\n        rect.w = is->video_ctx->width;\n        rect.h = is->video_ctx->height;\n        SDL_LockMutex(text_mutex);\n        SDL_RenderClear( renderer );\n        SDL_RenderCopy( renderer, texture, NULL, &rect);\n        SDL_RenderPresent( renderer );\n        SDL_UnlockMutex(text_mutex);\n    }\n}\n\nvoid video_refresh_timer(void *userdata) {\n    VideoState *is = (VideoState *)userdata;\n    VideoPicture *vp;\n    double actual_delay, delay, sync_threshold, ref_clock, diff;\n\n    if(is->video_st) {\n        if(is->pictq_size == 0) {\n            schedule_refresh(is, 1);\n            //fprintf(stderr, \"no picture in the queue!!!\\n\");\n        } else {\n            //fprintf(stderr, \"get picture from queue!!!\\n\");\n            vp = &is->pictq[is->pictq_rindex];\n\n            is->video_current_pts = vp->pts;\n            is->video_current_pts_time = av_gettime();\n            delay = vp->pts - is->frame_last_pts; /* the pts from last time */\n            if(delay <= 0 || delay >= 1.0) {\n                /* if incorrect delay, use previous one */\n                delay = is->frame_last_delay;\n            }\n            /* save for next time */\n            is->frame_last_delay = delay;\n            is->frame_last_pts = vp->pts;\n\n            /* update delay to sync to audio if not master source */\n            if(is->av_sync_type != AV_SYNC_VIDEO_MASTER) {\n                ref_clock = get_master_clock(is);\n                diff = vp->pts - ref_clock;\n\n                /* Skip or repeat the frame. Take delay into account\n       FFPlay still doesn't \"know if this is the best guess.\" */\n                sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;\n                if(fabs(diff) < AV_NOSYNC_THRESHOLD) {\n                    if(diff <= -sync_threshold) {\n                        delay = 0;\n                    } else if(diff >= sync_threshold) {\n                        delay = 2 * delay;\n                    }\n                }\n            }\n            is->frame_timer += delay;\n            /* computer the REAL delay */\n            actual_delay = is->frame_timer - (av_gettime() / 1000000.0);\n            if(actual_delay < 0.010) {\n                /* Really it should skip the picture instead */\n                actual_delay = 0.010;\n            }\n            schedule_refresh(is, (int)(actual_delay * 1000 + 0.5));\n\n            /* show the picture! */\n            video_display(is);\n\n            /* update queue for next picture! */\n            if(++is->pictq_rindex == VIDEO_PICTURE_QUEUE_SIZE) {\n                is->pictq_rindex = 0;\n            }\n            SDL_LockMutex(is->pictq_mutex);\n            is->pictq_size--;\n            SDL_CondSignal(is->pictq_cond);\n            SDL_UnlockMutex(is->pictq_mutex);\n        }\n    } else {\n        schedule_refresh(is, 100);\n    }\n}\n      \nvoid alloc_picture(void *userdata) {\n    int ret;\n\n    VideoState *is = (VideoState *)userdata;\n    VideoPicture *vp;\n\n    vp = &is->pictq[is->pictq_windex];\n    if(vp->bmp) {\n        // we already have one make another, bigger/smaller\n        avpicture_free(vp->bmp);\n        free(vp->bmp);\n        vp->bmp = NULL;\n    }\n\n    // Allocate a place to put our YUV image on that screen\n    SDL_LockMutex(text_mutex);\n\n    vp->bmp = (AVPicture*)malloc(sizeof(AVPicture));\n    ret = avpicture_alloc(vp->bmp, AV_PIX_FMT_YUV420P, is->video_ctx->width, is->video_ctx->height);\n    if (ret < 0) {\n        fprintf(stderr, \"Could not allocate temporary picture: %s\\n\", av_err2str(ret));\n    }\n\n    SDL_UnlockMutex(text_mutex);\n\n    vp->width = is->video_ctx->width;\n    vp->height = is->video_ctx->height;\n    vp->allocated = 1;\n}\n\nint queue_picture(VideoState *is, AVFrame *pFrame, double pts) {\n    VideoPicture *vp;\n\n    /* wait until we have space for a new pic */\n    SDL_LockMutex(is->pictq_mutex);\n    while(is->pictq_size >= VIDEO_PICTURE_QUEUE_SIZE &&\n          !is->quit) {\n        SDL_CondWait(is->pictq_cond, is->pictq_mutex);\n    }\n    SDL_UnlockMutex(is->pictq_mutex);\n\n    if(is->quit)\n        return -1;\n\n    // windex is set to 0 initially\n    vp = &is->pictq[is->pictq_windex];\n\n    /* allocate or resize the buffer! */\n    if(!vp->bmp ||\n       vp->width != is->video_ctx->width ||\n       vp->height != is->video_ctx->height) {\n\n        vp->allocated = 0;\n        alloc_picture(is);\n        if(is->quit) {\n            return -1;\n        }\n    }\n\n    /* We have a place to put our picture on the queue */\n    if(vp->bmp) {\n        vp->pts = pts;\n\n        // Convert the image into YUV format that SDL uses\n        sws_scale(is->video_sws_ctx, (uint8_t const * const *)pFrame->data,\n                  pFrame->linesize, 0, is->video_ctx->height,\n                  vp->bmp->data, vp->bmp->linesize);\n\n        /* now we inform our display thread that we have a pic ready */\n        if(++is->pictq_windex == VIDEO_PICTURE_QUEUE_SIZE) {\n            is->pictq_windex = 0;\n        }\n        SDL_LockMutex(is->pictq_mutex);\n        is->pictq_size++;\n        SDL_UnlockMutex(is->pictq_mutex);\n    }\n    return 0;\n}\n\ndouble synchronize_video(VideoState *is, AVFrame *src_frame, double pts) {\n    double frame_delay;\n\n    if(pts != 0) {\n        /* if we have pts, set video clock to it */\n        is->video_clock = pts;\n    } else {\n        /* if we aren't given a pts, set it to the clock */\n        pts = is->video_clock;\n    }\n    /* update the video clock */\n    frame_delay = av_q2d(is->video_ctx->time_base);\n    /* if we are repeating a frame, adjust clock accordingly */\n    frame_delay += src_frame->repeat_pict * (frame_delay * 0.5);\n    is->video_clock += frame_delay;\n    return pts;\n}\n\nint decode_video_thread(void *arg) {\n    VideoState *is = (VideoState *)arg;\n    AVPacket pkt1, *packet = &pkt1;\n    int frameFinished;\n    AVFrame *pFrame;\n    double pts;\n\n    pFrame = av_frame_alloc();\n\n    for(;;) {\n        if(packet_queue_get(&is->videoq, packet, 1) < 0) {\n            // means we quit getting packets\n            break;\n        }\n        pts = 0;\n\n        // Decode video frame\n        avcodec_decode_video2(is->video_ctx, pFrame, &frameFinished, packet);\n\n        if((pts = av_frame_get_best_effort_timestamp(pFrame)) != AV_NOPTS_VALUE) {\n        } else {\n            pts = 0;\n        }\n        pts *= av_q2d(is->video_st->time_base);\n\n        // Did we get a video frame?\n        if(frameFinished) {\n            pts = synchronize_video(is, pFrame, pts);\n            if(queue_picture(is, pFrame, pts) < 0) {\n                break;\n            }\n        }\n        av_free_packet(packet);\n    }\n    av_frame_free(&pFrame);\n    return 0;\n}\n\nint stream_component_open(VideoState *is, int stream_index) {\n    AVFormatContext *pFormatCtx = is->pFormatCtx;\n    AVCodecContext *codecCtx = NULL;\n    AVCodec *codec = NULL;\n    SDL_AudioSpec wanted_spec, spec;\n\n    if(stream_index < 0 || stream_index >= pFormatCtx->nb_streams) {\n        return -1;\n    }\n\n    codecCtx = avcodec_alloc_context3(NULL);\n\n    int ret = avcodec_parameters_to_context(codecCtx, pFormatCtx->streams[stream_index]->codecpar);\n    if (ret < 0)\n        return -1;\n\n    codec = avcodec_find_decoder(codecCtx->codec_id);\n    if(!codec) {\n        fprintf(stderr, \"Unsupported codec!\\n\");\n        return -1;\n    }\n\n    if(codecCtx->codec_type == AVMEDIA_TYPE_AUDIO) {\n        // Set audio settings from codec info\n        wanted_spec.freq = codecCtx->sample_rate;\n        wanted_spec.format = AUDIO_S16SYS;\n        wanted_spec.channels = 2;//codecCtx->channels;\n        wanted_spec.silence = 0;\n        wanted_spec.samples = SDL_AUDIO_BUFFER_SIZE;\n        wanted_spec.callback = audio_callback;\n        wanted_spec.userdata = is;\n\n        fprintf(stderr, \"wanted spec: channels:%d, sample_fmt:%d, sample_rate:%d \\n\",\n                2, AUDIO_S16SYS, codecCtx->sample_rate);\n\n        if(SDL_OpenAudio(&wanted_spec, &spec) < 0) {\n            fprintf(stderr, \"SDL_OpenAudio: %s\\n\", SDL_GetError());\n            return -1;\n        }\n        is->audio_hw_buf_size = spec.size;\n    }\n\n    if(avcodec_open2(codecCtx, codec, NULL) < 0) {\n        fprintf(stderr, \"Unsupported codec!\\n\");\n        return -1;\n    }\n\n    switch(codecCtx->codec_type) {\n        case AVMEDIA_TYPE_AUDIO:\n            is->audioStream = stream_index;\n            is->audio_st = pFormatCtx->streams[stream_index];\n            is->audio_ctx = codecCtx;\n            is->audio_buf_size = 0;\n            is->audio_buf_index = 0;\n            memset(&is->audio_pkt, 0, sizeof(is->audio_pkt));\n            packet_queue_init(&is->audioq);\n\n            //Out Audio Param\n            uint64_t out_channel_layout=AV_CH_LAYOUT_STEREO;\n\n            //AAC:1024  MP3:1152\n            int out_nb_samples= is->audio_ctx->frame_size;\n            //AVSampleFormat out_sample_fmt = AV_SAMPLE_FMT_S16;\n\n            int out_sample_rate=is->audio_ctx->sample_rate;\n            int out_channels=av_get_channel_layout_nb_channels(out_channel_layout);\n            //Out Buffer Size\n            /*\n    \t\tint out_buffer_size=av_samples_get_buffer_size(NULL,\n                                                   out_channels,\n                                                   out_nb_samples,\n                                                   AV_SAMPLE_FMT_S16,\n                                                   1);\n                                                   */\n\n            //uint8_t *out_buffer=(uint8_t *)av_malloc(MAX_AUDIO_FRAME_SIZE*2);\n            int64_t in_channel_layout=av_get_default_channel_layout(is->audio_ctx->channels);\n\n            struct SwrContext *audio_convert_ctx;\n            audio_convert_ctx = swr_alloc();\n            swr_alloc_set_opts(audio_convert_ctx,\n                               out_channel_layout,\n                               AV_SAMPLE_FMT_S16,\n                               out_sample_rate,\n                               in_channel_layout,\n                               is->audio_ctx->sample_fmt,\n                               is->audio_ctx->sample_rate,\n                               0,\n                               NULL);\n            fprintf(stderr, \"swr opts: out_channel_layout:%lld, out_sample_fmt:%d, out_sample_rate:%d, in_channel_layout:%lld, in_sample_fmt:%d, in_sample_rate:%d\",\n                    out_channel_layout, \n                    AV_SAMPLE_FMT_S16, \n                    out_sample_rate, \n                    in_channel_layout, \n                    is->audio_ctx->sample_fmt, \n                    is->audio_ctx->sample_rate);\n            swr_init(audio_convert_ctx);\n            is->audio_swr_ctx = audio_convert_ctx;\n\n            SDL_PauseAudio(0);\n            break;\n        case AVMEDIA_TYPE_VIDEO:\n            is->videoStream = stream_index;\n            is->video_st = pFormatCtx->streams[stream_index];\n            is->video_ctx = codecCtx;\n\n            is->frame_timer = (double)av_gettime() / 1000000.0;\n            is->frame_last_delay = 40e-3;\n            is->video_current_pts_time = av_gettime();\n\n            packet_queue_init(&is->videoq);\n            is->video_sws_ctx = sws_getContext(\n                \t\tis->video_ctx->width, is->video_ctx->height,\n                        is->video_ctx->pix_fmt, is->video_ctx->width,\n                        is->video_ctx->height, AV_PIX_FMT_YUV420P,\n                        SWS_BILINEAR, NULL, NULL, NULL);\n            is->video_tid = SDL_CreateThread(decode_video_thread, \"decode_video_thread\", is);\n            break;\n        default:\n            break;\n    }\n}\n\nint demux_thread(void *arg) {\n    int err_code;\n    char errors[1024] = {0,};\n\n    VideoState *is = (VideoState *)arg;\n    AVFormatContext *pFormatCtx;\n    AVPacket pkt1, *packet = &pkt1;\n\n    int video_index = -1;\n    int audio_index = -1;\n    int i;\n\n    is->videoStream=-1;\n    is->audioStream=-1;\n\n    global_video_state = is;\n\n    /* open input file, and allocate format context */\n    if ((err_code=avformat_open_input(&pFormatCtx, is->filename, NULL, NULL)) < 0) {\n        av_strerror(err_code, errors, 1024);\n        fprintf(stderr, \"Could not open source file %s, %d(%s)\\n\", is->filename, err_code, errors);\n        return -1;\n    }\n\n    is->pFormatCtx = pFormatCtx;\n\n    // Retrieve stream information\n    if(avformat_find_stream_info(pFormatCtx, NULL)<0)\n        return -1; // Couldn't find stream information\n\n    // Dump information about file onto standard error\n    av_dump_format(pFormatCtx, 0, is->filename, 0);\n\n    // Find the first video stream\n\n    for(i=0; i<pFormatCtx->nb_streams; i++) {\n        if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO &&\n           video_index < 0) {\n            video_index=i;\n        }\n        if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_AUDIO &&\n           audio_index < 0) {\n            audio_index=i;\n        }\n    }\n    if(audio_index >= 0) {\n        stream_component_open(is, audio_index);\n    }\n    if(video_index >= 0) {\n        stream_component_open(is, video_index);\n    }   \n\n    if(is->videoStream < 0 || is->audioStream < 0) {\n        fprintf(stderr, \"%s: could not open codecs\\n\", is->filename);\n        goto fail;\n    }\n\n    //creat window from SDL\n    win = SDL_CreateWindow(\"Media Player\",\n                           SDL_WINDOWPOS_UNDEFINED,\n                           SDL_WINDOWPOS_UNDEFINED,\n                           is->video_ctx->width, is->video_ctx->height,\n                           SDL_WINDOW_OPENGL|SDL_WINDOW_RESIZABLE);\n    if(!win) {\n        fprintf(stderr, \"SDL: could not set video mode - exiting\\n\");\n        exit(1);\n    }\n\n    renderer = SDL_CreateRenderer(win, -1, 0);\n\n    //IYUV: Y + U + V  (3 planes)\n    //YV12: Y + V + U  (3 planes)\n    Uint32 pixformat= SDL_PIXELFORMAT_IYUV;\n\n    //create texture for render\n    texture = SDL_CreateTexture(renderer,\n                                pixformat,\n                                SDL_TEXTUREACCESS_STREAMING,\n                                is->video_ctx->width,\n                                is->video_ctx->height);\n\n    // main decode loop\n    for(;;) {\n        if(is->quit) {\n            break;\n        }\n        // seek stuff goes here\n        if(is->audioq.size > MAX_AUDIOQ_SIZE ||\n           is->videoq.size > MAX_VIDEOQ_SIZE) {\n            SDL_Delay(10);\n            continue;\n        }\n        if(av_read_frame(is->pFormatCtx, packet) < 0) {\n            if(is->pFormatCtx->pb->error == 0) {\n                SDL_Delay(100); /* no error; wait for user input */\n                continue;\n            } else {\n                break;\n            }\n        }\n        // Is this a packet from the video stream?\n        if(packet->stream_index == is->videoStream) {\n            packet_queue_put(&is->videoq, packet);\n        } else if(packet->stream_index == is->audioStream) {\n            packet_queue_put(&is->audioq, packet);\n        } else {\n            av_free_packet(packet);\n        }\n    }\n    /* all done - wait for it */\n    while(!is->quit) {\n        SDL_Delay(100);\n    }\n\nfail:\n    if(1){\n        SDL_Event event;\n        event.type = FF_QUIT_EVENT;\n        event.user.data1 = is;\n        SDL_PushEvent(&event);\n    }\n    return 0;\n}\n\nint main(int argc, char *argv[]) {\n    SDL_Event       event;\n    VideoState      *is;\n\n    is = av_mallocz(sizeof(VideoState));\n    if(argc < 2) {\n        fprintf(stderr, \"Usage: test <file>\\n\");\n        exit(1);\n    }\n\n    yuvfd = fopen(\"testout.yuv\", \"wb+\");\n    audiofd = fopen(\"testout.pcm\", \"wb+\");\n    // Register all formats and codecs\n    av_register_all();\n\n    if(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {\n        fprintf(stderr, \"Could not initialize SDL - %s\\n\", SDL_GetError());\n        exit(1);\n    }\n\n    text_mutex = SDL_CreateMutex();\n    av_strlcpy(is->filename, argv[1], sizeof(is->filename));\n    is->pictq_mutex = SDL_CreateMutex();\n    is->pictq_cond = SDL_CreateCond();\n\n    schedule_refresh(is, 40);\n    is->av_sync_type = DEFAULT_AV_SYNC_TYPE;\n    is->parse_tid = SDL_CreateThread(demux_thread,\"demux_thread\", is);\n    if(!is->parse_tid) {\n        av_free(is);\n        return -1;\n    }\n    for(;;) {\n        SDL_WaitEvent(&event);\n        switch(event.type) {\n            case FF_QUIT_EVENT:\n            case SDL_QUIT:\n                is->quit = 1;\n                SDL_Quit();\n                return 0;\n                break;\n            case FF_REFRESH_EVENT:\n                video_refresh_timer(event.user.data1);\n                break;\n            default:\n                break;\n        }\n    }\n\n    fclose(yuvfd);\n    fclose(audiofd);\n    return 0;\n}\n```\n\n</details>\n\n## 7. 如何在 Android 下使用 FFmpeg\n\nAndroid 架构：\n\n<img src=\"/images/imageFFmpeg/Android架构.png\">\n\n内容：\n\n- Java 与 C 之间的相互调用\n- Android 下 FFmpeg 的编译\n- Android 下如何使用FFmpeg\n\n第一个 JNI 程序：\n\nTODO\n\nJNI 基本概念：\n\n- JNIEnv\n- JavaVM  一个Android APP只有一个 JavaVM， 一个 JavaVM 可以有多个JNIEnv\n- 线程  一个线程对应一个JNIEnv\n\nJava调用C/C++ 方法一：\n\n- 在Java层定义 native 关键字函数\n\n- 方法一：在C/C++层创建\n\n  Java_packname_classname_methodname 函数\n\nJava调用C/C++方法二：\n\n<img src=\"/images/imageFFmpeg/java调用c方法二.png\">\n\n<img src=\"/images/imageFFmpeg/注册Native方法的最佳时机.png\">\n\n什么是Signature：\n\n- Java与C/C++ 相互调用时，表式函数参数的描述符\n- 输入参数放在（）内，输出参数放在（）外\n- 多个参数之间顺序存放，且用 “；” 分割\n\n<img src=\"/images/imageFFmpeg/原始类型的Signature.png\">\n\n<img src=\"/images/imageFFmpeg/类的Signature.png\">\n\n <img src=\"/images/imageFFmpeg/例子.png\">\n\nC/C++ 调用 Java 方法：\n\n- FindClass\n- GetMethodID / GetFieldID\n- NewObject\n- `Call<TYPE>Method / [G/S]et<type>Field`\n\n### 7.1 [实战] Android 下的播放器\n\nTODO\n\n## 8. IOS 下使用 FFmpeg\n\nTODO\n\n## 9. 音视频进阶\n\n- FFmpeg Filter 的使用 \n- FFmpeg 裁剪与优化\n- 视频渲染（OpenGL / Metal）\n- 声音的特效\n- 网络传输\n- Webrtc - 实时互动、直播、P2P音视频传输\n- AR技术\n- OpenCV\n\n行业痛点：\n\n- 回音消除\n- 降噪\n- 视频秒开\n- 多人多视频实时互动\n- PC端/APP/网页实时视频互通\n- 实时互动与大并发负载\n\n## FFmpeg音视频同步原理与实现\n\n> [音视频同步解决方案](https://www.cnblogs.com/hoys/archive/2011/06/08/2075159.html)\n>\n> [一种基于FFMPEG的音视频同步算法](<http://www.xml-data.org/GDGYDXXB/html/20170411.htm>)\n\n### 音视频同步原理\n\n如果简单的按照音频的采样率与视频的帧率去播放，由于机器运行速度，解码效率等种种造成时间差异的因素影响，很难同步，音视频时间差将会呈现线性增长。所以要做音视频的同步，有三种方式：\n\n参考一个外部时钟，将音频与视频同步至此时间。我首先想到这种方式，但是并不好，由于某些生物学的原理，人对声音的变化比较敏感，但是对视觉变化不太敏感。所以频繁的去调整声音的播放会有些刺耳或者杂音吧影响用户体验。（ps：顺便科普生物学知识，自我感觉好高大上_）。\n\n- 以视频为基准，音频去同步视频的时间。不采用，理由同上。\n- 以音频为基准，视频去同步音频的时间。 所以这个办法了。\n\n所以，原理就是以音频时间为基准，判断视频快了还是慢了，从而调整视频速度。其实是一个动态的追赶与等待的过程。\n\n### 一些概念\n\n音视频中都有 `DTS` 与 `PTS`。\n\n- DTS ，Decoding Time Stamp，解码时间戳，告诉解码器packet的解码顺序。\n- PTS ，Presentation Time Stamp，显示时间戳，指示从packet中解码出来的数据的显示顺序。\n- 音频中二者是相同的，但是视频由于B帧（双向预测）的存在，会造成解码顺序与显示顺序并不相同，也就是视频中 DTS 与 PTS 不一定相同。\n\n时间基 : 看 FFmpeg 源码\n\n```c++\nAVRational time_base;\n/**\n* rational number numerator/denominator\n*/\ntypedef struct AVRational{\n   int num; ///< numerator\n   int den; ///< denominator\n} AVRational;\n```\n\n个人理解，其实就是 ffmpeg中 的用分数表示时间单位，num 为分子，den 为分母。并且 ffmpeg 提供了计算方法：\n\n```c++\n/**\n* Convert rational to double.\n* @param a rational to convert\n* @return (double) a\n*/\nstatic inline double av_q2d(AVRational a){\n   return a.num / (double) a.den;\n}\n```\n\n所以 视频中某帧的显示时间 计算方式为(单位为妙)：\n\n```c++\ntime = pts * av_q2d(time_base);\n```\n\n### 同步代码\n**音频部分**\n\nclock 为音频的播放时长（从开始到当前的时间）\n\n```c++\nif (packet->pts != AV_NOPTS_VALUE) {\n    audio->clock = av_q2d(audio->time_base) * packet->pts;\n}\n```\n\n然后加上此 packet 中数据需要播放的时间\n\n```c++\ndouble time = datalen/((double) 44100 *2 * 2);\naudio->clock = audio->clock +time;\n```\n\ndatalen 为数据长度。采样率为 44100，采样位数为 16，通道数为 2。所以 数据长度 / 每秒字节数。\n\nps：此处计算方式不是很完美，有很多问题，回头研究在再补上。\n\n**视频部分**\n\n<details><summary>先定义几个值：</summary>\n\n```c++\ndouble  last_play  //上一帧的播放时间\n   ,play             //当前帧的播放时间\n   , last_delay    // 上一次播放视频的两帧视频间隔时间\n   ,delay         //两帧视频间隔时间\n   ,audio_clock //音频轨道 实际播放时间\n   ,diff   //音频帧与视频帧相差时间\n   ,sync_threshold //合理的范围\n   ,start_time  //从第一帧开始的绝对时间\n   ,pts\n   ,actual_delay//真正需要延迟时间\n   start_time = av_gettime() / 1000000.0;\n//        获取pts\n       if ((pts = av_frame_get_best_effort_timestamp(frame)) == AV_NOPTS_VALUE) {\n           pts = 0;\n       }\n       play = pts * av_q2d(vedio->time_base);\n//        纠正时间\n       play = vedio->synchronize(frame, play);\n       delay = play - last_play;\n       if (delay <= 0 || delay > 1) {\n           delay = last_delay;\n       }\n       audio_clock = vedio->audio->clock;\n       last_delay = delay;\n       last_play = play;\n//音频与视频的时间差\n       diff = vedio->clock - audio_clock;\n//        在合理范围外  才会延迟  加快\n       sync_threshold = (delay > 0.01 ? 0.01 : delay);\n       if (fabs(diff) < 10) {\n           if (diff <= -sync_threshold) {\n               delay = 0;\n           } else if (diff >= sync_threshold) {\n               delay = 2 * delay;\n           }\n       }\n       start_time += delay;\n       actual_delay = start_time - av_gettime() / 1000000.0;\n       if (actual_delay < 0.01) {\n           actual_delay = 0.01;\n       }\n//  休眠时间 ffmpeg 建议这样写  为什么 要这样写 有待研究\n       av_usleep(actual_delay * 1000000.0 + 6000);\n纠正play （播放时间）的方法 repeat_pict / (2 * fps) 是ffmpeg注释里教的\nsynchronize(AVFrame *frame, double play) {\n   //clock是当前播放的时间位置\n   if (play != 0)\n       clock=play;\n   else //pst为0 则先把pts设为上一帧时间\n       play = clock;\n   //可能有pts为0 则主动增加clock\n   //需要求出扩展延时：\n   double repeat_pict = frame->repeat_pict;\n   //使用AvCodecContext的而不是stream的\n   double frame_delay = av_q2d(codec->time_base);\n   //fps \n   double fps = 1 / frame_delay;\n   //pts 加上 这个延迟 是显示时间  \n   double extra_delay = repeat_pict / (2 * fps);\n   double delay = extra_delay + frame_delay;\n   clock += delay;\n   return play;\n}\n```\n\n</details>\n\n## FFmpeg 痛点解决\n\n**回音消除解决方案：**\n\n> [语音自适应回声消除（AEC）算法](<https://blog.csdn.net/shichaog/article/details/71152743>)\n>\n> [回声消除(AEC)原理](<https://blog.csdn.net/longbei9029/article/details/81237402>)\n>\n> [音频降噪在 58 直播中的研究与实现](<https://www.infoq.cn/article/QOp4IOao_DJJ6eNsIOXp>)\n\n**视频秒开：**\n\n> [直播视频秒开及视频优化](<https://blog.csdn.net/ShareUs/article/details/79115816>)\n>\n> [视频直播秒开背后的技术与优化经验](<https://juejin.im/post/58eaf49a8d6d8100618bc238>)\n>\n> [短视频“秒播”那点事](https://segmentfault.com/a/1190000014405913)\n>\n> [百度LSS 音视频直播 秒开](<http://www.voidcn.com/article/p-qkdvttue-bmd.html>)\n>\n> [播放器的“妥协”造就了视频“秒开”的实现！](<https://zhuanlan.zhihu.com/p/34159910>)\n\n**多人视频实时互动：**\n\n> [WebRTC现状以及多人视频通话分析](<https://juejin.im/post/5cb008c26fb9a068547345eb>)\n>\n> [多人视频连麦——直播高效互动方式](<http://www.polyv.net/news/2019/02/hy0396/>)\n\n**实时互动与大并发负载：**\n\n> [RTP直播分发服务器集群方案](http://www.mediapro.cc/rtp转发服务器/)\n>\n> [海量用户实时互动直播架构探索](<https://zhuanlan.zhihu.com/p/23055659>)\n>\n> [直播开发过程中关于直播技术的架构问题](<http://blog.itpub.net/69907981/viewspace-2564479/>)\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"mxnet实现线性回归模型","url":"/2019-04-26/mxnet_practice_1/","content":"\n### mxnet线性回归demo\n```\nfrom IPython import display\nfrom matplotlib import pyplot as plt \nfrom mxnet import autograd, nd\nimport random\n\n# 特征个数\nnum_inputs = 2\n# 样本个数\nnum_examples = 1000\n# 权重\ntrue_w=[2,-3.4]\n# 偏差\ntrue_b=4.2\n# y=Xw+b+c 简单线性回归 c为噪音\n\n# 生成行长度为2的向量\nfeatures = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\nlables = true_w[0]*features[:, 0]+true_w[1]*features[:, 1] + true_b\nlables += nd.random.normal(scale=0.01, shape=lables.shape)\n\ndef use_svg_display():\n\tdisplay.set_matplotlib_formats('svg')\n\ndef set_figsize(figsize=(3.5,2.5)):\n\tuse_svg_display()\n\tplt.rcParams['figure.figsize'] = figsize\n\nset_figsize((7,5))\n# 画图显示点之间的关系\n# plt.scatter(features[:,1].asnumpy(), lables.asnumpy(), 1);\n# plt.show()\n\ndef data_iter(batch_size, features, lables):\n\tnum_examples = len(features)\n\tindices = list(range(num_examples))\n\t# 生成features长度的索引列表 并用shuffle打乱\n\trandom.shuffle(indices)\n\tfor i in range(0, num_examples, batch_size):\n\t\tj = nd.array(indices[i: min(i+batch_size, num_examples)])\n\t\t# 获取i到i+batch_size对应长度的索引值列表\n\t\t# 根据提供的索引值列表使用take获取对应的数组值\n\t\tyield features.take(j), lables.take(j)\n\n\nbatch_size = 10\n\n# for x,y in data_iter(batch_size, features, lables):\n# \tprint(x, y)\n# \tbreak;\n\n# 初始化权重 标准差0.01 均值0\nw = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n\n# 偏差0 \nb = nd.zeros(shape=(1,))\n\n# 创建梯度\nw.attach_grad()\nb.attach_grad()\n\n# 定义线性回归模型\ndef linreg(X, w, b):\n\treturn nd.dot(X, w) + b \n\n# 定义损失函数\ndef squared_loss(y_hat, y):\n\treturn (y_hat - y.reshape(y_hat.shape)) ** 2 /2\n\n# 定义sgd迭代模型进行随机梯度下降\ndef sgd(params, lr, batch_size):\n\tfor param in params:\n\t\tparam[:] = param - lr*param.grad/batch_size #?\n\n# 模型训练\nlr = 0.03 #学习率\nnum_epochs = 3 #迭代周期\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n\tfor X, y in data_iter(batch_size, features, lables):\n\t\twith autograd.record(): # autograd模块⾃动化计算求导\n\t\t\tl = loss(net(X, w, b), y)# 一个批量的损失\n\t\tl.backward() # 计算梯度 反向传播函数处理\n\t\tsgd([w,b], lr, batch_size) # 优化w 和 b\n\t# 将回归模型得到的解析解的值与标准值进行比较\n\ttrain_l = loss(net(features, w, b), lables)\n\tprint('epoch %d, loss %f' % (epoch+1, train_l.mean().asnumpy()))\n\n## 比较模型与真实值的 权重 与 偏差\nprint(true_w, w)\nprint(true_b, b)\n\n```\n\n\n\n\n\n","tags":["mxnet","pracitce"],"categories":["deeplearn"]},{"title":"新增hexo个人页面","url":"/2019-04-24/add_new_hexo_page/","content":"\n\n## 前言\n使用hexo博客创建个人页面/工具页面\n\n## 步骤\n1. 确认主题，我使用的是next主题，编辑themes\\next\\_config.yml\n2. 修改menu 新增分类与路径\n```\nmenu:\n  home: / || home\n  #about: /about/ || user\n  #tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n  #schedule: /schedule/ || calendar\n  #sitemap: /sitemap.xml || sitemap\n  #commonweal: /404/ || heartbeat\n  tools: /tools/ || tools # 新增工具分类\n```\n3.修改样式\n```\n- 编辑\n\\themes\\next\\source\\lib\\font-awesome\\css\\font-awesome.css\n\\themes\\next\\source\\lib\\font-awesome\\css\\font-awesome.min.css[真实生效的文件]\n- 添加\n.fa-tools:before {\n  content: \"\\f189\"; #自定义图标\n}\n4. 命令行执行hexo new page tools\n5. 编辑source\\tools\\index.md\n```\n---\ntitle: 小工具\ndate: 2019-05-16 10:35:17\ntype: tools\n---\n\n- [将普通文本保留样式转为markdown文本](./contentToMarkdown){:target=\"_blank\"}\n```\n6. 将自定义的文件夹/页面放在 public\\tools 下 这里是文件夹 contentToMarkdown\n7. 注意重新执行 hexo clean 会清空该自定义文件夹, 建议在source\\user_file 下保存一份用于替换清空或者hexo生成的文件,注意不要放在tools下，会自动添加hexo的html头\n8. 正常部署发布","tags":["hexo"],"categories":["运维"]},{"title":"音视频入门知识","url":"/2019-04-23/reference/FFmpeg/移动端音视频入门/","content":"\n> [H264基本原理](<https://www.jianshu.com/p/97b4dc8c7f00>)\n\n## 1. 万人直播架构讲解\n\n直播产品的种类：\n\n- 泛娱乐化直播\n\n  花椒、映客等娱乐直播，还有斗鱼、熊猫等游戏直播\n\n- 实时互动直播\n\n  音视频会议、教育直播等，像 思科、全时、声网\n\n<!-- more -->\n\n泛娱乐化直播架构\n\n> 信令服务器：创建房间、聊天、礼物。。。。\n>\n> 美女主播 --信令--> 信令服务器\n>\n> 信令服务器--rtmp流地址-->美女主播\n>\n> 美女主播 --推流--> 流媒体云CDN\n>\n> 观众 --信令--> 信令服务器：将观众加入到美女主播间\n>\n> 信令服务器--rmtp流地址--> 观众\n>\n> 观众 <--拉流--> 流媒体云CDN\n\n泛娱乐化直播架构\n\n基于TCP协议实现\n\n1. 发送信令到信令服务器, 服务器收到\\执行后, 返回给共享端一个流媒体云的地址\n2. 共享端采集自己音视频数据, 形成rtmp流, 推送到CDN网络(推流)\n3. 获取流媒体云地址\n4. 拉流\n\n<img src=\"/images/imageFFmpeg/泛娱乐化直播架构.png\">\n\n实时互动直播架构\n\n基于UDP实现\n\n1. 自有网络: UDP没有自有网络, 需自己搭建\n2. 多个节点: 为了保障服务的稳定性以及负载均衡\n3. 控制中心: 每个节点定期(心跳)向控制中心报告健康程度, 控制中心根据响应的数据做出决策\n4. 内总线: 数据安全性\\吞吐量等可靠性得以保障\n5. 媒体服务器: 将RTP协议的数据转换成RTMP协议的数据\n6. CDN网络: 根据用户需求进行拉流\n\n<img src=\"/images/imageFFmpeg/实时互动直播架构.png\">\n\n## 2. CDN网络介绍\n\n> CDN网络是为了解决什么问题而出现的？\n>\n> 总结为一句话：CDN网络是为了解决用户访问网络资源慢而出现的一个技术，两个原因：\n>\n> 1. 网络链路太长\n> 2. 人为因素（南电信北联通，利益相关）\n\nCDN构成：\n\n边缘结点：用户从边缘节点上获取数据\n\n二级节点：主干网节点，主要用于缓存、减转源站压力\n\n源站：CP(内容提供方)将内容放到源站\n\n查找顺序：边缘结点->二级节点->源站\n\n<img src=\"/images/imageFFmpeg/CDN网络.png\">\n\n## 3. 亲手搭建一套简单的直播系统\n\n安装nginx 配置rtmp\n\n```shell\n$ brew install nginx-full --with-rtmp-module\n\n#(这一步卡了我好久，安装nginx提示一直找不到nginx-full,网上相关的教程没更新，原因在于nginx仓库已搬迁)\n$ brew tap denji/nginx\n$ nginx -s reload 重启\n$ nginx  启动\n```\n\n```shell\n$ vi /usr/local/etc/nginx/nginx.conf\n```\n\n<img src=\"/images/imageFFmpeg/nginx-trmp配置.png\">\n\n```shell\n# 推流\n$ ffmpeg -re -i out.mp4 -c copy -f flv rtmp://server/live/streamName\n\n# 拉流\n$ ffmpeg -i rtmp://server/live/streamName -c copy dump.flv\n```\n\n<img src=\"/images/imageFFmpeg/FFMPEG直播命令.png\">\n\n<img src=\"/images/imageFFmpeg/测试流媒体服务器.png\">\n\n## 4. 音频基础知识\n\n<img src=\"/images/imageFFmpeg/声音三要素.png\">\n\n图一音量：甲乙的振动频率相同、振幅不同。图二音调：甲乙振幅相同、频率不同\n\n<img src=\"/images/imageFFmpeg/音量与音调.png\">\n\n<img src=\"/images/imageFFmpeg/音色.png\">\n\n<img src=\"/images/imageFFmpeg/人类听觉范围.png\">\n\n<img src=\"/images/imageFFmpeg/听觉-发声范围.png\">\n\n## 5. 音频的量化与编码\n\n模拟信号进行采样，采样时分频率的从模拟信号获取数据波形值，采样后，进行数据量化，量化后进行编码，把采样的十进制转化为计算机的二进制，也就是数字信号。\n\n模拟数据——》采样——》量化——》编码——》数字信号\n\n<img src=\"/images/imageFFmpeg/音频量化过程.png\">\n\n采样大小决定了音频的振幅的高度，采样时指一个采样用多少bit存放，常用的是16bit\n\n```shell\n# bit：位     一个二进制数据0或1，是1bit\n# byte：字节  存储空间的基本计量单位，如：MySQL中定义 VARCHAR(45) 即是指 45个字节；\n# 1 Byte = 8 Bit = 1 字节\n\n# 2^8 = 256, 2^16 = 65535\n```\n\naac通常44.1k采样率\n\n采样率:采样频率8k/秒、16k/秒、32k/秒、44.1k/秒、48k/秒\n\n> [什么是音频的采样率？采样率和音质有没有关系？](<https://www.zhihu.com/question/20351692>) - 知乎\n\n<img src=\"/images/imageFFmpeg/量化基本概念.png\">\n\n人能听到的声音范围是20hz-2whz\n\n码率 = 采样率 x 采样大小 x 声道数\n\n```shell\n# 宽带速率的单位用 bps(或b/s)表示\n# 1 B = 8 b   1 B/s = 8 b/s\n```\n\n<img src=\"/images/imageFFmpeg/码率计算.png\">\n\n原始的wav文件，大小是1411.2Kb/s\n\n做完aaclc的编码，大小是128Kb/s\n\n如果是aache-vr这种编码，大小是32Kb/s\n\n## 6. 音频压缩技术讲解\n\n音频压缩技术\n\n1、消除冗余数据（有损压缩技术）。\n\n压缩的主要方法是去除采集到的音频冗余信息，所谓冗余信息包括人耳听觉范围外的音频信号以及被掩蔽掉的音频信号\n\n信号的掩蔽可分为频域掩蔽和时域掩蔽\n\n频域掩蔽：一个强纯音会掩蔽在其附近同时发声的弱纯音。也称同时掩蔽\n\n时域掩蔽：在时间上相邻的声音之间也有掩蔽现象，主要原因是人的大脑处理信息需要花费时间。\n\n同步掩蔽效应和不同频率声音的频率和相对竟是有关，而时间掩蔽则仅仅和时间有关。如果两个声音在时间上特别接近，分辨会有困难（如两个声音音量相差较大且两个声音间隔时间低于5毫秒，则其中弱的那个声音会听不到）。\n\n2、哈夫曼无损编码\n\n<img src=\"/images/imageFFmpeg/音频压缩技术.png\">\n\n音频压缩：频域，时域。\n\n- 频域: 截取人耳能听到的频率范围，滤掉响度低的声音，去掉某个高频周围低频的声音；\n\n- 时域: 滤掉某个长时间说话中的低音\n\n<img src=\"/images/imageFFmpeg/音频冗余信息.png\">\n\n<img src=\"/images/imageFFmpeg/频域掩蔽效应.png\">\n\n<img src=\"/images/imageFFmpeg/时域掩蔽效应.png\">\n\n<img src=\"/images/imageFFmpeg/音频编码过程.png\">\n\n## 7. 音频编解码器选型\n\n网上测评结果：音频编解码器 opus > aac > vorbis \n\n音频编解码器：\n\n1：opus，\n\n- 口模型：实时互动，对实时性要求非常高 \n- 耳模型：高保真，对质量要求非常高\n\n至于什么时候使用那个模型，由opus自己内部来决定，同时，他是性能最好的，压缩率最好。\n\n2：AAC，经常用于泛娱乐化直播，因为其对实时性要求不是很高但是对音质要求可能较高，所以，选用AAC，当然也可以选用opus的耳模型\n\n3：sppex，最大的特点就是不仅可以编码音频，还可以对音频进行降噪，优化，尽可能的获取原音频数据\n\n4：G.711(722)，主要用于音视频会议，为了和固话进行相应的融合\n\n<img src=\"/images/imageFFmpeg/常见的音频编码器.png\">\n\n<img src=\"/images/imageFFmpeg/音频编码器性能对比.png\">\n\n## 8. AAC 讲解\n\ncdn，rtmp  支持 aac\n\nAAC 产生的目的是取代 MP3 格式：\n\nAAC 相对优点：压缩率高，损耗低\n\n<img src=\"/images/imageFFmpeg/AAC介绍.png\">\n\n<img src=\"/images/imageFFmpeg/AAC规格.png\">\n\naac 三种类型\naac\naacv1: aac+sbr(频率复用-高频部分采样率高，低频部分采样率低)\naacv2: aac+sbr+ps(声道关联，一个声道采集全部，一个声道只采集相对不同的声音)\n\nAAC规格描述（AAC、AAC HE、AAC HE V2）--> AAC+SBR=AAC HE V1, AAC + SBR + PS = AAC HE V2\n\n<img src=\"/images/imageFFmpeg/AAC规格描述.png\">\n\nAAC格式：\n\n1、ADIF(Audio Data Interchange Format):只能从头开始解码，常用在磁盘文件中。\n\n2、ADTS(Audio Data Transport Stream)：这种格式每一帧都有一个同步字，可以在音频流的任何位置开始解码，它似于数据流格式（缺点：文件比ADIF大，优点:每个帧都可以被单独解码播放）\n\n<img src=\"/images/imageFFmpeg/AAC格式.png\">\n\naac 编码库 ffmpeg AAC，libfdk AAC\n\n<img src=\"/images/imageFFmpeg/AAC编码库那个好.png\">\n\n## 9. 视频基本知识\n\nI帧：关键帧，采用帧内压缩技术\n\nP帧：向前参考帧，压缩时只参考前一个帧，属于帧间压缩技术\n\nB帧：双向参考帧，压缩时即参考前一帧也参考后一帧，属于帧间压缩技术\n\n一般实时互动都不会使用 B 帧\n\n<img src=\"/images/imageFFmpeg/H264基本概念.png\">\n\nGOF(group of frame): 一组帧，可以将一段时间内画面变化不大的所有帧划为一组帧\n\n<img src=\"/images/imageFFmpeg/GOF.png\">\n\nSPS与PPS（这两种都划为 I 帧）：\n\n- SPS(Sequence Parameter Set): \n\n  序列参数集，存放帧数、参考帧数目、解码图像尺寸、帧场编码模式选择标识等。\n\n- PPS(Picture Parameter Set):\n\n  图像参数集，存放熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系统数调整标识等\n\n<img src=\"/images/imageFFmpeg/SPS与PPS.png\">\n\n视频花屏/卡顿原因：\n\n1、如果 GOP 分组中的 P 帧丢失会造成解码端的图像发生错误（于是形成了花屏）。\n\n2、为了避免花屏问题的发生，一般如果发现 P 帧或者I帧丢失，就不显示本 GOP 内的所有帧，直到下一个 I 帧来后重新刷新图像（因为丢了一组数据，所以形成了卡顿）\n\n<img src=\"/images/imageFFmpeg/视频花屏卡顿的原因.png\">\n\n视频编码器：\n\n1、x264/x265。\n\n2、openH264(支持 SVC（分层传输） 技术)。\n\n3、vp8/vp9\n\n<img src=\"/images/imageFFmpeg/视频都有哪些视频编码器.png\">\n\n## 10. H264 宏块的划分与帧分组\n\nH264压缩技术\n\n1. 帧内预测压缩，解决的是空域数据冗余问题（将一幅图里的人眼不是很敏感的色彩、光亮等数据剔除）\n2. 帧间预测压缩，解决的是时域数据冗余问题（将一组图里面连续的重复性高的帧剔除）\n3. 整数离散余弦变换(DCT)，将空间上的相关性变为频域上无关的数据然后进行量化\n4. CABAC压缩，也叫上下文适应无损压缩\n\n<img src=\"/images/imageFFmpeg/H264压缩技术.png\">\n\n宏块的划分与分组：\n\nH264宏块划分与子块划分：宏块里面可以再包含很多子块\n\n<img src=\"/images/imageFFmpeg/H264宏块划分.png\">\n\n<img src=\"/images/imageFFmpeg/宏块划分完成.png\">\n\n子块划分：\n\n<img src=\"/images/imageFFmpeg/子块划分.png\">\n\n帧分组(一组连续的图片，一幅图片为一帧)\n\n<img src=\"/images/imageFFmpeg/帧分组.png\">\n\n## 11. 视频压缩技术详解\n\n- 帧间预测: \n\n  解决时间数据冗余，比较相邻两帧不同给出运动矢量 + 残差值\n\n- 帧内预测: \n\n  解决空间数据冗余，每一个宏块有一个预测模式，然后讲预测后的图像与原图比较算差值，最后存储预测模式和差值即可。帧内压缩是针对于 I 帧的\n\n### 11.1 帧间预测\n\n组内宏块查找：\n\n<img src=\"/images/imageFFmpeg/组内宏块查找.png\">\n\n<img src=\"/images/imageFFmpeg/运动估算.png\">\n\n<img src=\"/images/imageFFmpeg/运动矢量与补偿压缩.png\">\n\n### 11.2 帧内预测\n\n<img src=\"/images/imageFFmpeg/帧内预测.png\">\n\n<img src=\"/images/imageFFmpeg/计算帧内预测残差值.png\">\n\n<img src=\"/images/imageFFmpeg/预测模式与残差值压缩.png\">\n\n### 11.3 DCT 压缩\n\n<img src=\"/images/imageFFmpeg/DCT压缩.png\">\n\n<img src=\"/images/imageFFmpeg/压缩后的结果.png\">\n\n### 11.4 VLC 压缩\n\n<img src=\"/images/imageFFmpeg/VLC压缩.png\">\n\n### 11.5 CABAC 压缩\n\n<img src=\"/images/imageFFmpeg/CABAC压缩.png\">\n\n## 12. H264 结构与码流\n\nH264编码分层：\n\n1、NAL层（Network Abstraction Layer）, 视频数据网络抽象层。\n\n2、VCL层（Video Coding Layer），视频数据编码层，对原始数据进行压缩\n\n码流基本概念：\n\n1、SODB（String Of Data Bits）,原始数据比特流，长度不一定是8的倍数，它是由VCL层产生的。\n\n2、RBSP（Raw Byte Sequence Payload,SODB+trailing bits），算法是在SODB最后一位补1，不按字节对齐则补0。\n\n3、EBSP(Encapsulate Byte Sequence Payload)，需到两个连续的0x00就增加一个0x03。\n\n4、NALU，NAL Header(1B)+EBSP\n\n<img src=\"/images/imageFFmpeg/H264结构图.png\">\n\n以太网最大传输字节 1500 字节。\n\n<img src=\"/images/imageFFmpeg/H264编码分层.png\">\n\n<img src=\"/images/imageFFmpeg/码流基本概念一.png\">\n\n<img src=\"/images/imageFFmpeg/码流基本概念二.png\">\n\n一个H264帧最少要有一个切片(NAL Unit)\n\n<img src=\"/images/imageFFmpeg/NALUnit.png\">\n\n切片与宏块的关系：\n\n- 每个切片都包括切片头和切片数据，\n- 每个切片数据又包括了很多宏块，\n- 每个宏块又包括了宏块的类型、宏块的预测、编码的残渣数据等\n\n<img src=\"/images/imageFFmpeg/切片与宏.png\">\n\n<img src=\"/images/imageFFmpeg/H264切片.png\">\n\n<img src=\"/images/imageFFmpeg/H264码流分层.png\">\n\n## 13. NAL 单元详解\n\n<img src=\"/images/imageFFmpeg/NALHeader.png\">\n\n5 - 关键帧\n\n7- SPS 序列参数集\n\n8- PPS 图像参数集\n\n<img src=\"/images/imageFFmpeg/NALType一.png\">\n\n<img src=\"/images/imageFFmpeg/NALType二.png\">\n\n<img src=\"/images/imageFFmpeg/NAL类型介绍.png\">\n\n<img src=\"/images/imageFFmpeg/单一NALU的RTP包.png\">\n\n<img src=\"/images/imageFFmpeg/组合NALU的RTP包.png\">\n\n如：\n\nP帧B帧很多都是单一类型。\n\nSPS和PPS这两个NAL单元一般放在同一个RTP包里头\n\n<img src=\"/images/imageFFmpeg/分片NALU的RTP包.png\">\n\n<img src=\"/images/imageFFmpeg/FUHeader.png\">\n\n## 14. YUV 讲解\n\n<img src=\"/images/imageFFmpeg/图像除了RGB还是有YUV.png\">\n\n<img src=\"/images/imageFFmpeg/YUV.png\">\n\nYUV常见格式：YUV4:2:0、YUV4:2:2、YUV4:4:4\n\nRGB8:8:8\n\n<img src=\"/images/imageFFmpeg/YUV常见格式.png\">\n\n<img src=\"/images/imageFFmpeg/YUV420.png\">\n\n- UV 混存则为packed(打包存储)，\n- UV分开存则为planar(平面存储) \n\n<img src=\"/images/imageFFmpeg/YUV存储格式.png\">\n\n## 15. 总结\n\n<img src=\"/images/imageFFmpeg/小结.png\">\n\n<img src=\"/images/imageFFmpeg/音频小结.png\">\n\n<img src=\"/images/imageFFmpeg/视频小结.png\">\n\nrtmp 实时消息传输: tcp/ip 应用层协议  推送/直播  基本数据单元为消息\n\n1B 消息类型  2B 长度  3B 时间 4B  流id 消息体\n\n传输时 消息回被拆分成消息块 chunk chunk header + chunk data\n\nflv: 大块音视频 加入标记头信息   延迟表现和大规模并发成熟 \n\nHLS：分成5-10s 用m3u8索引管理 用于朋友圈分享  \n\nm3u8索引： 直播信号源--视频编码器（后台视频处理）--流切片器--各种ts媒体文件（分发模块）--索引文件（数据库）--客户端\n\n\n\ncdn网络 为了解决用户访问资源慢出现的技术\n\n边缘节点  二级节点（大城市） 源站\n\n搭建流媒体服务：\n\n准备流媒体服务器 linux max 编译安装nginx服务  配置rtmp服务并启动nginx服务\n\n\n\n声音三要素：音调 音量 音色\n\n音频量化(模数转换)：模拟数据 采样  量化 编码  数字信号  == 0101001110\n\n码率 = 采样率（1.6w/44.1/48k）x 采样大小(8位-电话/16位-常见) x 声道数（单/双）\n\n音频压缩： 有损消除冗余数据   哈夫曼无损编码\n\n音频编码： 时域转频域---心里声学模型---量化编码---比特流格式化---比特流\n\n音频编解码 ： opus（口 耳 实时互动 最快）  aac(直播用 次快)  speed(回音 降噪等)   g.711（固话）\n\naac : 取代mp3 加入 sir ps 技术  \n\naac lc 128k / aac he v2 64k /  aac he v2 32k/\n\naac 格式 ： adif 从头开始解码，用在磁盘文件中  adts 每一帧都有一个同步字，可以在任何位置解码\n\naac 编码库 ： libfdk_aac > ffmpeg aac >libfaac> libvo_aacenc\n\n\n\nH264： I帧 关键 帧内压缩  / p帧 向前参考1帧 / B帧 双向参考帧\n\nsps: 序列参数集/pps:图像参数集 \n\nGOF： 一组帧数  p帧丢失 会花屏卡顿\n\n视频编码器： x264/x265 /open h264(svc)/vp8/vp9\n\nh264 压缩技术-编码原理： 帧内预测压缩，空域冗余数据/帧间预测压缩，时域冗余数据/dcp整数离散余炫变换，傅立叶变换/cabac压缩\n\nh264结构：视频序列--图像--片--宏块--子快\n\nh264编码分层：nal 视频数据网络抽象层--vcl 视频数据编码层\n\n码率：sodb 原始比特流 / rbsp sodb最后补1 / ebsp 起始码增加一个起始位0x03 /  nalu nal+ebsp\n\nnal unit = nalu 头部 + 一个切片（头/数据） 切片 \n\nyuv格式：4：4:4/4:4:2/4：2:0 （平坦编码 /半平坦编码）\n\n<img src=\"/images/imageFFmpeg/后续.png\">\n\n<img src=\"/images/imageFFmpeg/音视频知识01.png\">\n\n<img src=\"/images/imageFFmpeg/音视频知识02.png\">\n\n<img src=\"/images/imageFFmpeg/音视频知识03.png\">\n\n<img src=\"/images/imageFFmpeg/行业痛点-01.png\">\n\n<img src=\"/images/imageFFmpeg/行业痛点-02.png\">\n\n\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"FFmpeg入门","url":"/2019-04-20/reference/FFmpeg/FFmpeg入门/","content":"\n> 本文以文档的形式来描述FFmpeg怎么入门，这也是为以后写文档做的一个大题框架格式。\n\n# 文档介绍\n\n## 文档目的\n\n整理出开源代码 ffmpeg 的资料，方便公司同事后续使用。\n\n<!-- more -->\n\n## 文档范围\n\n较为详细的介绍 ffmpeg 的功能、使用以及二次开发。\n\n## 读者对象\n\n希望了解 ffmpeg 知识，从事 USM 及 IPTV 的同事。\n\n## 参考文献\n\nTODO\n\n## 术语与缩写解释\n\n| 缩略语/术语 | 全 称               | 说 明                        |\n| ----------- | ------------------- | ---------------------------- |\n| ffmpeg      | Fast forword mpeg   | 音视频转换器                 |\n| ffplay      | Fast forword play   | 用 ffmpeg 实现的播放器       |\n| ffserver    | Fast forword server | 用 ffmpeg 实现的 rstp 服务器 |\n| ffprobe     | Fast forword probe  | 用来输入分析输入流。         |\n\n# FFmpeg 支持能力说明\n\n## FFmpeg 介绍及安装\n\n### FFmpeg 简介\n\nFFmpeg 是一个开源免费跨平台的视频和音频流方案，属于自由软件，采用 LGPL 或 GPL 许可证（依据你选择的组件）。它提供了录制、转换以及流化音视频的完整解决方案。它包含了非常先进的音频/视频编解码库 libavcodec，为了保证高可移植性和编解码质量，libavcodec 里很多 codec 都是从头开发的。\n\nFFmpeg 项目由以下几部分组成:\n\n（1）ffmpeg 视频文件转换命令行工具, 也支持经过实时电视卡抓取和编码成视频文件.\n\n（2）ffserver 基于 HTTP、RTSP 用于实时广播的多媒体服务器. 也支持时间平移\n\n（3）ffplay 用 SDL 和 FFmpeg 库开发的一个简单的媒体播放器\n\n（4）libavcodec 一个包含了所有 FFmpeg 音视频编解码器的库. 为了保证最优性能和高可复用性, 大多数编解码器从头开发的.\n\n（5）libavformat 一个包含了所有的普通音视格式的解析器和产生器的库\n\n### FFmpeg 安装\n\n将所有源代码压缩在一个文件夹中，例如 `/绝对路径/ffmpeg`。\n\n在终端输入以下指令：\n\n```shell\n$ cd /绝对路径/ffmpeg\n$ ./configure   #(此时，会出现问题。然后重新输入./configure –disable-yasm-)\n$ Make\n```\n\n至此，ffmpeg 安装编译通过，可以进行对音视频的操作。\n\nffplay 的编译需要依赖于 SDL 库，所以要想编译成功 ffplay，必须先安装 SDL 库，\n\n安装方法：下载最新版本的 SDL 相应版本的 SDL 源码，编译，即可生成 SDL 库。\n\n```shell\n# 首先下载 SDL 软件包\n$ configure --prefix=/usr/local\n$ make && make install\n```\n\n## FFmpeg 参数说明\n### 通用选项\n\n```shell\n-L license\n-h 帮助\n-fromats 显示可用的格式，编解码的，协议的。\n-f fmt 强迫采用格式 fmt\n-i filename 输入文件\n-y 覆盖输出文件\n-t duration 设置纪录时间 hh:mm:ss[.xxx]格式的记录时间也支持\n-ss position 搜索到指定的时间 [-]hh:mm:ss[.xxx]的格式也支持\n-title string 设置标题\n-author string 设置作者\n-copyright string 设置版权\n-comment string 设置评论\n-target type 设置目标文件类型(vcd，svcd，dvd) 所有的格式选项（比特率，编解码以及缓冲区\t大小）自动设置 ，只需要输入如下的就可以了：\n\tffmpeg -i myfile.avi -target vcd /tmp/vcd.mpg\n-hq 激活高质量设置\n-itsoffset offset 设置以秒为基准的时间偏移，该选项影响所有后面的输入文件。该偏移被加到输\t 入文件的时戳，定义一个正偏移意味着相应的流被延迟了 offset 秒。\n\t[-]hh:mm:ss[.xxx] 的格式也支持\n```\n\n### 视频选项\n\n```shell\n-b bitrate 设置比特率，缺省 200kb/s\n-r fps 设置帧频 缺省 25\n-s size 设置帧大小 格式为 WXH 缺省 160X128.下面的简写也可以直接使用：\n\tSqcif 128X96 qcif 176X144 cif 252X288 4cif 704X576\n-aspect aspect 设置横纵比 4:3 16:9 或 1.3333 1.7777\n-croptop size 设置顶部切除带大小 像素单位\n-cropbottom size –cropleft size –cropright size 底部，左边，右边切除带大小。\n-padtop size 设置顶部补齐的大小 像素单位\n-padbottom size –padleft size –padright size –padcolor color 设置补齐条大小\n\t和颜色(hex，6 个 16 进制的数，红:绿:兰排列，比如 000000 代表黑色)\n-vn 不做视频记录\n-bt tolerance 设置视频码率容忍度 kbit/s\n-maxrate bitrate 设置最大视频码率容忍度\n-minrate bitreate 设置最小视频码率容忍度\n-bufsize size 设置码率控制缓冲区大小\n-vcodec codec 强制使用 codec 编解码方式。 如果用 copy 表示原始编解码数据必须被拷贝。\n-sameq 使用同样视频质量作为源（VBR）\n-pass n 选择处理遍数（1 或者 2）。两遍编码非常有用。第一遍生成统计信息，第二遍生成精确的请求的码率\n-passlogfile file 选择两遍的纪录文件名为 file\n```\n\n### 高级视频选项\n\n```shell\n-g gop_size 设置图像组大小\n-intra 仅适用帧内编码\n-qscale q 使用固定的视频量化标度(VBR)\n-qmin q 最小视频量化标度(VBR)\n-qmax q 最大视频量化标度(VBR)\n-qdiff q 量化标度间最大偏差 (VBR)\n-qblur blur 视频量化标度柔化(VBR)\n-qcomp compression 视频量化标度压缩(VBR)\n-rc_init_cplx complexity 一遍编码的初始复杂度\n-b_qfactor factor 在 p 和 b 帧间的 qp 因子\n-i_qfactor factor 在 p 和 i 帧间的 qp 因子\n-b_qoffset offset 在 p 和 b 帧间的 qp 偏差\n-i_qoffset offset 在 p 和 i 帧间的 qp 偏差\n-rc_eq equation 设置码率控制方程 默认 tex^qComp\n-rc_override override 特定间隔下的速率控制重载\n-me method 设置运动估计的方法 可用方法有 zero phods log x1 epzs(缺省) full\n-dct_algo algo 设置 dct 的算法可用的有 0 FF_DCT_AUTO 缺省的 DCT \n\t1 FF_DCT_FASTINT \n\t2 FF_DCT_INT \n\t3 FF_DCT_MMX \n\t4 FF_DCT_MLIB \n\t5 FF_DCT_ALTIVEC\n-idct_algo algo 设置 idct 算法。可用的有 0 FF_IDCT_AUTO 缺省的 IDCT \n\t1 FF_IDCT_INT \n\t2 FF_IDCT_SIMPLE \n\t3 FF_IDCT_SIMPLEMMX \n\t4 FF_IDCT_LIBMPEG2MMX \n\t5 FF_IDCT_PS2 \n\t6 FF_IDCT_MLIB \n\t7 FF_IDCT_ARM \n\t8 FF_IDCT_ALTIVEC \n\t9 FF_IDCT_SH4 \n\t10 FF_IDCT_SIMPLEARM\n-er n 设置错误残留为 n 1 FF_ER_CAREFULL 缺省 \n\t2 FF_ER_COMPLIANT \n\t3 FF_ER_AGGRESSIVE \n\t4 FF_ER_VERY_AGGRESSIVE\n-ec bit_mask 设置错误掩蔽为 bit_mask ， 该值为如下值的位掩码 \n\t1 FF_EC_GUESS_MVS (default=enabled) \n\t2 FF_EC_DEBLOCK (default=enabled)\n-bf frames 使用 frames B 帧，支持 mpeg1，mpeg2，mpeg4\n-mbd mode 宏块决策 0 FF_MB_DECISION_SIMPLE 使 用 mb_cmp \n\t1 FF_MB_DECISION_BITS \n\t2 FF_MB_DECISION_RD\n-4mv 使用 4 个运动矢量 仅用于 mpeg4\n-part 使用数据划分 仅用于 mpeg4\n-bug param 绕过没有被自动监测到编码器的问题\n-strict strictness 跟标准的严格性\n-aic 使能高级帧内编码 h263+\n-umv 使能无限运动矢量 h263+\n-deinterlace 不采用交织方法\n-interlace 强迫交织法编码 仅对 mpeg2 和 mpeg4 有效。当你的输入是交织的并且你想要保持交\t织以最小图像损失的时候采用该选项。可选的方法是不交织，但是损失更大\n-psnr 计算压缩帧的 psnr\n-vstats 输出视频编码统计到 vstats_hhmmss.log\n-vhook module 插入视频处理模块 module 包括了模块名和参数，用空格分开\n```\n\n### 音频选项\n\n```shell\n-ab bitrate 设置音频码率\n-ar freq 设置音频采样率\n-ac channels 设置通道 缺省为 1\n-an 不使能音频纪录\n-acodec codec 使用 codec 编解码\n```\n\n### 音频/视频捕获选项\n\n```shell\n-vd device 设置视频捕获设备。比如/dev/video0\n-vc channel 设置视频捕获通道 DV1394 专用\n-tvstd standard 设置电视标准 NTSC PAL(SECAM)\n-dv1394 设置 DV1394 捕获\n-av device 设置音频设备 比如/dev/dsp\n```\n\n### 高级选项\n\n```shell\n-map file:stream 设置输入流映射\n-debug 打印特定调试信息\n-benchmark 为基准测试加入时间\n-hex 倾倒每一个输入包\n-bitexact 仅使用位精确算法 用于编解码测试\n-ps size 设置包大小，以 bits 为单位\n-re 以本地帧频读数据，主要用于模拟捕获设备\n-loop 循环输入流。只工作于图像流，用于 ffserver 测试\n```\n\n### FFmpeg 参数实例\n\n```shell\n$ ./ffmpeg -y -i /rootVideoConverter/123.avi -ab 56 -ar 22050 -b 1500 -r 15 -qscale 10 –s 480x350 /root/VideoConverter/234.flv\n============================================================\n-y （覆盖输出文件，即如果 1.***文件已经存在的话，不经提示就覆盖掉了）\n-i \"1.avi\"（输入文件是和 ffmpeg 在同一目录下的 1.avi 文件，可以自己加路径，改名字）\n-s （输出的分辨率，注意片源一定要是 16:9 的不然会变形）\n-r （帧数，一般就用这个吧）\n-b 1500（视频数据流量，用-b xxxx 的指令则使用固定码率，数字随便改，1500 以上没效果；\n\t还可以用动态码率如：-qscale 4 和-qscale 6，4 的质量比 6 高）\n-acodec aac（音频编码用 AAC）\n-ac 2 （声道数 1 或 2）\n-ar 24000（声音的采样频率，好像 PSP 只能支持 24000Hz）\n-ab 128 （ 音频数据流量，一般选择 32、64、96、128）\n-vol 200（200%的音量，自己改）\n-f psp （输出 psp 专用格式）\n-muxvb 768（给 PSP 机器识别的码率，一般选择 384、512 和 768）\n-ab bitrate 设置音频码率\n-ar freq 设置音频采样率\n-ss 指定时间点开始转换任务，(time_off set the start time offset)，-ss 后跟的时间单位为秒 .\n-s 320x240 指定分辨率\n-r 29.97 桢速率\n-bitexact 使用标准比特率\n-vcodec xvid 使用 xvid 压缩\n```\n\n\n## FFmpeg 支持能力说明\n\n### FFmpeg 对编码解码器的支持\n\nffmpeg 支持的编解码器种类共有 280 多种，涵盖了几乎所有常见音视频编码格式，能解码几乎所有的音视频，每种音视频编解码器的实现都在 libavcodec 目录下有具体的C 语言实现，具体的支持情况参见：\n\n> [ffmpeg支持的编解码器]()\n\n注：编码器和解码器的名称不是完全匹配的，因此有些编码器没有对应相同名称的解码器，反之，\n解码器也一样。即使编码和解码都支持也不一定是完全对应的，例如 h263 解码器对应有 h263p 和 h263 编码器。\n\n### FFmpeg 对容器格式的支持\n\nffmpeg 支持对绝大多数的容器格式的读写操作，共计 190 多种，涵盖了互联网上各种常见媒体格式及日常生活中及专业应用中的各种媒体格式。详细的支持情况参见：\n\n> [FFmpeg支持的媒体文件格式]()\n\n### FFmpeg 对过滤器的支持\n\n| Filters     | 说明                                                         |\n| ----------- | ------------------------------------------------------------ |\n| aformat     | Convert the input audio to one of the specified formats.     |\n| anull       | Pass the source unchanged to the output.                     |\n| aresample   | Resample audio data.                                         |\n| ashowinfo   | Show textual information for each audio frame.               |\n| abuffer     | Buffer audio frames， and make them accessible to the filterchain. |\n| anullsrc    | Null audio source， never return audio frames.               |\n| abuffersink | Buffer audio frames， and make them available to the end of the filter graph. |\n| anullsink   | Do absolutely nothing with the input audio.                  |\n| copy        | Copy the input video unchanged to the output.                |\n| crop        | Crop the input video to `width:height:x:y`.                  |\n| drawbox     | Draw a colored box on the input video.                       |\n| fade        | Fade in/out input video                                      |\n| fieldorder  | Set the field order.                                         |\n| fifo        | Buffer input images and send them when they are requested.   |\n| format      | Convert the input video to one of the specified pixel formats. |\n| gradfun     | Debands video quickly using gradients.                       |\n| hflip       | Horizontally flip the input video.                           |\n| lut         | Compute and apply a lookup table to the RGB/YUV input video. |\n| lutrgb      | Compute and apply a lookup table to the RGB input video.     |\n| lutyuv      | Compute and apply a lookup table to the YUV input video.     |\n| negate      | Negate input video.                                          |\n| noformat    | Force libavfilter not to use any of the specified pixel formats for the input to the next filter. |\n| null        | Pass the source unchanged to the output.                     |\n| overlay     | Overlay a video source on top of the input.                  |\n| pad         | Pad input image to `width:height[:x:y[:color]] (default x and y: 0， default color: black)`. |\n| pixdesctest | Test pixel format definitions.                               |\n| scale       | Scale the input video to width:height size and/or convert the image format. |\n| select      | Select frames to pass in output.                             |\n| setdar      | Set the frame display aspect ratio.                          |\n| setpts      | Set PTS for the output video frame.                          |\n| setsar      | Set the pixel sample aspect ratio.                           |\n| settb       | Set timebase for the output link.                            |\n| showinfo    | Show textual information for each video frame.               |\n| slicify     | Pass the images of input video on to next video filter as multiple slices. |\n| split       | Pass on the input to two outputs.                            |\n| transpose   | Transpose input video.                                       |\n| unsharp     | Sharpen or blur the input video.                             |\n| vflip       | Flip the input video vertically.                             |\n| buffer      | Buffer video frames， and make them accessible to the filterchain. |\n| color       | Provide an uniformly colored input， syntax is: [color[:size[:rate]]] |\n| movie       | Read from a movie source.                                    |\n| nullsrc     | Null video source， never return images.                     |\n| rgbtestsrc  | Generate RGB test pattern.                                   |\n| testsrc     | Generate test pattern.                                       |\n| buffersink  | Buffer video frames， and make them available to the end of the filter graph. |\n| nullsink    | Do absolutely nothing with the input video.                  |\n\n### FFmpeg 对图像颜色空间的支持\n\nffmpeg 支持常见的图像颜色空间，并且在 libavswcale 中定义了颜色空间转换的相关函数实现各种颜色模式的互转。具体的支持情况见:\n\n> [FFmpeg支持的图像颜色空间]()\n\n## FFmpeg 功能及使用说明\n\n### ffplay 对多媒体的支持能力验证\n\n**一、视频**\n\n`3gp 177X144` 支持播放，在 windows 下播放正常，但是在 linux 下面偶有 BUG 如果发现画面无法显示而声音可以播放的情况下 可以试着切换全屏或者切换分辨率。\n\n```shell\nAVI \t208X176 \t\t支持\n \t\t320X240 \t\t支持\n \t\t720X400 \t\t支持\n \t\t720X576 \t\t支持\nDAT 3\t52X288 \t\t\t支持\nDiVX \t720X576 \t\t支持\nMKV \t320X240 \t\t支持\n\t\t352X288 \t\t支持\n \t\t704X304 \t\t支持\n \t\t720X576 \t\t支持\nMP4 \t320X240 \t\t支持\n\t\t352X288 \t\t支持\n        720X400 \t\t支持\nMPG \t320X240 \t\t支持\n \t\t352X288 \t\t支持\n \t\t480X576 \t\t支持\n \t\t720X576 \t\t支持\n \t\t720X480 \t\t支持\nVOB \t352X288 \t\t支持\nXVID \t720X576 \t\t支持\nMOV \t\t\t\t\t支持\nRMVB \t\t\t\t\t支持 \n```\n\n**二、音频**\n\n```shell\nAC3 \t48KHZ \t\t\t\t支持\nAPE \t11KHZ \t\t\t\t支持\n \t\t22KHZ \t\t\t\t支持\n \t\t44KHZ \t\t\t\t支持\n \t\t48KHZ \t\t\t\t支持\nMP3 \t32KHZ \t64Kbps \t\t支持\n        32KHZ \t128KbpS \t支持\n        32KHZ \t160Kbps \t支持\n        32KHZ \t192Kbps \t支持\n        32KHZ \t320Kbps \t支持\n        44KHZ\t64Kbps \t\t支持\n        44KHZ \t128Kbps \t支持\n        44KHZ \t160Kbps \t支持\n        44KHZ \t192Kbps \t支持\n        44KHZ \t320Kbps \t支持\n        48KHZ \t64Kbps \t\t支持\n        48KHZ \t128Kbps \t支持\n        48KHZ \t160Kbps \t支持\n        48KHZ \t192Kbps \t支持\n        48KHZ \t320Kbps \t支持\nOGG \t32KHZ \t128Kbps \t支持\n        32KHZ \t192Kbps \t支持\n        44KHZ \t64Kbps \t\t支持\n        44KHZ \t128Kbps \t支持\n        44KHZ \t192Kbps \t支持\n        48KHZ \t64Kbps \t\t支持\n        44KHZ \t128Kbps \t支持\n        44KHZ \t192Kbps \t支持\nWAV \t11KHZ \t\t\t\t支持\n        22KHZ \t16Kbps \t\t支持\n        44KHZ \t16Kbps \t\t支持\n        48KHZ \t16Kbps \t\t支持\nWMA \t8KHZ \t16Kbps \t\t支持\n        11KHZ \t16Kbps \t\t支持\n        16KHZ \t16Kbps \t\t支持\n        22KHZ \t16Kbps \t\t支持\n        44KHZ \t16Kbps \t\t支持\n        48KHZ \t16Kbps \t\t支持\n```\n\n**三、图像**\n\n```shell\nPNG \t\t支持\nJPG \t\t支持\nJPEG \t\t支持\nGIF \t\t支持\nBMP \t\t支持\n```\n\n\n### FFmpeg 格式转换\n\n**第一步：准备媒体**\n\n前面已经讲的很清楚了，ffmpeg 如何安装不在赘述。准备好相应的文件，如图 2-1所示。\n\n![图 2-1](/images/imageFFmpeg/Thor/2-1.png)\n\n**第二步：启动 ffmpeg**\n\n由于做的是格式转换，在 ffserver 上不能直观的看见结果，故我是在 linux 下进行的。打开终端，值得一提的是格式转换需要超级用户才能进行，故在命令行输入：`su，<回车>`，输入密码进入超级用户，本例中，以 FFmpeg 将 test.avi 转换为 test.mpg。在命令行中输入：\n\n```shell\n$ ./ffmpeg –i test.avi –r 25 –s 720x400 test.mpg\n```\n\n其中原格式分辨率为 320x240，将转为 720x400，-r 前面已经解释其含义，表示设置帧频为 25。转换成功后如图 2-2 所示，前后两种格式播放效果如图 2-3 所示。相应的，转换为其他格式做相应的变化即可。\n\n同时还可以在转换格式时进行强制的音视频转换，如 `–vcodec + 格式`，将会强制将视频按指定格式编码，`-acodec +格式`，将会强制按指定格式编码音频信息。在转换中有很多其他参数可以指定，如码率、分辨率、帧率等，具体按照 ffmpeg 的参数说明指定参数即可。但有一条转低不转高的原则需要注意，即品质差的音视频转换不建议转换到品质好的音视频。\n\n![图 2-2](/images/imageFFmpeg/Thor/2-2.png)\n\n![图 2-3](/images/imageFFmpeg/Thor/2-3.png)\n\n再说说如何在转换视频的时候将音频合成到视频中，且覆盖其原来的音频。这个现在摸索出两种方法。\n\n**方法一**：需要两条命令实现，先在命令行中输入：\n\n```shell\n$ ./ffmpeg –i test.avi -an –r 25 test.mpg\n```\n\n此时将生成一个没有声音的 test.mpg 视频，再在命令行中输入：\n\n```shell\n$ ./ffmpeg –i test.mpg –i test.mp3 –r 25 test1.mpg\n```\n\n此时将会生成一个名为 test1.mpg 的视频。该视频播放时视频为 test.avi 的视频，但音频变为了 test.mp3 的音频了。\n\n**方法二**：只要一条指令即可实现。在命令行中输入：\n\n```shell\n$ ./ffmpeg –i test.avi –i test.mp3 –vcodec copy –acodec copy –r 25 test2.mpg\n```\n\n此时将会生成一个名为 test2.mpg 的视频，播放时其视频为 test.avi 的视频，音频为 test.mp3。`–vcodec copy` 为 force video codec(‘copy’ to copy stream)。\n\n有一点需要注意，文件命名不能有空格，否则会导致编译时不能通过。另外，`-an` 为不能使音频记录。\n\n**第三步：播放媒体**\n\n播放我们转换的媒体，看看是否满足我们当初的愿望，不出什么差错的话，是完全能够满足我们的要求的。\n\n### FFmpeg 视频截图\n\n截取一张 `300x200` 尺寸大小的格式为 jpg 的一张图片：\n\n```shell\n$ ./ffmpeg –i test.avi –y –f image2 –t 0.001 –s 300x200 test.jpg\n```\n\n要截取指定时间的图片，如 5 秒之后的：\n\n```shell\n$ ./ffmpeg –i test.avi –y –f image2 –ss 5 –t 0.001 –s 300x200 test.jpg\n```\n\n其中，`-ss` 后的单位为秒，也可写成：`-ss 00:00:05`。\n\n把视频的前 30 帧转换为一个动态的 gif 图。需要说明的是，转换成功之后，如果用 ffplay 播放是看不出效果的，建议换成其他图片播放器播放。其转换命令为：\n\n```shell\n$ ./ffmpeg –i test.avi –vframes 30 –pix_fmt rgb24–y –f gif test.gif\n```\n\n也可以从视频中的第 10 秒开始截取后面的 5 秒内容转换为一个无限重播的动态 gif 图。其命令为：\n\n```shell\n$ ./ffmpeg –i test.avi –pix_fmt rgb24 –ss 10 –t 5 –y –f gif test.gif\n```\n\n上面两种动态 gif 都是只播一次，想让其一直播，可再加一个参数：`-loop_output 0`。\n\n### FFmpeg 屏幕录制\n\n屏幕录制其命令为：\n\n```shell\n$ ./ffmpeg -f x11grab -r 25 -s wxga -i :0.0 /tmp/outputFile.mpg\n```\n\n其他相关参数可自行添加。需要说明的是，各个版本的 ffmpeg 对屏幕录制的命令不一。如果你只想录制一个应用程序窗口或者桌面上的一个固定区域，那么可以指定偏移位置和区域大小。使用 `xwininfo -frame` 命令可以完成查找上述参数。\n\n注：ffmpeg 的屏幕录制功能只能在 Linux 环境下有效。并且在配置时需要添加 `–enable-x11grub` 指令，默认关闭。\n\n### FFmpeg 音视频采集\n\n把摄像头的实时视频录制下来，存储为文件\n\n```shell\n$ ./ffmpeg -f video4linux -s 320x240 -r 10 -i /dev/video0 test.asf\n```\n\n录音，其命令为：\n\n```shell\n$ ./ffmpeg –i /dev/dsp -f oss test.mp3\n```\n\n## FFmpeg 应用实例\n\n### 用 FFserver 从文件生成流媒体\n\n**一、安装 ffmpeg**\n\n在 ubuntu 下，运行 `sudo apt-get ffmpeg` 安装 ffmpeg，在其他 linux 操作系统下，见 ffmpeg 的编译过程（编译完成后可执行自动安装）。\n\n**二、准备预播放的媒体文件**\n\n如 test.Mp3，在本文档中，默认放入用户文件夹下得 Music 文件夹内.(直接从设备采集不在本文档叙述范围之内)\n\n**三、修改 ffserver 配置信息**\n\nffserver 配置文件为: `/etc/ffserver.conf` 打开，填写配置信息.配置信息包括三方面:\n\n（1）端口绑定等基本信息，在 ·/etc/ffserver.conf· 中有详细注释，在此不再重复，最终配置信息为：\n\n```shell\nPort 8090\nBindAddress 0.0.0.0\nMaxHTTPConnections 2000\nMaxClients 1000\nMaxBandwidth 1000\n```\n\n（2）媒体文件配置信息.本信息根据具体的媒体文件类型直接在配置文件中取消注释掉相应文件类型的配置信息，然后填写文件路径即可:\n\n```shell\n# MP3 audio\n<Stream test.mp3>\nFile \"/home/xiaoma/Music/test.mp3\"\nFormat mp2\nNoVideo\n</Stream>\n```\n\n**四、启动 ffserver**\n\n在终端中运行: `sudo ffserver -f /etc/ffserver.conf`  启动 ffserver.\n\n**五、播放流媒体**\n\n在浏览器中输入 [http://127.0.0.1:8090/test.mp3]() 即可播放音乐.\n\n在终端中输入 `ffplay http://localhost:8090/test.mp3` 可播放流媒体.\n\n### 用 FFserver 从设备生成实时流\n\n**一、准备媒体**\n\n按照上节步骤安装 ffmpeg，保证摄像头和声卡可用，将从摄像头和声卡获取音视频信息。\n\n**二、修改 ffserver 配置信息**\n\nffserver 配置文件为: `/etc/ffserver.conf` 打开，填写配置信息.配置信息包括三方面:\n\n（1）端口绑定等基本信息，在 `/etc/ffserver.conf` 中有详细注释，在此不再重复，最终配\n置信息为:\n\n```shell\nPort 8090\nBindAddress 0.0.0.0\nMaxHTTPConnections 2000\nMaxClients 1000\nMaxBandwidth 1000\n```\n\n（2）fend(传冲信息)，在文件播放中，基本不用动本配置信息，只需要根据具体情况分配缓冲文件.最终配置信息如下:\n\n```shell\n<Feed feed1.ffm>\nFile /tmp/feed1.ffm\nFileMaxSize 2M\nACL allow 127.0.0.1\n</Feed>\n```\n\n（3）媒体文件配置信息.本信息根据具体的媒体文件类型直接在配置文件中取消注释掉相应文件类型的配置信息，然后填写文件路径即可:\n\n(中间会有很多很多配置信息，都是关于音视频的，有些配置还不懂，慢慢摸索吧)\n\n```shell\n<Stream test1.mpg>\nFeed feed1.ffm\nFormat mpeg\nAudioBitRate 32\nAudioChannels 1\nAudioSampleRate 44100\nVideoBitRate 64\nVideoBufferSize 40\nVideoFrameRate 3\nVideoSize 160x128\nVideoGopSize 12\n</Stream>\n# Flash\n<Stream test.swf>\nFeed feed1.ffm\nFormat swf\nVideoFrameRate 2\nVideoIntraOnly\nNoAudio\n</Stream>\n```\n\n**三、启动 FFserver**\n\n在终端中运行: `sudo ffserver -f /etc/ffserver.conf` 启动 ffserver.\n\n**四、启动 ffmpeg**\n\n本例中，以 ffmpeg 作为实时摄像头采集输入.在命令行中输入:\n\n```shell\n$ ./ffmpeg -f video4linux2 -r 25 -i /dev/video0 /tmp/feed1.ffm\n```\n\n如果有音频设备，则采集音频的命令如下:\n\n```shell\n$ ./ffmpeg -f oss -i /dev/dsp -f video4linux2 -r 25 -i /dev/video0 /tmp/feed1.ffm\n```\n\n(音频格式参数自己配置)\n\n**五、播放流媒体**\n\n在浏览器中输入 [http://127.0.0.1:8090/test1.mpg]() 即可播放音乐.\n\n在终端中输入 `ffplay http://localhost:8090/test.swf` 可播放流媒体.\n\n# FFmpeg 架构\n\n## FFmpeg 文件结构\n\n![markdown table](/images/imageFFmpeg/Thor/markdown-table.png)\n\n<table>\n    <tr>\n        <td>目录</td>\n        <td>文件</td>\n        <td>简要说明</td>\n    </tr>\n    <tr>\n        <td rowspan=\"14\">\n            <font style=\"color:red;font-weight:bold;\">libavformat</font> </br></br>\n            主要存放ffmpeg</br>\n            支持的各种编解码</br>\n            器的实现及ffmpeg</br>\n            编解码功能相关的</br>\n            数据结构定义及函</br>\n\t\t\t数定义和声明\n        </td>\n   \t\t<td >allcodecs.c</td>\n    \t<td>简单的注册类函数</td>\n    </tr>\n    <tr>\n        <td >avcodec.h</td>\n        <td >编解码相关结构体定义和函数原型声明</td>\n    </tr>\n    <tr>\n        <td >dsputil.c</td>\n        <td >限幅数组初始化</td>\n    </tr>\n    <tr>\n        <td >dsputil.h</td>\n        <td >限幅数组声明</td>\n    </tr>\n    <tr>\n        <td >imgconvert.c</td>\n        <td >颜色空间转换相关函数实现</td>\n    </tr>\n    <tr>\n        <td >imgconvert_template.h</td>\n        <td >颜色空间转换相关结构体定义和函数声明</td>\n    </tr>\n    <tr>\n        <td >utils_codec.c </td>\n        <td >一些解码相关的工具类函数的实现</td>\n    </tr>\n    <tr>\n        <td >mpeg4audio.c</td>\n        <td >mpeg4 音频编解码器的函数实现</td>\n    </tr>\n    <tr>\n        <td >mpeg4audio.h</td>\n        <td >mpeg4 音频编解码器的函数声明</td>\n    </tr>\n    <tr>\n        <td >mpeg4data.h</td>\n        <td >mpeg4 音视频编解码器的公用的函数声明及数据结构定义</td>\n    </tr>\n    <tr>\n        <td >mpeg4video.c </td>\n        <td >mpeg4 视频编解码器的函数实现</td>\n    </tr>\n    <tr>\n        <td >mpeg4video.h</td>\n        <td >mpeg4 视频编解码器的函数的声明及先关数据结构的定义</td>\n    </tr>\n    <tr>\n        <td >mpeg4videodec.c</td>\n        <td >mpeg4 视频解码器的函数实现</td>\n    </tr>\n    <tr>\n        <td >mpeg4videoenc.c</td>\n        <td >mpeg4 视频编码器的函数实现</td>\n    </tr>\n    <tr>\n        <td rowspan=\"14\">\n            <font style=\"color:red;font-weight:bold;\">libavformat</font> </br></br>\n            主要存放ffmpeg支</br>\n            持的各种媒体格式</br>\n\t\t\tMUXER/DEMUXER</br>\n\t\t\t和数据流协议的定</br>\n\t\t\t义和实现文件以及</br>\n\t\t\tffmpeg解复用相</br>\n\t\t\t关的数据结构及</br>\n\t\t\t函数定\n        </td>\n   \t\t<td >allformats.c</td>\n    \t<td>简单注册类函数</td>\n    </tr>\n    <tr>\n        <td >avformat.h</td>\n        <td >文件和媒体格式相关函数声明和数据结构定义</td>\n    </tr>\n    <tr>\n        <td >avio.c</td>\n        <td >无缓冲 IO 相关函数实现</td>\n    </tr>\n    <tr>\n        <td >avio.h</td>\n        <td >无缓冲 IO 相关结构定义和函数声明</td>\n    </tr>\n    <tr>\n        <td >aviobuf.c</td>\n        <td >有缓冲数据 IO 相关函数实现</td>\n    </tr>\n\t<tr>\n        <td >cutils.c</td>\n        <td >简单的字符串操作函数</td>\n    </tr>\n    <tr>\n        <td >utils_format.c</td>\n        <td >文件和媒体格式相关的工具函数的实现</td>\n    </tr>\n    <tr>\n        <td >file.c</td>\n        <td >文件 io 相关函数</td>\n    </tr>\n    <tr>\n        <td >......</td>\n        <td >其他相关媒体流 IO 的函数和数据结构实现文件。如：rtsp、http 等。</td>\n    </tr>\n\t<tr>\n        <td >avi.c</td>\n        <td >AVI 格式的相关函数定西</td>\n    </tr>\n    <tr>\n        <td >avi.h</td>\n        <td >AVI 格式的相关函数声明及数据结构定义</td>\n    </tr>\n    <tr>\n        <td >avidec.c</td>\n        <td >AVI 格式 DEMUXER 相关函数定义</td>\n    </tr>\n    <tr>\n        <td >avienc.c</td>\n        <td >AVI 格式 MUXER 相关函数定义</td>\n    </tr>\n\t<tr>\n        <td >......</td>\n        <td >其他媒体格式的 muxer/demuxer 相关函数及数据结构定义和声明文件</td>\n    </tr>\n    <tr>\n        <td rowspan=\"14\">\n            <font style=\"color:red;font-weight:bold;\">libavutil</font> </br></br>\n\t\t\t主要存放ffmpeg</br>\n\t\t\t工具类函数的定义\n        </td>\n   \t\t<td >avutil.h</td>\n    \t<td>简单的像素格式宏定义</td>\n    </tr>\n    <tr>\n        <td >bswap.h</td>\n        <td >简单的大小端转换函数的实现</td>\n    </tr>\n    <tr>\n        <td >commom.h</td>\n        <td >公共的宏定义和简单函数的实现</td>\n    </tr>\n<tr>\n        <td >mathematics.c</td>\n        <td >数学运算函数实现</td>\n    </tr>\n    <tr>\n        <td >rational.h</td>\n        <td >分数相关表示的函数实现</td>\n    </tr>\n</table>\n\n\n##  I\\O 模块分析\n\n### 概述\n\nffmpeg 项目的数据 IO 部分主要是在 libavformat 库中实现，某些对于内存的操作部分在 libavutil 库中。数据 IO 是基于文件格式（Format）以及文件传输协议(Protocol)的，与具体的编解码标准无关。\n\nffmpeg 工程转码时数据 IO 层次关系如图所示：\n\n![ffmpeg转码数据IO流程](/images/imageFFmpeg/Thor/ffmpeg转码数据IO流程.png)\n\n对于上面的数据 IO 流程，具体可以用下面的例子来说明，我们从一个 http 服务器获取音视频数据，格式是 flv 的，需要通过转码后变成 avi 格式，然后通过 udp 协议进行发布。其过程就如下所示：\n\n- 读入 http 协议数据流，根据 http 协议获取真正的文件数据（去除无关报文信息）；\n- 根据 flv 格式对数据进行解封装；\n- 读取帧进行转码操作；\n- 按照目标格式 avi 进行封装；\n- 通过 udp 协议发送出去。\n\n### 相关数据结构介绍\n\n在 libavformat 库中与数据 IO 相关的数据结构主要有 URLProtocol、URLContext、ByteIOContext、AVFormatContext 等，各结构之间的关系如图所示。\n\n![libavformat库中IO相关数据结构之间的关系](/images/imageFFmpeg/Thor/libavformat库中IO相关数据结构之间的关系.png)\n\n**1、URLProtocol 结构**\n\n表示广义的输入文件，该结构体提供了很多的功能函数，每一种广义的输入文件（如：file、pipe、tcp、rtp 等等）对应着一个 `URLProtocol` 结构，在 `av_register_all()` 中将该结构体初始化为一个链表，表头为 `avio.c` 里的 `URLProtocol *first_protocol = NULL;` 保存所有支持的输入文件协议，该结构体的定义如下：\n\n```c\ntypedef struct URLProtocol\n{\n    const char *name;\n    int (*url_open)(URLContext *h， const char *url， int flags);\n    int (*url_read)(URLContext *h， unsigned char *buf， int size);\n    int (*url_write)(URLContext *h， const unsigned char *buf， int size);\n    int64_t (*url_seek)(URLContext *h， int64_t pos， int whence);\n    int (*url_close)(URLContext *h);\n    struct URLProtocol *next;\n    int (*url_read_pause)(URLContext *h， int pause);\n    int64_t (*url_read_seek)(URLContext *h， int stream_index，\n                             int64_t timestamp， int flags);\n    int (*url_get_file_handle)(URLContext *h);\n    int priv_data_size;\n    const AVClass *priv_data_class;\n    int flags;\n    int (*url_check)(URLContext *h， int mask);\n} URLProtocol;\n```\n\n注意到，`URLProtocol` 是一个链表结构，这是为了协议的统一管理，ffmpeg 项目中将所有的用到的协议都存放在一个全局变量 first_protocol 中，协议的注册是在 `av_register_all` 中完成的，新添加单个协议可以调用 `av_register_protocol2` 函数实现。而协议的注册就是将具体的协议对象添加至 `first_protocol` 链表的末尾。\n\n`URLProtocol` 在各个具体的文件协议中有一个具体的实例，如在 file 协议中定义为：\n\n```c\nURLProtocol ff_file_protocol = {\n    .name = \"file\"，\n\t.url_open = file_open，\n\t.url_read = file_read，\n    .url_write = file_write，\n    .url_seek = file_seek，\n    .url_close = file_close，\n    .url_get_file_handle = file_get_handle，\n    .url_check = file_check，\n};\n```\n\n**2、URLContext 结构**\n\nURLContext 提供了与当前打开的具体的文件协议（URL）相关数据的描述，在该结构中定义了指定当前 URL（即 filename 项）所要用到的具体的 URLProtocol，即：提供了一个在 URLprotocol 链表中找到具体项的依据，此外还有一些其它的标志性的信息，如 flags， is_streamed 等。它可以看成某一种协议的载体。其结构定义如下：\n\n```c\ntypedef struct URLContext\n{\n    const AVClass *av_class; ///< information for av_log(). Set by url_open().\n    struct URLProtocol *prot;\n    int flags;\n    int is_streamed; /**< true if streamed (no seek possible)， default = false */\n    int max_packet_size; void *priv_data;\n    char *filename; /**< specified URL */\n    int is_connected;\n} URLContext;\n```\n\n那么 ffmpeg 依据什么信息初始化 URLContext？然后又是如何初始化 URLContext的呢？\n\n在打开一个 URL 时，全局函数 ffurl_open 会根据 filename 的前缀信息来确定 URL所使用的具体协议，并为该协议分配好资源，再调用 ffurl_connect 函数打开具体协议，即调用协议的 url_open，调用关系如下：\n\n```c\nint av_open_input_file(AVFormatContext **ic_ptr， const char *filename，\n \t\t\t\t\t\tAVInputFormat *fmt，int buf_size，AVFormatParameters *ap)\nint avformat_open_input(AVFormatContext **ps ， const char *filename ，\n\t\t\t\t\t\tAVInputFormat *fmt， AVDictionary **options)\nstatic int init_input(AVFormatContext *s， const char *filename)\n\nint avio_open(AVIOContext **s， const char *filename， int flags)\nint ffurl_open(URLContext **puc， const char *filename， int flags)\nint ffurl_alloc(URLContext **puc， const char *filename， int flags)\nstatic int url_alloc_for_protocol(URLContext **puc， struct URLProtocol *up，\n\t\t\t\t\t\t\t\t\tconst char *filename， int flags)\n```\n\n\n浅蓝色部分的函数完成了 URLContext 函数的初始化，URLContext 使 ffmpeg 外所暴露的接口是统一的，而不是对于不同的协议用不同的函数，这也是面向对象思维的体现。在此结构中还有一个值得说的是 priv_data 项，这是结构的一个可扩展项，具体协议可以根据需要添加相应的结构，将指针保存在这就行。\n\n**3、AVIOContext 结构**\n\nAVIOContext（即：ByteIOContext）是由 URLProtocol 和 URLContext 结构扩展而来，也是 ffmpeg 提供给用户的接口，它将以上两种不带缓冲的读取文件抽象为带缓冲的读取和写入，为用户提供带缓冲的读取和写入操作。数据结构定义如下：\n\n```c\ntypedef struct\n{\n    unsigned char *buffer; /**< Start of the buffer. */\n    int buffer_size; /**< Maximum buffer size */\n    unsigned char *buf_ptr; /**< Current position in the buffer */\n    unsigned char *buf_end;\n    void *opaque; //关联 URLContext\n    int (*read_packet)(void *opaque， uint8_t *buf， int buf_size);\n    int (*write_packet)(void *opaque， uint8_t *buf， int buf_size);\n    int64_t (*seek)(void *opaque， int64_t offset， int whence);\n    int64_t pos;\n    int must_flush;\n    int eof_reached; /**< true if eof reached */\n    int write_flag; /**< true if open for writing */\n    int max_packet_size;\n    unsigned long checksum;\n    unsigned char *checksum_ptr;\n    unsigned long (*update_checksum)(unsigned long checksum， const uint8_t *buf，\n                                     unsigned int size);\n    int error;\n    int (*read_pause)(void *opaque， int pause);\n  \tint64_t (*read_seek)(void *opaque， int stream_index，int64_t timestamp，int flags);\n    int seekable;\n} AVIOContext;\n```\n\n结构简单的为用户提供读写容易实现的四个操作，read_packet write_packet read_pause read_seek，极大的方便了文件的读取，四个函数在加了缓冲机制后被中转到，URLContext 指向的实际的文件协议读写函数中。\n\n下面给出 0.8 版本中是如何将 AVIOContext 的读写操作中转到实际文件中的。\n\n在 avio_open（）函数中调用了 ffio_fdopen（）函数完成了对 AVIOContex 的初始化，其调用过程如下：\n\n```c\nint avio_open(AVIOContext **s， const char *filename， int flags)\nffio_fdopen(s， h); //h 是 URLContext 指针\nffio_init_context(*s， buffer， buffer_size，h->flags & AVIO_FLAG_WRITE， h，\n\t\t\t\t\t(void*)ffurl_read，(void*)ffurl_write，(void*)ffurl_seek)\n```\n\n函数调用完成了对 AVIOContext 的初始化，在初始化的过程中，将AVIOContext 的 read_packet 、 write_packet 、 seek 分别初始化为： ffurl_read ffurl_write ffurl_seek ， 而这三个函数又将具体的读写操作中转为：\n\n`h->prot->url_read、h->prot->url_write、h->prot->url_seek`，另外两个变量初始化时也被相应的中转，如下：\n\n```c\n(*s)->read_pause = (int (*)(void *， int))h->prot->url_read_pause;\n(*s)->read_seek = (int64_t (*)(void *， int， int64_t， int))h->prot->url_read_seek;\n```\n\n所以，可以简要的描述为：AVIOContext 的接口口是加了缓冲后的 URLProtocol 的函数接口。\n\n在 aviobuf.c 中定义了一系列关于 ByteIOContext 这个结构体的函数，如下\n\n**put_xxx 系列：**\n\n```c\nvoid put_byte(ByteIOContext *s， int b);\nvoid put_buffer(ByteIOContext *s， const unsigned char *buf， int size);\nvoid put_le64(ByteIOContext *s， uint64_t val);\nvoid put_be64(ByteIOContext *s， uint64_t val);\nvoid put_le32(ByteIOContext *s， unsigned int val);\nvoid put_be32(ByteIOContext *s， unsigned int val);\nvoid put_le24(ByteIOContext *s， unsigned int val);\nvoid put_be24(ByteIOContext *s， unsigned int val);\nvoid put_le16(ByteIOContext *s， unsigned int val);\nvoid put_be16(ByteIOContext *s， unsigned int val);\nvoid put_tag(ByteIOContext *s， const char *tag);\n```\n\n**get_xxx 系列：**\n\n```c\nint get_buffer(ByteIOContext *s， unsigned char *buf， int size);\nint get_partial_buffer(ByteIOContext *s， unsigned char *buf， int size);\nint get_byte(ByteIOContext *s);\nunsigned int get_le24(ByteIOContext *s);\nunsigned int get_le32(ByteIOContext *s);\nuint64_t get_le64(ByteIOContext *s);\nunsigned int get_le16(ByteIOContext *s);\nchar *get_strz(ByteIOContext *s， char *buf， int maxlen);\nunsigned int get_be16(ByteIOContext *s);\nunsigned int get_be24(ByteIOContext *s);\nunsigned int get_be32(ByteIOContext *s);\nuint64_t get_be64(ByteIOContext *s);\n```\n\n这些 put_xxx 及 get_xxx 函数是用于从缓冲区 buffer 中写入或者读取若干个字节，对于读写整型数据，分别实现了大端和小端字节序的版本。而缓冲区 buffer 中的数据又是从何而来呢，有一个 fill_buffer 的函数，在 fill_buffer 函数中调用了ByteIOContext 结构的 read_packet 接口。在调用 put_xxx 函数时，并没有直接进行真\n正写入操作，而是先缓存起来，直到缓存达到最大限制或调用 flush_buffer 函数对缓冲区进行刷新，才使用 write_packet 函数进行写入操作。\n\n## Demuxer 和 muxer 模块分析\n\n### 概述\n\nffmpeg 的 demuxer 和 muxer 接口分别在 AVInputFormat 和 AVOutputFormat 两个结构体中实现，在 av_register_all()函数中将两个结构分别静态初始化为两个链表，保存在全局变量：first_iformat 和 first_oformat 两个变量中。在 FFmpeg 的文件转换或者打开过程中，首先要做的就是根据传入文件和传出文件的后缀名匹配合适的 demuxer和 muxer，得到合适的信息后保存在 AVFormatContext 中。\n\n### 相关数据结构介绍\n\n**1、AVInputFormat**\n\n该结构被称为 demuxer，是音视频文件的一个解封装器，它的定义如下：\n\n```c\ntypedef struct AVInputFormat\n{\n    const char *name;\n    const char *long_name;\n    int priv_data_size; //具体文件容器格式对应的 Context 的大小，如：avicontext\n    int (*read_probe)(AVProbeData *);\n    int (*read_header)(struct AVFormatContext *，AVFormatParameters *ap);\n    int (*read_packet)(struct AVFormatContext *， AVPacket *pkt);\n    int (*read_close)(struct AVFormatContext *);\n#if FF_API_READ_SEEK\n    attribute_deprecated int (*read_seek)(struct AVFormatContext *，\n                               int stream_index，int64_t timestamp，int flags);\n#endif\n    int64_t (*read_timestamp)(struct AVFormatContext *s， int stream_index，\n                              int64_t *pos， int64_t pos_limit);\n    int flags;\n    const char *extensions;\n    int value;\n    int (*read_play)(struct AVFormatContext *);\n    int (*read_pause)(struct AVFormatContext *);\n    const struct AVCodecTag * const *codec_tag;\n    int (*read_seek2)(struct AVFormatContext *s， int stream_index， int64_t min_ts，\n                      int64_t ts， int64_t max_ts， int flags);\n#if FF_API_OLD_METADATA2\n    const AVMetadataConv *metadata_conv;\n#endif\n    const AVClass *priv_class; ///< AVClass for the private context\n    struct AVInputFormat *next;\n} AVInputFormat;\n```\n\n对于不同的文件格式要实现相应的函数接口，这样每一种格式都有一个对应的demuxer，所有的 demuxer 都保存在全局变量 first_iformat 中。红色表示提供的接口。\n\n**2、AVOutputFormat**\n\n该结构与 AVInputFormat 类似也是在编译时静态初始化，组织为一个链表结构，提供了多个 muxer 的函数接口。\n\n```c\nint (*write_header)(struct AVFormatContext *);\nint (*write_packet)(struct AVFormatContext *， AVPacket *pkt);\nint (*write_trailer)(struct AVFormatContext *);\n```\n\n对于不同的文件格式要实现相应的函数接口，这样每一种格式都有一个对应的 muxer，所有的 muxer 都保存在全局变量 first_oformat 中。\n\n**3、AVFormatContext**\n\n该结构表示与程序当前运行的文件容器格式使用的上下文，着重于所有文件容器共有的属性，在运行时动态的确定其值，是 AVInputFormat 和 AVOutputFormat 的载体，但同一个结构对象只能使 AVInputFormat 和 AVOutputFormat 中的某一个有效。每一个输入和输出文件，都在\n\n```c\nstatic AVFormatContext *output_files[MAX_FILES] 和\nstatic AVFormatContext *input_files[MAX_FILES];\n```\n\n定义的指针数组全局变量中有对应的实体。对于输入和输出，因为共用的是同一个结构体，所以需要分别对该结构中如下定义的 iformat 或 oformat 成员赋值。在转码时读写数据是通过 AVFormatContext 结构进行的。定义如下：\n\n```c\ntypedef struct AVFormatContext\n{\n    const AVClass *av_class;\n    struct AVInputFormat *iformat; //指向具体的 demuxer\n    struct AVOutputFormat *oformat; //指向具体的 muxer\n    void *priv_data; //具体文件容器格式的 Context 如：avicontext\n    AVIOContext *pb; //广义的输入输出；\n    unsigned int nb_streams; //本次打开的文件容器中流的数量\n    AVStream **streams; //每个流的相关描述\n    char filename[1024]; // input or output filename */\n    int64_t timestamp;\n    int ctx_flags;\n    struct AVPacketList *packet_buffer;\n    ......\n    enum CodecID video_codec_id;\n    enum CodecID audio_codec_id;\n    enum CodecID subtitle_codec_id;\n    unsigned int max_index_size;\n    unsigned int max_picture_buffer;\n    ......\n    struct AVPacketList *raw_packet_buffer;\n    struct AVPacketList *raw_packet_buffer_end;\n    struct AVPacketList *packet_buffer_end;\n    ......\n} AVFormatContext;\n```\n\n注释部分的成员是 AVFormatContext 中最为重要的成员变量，这些变量的初始化是ffmpeg 能正常工作的必要条件，那么，AVFormatContext 是如何被初始化的呢？文件的格式是如何被探测到的呢？\n\n首先我们来探讨：\n\n```c\nstruct AVInputFormat *iformat; //指向具体的 demuxer\nstruct AVOutputFormat *oformat; //指向具体的 muxer\nvoid *priv_data; //具体文件容器格式的 Context 如：avicontext\n```\n\n\n三个成员的初始化。\n\n在 avformat_open_input() 函数中调用了 init_input() 函数，然后用调用了av_probe_input_format（）函数实现了对 AVFormatContext 的初始化。其调用关系如下：\n\n```c\nint av_open_input_file(AVFormatContext **ic_ptr， const char *filename，\n\t\t\t\t\t\tAVInputFormat *fmt，int buf_size，AVFormatParameters *ap)；\nint avformat_open_input(ic_ptr， filename， fmt， &opts)；\nstatic int init_input(s， filename)；\nav_probe_input_format(&pd， 0)；\nav_probe_input_format (AVProbeData *pd， int is_opened， int *score_max)\n```\n\n函数用途是根据传入的 probe data 数据，依次调用每个 demuxer 的 read_probe 接口，来进行该 demuxer 是否和传入的文件内容匹配的判断。与 demuxer 的匹配不同，muxer的匹配是调用 guess_format 函数，根据 main( ) 函数的 argv 里的输出文件后缀名来进行的。至此完成了前三个重要成员的初始化，具体的做法就不在深入分析。\n\n下面分别给出 av_read_frame 函数以及 av_write_frame 函数的基本流程。\n\n```c\nint av_read_frame(AVFormatContext *s， AVPacket *pkt);\n\t-> av_read_frame_internel\n\t\t-> av_read_packet\n\t\t\t-> iformat->read_packet（在实现中会丢弃多余信息）\n\t\t\t\t-> av_get_packet\n\t\t\t\t\t-> get_xxx\n\t\t\t\t\t\nint av_write_frame(AVFormatContext *s， AVPacket *pkt);\n\t-> oformat->write_packet\n\t\t-> put_xxx\n```\n\n由上可见，对 AVFormatContext 的读写操作最终是通过 ByteIOContext 来实现的，这样，AVFormatContext 与 URLContext 就由 ByteIOContext 结构联系到一起了。在AVFormat 结构体中有一个 packet 的缓冲区 raw_packet_buffer，是 AVPackList 的指针类型，av_read_packet 函数将读到的包添加至 raw_packet_buffer 链表末尾。\n\n## Decoder/Encoder 模块\n\n### 概述\n\n编解码模块主要包含的数据结构为：AVCodec、AVCodecContext 每一个解码类型都会有自己的 Codec 静态对像，Codec 的 int priv_data_size 记录该解码器上下文的结构大小，如 MsrleContext 。这些都是编译时确定的，程序运行时通过avcodec_register_all()将所有的解码器注册成一个链表。在 av_open_input_stream()函数中调用 AVInputFormat 的 read_header()中读文件头信息时，会读出数据流的CodecID，即确定了他的解码器 Codec。\n\n在 main()函数中除了解析传入参数并初始化 demuxer 与 muxer 的 parse_options( )函数以外，其他的功能都是在 av_encode( )函数里完成的。在 libavcodec\\utils.c 中有如下二个函数 : `AVCodec *avcodec_find_encoder(enum CodecID id)` 和 `AVCodec *avcodec_find_decoder(enum CodecID id)` 他们的功能就是根据传入的 CodecID，找到匹配的 encoder 和 decoder。在 av_encode( )函数的开头，首先初始化各个 AVInputStream和 AVOutputStream，然后分别调用上述二个函数，并将匹配上的 encoder 与 decoder 分\n别保存在:\n\n```c\nAVInputStream->AVStream *st->AVCodecContext *codec->struct AVCodec *codec\n与 AVOutputStream->AVStream *st->AVCodecContext *codec->struct AVCodec *codec 变量。\n```\n\n### 相关数据结构的初始化\n\nAVCodecContext 结构\n\nAVCodecContext 保存 AVCodec 指针和与 codec 相关数据，如 video 的 width、height，audio 的 sample rate 等。\n\nAVCodecContext 中的 codec_type，codec_id 二个变量对于 encoder/decoder 的匹配来说，最为重要。\n\n```c\nenum CodecType codec_type; /* see CODEC_TYPE_xxx */\nenum CodecID codec_id; /* see CODEC_ID_xxx */\n```\n\n如上所示，codec_type 保存的是 CODEC_TYPE_VIDEO，CODEC_TYPE_AUDIO 等媒体类型，codec_id 保存的是 CODEC_ID_FLV1，CODEC_ID_VP6F 等编码方式。\n\n以支持 flv 格式为例，在前述的 av_open_input_file(…… ) 函数中，匹配到正确的 AVInputFormat demuxer 后，通过 av_open_input_stream( )函数中调用 AVInputFormat的 read_header 接口来执行 flvdec.c 中的 flv_read_header( )函数。flv_read_header( )函数内，根据文件头中的数据，创建相应的视频或音频 AVStream，并设置 AVStream 中AVCodecContext 的正确的 codec_type 值。codec_id 值是在解码过程。flv_read_packet( )\n函数执行时根据每一个 packet 头中的数据来设置的。\n\n以 avidec 为例 有如下初始化，我们主要知道的就是 code_id 和 code_type 该字段关联具体的解码器，和解码类型（音视频或 subtitle）\n\n```c\nif (st->codec->stream_codec_tag == AV_RL32(\"Axan\"))\n{\n    st->codec->codec_id = CODEC_ID_XAN_DPCM;\n    st->codec->codec_tag = 0;\n}\nif (amv_file_format)\n{\n    st->codec->codec_id = CODEC_ID_ADPCM_IMA_AMV;\n    ast->dshow_block_align = 0;\n}\n    \tbreak;\n    case AVMEDIA_TYPE_SUBTITLE:\n        st->codec->codec_type = AVMEDIA_TYPE_SUBTITLE;\n        st->request_probe= 1;\n    \tbreak;\n    default:\n        st->codec->codec_type = AVMEDIA_TYPE_DATA;\n        st->codec->codec_id= CODEC_ID_NONE;\n        st->codec->codec_tag= 0;\n        avio_skip(pb， size);\n```\n\n\n## 其他重要数据结构的初始化\n\n### AVStream\n\nAVStream 结构保存与数据流相关的编解码器，数据段等信息。比较重要的有如下二个成员：\n\n```c\nAVCodecContext *codec; /**< codec context */\nvoid *priv_data;\n```\n\n其中 codec 指针保存的就是上节所述的 encoder 或 decoder 结构。priv_data 指针保存的是和具体编解码流相关的数据，如下代码所示，在 ASF 的解码过程中，priv_data保存的就是 ASFStream 结构的数据。\n\n```c\nAVStream *st;\nASFStream *asf_st;\n......\nst->priv_data = asf_st;\n```\n\n\n### AVInputStream/ AVOutputStream\n\n根据输入和输出流的不同，前述的 AVStream 结构都是封装在 AVInputStream 和AVOutputStream 结构中，在 av_encode( )函数中使用。AVInputStream 中还保存的有与时间有关的信息。AVOutputStream 中还保存有与音视频同步等相关的信息。\n\n### AVPacket\n\nAVPacket 结构定义如下，其是用于保存读取的 packet 数据。\n\n```c\ntypedef struct AVPacket\n{\n    int64_t pts; ///< presentation time stamp in time_base units\n    int64_t dts; ///< decompression time stamp in time_base units\n    uint8_t *data;\n    int size;\n    int stream_index;\n    int flags;\n    int duration; ///< presentation duration in time_base units\n    void (*destruct)(struct AVPacket *);\n    void *priv;\n    int64_t pos; ///< byte position in stream， -1 if unknown\n} AVPacket;\n```\n\n在 av_encode() 函数中，调用 AVInputFormat 的 `(*read_packet)(struct AVFormatContext *， AVPacket *pkt)` 接口，读取输入文件的一帧数据保存在当前输入 AVFormatContext 的 AVPacket 成员中。\n\n# FFmpeg 裁剪说明\n\n本文对 ffmpeg 进行裁剪采用的是配置所需的接口，不需要的不配置，而不是采用修改源代码的方式。\n\n## configure 参数\n\n###  通用选项\n\n在 linux 下进入终端，找到 ffmpeg 解压位置，输入如下命令：\n\n```shell\n$ ./configure –help\n```\n\n得到 configure 的基本选项参数，其并没有中文解释。\n\n```shell\n--help 显示此帮助信息|print this message\n--log[=FILE|yes|no] 记录测试并输出到 config.err 文件|log tests and output to FILE [config.err]\n--prefix=PREFIX 安装程序到指定目录（默认/usr/local）|install in PREFIX [/usr/local]\n--libdir=DIR 安装库到指定目录（默认 prefix/lib）|install libs in DIR [PREFIX/lib]\n--shlibdir=DIR 指定共享库路径（默认 prefix/lib）|install shared libs in DIR [PREFIX/lib]\n--incdir=DIR 指定 includes 路径（默认 prefix/include/ffmpeg）|install includes in DIR[PREFIX/include/ffmpeg]\n--mandir=DIR 指定 man page 路径（默认 prefix/man）install man page in DIR [PREFIX/man]\n--enable-mp3lame 启用 mp3 编码 libmp3lame（默认关闭）enable MP3 encoding via libmp3lame[default=no]\n--enable-libogg 启用 ogg 支持 libogg（默认关闭）enable Ogg support via libogg [default=no]\n--enable-vorbis 启用 Vorbis 支持 libvorbis（默认关闭）enable Vorbis support via libvorbis [default=no]\n--enable-faad 启用 faad 支持 libfaad（默认关闭）enable FAAD support via libfaad [default=no]\n--enable-faadbin 启用 faad 运行时链接支持（默认关闭）build FAAD support with runtime linking[default=no]\n--enable-faac 启用 faac 支持 libfaac（默认关闭）enable FAAC support via libfaac [default=no]\n--enable-libgsm 启用 GSM 支持 libgsm（默认关闭）enable GSM support via libgsm [default=no]\n--enable-xvid 启用 xvid 支持 xvidcore（默认关闭）enable XviD support via xvidcore [default=no]\n--enable-x264 启 用 H.264 编码（默认关闭） enable H.264 encoding via x264 [default=no]\n--enable-mingw32 启用 MinGW 本地/交叉 win 环境编译|enable MinGW native/cross Windows compile\n--enable-mingwce 启用 MinGW 本地/交叉 winCE 环境编译 enable MinGW native/cross WinCE compile\n--enable-a52 启用 A52 支持（默认关闭）enable GPLed A52 support [default=no]\n--enable-a52bin 启用运行时打开 liba52.so.0（默认关闭）open liba52.so.0 at runtime [default=no]\n--enable-dts 启用 DTS 支持（默认关闭）enable GPLed DTS support [default=no]\n--enable-pp 启用后加工支持（默认关闭）enable GPLed postprocessing support [default=no]\n--enable-static 构建静态库（默认启用）build static libraries [default=yes]\n--disable-static 禁止构建静态库（默认关闭）do not build static libraries [default=no]\n--enable-shared 构建共享库（默认关闭）build shared libraries [default=no]\n--disable-shared 禁止构建共享库（默认启用）do not build shared libraries [default=yes]\n--enable-amr_nb 启用 amr_nb float 音频编解码器|enable amr_nb float audio codec\n--enable-amr_nb-fixed 启用 fixed amr_nb codec | use fixed point for amr-nb codec\n--enable-amr_wb 启用 amr_wb float 音频编解码器|enable amr_wb float audio codec\n--enable-amr_if2 启用 amr_wb IF2 音频编解码器|enable amr_wb IF2 audio codec\n--enable-sunmlib 启用 Sun medialib（默认关闭） | use Sun medialib [default=no]\n--enable-pthreads 启用 pthreads（多线程）（默认关闭）use pthreads [default=no]\n--enable-dc1394 启用 libdc1394、libraw1394 抓取 IIDC-1394（默认关闭）enable IIDC-1394 grabbing using libdc1394 and libraw1394 [default=no]\n--enable-swscaler 启用计数器支持？（默认关闭）software scaler support [default=no]\n--enable-avisynth 允许读取 AVISynth 脚本本件（默认关闭）allow reading AVISynth script files [default=no]\n--enable-gpl 允许使用 GPL（默认关闭）allow use of GPL code， the resulting libav* and ffmpeg will be under GPL [default=no] Advanced options (experts only): 高级选项参数（供专业人员使用）\n--source-path=PATH 源码的路径（当前为/root/flv/ffmpeg）| path to source code [/root/flv/ffmpeg]\n--cross-prefix=PREFIX 为编译工具指定路径 | use PREFIX for compilation tools []\n--cross-compile 假定使用了交叉编译 | assume a cross-compiler is used\n--cc=CC 指定使用何种 C 编译器（默认 gcc）use C compiler CC [gcc]\n--make=MAKE 使用特定的 make | use specified make [make]\n--extra-cflags=ECFLAGS 添加 ECFLAGS 到 CFLAGS | add ECFLAGS to CFLAGS []\n--extra-ldflags=ELDFLAGS 添加 ELDFLAGS 到 LDFLAGS（默认-Wl，--as-needed）| add ELDFLAGS to LDFLAGS [ -Wl，--as-needed]\n--extra-libs=ELIBS 添加 ELIBS | add ELIBS []\n--build-suffix=SUFFIX 为专用程序添加后缀 | suffix for application specific build []\n--arch=ARCH 选择机器架构（默认 x86）select architecture [x86]\n--cpu=CPU 选用最低的 cpu（影响指令的选择，可以在老 CPU 上出错）\n\t| selects the minimum cpu required (affects instruction selection， may crash on older CPUs)\n--powerpc-perf-enable 启用 PPC 上面的性能报告（需要启用 PMC）enable performance report on PPC (requires enabling PMC)\n--disable-mmx 禁用 MMX | disable MMX usage\n--disable-armv5te 禁用 armv5te | disable armv5te usage\n--disable-iwmmxt 禁用 iwmmxt | disable iwmmxt usage\n--disable-altivec 禁用 AltiVec | disable AltiVec usage\n--disable-audio-oss 禁用 OSS 音频支持（默认启用）disable OSS audio support [default=no]\n--disable-audio-beos 禁用 BeOS 音频支持（默认启用）disable BeOS audio support [default=no]\n--disable-v4l 禁用 video4linux 提取（默认启用）disable video4linux grabbing [default=no]\n--disable-v4l2 禁用 video4linux2 提取（默认启用）disable video4linux2 grabbing [default=no]\n--disable-bktr 禁用 bktr 视频提取（默认启用）disable bktr video grabbing [default=no]\n--disable-dv1394 禁用 DV1394 提取（默认启用）disable DV1394 grabbing [default=no]\n--disable-network 禁用网络支持（默认支持）disable network support [default=no]\n--disable-ipv6 禁用 ipv6 支持（默认支持）disable ipv6 support [default=no]\n--disable-zlib 禁用 zlib（默认支持）disable zlib [default=no]\n--disable-simple_idct 禁用 simple IDCT 例程（默认启用）disable simple IDCT routines [default=no]\n--disable-vhook 禁用 video hooking 支持 | disable video hooking support\n--enable-gprof enable profiling with gprof [no]\n--disable-debug 禁用调试符号 | disable debugging symbols\n--disable-opts 禁用编译器最优化 | disable compiler optimizations\n--disable-mpegaudio-hp 启用更快的解码 MPEG 音频（但精确度较低）（默认禁用）faster (but less accurate) MPEG audio decoding [default=no]\n--disable-protocols 禁用 I/O 协议支持（默认启用）disable I/O protocols support [default=no]\n--disable-ffserver 禁用生成 ffserver | disable ffserver build\n--disable-ffplay 禁用生成 ffplay | disable ffplay build\n--enable-small 启用优化文件尺寸大小（牺牲速度）optimize for size instead of speed\n--enable-memalign-hack 启用模拟内存排列，由内存调试器干涉？ | emulate memalign，interferes with memory debuggers\n--disable-strip 禁用剥离可执行程序和共享库 | disable stripping of executables and shared libraries\n--disable-encoder=NAME 禁用 XX 编码器 | disables encoder NAME\n--enable-encoder=NAME 启用 XX 编码器 | enables encoder NAME\n--disable-decoder=NAME 禁用 XX 解码器 | disables decoder NAME\n--enable-decoder=NAME 启用 XX 解码器 | enables decoder NAME\n--disable-encoders 禁用所有编码器 | disables all encoders\n--disable-decoders 禁用所有解码器 | disables all decoders\n--disable-muxer=NAME 禁用 XX 混音器 | disables muxer NAME\n--enable-muxer=NAME 启用 XX 混音器 | enables muxer NAME\n--disable-muxers 禁用所有混音器 | disables all muxers\n--disable-demuxer=NAME 禁用 XX 解轨器 | disables demuxer NAME\n--enable-demuxer=NAME 启用 XX 解轨器 | enables demuxer NAME\n--disable-demuxers 禁用所有解轨器 | disables all demuxers\n--enable-parser=NAME 启用 XX 剖析器 | enables parser NAME\n--disable-parser=NAME 禁用 XX 剖析器 | disables parser NAME\n--disable-parsers 禁用所有剖析器 | disables all parsers\n```\n\n### 基本选项介绍\n\n以下为配置 ffmpeg 的基本选项，其含义如下：\n\n```shell\n--cache-file=FILE\n```\n\nconfigure 会在你的系统上测试存在的特性(或者 bug!)。为了加速随后进行的配置，测试的结果会存储在一个 cache file 里。当 configure 到每个子树里都有 configure 脚本的复杂的源码树时，一个很好的 cache file 的存在会有很大帮助。\n\n```shell\n--help\n```\n\n输出帮助信息。即使是有经验的用户也偶尔需要使用使用 `--help` 选项，因为一个复杂的项目会包含附加的选项。例如，GCC 包里的 configure 脚本就包含了允许你控制是否生成和在 GCC 中使用 GNU 汇编器的选项。\n\n```shell\n--no-create\n```\n\nconfigure 中的一个主要函数会制作输出文件。此选项阻止 configure 生成这个文件。你可以认为这是一种演习(dry run)，尽管缓存(cache)仍然被改写了。\n\n```shell\n--quiet\n--silent\n```\n\n当 configure 进行他的测试时，会输出简要的信息来告诉用户正在作什么。这样做是因为 configure 可能会比较慢，没有这种输出的话用户将会被扔在一旁疑惑正在发生什么。使用这两个选项中的任何一个都会把你扔到一旁。(译注：这两句话比较有意思，原文是这样的：If there was no such output， the user would be left wondering what is happening. By using this option， you too can be left wondering!)\n\n```shell\n--version\n```\n\n打印用来产生 'configure' 脚本的 Autoconf 的版本号。\n\n```shell\n--prefix=PEWFIX\n```\n\n`--prefix` 是最常用的选项。制作出的 Makefile 会查看随此选项传递的参数，当一个包在安装时可以彻底的重新安置他的结构独立部分。举一个例子，当安装一个包，例如说Emacs，下面的命令将会使 Emacs Lisp file 被安装到\"/opt/gnu/share\"：\n\n```shell\n$ ./configure --prefix=/opt/gnu\n```\n\n````shell\n--exec-prefix=EPREFIX\n````\n\n与 `--prefix` 选项类似，但是他是用来设置结构倚赖的文件的安装位置。编译好的 emacs 二进制文件就是这样一个问件。如果没有设置这个选项的话，默认使用的选项值将被设为和 `--prefix` 选项值一样。\n\n```shell\n--bindir=DIR\n```\n\n指定二进制文件的安装位置。这里的二进制文件定义为可以被用户直接执行的程序。\n\n```shell\n--sbindir=DIR\n```\n\n指定超级二进制文件的安装位置。这是一些通常只能由超级用户执行的程序。\n\n```shell\n--libexecdir=DIR\n```\n\n指定可执行支持文件的安装位置。与二进制文件相反，这些文件从来不直接由用户执行，但是可以被上面提到的二进制文件所执行。\n\n```shell\n--datadir=DIR\n```\n\n指定通用数据文件的安装位置。\n\n```shell\n--sysconfdir=DIR\n```\n\n指定在单个机器上使用的只读数据的安装位置。\n\n```shell\n--sharedstatedir=DIR\n```\n\n指定可以在多个机器上共享的可写数据的安装位置。\n\n```shell\n--localstatedir=DIR\n```\n\n指定只能单机使用的可写数据的安装位置。\n\n```shell\n--libdir=DIR\n```\n\n指定库文件的安装位置。\n\n```shell\n--includedir=DIR\n```\n\n指定 C 头文件的安装位置。其他语言如 C++的头文件也可以使用此选项。\n\n```shell\n--oldincludedir=DIR\n```\n\n指定为除 GCC 外编译器安装的 C 头文件的安装位置。\n\n```shell\n--infodir=DIR\n```\n\n指定 Info 格式文档的安装位置。Info 是被 GNU 工程所使用的文档格式。\n\n```shell\n--mandir=DIR\n```\n\n指定手册页的安装位置。\n\n```shell\n--srcdir=DIR\n```\n\n这个选项对安装没有作用。他会告诉 configure 源码的位置。一般来说不用指定此选项，因为 configure 脚本一般和源码文件在同一个目录下。\n\n```shell\n--program-prefix=PREFIX\n```\n\n指定将被加到所安装程序的名字上的前缀。例如，使用 `--program-prefix=g` 来 configure一个名为 tar 的程序将会使安装的程序被命名为 gtar。当和其他的安装选项一起使用时，这个选项只有当他被 Makefile.in 文件使用时才会工作。\n\n```shell\n--program-suffix=SUFFIX\n```\n\n指定将被加到所安装程序的名字上的后缀。\n\n```shell\n--program-transform-name=PROGRAM\n```\n\n这里的 PROGRAM 是一个 sed 脚本。当一个程序被安装时，他的名字将经过 `sed -e PROGRAM` 来产生安装的名字。\n\n```shell\n--build=BUILD\n```\n\n指定软件包安装的系统平台。如果没有指定，默认值将是 `--host` 选项的值。\n\n```shell\n--host=HOST\n```\n\n指定软件运行的系统平台。如果没有指定，将会运行 config.guess 来检测。\n\n```shell\n--target=GARGET\n```\n\n指定软件面向(target to)的系统平台。这主要在程序语言工具如编译器和汇编器上下文中起作用。如果没有指定，默认将使用 `--host` 选项的值。\n\n```shell\n--disable-FEATURE\n```\n\n一些软件包可以选择这个选项来提供为大型选项的编译时配置，例如使用 Kerberos认证系统或者一个实验性的编译器最优配置。如果默认是提供这些特性，可以使用 `--disable-FEATURE` 来禁用它，这里 FEATURE 是特性的名字。例如：\n\n```shell\n$ ./configure --disable-gui\n```\n\n```shell\n--enable-FEATURE[=ARG]\n```\n\n相反的，一些软件包可能提供了一些默认被禁止的特性，可以使用 `--enable-FEATURE` 来起用它。这里 FEATURE 是特性的名字。一个特性可能会接受一个可选的参数。例如：\n\n```shell\n$ ./configure --enable-buffers=128\n```\n\n`--enable-FEATURE=no` 与上面提到的 `--disable-FEATURE` 是同义的。\n\n```shell\n--with-PACKAGE[=ARG]\n```\n\n在自由软件社区里，有使用已有软件包和库的优秀传统。当用 configure 来配置一个源码树时，可以提供其他已经安装的软件包的信息。例如，倚赖于 Tcl 和 Tk 的 BLT 器件工具包。要配置 BLT，可能需要给 configure 提供一些关于我们把 Tcl 和 Tk 装的何处的信息：\n\n```shell\n$ ./configure --with-tcl=/usr/local --with-tk=/usr/local\n```\n\n`--with-PACKAGE=no` 与下面将提到的 `--without-PACKAGE` 是同义的。\n\n```shell\n--without-PACKAGE\n```\n\n有时候你可能不想让你的软件包与系统已有的软件包交互。例如，你可能不想让你的新编译器使用 GNU ld。通过使用这个选项可以做到这一点：\n\n```shell\n$ ./configure --without-gnu-ld\n```\n\n```shell\n--x-includes=DIR\n```\n\n这个选项是 `--with-PACKAGE` 选项的一个特例。在 Autoconf 最初被开发出来时，流行使用 configure 来作为 Imake 的一个变通方法来制作运行于 X 的软件。--x-includes 选项提供了向 configure 脚本指明包含 X11 头文件的目录的方法。\n\n```shell\n--x-libraries=DIR\n```\n\n类似的，`--x-libraries` 选项提供了向 configure 脚本指明包含 X11 库的目录的方法。\n\n## FFmpeg 裁剪优化实例\n\n对 ffmpeg 的裁剪优化主要是对 ffplay 的裁剪优化，我们制定的需求是能播放测试文件（视频为 mpeg4 编码、音频为 mp2 编码，且为 AVI 复用），根据需求，找到相应的选项，或禁用或启用，最后的命令如下：\n\n```shell\n$ ./configure --disable-yasm --disable-parsers --disable-decoders\n--disable-encoders --enable-decoder=mpeg4 --disable-muxers\n--disable-demuxers --enable-demuxer=avi --enable-decoder=mp2\n--disable-protocols --enable-protocol=file --disable-filters --disable-bsfs\n```\n\n其中针对需求，\n\n- `--disable-parsers` 为禁用所有解析器，\n\n- `--disable-decoders` 为禁用所有解码器，\n\n- `--disable-encoders` 为禁用所有编码器，\n\n- `--enable-decoder=mpeg4` 为启用 mpeg4 的编码器 ， \n\n- `--disable-muxers` 为禁用所有复用， \n\n- `--disable-demuxers` 为禁用所有解复用，\n\n- `--enable-demuxer=avi` 为启用 AVI 复用，\n\n- `--enable-decoder=mp2` 为启用 mp2 编码，\n\n- `--disable-protocols` 为禁用所有协议， \n\n- `--enable-protocol=file` 为启用文件协议，\n\n- `--disable-filters` 为禁用所有过滤器，\n\n- `--disable-bsfs` 为禁用所有码流过滤器。\n\n通过以上配置之后，编译，安装，就生成了我们要求的 ffplay，其大小为 1.8M（1864012 字节）。此次是在 linux 环境下进行的，在以后的配置中，如果需要其他的什么编码器或什么的，按照选项要求进行配置即可。\n\n## 裁剪优化前后文件比较\n\n前面已经提到本次裁剪优化的内容。经过裁剪优化之后，对其文件夹进行比较，主要有 3 个地方不同，分别是 config.fate、config.h 和 config.mak。在 config.fate 中，其记录的是配置命令，由于前后两次配置命令不同，故相应内容也不同。在config.h 中，其主要是根据配置命令来改变相应预定义的值，达到裁剪优化之效果。在 config.mak 中，改变的也是配置命令中需要改变的选项。\n\n# FFmpeg SDK \n\nFFMpeg 中比较重要的函数以及数据结构如下：\n\n**1、数据结构：**\n\n**(1) AVFormatContext**\n\n**(2) AVOutputFormat**\n\n**(3) AVInputFormat**\n\n**(4) AVCodecContext**\n\n**(5) AVCodec**\n\n**(6) AVFrame**\n\n**(7) AVPacket**\n\n**(8) AVPicture**\n\n(9) AVStream\n\n**2、初始化函数：**\n\n(1) av_register_all()\n\n(2) avcodec_open()\n\n(3) avcodec_close()\n\n(4) av_open_input_file()\n\n(5) av_find_input_format()\n\n(6) av_find_stream_info()\n\n(7) av_close_input_file()\n\n**3、音视频编解码函数：**\n\n(1) avcodec_find_decoder()\n\n(2) avcodec_alloc_frame()\n\n(3) avpicture_get_size()\n\n(4) avpicture_fill()\n\n(5) img_convert()\n\n(6) avcodec_alloc_context()\n\n(7) avcodec_decode_video()\n\n(8) av_free_packet()\n\n(9) av_free()\n\n**4、文件操作：**\n\n(1) avnew_steam()\n\n(2) av_read_frame()\n\n(3) av_write_frame()\n\n(4) dump_format()\n\n**5、其他函数：**\n\n(1) avpicture_deinterlace()\n\n(2) ImgReSampleContext() \n\n# FFmpeg 编译\n\n```shell\n$ git clone http://source.ffmpeg.org/git/ffmpeg.git ffmpeg\n$ cd ffmpeg\n$ ./configure --prefix=./install --enable-gpl --enable-nonfree \\\n--enable-libass --enable-libfdk-aac --enable-libfreetype \\\n--enable-libmp3lame --enable-libopus --enable-libtheora \\\n--enable-libvorbis --enable-libvpx --enable-libx264 --enable-libxvid \\\n--enable-shared --enable-static\n$ make && sudo make install\n```\n\n注意：在执行各自的 configure 创建编译配置文件时，最好都强制带上 --enable-static 和 --enable-shared 参数以确保生成静态库和动态库。另外因为是在 Mac OS X 环境下编译，因此在各自编译完后，都要执行 sudo make install，安装到默认的 /usr/local 目录下相应位置（Mac OS X 下不推荐 /usr），因此不要在 configure 时指定 --prefix，就用默认的 /usr/local 目录前缀即可。完成编译安装后，FFmpeg 的头文件将会复制到 /usr/local/include 下面相应位置，静态库及动态库会被复制到 /usr/local/lib 目录下，FFmpeg 的可执行程序（ffmpeg、ffprobe、ffserver）会被复制到 /usr/local/bin 目录下，这样 FFmpeg 的开发环境就构建好了。\n\n\n\n\n\n","tags":["FFmpeg"],"categories":["FFmpeg"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/2.generative_chatbot/4.transformer/","content":"\n# Transformer详解\n\n\n本翻译内容可以查看[BERT大火却不懂Transformer?读这一篇就够了](https://www.jiqizhixin.com/articles/2019-01-09-18)\n\n![](./img/transformer.png)\n\n\n\n\n```python\n\n```\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/2.generative_chatbot/2.seq2seq_details/","content":"\n# 图解seq2seq\n\n\n### 1.seq2seq（序列到序列模型）简介\n对于很多自然语言处理任务，比如**聊天机器人，机器翻译，自动文摘，智能问答**等，传统的解决方案都是**检索式(从候选集中选出答案)**，这对素材的完善程度要求很高，随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，它是目前自然语言处理技术中非常重要和流行的一个模型，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务的先河，并且被证实在各主流语言之间的相互翻译以及语音助手中人机短问快答的应用中有着非常好的表现，我们在这个notebook中主要给大家以动图的方式展示一下seq2seq模型的一些细节。\n\n参考资料:[图解seq2seq](https://zhuanlan.zhihu.com/p/40920384)\n\n### seq2seq模型\n\n\nseq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。\n\n![](./img/pic1-edit.jpg)\n\n输入： $x = (x_1,...,x_{T_x})$\n\n输出： $y = (y_1,...,y_{T_y})$\n\n(1) $h_t = RNN_{enc}(x_t, h_{t-1})$ , Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。\n\n(2) $s_t = RNN_{dec}(\\hat{y_{t-1}},s_{t-1})$ ， Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。\n\n(3) $c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$ , context vector是一个对于encoder输出的hidden states的一个加权平均。\n\n(4) $\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$ , 每一个encoder的hidden states对应的权重。\n\n(5) $e_{ij} = score(s_i, h_j)$ , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)\n\n(6) $\\hat{s_t} = tanh(W_c[c_t;s_t])$, 将context vector 和 decoder的hidden states 串起来。\n\n(7) $p(y_t|y_{<t},x) = softmax(W_s\\hat{s_t})$ ，计算最后的输出概率。\n\n![](./img/pic2-edit.jpg)\n\n(1) $h_t = RNN_{enc}(x_t, h_{t-1})$ , Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。\n\n![](./img/pic3-edit.jpg)\n\n(2) $s_t = RNN_{dec}(\\hat{y_{t-1}},s_{t-1})$ ， Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。\n\n![](./img/pic4-edit.jpg)\n\n(3) $c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$ , context vector是一个对于encoder输出的hidden states的一个加权平均。\n\n(4) $\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$ , 每一个encoder的hidden states对应的权重。\n\n(5) $e_{ij} = score(s_i, h_j)$ , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)\n\n![](./img/pic5-edit.jpg)\n\n下一个时间点\n\n![](./img/pic6-edit.jpg)\n\n(6) $\\hat{s_t} = tanh(W_c[c_t;s_t])$, 将context vector 和 decoder的hidden states 串起来。\n\n(7) $p(y_t|y_{<t},x) = softmax(W_s\\hat{s_t})$ ，计算最后的输出概率。\n\n![](./img/pic7-edit.jpg)\n\n在luong中提到了三种score的计算方法。这里图解前两种：\n![](./img/score.png)\n\n#### 第1种\n输入是encoder的所有hidden states H: 大小为(hid dim, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim, 1）。\n\n第一步：旋转H为（sequence length, hid dim) 与s做点乘得到一个 大小为(sequence length, 1)的分数。\n\n第二步：对分数做softmax得到一个合为1的权重。\n\n第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。\n\n![](./img/pic8-edit.jpg)\n\n#### 第2种\n输入是encoder的所有hidden states H: 大小为(hid dim1, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim2, 1）。此处两个hidden state的纬度并不一样。\n\n第一步：旋转H为（sequence length, hid dim1) 与 Wa [大小为 hid dim1, hid dim 2)] 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数。\n\n第二步：对分数做softmax得到一个合为1的权重。\n\n第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。\n\n![](./img/pic9-edit.jpg)\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/2.generative_chatbot/1.seq2seq_visualization/","content":"\n# 图解seq2seq\n\n\n### 1.seq2seq（序列到序列模型）简介\n对于很多自然语言处理任务，比如**聊天机器人，机器翻译，自动文摘，智能问答**等，传统的解决方案都是**检索式(从候选集中选出答案)**，这对素材的完善程度要求很高，随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，它是目前自然语言处理技术中非常重要和流行的一个模型，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务的先河，并且被证实在各主流语言之间的相互翻译以及语音助手中人机短问快答的应用中有着非常好的表现，我们在这个notebook中主要给大家以动图的方式展示一下seq2seq模型的一些细节。\n\n参考资料:[Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n\n### 2.编码解码模型\n\n\n序列到序列的模型是非常有意思的NLP模型，我们的很多NLP任务，是文本到文本的映射(对应)，这个过程就像是下面图里展示的过程。当然seq2seq模型不仅仅是用在NLP中的模型，它的输入也可以是语音信号或者图像表示。\n\n![](./img/[1]_seq2seq_1.gif)\n\n更具体一点，在NLP的任务中，其实输入的是文本序列，输出的很多时候也是文本序列，下图所示的是一个典型的机器翻译任务中，输入的文本序列(源语言表述)到输出的文本序列(目标语言表述)之间的变换。\n\n![](./img/[2]_seq2seq_2.gif)\n\n更细节一点的结构是一个“编码解码器”结构，编码器处理输入序列中的每个元素(在这里可能是1个词)，将捕获的信息编译成向量（称为上下文内容向量）。在处理整个输入序列之后，编码器将上下文发送到解码器，解码器逐项开始产生输出序列。\n\n![](./img/[3]_seq2seq_3.gif)\n\n在机器翻译的场景下，是下面这样的。\n\n![](./img/[4]_seq2seq_4.gif)\n\n所谓的上下文向量其实就是\n\n![](./img/context.png)\n\n输入的数据(文本序列)中的每个元素(词)通常会被编码成一个稠密的向量，这个过程叫做word embedding，如下图所示\n\n![](./img/embedding_seq2seq.png)\n\n我们的encoder和decoder都会借助于循环神经网络(RNN)这类特殊的神经网络完成，循环神经网络会接受每个位置(时间点)上的输入，同时经过处理进行信息融合，并可能会在某些位置(时间点)上输出。如下图所示。\n\n![](./img/[5]_RNN_1.gif)\n\n所以动态地展示整个编码器和解码器，分拆的步骤过程大概是下面这个样子。\n\n![](./img/[6]_seq2seq_6.gif)\n\n更详细地展开，其实是这样的。\n\n![](./img/[7]_seq2seq_7.gif)\n\n在更多的时候，我们考虑到提升效果，不会寄希望于把所有的内容都放到一个上下文向量(context vector)中，而是会采用一个叫做**注意力模型**的模型来动态处理和解码，动态的图如下所示。\n\n![](./img/[8]_seq2seq_8.gif)\n\n所谓的注意力机制，可以粗略地理解为是一种对于输入的信息，根据重要程度进行不同权重的加权处理(通常加权的权重来源于softmax后的结果)的机制，如下图所示，是一个在解码阶段，简单地对编码器中的hidden states进行不同权重的加权处理的过程。\n\n![](./img/[9]_seq2seq_9.gif)\n\n更详细一点的**注意力解码**过程如下图所示。\n\n- 带注意力的解码器RNN接收<END>的嵌入(embedding)和一个初始的解码器隐藏状态(hidden state)。\n- RNN处理输入，产生输出和新的隐藏状态向量（h4），输出被摒弃不用。\n- attention的步骤：使用编码器隐藏状态(hidden state)和h4向量来计算该时间步长的上下文向量（C4）。\n- 把h4和C4拼接成一个向量。\n- 把拼接后的向量连接全连接层和softmax完成解码\n- 每个时间点上重复这个操作\n\n![](./img/attention_tensor_dance.gif)\n\n也可以把这个动态解码的过程展示成下述图所示的过程。\n\n![](./img/[11]_seq2seq_9.gif)\n\n注意力机制是一个很神奇地可以学习源语言和目标语言之间词和词对齐关系的方式。如下图所示。\n\n![](./img/attention_sentence.png)\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/10Visual_Text_Task_Image_Caption_and_VQA/Chapter1_Image_Caption/Image_Caption/","content":"\n# 看图说话 （Image Captioning）\n\n\n## 本章概述\n    1.1 “看图说话”问题介绍\n    1.2 简易CNN+RNN编码解码模型完成图片短文本描述原理\n    1.3 注意力模型与“看图说话”优化\n    1.4 【实战】基于CNN+RNN的编解码“看图说话”与beam-search优化\n    1.5 【实战】基于attention model的“看图说话”实现\n\n\n## 1.1 “看图说话”问题介绍\n- image captioning 任务的定义是对一幅图片生成相对于图片内容的文本描述。一个AI系统不仅需要对图片进行识别，也需要理解和解释看到的图片内容，并且能够像人一样描述出图片中的对象之间的关系。\n- 输入：一张图片\n- 输出：一句文本描述\n\n\n## 1.1 “看图说话”问题介绍\n- 最早的image captioning 系统是2014年Circa 提出的，该系统使用多层感知系统（multi-layer perceptron MLP)将图片特征和词向量特征，去输出一串连贯的文字描述。\n- 可以将image caption与机器翻译做一个类比：\n    - image caption的输入是一张图片\n    - machine translation的输入是一句外语句子\n    - 两者的输出都是一个句子\n\n## 1.1 “看图说话”问题介绍\n- 常见的image captioning 系统的评估指标使用的是\n    - BLEU， 是常见的机器翻译系统的评估指标，计算的是一句预测的文字与人类标注的参考文字之间的n-gram 重合度（overlap）。\n    - METEOR， 也是常见的机器翻译系统的评估指标，其通过建立一个短语词表（phrase table），考虑了输出文本是否使用了相似短语。\n    - CIDEr， 考虑了句子中的文字与图片的相关性\n    - ROUGE-L，是text summerization的评估指标\n\n## 1.1 “看图说话”问题介绍\n- 常见的image captioning 系统的标准测试数据集包括\n    - Flickr 8k\n    - Flickr 30k\n    - MS COCO\n\n## 1.2 简易CNN+RNN编码解码模型完成图片短文本描述原理\n- 常见的image captioning 系统是由一个CNN+RNN的编码解码模型完成\n- 类比一下machine translation系统，通常由一个RNN encoder + RNN decoder组成\n![](./img/rnn-rnn.png)\n- image captioning系统，通常由一个CNN encoder + RNN decoder组成\n![](./img/cnn-rnn.png)\n\n\n## 1.2 简易CNN+RNN编码解码模型完成图片短文本描述原理\n- CNN 能够提取一张图片的特征，其特征能用来做图片分类，目标识别，图片分割，及其他视觉任务。\n- Vinyals et al. (2014) Show and Tell: A Neural Image Caption Generator 这篇文章将seq2seq模型中的LSTM encoder换成CNN encoder，用于提取图片的信息，得到一个固定长度的内容向量（context vector），之后通过一个RNN decoder，将信息使用文字的方式解码出来。\n![](./img/cnn-lstm.png)\n\n## 1.2 简易CNN+RNN编码解码模型完成图片短文本描述原理\n- Donahue et al. (2014) Long-term Recurrent Convolutional Networks for Visual Recognition and Description 这篇文章使用了VGG Net作为CNN 去提取图片信息，在输入到一个LSTM decoder中输出文本。同时该文章还将这项技术应用到video captioning中。\n![](./img/cnn-lstm-video.png)\n\n## 1.2 简易CNN+RNN编码解码模型完成图片短文本描述原理\n- 对比视频识别，看图说话，看视频说话\n![](./img/img-video.png)\n\n## 1.2 另一种完成图片短文本描述原理\n- Fang et al 2014， From Captions to Visual Concepts and Back, 提供了另一个image caption系统的思路。\n    1. 预测文字： 使用一个CNN去做目标识别，并且根据bounding box生成可能出现的文字\n    2. 生成句子：通过一个统计语言模型，生成很多个可能的句子集合\n    3. 重新排序已经生成的句子： 通过学习一个Deep Multimodal Similarity Model （DMSM）去重新排序所有可能的句子集合，取最高分数的句子作为系统输出。\n   \n![](./img/ms.png)\n\n## 1.3 注意力机制与beam search优化\n- 注意力机制 - Kelvin et al. (2014) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\n    - 类比人看图说话：当人在解说一幅图片的时候，每预测一个字，会关注到图片上的不同位置。\n    - 在解码器预测文字的时候，会关注到跟当前文字内容和图片最相关的位置。\n    - 举例：a woman standing in a living room holding a Wii remote .\n![](./img/example.gif)\n\n## 1.3 注意力机制与beam search优化\n- soft attention: 对一张图片中每个关注的地方都有一个关注权重，这个权重是介于0和1之间的数值。 \n- hard attention: 对一张图片中每个关注的地方都有一个关注权重，这个权重是0或者1，1表示关注该区域。\n![](./img/example1.png)\n\n## 1.3 注意力机制与beam search优化\n- 关注的名词会对应于图片中高亮的位置\n![](./img/example2.png)\n\n## 1.3 注意力机制与beam search优化\n- 注意力机制 \n    - 一张图片作为输入，CNN不仅可以通过最后一层全连接层（fully-connected layer）抓取出一个固定长度的向量，也即是context vector。还可以抓取出中间卷积层中的向量\n![](./img/cnn.jpeg)\n\n## 1.3 注意力机制与beam search优化\n- 注意力机制 \n    - 一张图片的卷积层中的向量有14x14=196个feature maps $a_i, i=1...196$，每个feature map对应于每个图片中不同的高亮位置。\n    - 注意力机制通过计算每个feature map与当前的hidden state计算两者之间的相关度，这里的hidden state $h_{t-1}$ 总结了已经生成的1到t-1个单词的内容。\n    $$e_{ti}=f_{att}(a_i, h_{t-1}) \\\\ \\alpha_{ti} = {\\exp(e_{ti}) \\over \\sum_{k=1}^L \\exp(e_{tk}) }$$\n    - 之后通过加权求和得到注意力内容向量 $\\hat{z}_t$。\n    $$\\hat{z}_t=\\phi(\\{a_i\\}, \\{\\alpha_i\\})$$\n![](./img/show_attend.png)\n\n## 1.3 注意力机制与beam search优化\n- 注意力机制 \n    - 通过将196个feature maps求平均值去初始化LSTM中的 memory cell $c_0, h_0$ \n    - 根据图片及已经生成的部分单词，去预测下一个单词\n    $$c_0 = f_{init,c}({1\\over L} \\sum_i^L a_i) \\\\ h_0 = f_{init,h}({1\\over L} \\sum_i^L a_i) \\\\  p(y_t|a, y_1^{t-1}) \\propto \\exp(L_o (E y_{t-1} + L_h h_t + L_z \\hat{z}_t))$$\n![](./img/show_attend.png)\n\n## 1.3 注意力机制与beam search优化\n- beam search 优化\n    - 每次预测下一个单词的时候，计算当前所有路径的data log-likelihood并进行排序, 只保留data log-likelihood 最大值的K个beams。\n![](./img/beam.png)\n\n## 1.4 【实战】基于CNN+RNN的编解码“看图说话”与beam-search优化\n- tensorflow 官方源代码：https://github.com/tensorflow/models/tree/master/research/im2txt\n    - Bazel (http://bazel.io/docs/install.html)\n    - Python 2.7\n    - TensorFlow 1.0 or greater (https://www.tensorflow.org/install/)\n    - NumPy (http://www.scipy.org/install.html)\n    - Natural Language Toolkit (NLTK):\n        - NLTK (http://www.nltk.org/install.html)\n        - install the NLTK data package \"punkt\" (http://www.nltk.org/data.html)\n    - Unzip\n- 可以使用conda安装环境\n```bash \nconda env create --file im2txt/conda-env/ubuntu-18-04-environment.yaml\nconda activate im2txt\n```\n\n\n```bash\n%%bash\n! git clone https://github.com/tensorflow/models.git\n\n! home=\"./models/research/\"    #文件夹models存放地址\n! MSCOCO_DIR=\"$home/im2txt/data/mscoco\"\n\n# 预测脚本\n! cd $home/im2txt\n! bazel build //im2txt:download_and_preprocess_mscoco\n\n# 下载MSCOCO数据并进行数据预处理\n! bazel-bin/im2txt/download_and_preprocess_mscoco \"${MSCOCO_DIR}\"\n\n```\n\n\n```bash\n%%bash\n! home=\"$PWD/models/research/\"\n! MSCOCO_DIR=\"$home/im2txt/data/mscoco\"\n\n# 保存 Inception v3 checkpoint的地址\n! INCEPTION_DIR=\"$home/im2txt/data\"\n! mkdir -p ${INCEPTION_DIR}\n\n# 下载预训练好的CNN模型\n! wget \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"\n! tar -xvf \"inception_v3_2016_08_28.tar.gz\" -C ${INCEPTION_DIR}\n! rm \"inception_v3_2016_08_28.tar.gz\"\n```\n\n    inception_v3.ckpt\n    \n\n    fatal: destination path 'models' already exists and is not an empty directory.\n    --2019-03-15 06:34:59--  http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n    Resolving download.tensorflow.org (download.tensorflow.org)... 216.58.200.240, 2404:6800:4008:802::2010\n    Connecting to download.tensorflow.org (download.tensorflow.org)|216.58.200.240|:80... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 100885009 (96M) [application/x-tar]\n    Saving to: 'inception_v3_2016_08_28.tar.gz'\n    \n         0K .......... .......... .......... .......... ..........  0%  440K 3m44s\n        50K .......... .......... .......... .......... ..........  0% 1.01M 2m39s\n       100K .......... .......... .......... .......... ..........  0% 1.60M 2m6s\n       150K .......... .......... .......... .......... ..........  0% 1.91M 1m47s\n       200K .......... .......... .......... .......... ..........  0% 3.07M 92s\n       250K .......... .......... .......... .......... ..........  0% 3.53M 81s\n       300K .......... .......... .......... .......... ..........  0% 4.15M 73s\n       350K .......... .......... .......... .......... ..........  0% 4.46M 66s\n       400K .......... .......... .......... .......... ..........  0% 5.22M 61s\n       450K .......... .......... .......... .......... ..........  0% 5.47M 57s\n       500K .......... .......... .......... .......... ..........  0% 6.34M 53s\n       550K .......... .......... .......... .......... ..........  0% 7.94M 49s\n       600K .......... .......... .......... .......... ..........  0% 7.30M 47s\n       650K .......... .......... .......... .......... ..........  0% 8.60M 44s\n       700K .......... .......... .......... .......... ..........  0% 9.60M 42s\n       750K .......... .......... .......... .......... ..........  0% 11.2M 40s\n       800K .......... .......... .......... .......... ..........  0% 10.5M 38s\n       850K .......... .......... .......... .......... ..........  0% 11.4M 36s\n       900K .......... .......... .......... .......... ..........  0% 11.4M 35s\n       950K .......... .......... .......... .......... ..........  1% 13.5M 33s\n      1000K .......... .......... .......... .......... ..........  1% 11.2M 32s\n      1050K .......... .......... .......... .......... ..........  1% 14.9M 31s\n      1100K .......... .......... .......... .......... ..........  1% 12.0M 30s\n      1150K .......... .......... .......... .......... ..........  1% 15.8M 29s\n      1200K .......... .......... .......... .......... ..........  1% 18.1M 28s\n      1250K .......... .......... .......... .......... ..........  1% 13.5M 27s\n      1300K .......... .......... .......... .......... ..........  1% 15.4M 26s\n      1350K .......... .......... .......... .......... ..........  1% 16.9M 26s\n      1400K .......... .......... .......... .......... ..........  1% 21.1M 25s\n      1450K .......... .......... .......... .......... ..........  1% 18.9M 24s\n      1500K .......... .......... .......... .......... ..........  1% 19.4M 24s\n      1550K .......... .......... .......... .......... ..........  1% 20.9M 23s\n      1600K .......... .......... .......... .......... ..........  1% 20.6M 22s\n      1650K .......... .......... .......... .......... ..........  1% 18.9M 22s\n      1700K .......... .......... .......... .......... ..........  1% 23.5M 21s\n      1750K .......... .......... .......... .......... ..........  1% 23.3M 21s\n      1800K .......... .......... .......... .......... ..........  1% 24.5M 20s\n      1850K .......... .......... .......... .......... ..........  1% 25.4M 20s\n      1900K .......... .......... .......... .......... ..........  1% 28.9M 19s\n      1950K .......... .......... .......... .......... ..........  2% 21.0M 19s\n      2000K .......... .......... .......... .......... ..........  2% 23.5M 19s\n      2050K .......... .......... .......... .......... ..........  2% 26.2M 18s\n      2100K .......... .......... .......... .......... ..........  2% 30.7M 18s\n      2150K .......... .......... .......... .......... ..........  2% 19.2M 18s\n      2200K .......... .......... .......... .......... ..........  2% 35.9M 17s\n      2250K .......... .......... .......... .......... ..........  2% 26.9M 17s\n      2300K .......... .......... .......... .......... ..........  2% 28.1M 17s\n      2350K .......... .......... .......... .......... ..........  2% 37.5M 16s\n      2400K .......... .......... .......... .......... ..........  2% 26.3M 16s\n      2450K .......... .......... .......... .......... ..........  2% 31.4M 16s\n      2500K .......... .......... .......... .......... ..........  2% 25.7M 16s\n      2550K .......... .......... .......... .......... ..........  2% 25.6M 15s\n      2600K .......... .......... .......... .......... ..........  2% 54.7M 15s\n      2650K .......... .......... .......... .......... ..........  2% 29.3M 15s\n      2700K .......... .......... .......... .......... ..........  2% 30.3M 15s\n      2750K .......... .......... .......... .......... ..........  2% 44.1M 14s\n      2800K .......... .......... .......... .......... ..........  2% 30.9M 14s\n      2850K .......... .......... .......... .......... ..........  2% 53.2M 14s\n      2900K .......... .......... .......... .......... ..........  2% 30.8M 14s\n      2950K .......... .......... .......... .......... ..........  3% 48.1M 14s\n      3000K .......... .......... .......... .......... ..........  3% 27.7M 13s\n      3050K .......... .......... .......... .......... ..........  3% 53.6M 13s\n      3100K .......... .......... .......... .......... ..........  3% 31.2M 13s\n      3150K .......... .......... .......... .......... ..........  3% 66.3M 13s\n      3200K .......... .......... .......... .......... ..........  3% 36.1M 13s\n      3250K .......... .......... .......... .......... ..........  3% 36.2M 13s\n      3300K .......... .......... .......... .......... ..........  3% 55.0M 12s\n      3350K .......... .......... .......... .......... ..........  3% 50.4M 12s\n      3400K .......... .......... .......... .......... ..........  3% 58.1M 12s\n      3450K .......... .......... .......... .......... ..........  3% 37.6M 12s\n      3500K .......... .......... .......... .......... ..........  3% 48.4M 12s\n      3550K .......... .......... .......... .......... ..........  3% 34.6M 12s\n      3600K .......... .......... .......... .......... ..........  3% 53.1M 11s\n      3650K .......... .......... .......... .......... ..........  3% 45.4M 11s\n      3700K .......... .......... .......... .......... ..........  3% 83.9M 11s\n      3750K .......... .......... .......... .......... ..........  3% 44.5M 11s\n      3800K .......... .......... .......... .......... ..........  3% 36.7M 11s\n      3850K .......... .......... .......... .......... ..........  3% 45.5M 11s\n      3900K .......... .......... .......... .......... ..........  4% 73.5M 11s\n      3950K .......... .......... .......... .......... ..........  4% 31.7M 11s\n      4000K .......... .......... .......... .......... ..........  4% 82.8M 10s\n      4050K .......... .......... .......... .......... ..........  4% 83.6M 10s\n      4100K .......... .......... .......... .......... ..........  4% 42.3M 10s\n      4150K .......... .......... .......... .......... ..........  4% 27.7M 10s\n      4200K .......... .......... .......... .......... ..........  4% 67.0M 10s\n      4250K .......... .......... .......... .......... ..........  4%  102M 10s\n      4300K .......... .......... .......... .......... ..........  4% 48.1M 10s\n      4350K .......... .......... .......... .......... ..........  4% 46.6M 10s\n      4400K .......... .......... .......... .......... ..........  4% 66.9M 10s\n      4450K .......... .......... .......... .......... ..........  4% 31.4M 10s\n      4500K .......... .......... .......... .......... ..........  4% 63.7M 9s\n      4550K .......... .......... .......... .......... ..........  4% 84.8M 9s\n      4600K .......... .......... .......... .......... ..........  4% 55.7M 9s\n      4650K .......... .......... .......... .......... ..........  4% 53.3M 9s\n      4700K .......... .......... .......... .......... ..........  4% 96.6M 9s\n      4750K .......... .......... .......... .......... ..........  4% 45.4M 9s\n      4800K .......... .......... .......... .......... ..........  4%  109M 9s\n      4850K .......... .......... .......... .......... ..........  4% 39.5M 9s\n      4900K .......... .......... .......... .......... ..........  5% 56.4M 9s\n      4950K .......... .......... .......... .......... ..........  5%  109M 9s\n      5000K .......... .......... .......... .......... ..........  5% 56.6M 9s\n      5050K .......... .......... .......... .......... ..........  5% 42.6M 9s\n      5100K .......... .......... .......... .......... ..........  5% 93.5M 9s\n      5150K .......... .......... .......... .......... ..........  5% 49.2M 8s\n      5200K .......... .......... .......... .......... ..........  5% 65.3M 8s\n      5250K .......... .......... .......... .......... ..........  5% 46.2M 8s\n      5300K .......... .......... .......... .......... ..........  5% 60.5M 8s\n      5350K .......... .......... .......... .......... ..........  5%  152M 8s\n      5400K .......... .......... .......... .......... ..........  5% 70.0M 8s\n      5450K .......... .......... .......... .......... ..........  5% 75.2M 8s\n      5500K .......... .......... .......... .......... ..........  5% 52.2M 8s\n      5550K .......... .......... .......... .......... ..........  5% 49.6M 8s\n      5600K .......... .......... .......... .......... ..........  5%  135M 8s\n      5650K .......... .......... .......... .......... ..........  5% 88.0M 8s\n      5700K .......... .......... .......... .......... ..........  5% 56.8M 8s\n      5750K .......... .......... .......... .......... ..........  5% 82.8M 8s\n      5800K .......... .......... .......... .......... ..........  5% 57.8M 8s\n      5850K .......... .......... .......... .......... ..........  5%  152M 8s\n      5900K .......... .......... .......... .......... ..........  6% 87.7M 7s\n      5950K .......... .......... .......... .......... ..........  6% 39.6M 7s\n      6000K .......... .......... .......... .......... ..........  6%  300M 7s\n      6050K .......... .......... .......... .......... ..........  6% 5.29M 7s\n      6100K .......... .......... .......... .......... ..........  6% 71.3M 7s\n      6150K .......... .......... .......... .......... ..........  6% 39.4M 7s\n      6200K .......... .......... .......... .......... ..........  6% 16.0M 7s\n      6250K .......... .......... .......... .......... ..........  6% 45.0M 7s\n      6300K .......... .......... .......... .......... ..........  6% 44.4M 7s\n      6350K .......... .......... .......... .......... ..........  6%  259M 7s\n      6400K .......... .......... .......... .......... ..........  6% 48.1M 7s\n      6450K .......... .......... .......... .......... ..........  6% 55.0M 7s\n      6500K .......... .......... .......... .......... ..........  6%  103M 7s\n      6550K .......... .......... .......... .......... ..........  6%  103M 7s\n      6600K .......... .......... .......... .......... ..........  6%  314M 7s\n      6650K .......... .......... .......... .......... ..........  6%  248M 7s\n      6700K .......... .......... .......... .......... ..........  6%  264M 7s\n      6750K .......... .......... .......... .......... ..........  6% 99.8M 7s\n      6800K .......... .......... .......... .......... ..........  6%  261M 7s\n      6850K .......... .......... .......... .......... ..........  7%  279M 7s\n      6900K .......... .......... .......... .......... ..........  7%  290M 7s\n      6950K .......... .......... .......... .......... ..........  7%  242M 7s\n      7000K .......... .......... .......... .......... ..........  7% 66.9M 7s\n      7050K .......... .......... .......... .......... ..........  7%  272M 6s\n      7100K .......... .......... .......... .......... ..........  7% 45.1M 6s\n      7150K .......... .......... .......... .......... ..........  7% 62.0M 6s\n      7200K .......... .......... .......... .......... ..........  7% 75.7M 6s\n      7250K .......... .......... .......... .......... ..........  7% 29.4M 6s\n      7300K .......... .......... .......... .......... ..........  7%  104M 6s\n      7350K .......... .......... .......... .......... ..........  7%  103M 6s\n      7400K .......... .......... .......... .......... ..........  7% 34.7M 6s\n      7450K .......... .......... .......... .......... ..........  7% 40.7M 6s\n      7500K .......... .......... .......... .......... ..........  7% 7.01M 6s\n      7550K .......... .......... .......... .......... ..........  7% 9.53M 6s\n      7600K .......... .......... .......... .......... ..........  7% 89.2M 6s\n      7650K .......... .......... .......... .......... ..........  7%  286M 6s\n      7700K .......... .......... .......... .......... ..........  7% 90.8M 6s\n      7750K .......... .......... .......... .......... ..........  7%  108M 6s\n      7800K .......... .......... .......... .......... ..........  7% 99.4M 6s\n      7850K .......... .......... .......... .......... ..........  8%  126M 6s\n      7900K .......... .......... .......... .......... ..........  8%  106M 6s\n      7950K .......... .......... .......... .......... ..........  8% 58.2M 6s\n      8000K .......... .......... .......... .......... ..........  8% 56.8M 6s\n      8050K .......... .......... .......... .......... ..........  8%  331M 6s\n      8100K .......... .......... .......... .......... ..........  8%  274M 6s\n      8150K .......... .......... .......... .......... ..........  8%  283M 6s\n      8200K .......... .......... .......... .......... ..........  8%  250M 6s\n      8250K .......... .......... .......... .......... ..........  8%  327M 6s\n      8300K .......... .......... .......... .......... ..........  8% 94.3M 6s\n      8350K .......... .......... .......... .......... ..........  8%  326M 6s\n      8400K .......... .......... .......... .......... ..........  8% 70.1M 6s\n      8450K .......... .......... .......... .......... ..........  8%  257M 6s\n      8500K .......... .......... .......... .......... ..........  8% 87.0M 6s\n      8550K .......... .......... .......... .......... ..........  8%  344M 6s\n      8600K .......... .......... .......... .......... ..........  8%  297M 6s\n      8650K .......... .......... .......... .......... ..........  8% 52.0M 6s\n      8700K .......... .......... .......... .......... ..........  8%  382M 5s\n      8750K .......... .......... .......... .......... ..........  8%  133M 5s\n      8800K .......... .......... .......... .......... ..........  8%  103M 5s\n      8850K .......... .......... .......... .......... ..........  9%  171M 5s\n      8900K .......... .......... .......... .......... ..........  9% 72.2M 5s\n      8950K .......... .......... .......... .......... ..........  9%  305M 5s\n      9000K .......... .......... .......... .......... ..........  9%  101M 5s\n      9050K .......... .......... .......... .......... ..........  9% 3.31M 5s\n      9100K .......... .......... .......... .......... ..........  9% 86.3M 5s\n      9150K .......... .......... .......... .......... ..........  9%  274M 5s\n      9200K .......... .......... .......... .......... ..........  9%  395M 5s\n      9250K .......... .......... .......... .......... ..........  9% 99.1M 5s\n      9300K .......... .......... .......... .......... ..........  9%  307M 5s\n      9350K .......... .......... .......... .......... ..........  9%  256M 5s\n      9400K .......... .......... .......... .......... ..........  9% 58.6M 5s\n      9450K .......... .......... .......... .......... ..........  9%  312M 5s\n      9500K .......... .......... .......... .......... ..........  9%  340M 5s\n      9550K .......... .......... .......... .......... ..........  9% 63.6M 5s\n      9600K .......... .......... .......... .......... ..........  9%  212M 5s\n      9650K .......... .......... .......... .......... ..........  9% 69.8M 5s\n      9700K .......... .......... .......... .......... ..........  9% 12.2M 5s\n      9750K .......... .......... .......... .......... ..........  9%  248M 5s\n      9800K .......... .......... .......... .......... ..........  9% 97.7M 5s\n      9850K .......... .......... .......... .......... .......... 10%  119M 5s\n      9900K .......... .......... .......... .......... .......... 10%  343M 5s\n      9950K .......... .......... .......... .......... .......... 10%  373M 5s\n     10000K .......... .......... .......... .......... .......... 10%  322M 5s\n     10050K .......... .......... .......... .......... .......... 10%  230M 5s\n     10100K .......... .......... .......... .......... .......... 10%  380M 5s\n     10150K .......... .......... .......... .......... .......... 10%  121M 5s\n     10200K .......... .......... .......... .......... .......... 10%  361M 5s\n     10250K .......... .......... .......... .......... .......... 10%  298M 5s\n     10300K .......... .......... .......... .......... .......... 10%  157M 5s\n     10350K .......... .......... .......... .......... .......... 10% 45.7M 5s\n     10400K .......... .......... .......... .......... .......... 10%  146M 5s\n     10450K .......... .......... .......... .......... .......... 10% 45.4M 5s\n     10500K .......... .......... .......... .......... .......... 10% 43.8M 5s\n     10550K .......... .......... .......... .......... .......... 10% 6.31M 5s\n     10600K .......... .......... .......... .......... .......... 10%  123M 5s\n     10650K .......... .......... .......... .......... .......... 10% 15.0M 5s\n     10700K .......... .......... .......... .......... .......... 10%  348M 5s\n     10750K .......... .......... .......... .......... .......... 10%  100M 5s\n     10800K .......... .......... .......... .......... .......... 11% 31.2M 5s\n     10850K .......... .......... .......... .......... .......... 11%  343M 5s\n     10900K .......... .......... .......... .......... .......... 11%  354M 5s\n     10950K .......... .......... .......... .......... .......... 11% 34.5M 5s\n     11000K .......... .......... .......... .......... .......... 11%  151M 5s\n     11050K .......... .......... .......... .......... .......... 11%  368M 5s\n     11100K .......... .......... .......... .......... .......... 11% 57.5M 5s\n     11150K .......... .......... .......... .......... .......... 11%  276M 5s\n     11200K .......... .......... .......... .......... .......... 11% 9.29M 5s\n     11250K .......... .......... .......... .......... .......... 11% 43.5M 5s\n     11300K .......... .......... .......... .......... .......... 11% 75.8M 5s\n     11350K .......... .......... .......... .......... .......... 11%  296M 5s\n     11400K .......... .......... .......... .......... .......... 11%  304M 4s\n     11450K .......... .......... .......... .......... .......... 11%  314M 4s\n     11500K .......... .......... .......... .......... .......... 11%  329M 4s\n     11550K .......... .......... .......... .......... .......... 11%  276M 4s\n     11600K .......... .......... .......... .......... .......... 11%  312M 4s\n     11650K .......... .......... .......... .......... .......... 11%  309M 4s\n     11700K .......... .......... .......... .......... .......... 11%  360M 4s\n     11750K .......... .......... .......... .......... .......... 11%  296M 4s\n     11800K .......... .......... .......... .......... .......... 12%  290M 4s\n     11850K .......... .......... .......... .......... .......... 12%  356M 4s\n     11900K .......... .......... .......... .......... .......... 12%  356M 4s\n     11950K .......... .......... .......... .......... .......... 12%  287M 4s\n     12000K .......... .......... .......... .......... .......... 12%  389M 4s\n     12050K .......... .......... .......... .......... .......... 12% 7.06M 4s\n     12100K .......... .......... .......... .......... .......... 12% 11.0M 4s\n     12150K .......... .......... .......... .......... .......... 12% 67.2M 4s\n     12200K .......... .......... .......... .......... .......... 12% 24.5M 4s\n     12250K .......... .......... .......... .......... .......... 12%  267M 4s\n     12300K .......... .......... .......... .......... .......... 12% 7.70M 4s\n     12350K .......... .......... .......... .......... .......... 12% 78.3M 4s\n     12400K .......... .......... .......... .......... .......... 12% 74.0M 4s\n     12450K .......... .......... .......... .......... .......... 12% 9.92M 4s\n     12500K .......... .......... .......... .......... .......... 12%  285M 4s\n     12550K .......... .......... .......... .......... .......... 12%  109M 4s\n     12600K .......... .......... .......... .......... .......... 12%  369M 4s\n     12650K .......... .......... .......... .......... .......... 12%  362M 4s\n     12700K .......... .......... .......... .......... .......... 12%  341M 4s\n     12750K .......... .......... .......... .......... .......... 12%  340M 4s\n     12800K .......... .......... .......... .......... .......... 13%  379M 4s\n     12850K .......... .......... .......... .......... .......... 13%  336M 4s\n     12900K .......... .......... .......... .......... .......... 13%  373M 4s\n     12950K .......... .......... .......... .......... .......... 13%  315M 4s\n     13000K .......... .......... .......... .......... .......... 13%  406M 4s\n     13050K .......... .......... .......... .......... .......... 13%  390M 4s\n     13100K .......... .......... .......... .......... .......... 13%  348M 4s\n     13150K .......... .......... .......... .......... .......... 13%  321M 4s\n     13200K .......... .......... .......... .......... .......... 13%  360M 4s\n     13250K .......... .......... .......... .......... .......... 13%  303M 4s\n     13300K .......... .......... .......... .......... .......... 13%  280M 4s\n     13350K .......... .......... .......... .......... .......... 13%  244M 4s\n     13400K .......... .......... .......... .......... .......... 13%  306M 4s\n     13450K .......... .......... .......... .......... .......... 13%  266M 4s\n     13500K .......... .......... .......... .......... .......... 13%  289M 4s\n     13550K .......... .......... .......... .......... .......... 13% 12.3M 4s\n     13600K .......... .......... .......... .......... .......... 13% 47.5M 4s\n     13650K .......... .......... .......... .......... .......... 13% 30.3M 4s\n     13700K .......... .......... .......... .......... .......... 13% 4.82M 4s\n     13750K .......... .......... .......... .......... .......... 14% 90.4M 4s\n     13800K .......... .......... .......... .......... .......... 14%  278M 4s\n     13850K .......... .......... .......... .......... .......... 14% 84.2M 4s\n     13900K .......... .......... .......... .......... .......... 14%  247M 4s\n     13950K .......... .......... .......... .......... .......... 14%  304M 4s\n     14000K .......... .......... .......... .......... .......... 14% 71.4M 4s\n     14050K .......... .......... .......... .......... .......... 14%  283M 4s\n     14100K .......... .......... .......... .......... .......... 14% 55.8M 4s\n     14150K .......... .......... .......... .......... .......... 14%  263M 4s\n     14200K .......... .......... .......... .......... .......... 14%  255M 4s\n     14250K .......... .......... .......... .......... .......... 14% 55.5M 4s\n     14300K .......... .......... .......... .......... .......... 14%  221M 4s\n     14350K .......... .......... .......... .......... .......... 14%  228M 4s\n     14400K .......... .......... .......... .......... .......... 14%  301M 4s\n     14450K .......... .......... .......... .......... .......... 14%  239M 4s\n     14500K .......... .......... .......... .......... .......... 14%  240M 4s\n     14550K .......... .......... .......... .......... .......... 14% 6.10M 4s\n     14600K .......... .......... .......... .......... .......... 14%  328M 4s\n     14650K .......... .......... .......... .......... .......... 14% 64.1M 4s\n     14700K .......... .......... .......... .......... .......... 14%  171M 4s\n     14750K .......... .......... .......... .......... .......... 15% 55.7M 4s\n     14800K .......... .......... .......... .......... .......... 15%  140M 4s\n     14850K .......... .......... .......... .......... .......... 15% 75.7M 4s\n     14900K .......... .......... .......... .......... .......... 15% 36.8M 4s\n     14950K .......... .......... .......... .......... .......... 15%  305M 4s\n     15000K .......... .......... .......... .......... .......... 15%  343M 4s\n     15050K .......... .......... .......... .......... .......... 15%  355M 4s\n     15100K .......... .......... .......... .......... .......... 15%  297M 4s\n     15150K .......... .......... .......... .......... .......... 15%  369M 4s\n     15200K .......... .......... .......... .......... .......... 15% 5.36M 4s\n     15250K .......... .......... .......... .......... .......... 15%  286M 4s\n     15300K .......... .......... .......... .......... .......... 15% 87.7M 4s\n     15350K .......... .......... .......... .......... .......... 15%  291M 4s\n     15400K .......... .......... .......... .......... .......... 15%  313M 4s\n     15450K .......... .......... .......... .......... .......... 15%  379M 4s\n     15500K .......... .......... .......... .......... .......... 15%  285M 4s\n     15550K .......... .......... .......... .......... .......... 15%  393M 4s\n     15600K .......... .......... .......... .......... .......... 15%  101M 4s\n     15650K .......... .......... .......... .......... .......... 15% 36.5M 4s\n     15700K .......... .......... .......... .......... .......... 15% 28.2M 4s\n     15750K .......... .......... .......... .......... .......... 16% 47.7M 4s\n     15800K .......... .......... .......... .......... .......... 16%  295M 4s\n     15850K .......... .......... .......... .......... .......... 16% 6.51M 4s\n     15900K .......... .......... .......... .......... .......... 16% 50.4M 4s\n     15950K .......... .......... .......... .......... .......... 16% 15.4M 4s\n     16000K .......... .......... .......... .......... .......... 16%  314M 4s\n     16050K .......... .......... .......... .......... .......... 16%  324M 4s\n     16100K .......... .......... .......... .......... .......... 16%  304M 3s\n     16150K .......... .......... .......... .......... .......... 16%  296M 3s\n     16200K .......... .......... .......... .......... .......... 16%  404M 3s\n     16250K .......... .......... .......... .......... .......... 16%  347M 3s\n     16300K .......... .......... .......... .......... .......... 16%  363M 3s\n     16350K .......... .......... .......... .......... .......... 16%  252M 3s\n     16400K .......... .......... .......... .......... .......... 16%  367M 3s\n     16450K .......... .......... .......... .......... .......... 16%  352M 3s\n     16500K .......... .......... .......... .......... .......... 16%  266M 3s\n     16550K .......... .......... .......... .......... .......... 16%  220M 3s\n     16600K .......... .......... .......... .......... .......... 16%  316M 3s\n     16650K .......... .......... .......... .......... .......... 16%  378M 3s\n     16700K .......... .......... .......... .......... .......... 17% 35.7M 3s\n     16750K .......... .......... .......... .......... .......... 17%  369M 3s\n     16800K .......... .......... .......... .......... .......... 17% 19.0M 3s\n     16850K .......... .......... .......... .......... .......... 17% 21.9M 3s\n     16900K .......... .......... .......... .......... .......... 17%  289M 3s\n     16950K .......... .......... .......... .......... .......... 17% 59.6M 3s\n     17000K .......... .......... .......... .......... .......... 17%  253M 3s\n     17050K .......... .......... .......... .......... .......... 17%  152M 3s\n     17100K .......... .......... .......... .......... .......... 17% 34.8M 3s\n     17150K .......... .......... .......... .......... .......... 17%  354M 3s\n     17200K .......... .......... .......... .......... .......... 17%  370M 3s\n     17250K .......... .......... .......... .......... .......... 17% 68.2M 3s\n     17300K .......... .......... .......... .......... .......... 17%  323M 3s\n     17350K .......... .......... .......... .......... .......... 17% 3.65M 3s\n     17400K .......... .......... .......... .......... .......... 17%  196M 3s\n     17450K .......... .......... .......... .......... .......... 17%  223M 3s\n     17500K .......... .......... .......... .......... .......... 17%  217M 3s\n     17550K .......... .......... .......... .......... .......... 17%  293M 3s\n     17600K .......... .......... .......... .......... .......... 17%  275M 3s\n     17650K .......... .......... .......... .......... .......... 17%  224M 3s\n     17700K .......... .......... .......... .......... .......... 18%  222M 3s\n     17750K .......... .......... .......... .......... .......... 18%  233M 3s\n     17800K .......... .......... .......... .......... .......... 18%  255M 3s\n     17850K .......... .......... .......... .......... .......... 18%  276M 3s\n     17900K .......... .......... .......... .......... .......... 18%  210M 3s\n     17950K .......... .......... .......... .......... .......... 18%  258M 3s\n     18000K .......... .......... .......... .......... .......... 18% 8.55M 3s\n     18050K .......... .......... .......... .......... .......... 18%  167M 3s\n     18100K .......... .......... .......... .......... .......... 18%  264M 3s\n     18150K .......... .......... .......... .......... .......... 18%  273M 3s\n     18200K .......... .......... .......... .......... .......... 18%  745K 3s\n     18250K .......... .......... .......... .......... .......... 18% 25.7M 3s\n     18300K .......... .......... .......... .......... .......... 18% 8.11M 3s\n     18350K .......... .......... .......... .......... .......... 18%  109M 3s\n     18400K .......... .......... .......... .......... .......... 18% 6.35M 3s\n     18450K .......... .......... .......... .......... .......... 18%  102M 3s\n     18500K .......... .......... .......... .......... .......... 18%  276M 3s\n     18550K .......... .......... .......... .......... .......... 18% 52.5M 3s\n     18600K .......... .......... .......... .......... .......... 18% 5.87M 3s\n     18650K .......... .......... .......... .......... .......... 18% 45.8M 3s\n     18700K .......... .......... .......... .......... .......... 19% 27.6M 3s\n     18750K .......... .......... .......... .......... .......... 19% 4.87M 3s\n     18800K .......... .......... .......... .......... .......... 19%  207M 3s\n     18850K .......... .......... .......... .......... .......... 19%  225M 3s\n     18900K .......... .......... .......... .......... .......... 19%  137M 3s\n     18950K .......... .......... .......... .......... .......... 19% 3.50M 4s\n     19000K .......... .......... .......... .......... .......... 19%  232M 4s\n     19050K .......... .......... .......... .......... .......... 19%  265M 3s\n     19100K .......... .......... .......... .......... .......... 19% 39.1M 3s\n     19150K .......... .......... .......... .......... .......... 19% 22.4M 3s\n     19200K .......... .......... .......... .......... .......... 19%  262M 3s\n     19250K .......... .......... .......... .......... .......... 19% 10.1M 3s\n     19300K .......... .......... .......... .......... .......... 19%  203M 3s\n     19350K .......... .......... .......... .......... .......... 19%  175M 3s\n     19400K .......... .......... .......... .......... .......... 19% 34.7M 3s\n     19450K .......... .......... .......... .......... .......... 19% 8.39M 3s\n     19500K .......... .......... .......... .......... .......... 19%  249M 3s\n     19550K .......... .......... .......... .......... .......... 19%  346K 4s\n     19600K .......... .......... .......... .......... .......... 19%  167M 4s\n     19650K .......... .......... .......... .......... .......... 19%  263M 4s\n     19700K .......... .......... .......... .......... .......... 20%  258M 4s\n     19750K .......... .......... .......... .......... .......... 20%  269M 4s\n     19800K .......... .......... .......... .......... .......... 20%  744K 4s\n     19850K .......... .......... .......... .......... .......... 20%  236M 4s\n     19900K .......... .......... .......... .......... .......... 20%  260M 4s\n     19950K .......... .......... .......... .......... .......... 20%  251M 4s\n     20000K .......... .......... .......... .......... .......... 20%  237M 4s\n     20050K .......... .......... .......... .......... .......... 20%  289M 4s\n     20100K .......... .......... .......... .......... .......... 20%  229M 4s\n     20150K .......... .......... .......... .......... .......... 20%  301M 4s\n     20200K .......... .......... .......... .......... .......... 20%  223M 4s\n     20250K .......... .......... .......... .......... .......... 20%  308M 4s\n     20300K .......... .......... .......... .......... .......... 20%  270M 4s\n     20350K .......... .......... .......... .......... .......... 20%  262M 4s\n     20400K .......... .......... .......... .......... .......... 20%  240M 4s\n     20450K .......... .......... .......... .......... .......... 20%  254M 4s\n     20500K .......... .......... .......... .......... .......... 20%  262M 4s\n     20550K .......... .......... .......... .......... .......... 20%  271M 4s\n     20600K .......... .......... .......... .......... .......... 20%  225M 4s\n     20650K .......... .......... .......... .......... .......... 21%  291M 4s\n     20700K .......... .......... .......... .......... .......... 21%  259M 4s\n     20750K .......... .......... .......... .......... .......... 21%  296M 4s\n     20800K .......... .......... .......... .......... .......... 21%  221M 4s\n     20850K .......... .......... .......... .......... .......... 21%  284M 4s\n     20900K .......... .......... .......... .......... .......... 21%  285M 4s\n     20950K .......... .......... .......... .......... .......... 21%  269M 4s\n     21000K .......... .......... .......... .......... .......... 21%  260M 4s\n     21050K .......... .......... .......... .......... .......... 21%  273M 4s\n     21100K .......... .......... .......... .......... .......... 21%  262M 4s\n     21150K .......... .......... .......... .......... .......... 21%  269M 4s\n     21200K .......... .......... .......... .......... .......... 21%  228M 4s\n     21250K .......... .......... .......... .......... .......... 21%  273M 4s\n     21300K .......... .......... .......... .......... .......... 21%  265M 4s\n     21350K .......... .......... .......... .......... .......... 21%  269M 4s\n     21400K .......... .......... .......... .......... .......... 21%  245M 4s\n     21450K .......... .......... .......... .......... .......... 21%  270M 4s\n     21500K .......... .......... .......... .......... .......... 21%  218M 4s\n     21550K .......... .......... .......... .......... .......... 21%  257M 4s\n     21600K .......... .......... .......... .......... .......... 21%  232M 4s\n     21650K .......... .......... .......... .......... .......... 22%  284M 4s\n     21700K .......... .......... .......... .......... .......... 22%  271M 4s\n     21750K .......... .......... .......... .......... .......... 22%  288M 4s\n     21800K .......... .......... .......... .......... .......... 22%  224M 4s\n     21850K .......... .......... .......... .......... .......... 22%  289M 4s\n     21900K .......... .......... .......... .......... .......... 22%  278M 4s\n     21950K .......... .......... .......... .......... .......... 22%  280M 4s\n     22000K .......... .......... .......... .......... .......... 22%  237M 4s\n     22050K .......... .......... .......... .......... .......... 22%  264M 4s\n     22100K .......... .......... .......... .......... .......... 22%  282M 4s\n     22150K .......... .......... .......... .......... .......... 22%  282M 4s\n     22200K .......... .......... .......... .......... .......... 22% 6.15M 4s\n     22250K .......... .......... .......... .......... .......... 22%  216M 4s\n     22300K .......... .......... .......... .......... .......... 22% 1.13M 4s\n     22350K .......... .......... .......... .......... .......... 22%  309M 4s\n     22400K .......... .......... .......... .......... .......... 22%  243M 4s\n     22450K .......... .......... .......... .......... .......... 22%  346M 4s\n     22500K .......... .......... .......... .......... .......... 22%  244M 4s\n     22550K .......... .......... .......... .......... .......... 22%  329M 4s\n     22600K .......... .......... .......... .......... .......... 22%  271M 4s\n     22650K .......... .......... .......... .......... .......... 23%  441M 4s\n     22700K .......... .......... .......... .......... .......... 23%  247M 4s\n     22750K .......... .......... .......... .......... .......... 23%  374M 4s\n     22800K .......... .......... .......... .......... .......... 23%  386M 4s\n     22850K .......... .......... .......... .......... .......... 23%  296M 4s\n     22900K .......... .......... .......... .......... .......... 23%  370M 4s\n     22950K .......... .......... .......... .......... .......... 23%  234M 4s\n     23000K .......... .......... .......... .......... .......... 23%  393M 4s\n     23050K .......... .......... .......... .......... .......... 23%  281M 4s\n     23100K .......... .......... .......... .......... .......... 23%  358M 4s\n     23150K .......... .......... .......... .......... .......... 23%  397M 4s\n     23200K .......... .......... .......... .......... .......... 23%  289M 4s\n     23250K .......... .......... .......... .......... .......... 23%  421M 4s\n     23300K .......... .......... .......... .......... .......... 23%  239M 4s\n     23350K .......... .......... .......... .......... .......... 23%  619K 4s\n     23400K .......... .......... .......... .......... .......... 23% 63.4M 4s\n     23450K .......... .......... .......... .......... .......... 23% 90.2M 4s\n     23500K .......... .......... .......... .......... .......... 23% 81.4M 4s\n     23550K .......... .......... .......... .......... .......... 23% 54.6M 4s\n     23600K .......... .......... .......... .......... .......... 24% 52.3M 4s\n     23650K .......... .......... .......... .......... .......... 24%  759K 4s\n     23700K .......... .......... .......... .......... .......... 24% 14.8M 4s\n     23750K .......... .......... .......... .......... .......... 24% 22.2M 4s\n     23800K .......... .......... .......... .......... .......... 24% 19.7M 4s\n     23850K .......... .......... .......... .......... .......... 24% 2.84M 4s\n     23900K .......... .......... .......... .......... .......... 24%  432M 4s\n     23950K .......... .......... .......... .......... .......... 24% 63.0M 4s\n     24000K .......... .......... .......... .......... .......... 24% 94.5M 4s\n     24050K .......... .......... .......... .......... .......... 24% 60.7M 4s\n     24100K .......... .......... .......... .......... .......... 24%  132M 4s\n     24150K .......... .......... .......... .......... .......... 24% 99.3M 4s\n     24200K .......... .......... .......... .......... .......... 24% 60.1M 4s\n     24250K .......... .......... .......... .......... .......... 24% 84.2M 4s\n     24300K .......... .......... .......... .......... .......... 24% 70.9M 4s\n     24350K .......... .......... .......... .......... .......... 24% 72.0M 4s\n     24400K .......... .......... .......... .......... .......... 24%  780K 4s\n     24450K .......... .......... .......... .......... .......... 24% 3.93M 4s\n     24500K .......... .......... .......... .......... .......... 24%  275M 4s\n     24550K .......... .......... .......... .......... .......... 24%  308M 4s\n     24600K .......... .......... .......... .......... .......... 25%  131M 4s\n     24650K .......... .......... .......... .......... .......... 25%  330M 4s\n     24700K .......... .......... .......... .......... .......... 25%  114M 4s\n     24750K .......... .......... .......... .......... .......... 25% 63.5M 4s\n     24800K .......... .......... .......... .......... .......... 25%  109M 4s\n     24850K .......... .......... .......... .......... .......... 25%  104M 4s\n     24900K .......... .......... .......... .......... .......... 25% 83.0M 4s\n     24950K .......... .......... .......... .......... .......... 25%  353M 4s\n     25000K .......... .......... .......... .......... .......... 25% 76.0M 4s\n     25050K .......... .......... .......... .......... .......... 25% 89.9M 4s\n     25100K .......... .......... .......... .......... .......... 25%  112M 4s\n     25150K .......... .......... .......... .......... .......... 25%  152M 4s\n     25200K .......... .......... .......... .......... .......... 25%  107M 4s\n     25250K .......... .......... .......... .......... .......... 25% 68.6M 4s\n     25300K .......... .......... .......... .......... .......... 25%  308M 4s\n     25350K .......... .......... .......... .......... .......... 25%  354M 4s\n     25400K .......... .......... .......... .......... .......... 25%  364M 4s\n     25450K .......... .......... .......... .......... .......... 25%  390M 4s\n     25500K .......... .......... .......... .......... .......... 25%  280M 4s\n     25550K .......... .......... .......... .......... .......... 25%  373M 4s\n     25600K .......... .......... .......... .......... .......... 26%  291M 4s\n     25650K .......... .......... .......... .......... .......... 26%  350M 4s\n     25700K .......... .......... .......... .......... .......... 26%  344M 4s\n     25750K .......... .......... .......... .......... .......... 26%  337M 4s\n     25800K .......... .......... .......... .......... .......... 26%  372M 4s\n     25850K .......... .......... .......... .......... .......... 26%  314M 4s\n     25900K .......... .......... .......... .......... .......... 26%  315M 4s\n     25950K .......... .......... .......... .......... .......... 26%  364M 4s\n     26000K .......... .......... .......... .......... .......... 26%  338M 4s\n     26050K .......... .......... .......... .......... .......... 26%  409M 4s\n     26100K .......... .......... .......... .......... .......... 26%  313M 4s\n     26150K .......... .......... .......... .......... .......... 26%  355M 4s\n     26200K .......... .......... .......... .......... .......... 26%  349M 4s\n     26250K .......... .......... .......... .......... .......... 26%  342M 4s\n     26300K .......... .......... .......... .......... .......... 26%  307M 4s\n     26350K .......... .......... .......... .......... .......... 26%  338M 4s\n     26400K .......... .......... .......... .......... .......... 26%  349M 4s\n     26450K .......... .......... .......... .......... .......... 26% 1.11M 4s\n     26500K .......... .......... .......... .......... .......... 26% 58.1M 4s\n     26550K .......... .......... .......... .......... .......... 26% 66.9M 4s\n     26600K .......... .......... .......... .......... .......... 27% 47.1M 4s\n     26650K .......... .......... .......... .......... .......... 27%  449M 4s\n     26700K .......... .......... .......... .......... .......... 27% 47.6M 4s\n     26750K .......... .......... .......... .......... .......... 27% 53.2M 4s\n     26800K .......... .......... .......... .......... .......... 27% 38.5M 4s\n     26850K .......... .......... .......... .......... .......... 27% 58.1M 4s\n     26900K .......... .......... .......... .......... .......... 27%  119M 4s\n     26950K .......... .......... .......... .......... .......... 27% 64.1M 4s\n     27000K .......... .......... .......... .......... .......... 27% 47.4M 4s\n     27050K .......... .......... .......... .......... .......... 27% 66.6M 4s\n     27100K .......... .......... .......... .......... .......... 27% 55.9M 4s\n     27150K .......... .......... .......... .......... .......... 27% 43.1M 4s\n     27200K .......... .......... .......... .......... .......... 27%  107M 4s\n     27250K .......... .......... .......... .......... .......... 27% 64.2M 4s\n     27300K .......... .......... .......... .......... .......... 27% 55.3M 4s\n     27350K .......... .......... .......... .......... .......... 27% 50.8M 4s\n     27400K .......... .......... .......... .......... .......... 27% 49.9M 4s\n     27450K .......... .......... .......... .......... .......... 27% 57.1M 4s\n     27500K .......... .......... .......... .......... .......... 27%  357K 4s\n     27550K .......... .......... .......... .......... .......... 28%  127M 4s\n     27600K .......... .......... .......... .......... .......... 28% 96.7M 4s\n     27650K .......... .......... .......... .......... .......... 28%  255M 4s\n     27700K .......... .......... .......... .......... .......... 28%  195M 4s\n     27750K .......... .......... .......... .......... .......... 28%  239M 4s\n     27800K .......... .......... .......... .......... .......... 28%  237M 4s\n     27850K .......... .......... .......... .......... .......... 28%  280M 4s\n     27900K .......... .......... .......... .......... .......... 28%  162M 4s\n     27950K .......... .......... .......... .......... .......... 28%  303M 4s\n     28000K .......... .......... .......... .......... .......... 28% 2.01M 4s\n     28050K .......... .......... .......... .......... .......... 28%  747K 4s\n     28100K .......... .......... .......... .......... .......... 28%  101M 4s\n     28150K .......... .......... .......... .......... .......... 28% 43.4M 4s\n     28200K .......... .......... .......... .......... .......... 28%  133M 4s\n     28250K .......... .......... .......... .......... .......... 28%  119M 4s\n     28300K .......... .......... .......... .......... .......... 28%  183M 4s\n     28350K .......... .......... .......... .......... .......... 28%  245M 4s\n     28400K .......... .......... .......... .......... .......... 28% 6.71M 4s\n     28450K .......... .......... .......... .......... .......... 28%  294M 4s\n     28500K .......... .......... .......... .......... .......... 28%  250M 4s\n     28550K .......... .......... .......... .......... .......... 29%  346M 4s\n     28600K .......... .......... .......... .......... .......... 29%  357M 4s\n     28650K .......... .......... .......... .......... .......... 29%  293M 4s\n     28700K .......... .......... .......... .......... .......... 29%  355M 4s\n     28750K .......... .......... .......... .......... .......... 29%  332M 4s\n     28800K .......... .......... .......... .......... .......... 29%  299M 4s\n     28850K .......... .......... .......... .......... .......... 29%  256M 4s\n     28900K .......... .......... .......... .......... .......... 29%  233M 4s\n     28950K .......... .......... .......... .......... .......... 29%  274M 4s\n     29000K .......... .......... .......... .......... .......... 29%  149M 4s\n     29050K .......... .......... .......... .......... .......... 29%  145M 4s\n     29100K .......... .......... .......... .......... .......... 29%  272M 4s\n     29150K .......... .......... .......... .......... .......... 29%  131M 4s\n     29200K .......... .......... .......... .......... .......... 29%  310M 4s\n     29250K .......... .......... .......... .......... .......... 29%  219M 4s\n     29300K .......... .......... .......... .......... .......... 29%  274M 4s\n     29350K .......... .......... .......... .......... .......... 29%  271M 4s\n     29400K .......... .......... .......... .......... .......... 29%  220M 4s\n     29450K .......... .......... .......... .......... .......... 29%  235M 4s\n     29500K .......... .......... .......... .......... .......... 29%  217M 4s\n     29550K .......... .......... .......... .......... .......... 30%  228M 4s\n     29600K .......... .......... .......... .......... .......... 30%  271M 4s\n     29650K .......... .......... .......... .......... .......... 30%  194M 4s\n     29700K .......... .......... .......... .......... .......... 30%  253M 4s\n     29750K .......... .......... .......... .......... .......... 30%  258M 4s\n     29800K .......... .......... .......... .......... .......... 30%  266M 4s\n     29850K .......... .......... .......... .......... .......... 30%  236M 4s\n     29900K .......... .......... .......... .......... .......... 30%  216M 4s\n     29950K .......... .......... .......... .......... .......... 30%  279M 4s\n     30000K .......... .......... .......... .......... .......... 30%  266M 4s\n     30050K .......... .......... .......... .......... .......... 30%  211M 4s\n     30100K .......... .......... .......... .......... .......... 30%  278M 4s\n     30150K .......... .......... .......... .......... .......... 30% 1.27M 4s\n     30200K .......... .......... .......... .......... .......... 30%  194M 4s\n     30250K .......... .......... .......... .......... .......... 30%  210M 4s\n     30300K .......... .......... .......... .......... .......... 30%  237M 4s\n     30350K .......... .......... .......... .......... .......... 30%  202M 4s\n     30400K .......... .......... .......... .......... .......... 30%  248M 4s\n     30450K .......... .......... .......... .......... .......... 30%  264M 4s\n     30500K .......... .......... .......... .......... .......... 31%  211M 4s\n     30550K .......... .......... .......... .......... .......... 31%  224M 4s\n     30600K .......... .......... .......... .......... .......... 31% 6.51M 4s\n     30650K .......... .......... .......... .......... .......... 31% 22.9M 4s\n     30700K .......... .......... .......... .......... .......... 31% 21.0M 4s\n     30750K .......... .......... .......... .......... .......... 31% 35.1M 4s\n     30800K .......... .......... .......... .......... .......... 31% 20.0M 4s\n     30850K .......... .......... .......... .......... .......... 31% 27.9M 4s\n     30900K .......... .......... .......... .......... .......... 31% 31.3M 4s\n     30950K .......... .......... .......... .......... .......... 31% 21.7M 4s\n     31000K .......... .......... .......... .......... .......... 31% 33.7M 4s\n     31050K .......... .......... .......... .......... .......... 31% 20.5M 4s\n     31100K .......... .......... .......... .......... .......... 31% 23.2M 4s\n     31150K .......... .......... .......... .......... .......... 31% 23.9M 4s\n     31200K .......... .......... .......... .......... .......... 31% 30.8M 4s\n     31250K .......... .......... .......... .......... .......... 31% 23.1M 4s\n     31300K .......... .......... .......... .......... .......... 31% 2.47M 4s\n     31350K .......... .......... .......... .......... .......... 31% 21.0M 4s\n     31400K .......... .......... .......... .......... .......... 31% 26.2M 4s\n     31450K .......... .......... .......... .......... .......... 31% 30.9M 4s\n     31500K .......... .......... .......... .......... .......... 32% 21.7M 4s\n     31550K .......... .......... .......... .......... .......... 32% 34.5M 4s\n     31600K .......... .......... .......... .......... .......... 32% 21.3M 4s\n     31650K .......... .......... .......... .......... .......... 32% 20.3M 4s\n     31700K .......... .......... .......... .......... .......... 32% 27.6M 4s\n     31750K .......... .......... .......... .......... .......... 32% 30.8M 4s\n     31800K .......... .......... .......... .......... .......... 32% 21.7M 4s\n     31850K .......... .......... .......... .......... .......... 32% 31.5M 4s\n     31900K .......... .......... .......... .......... .......... 32% 21.0M 4s\n     31950K .......... .......... .......... .......... .......... 32% 27.5M 4s\n     32000K .......... .......... .......... .......... .......... 32% 31.9M 4s\n     32050K .......... .......... .......... .......... .......... 32% 20.4M 4s\n     32100K .......... .......... .......... .......... .......... 32% 24.0M 4s\n     32150K .......... .......... .......... .......... .......... 32% 30.4M 4s\n     32200K .......... .......... .......... .......... .......... 32% 22.6M 4s\n     32250K .......... .......... .......... .......... .......... 32%  712K 4s\n     32300K .......... .......... .......... .......... .......... 32%  373K 4s\n     32350K .......... .......... .......... .......... .......... 32% 11.3M 4s\n     32400K .......... .......... .......... .......... .......... 32%  221M 4s\n     32450K .......... .......... .......... .......... .......... 32%  242M 4s\n     32500K .......... .......... .......... .......... .......... 33%  142M 4s\n     32550K .......... .......... .......... .......... .......... 33%  156M 4s\n     32600K .......... .......... .......... .......... .......... 33%  148M 4s\n     32650K .......... .......... .......... .......... .......... 33%  250M 4s\n     32700K .......... .......... .......... .......... .......... 33%  229M 4s\n     32750K .......... .......... .......... .......... .......... 33%  197M 4s\n     32800K .......... .......... .......... .......... .......... 33%  209M 4s\n     32850K .......... .......... .......... .......... .......... 33%  169M 4s\n     32900K .......... .......... .......... .......... .......... 33%  185M 4s\n     32950K .......... .......... .......... .......... .......... 33%  216M 4s\n     33000K .......... .......... .......... .......... .......... 33%  156M 4s\n     33050K .......... .......... .......... .......... .......... 33%  274M 4s\n     33100K .......... .......... .......... .......... .......... 33%  176M 4s\n     33150K .......... .......... .......... .......... .......... 33%  195M 4s\n     33200K .......... .......... .......... .......... .......... 33%  284M 4s\n     33250K .......... .......... .......... .......... .......... 33%  159M 4s\n     33300K .......... .......... .......... .......... .......... 33%  166M 4s\n     33350K .......... .......... .......... .......... .......... 33%  169M 4s\n     33400K .......... .......... .......... .......... .......... 33%  206M 4s\n     33450K .......... .......... .......... .......... .......... 34%  197M 4s\n     33500K .......... .......... .......... .......... .......... 34%  157M 4s\n     33550K .......... .......... .......... .......... .......... 34%  252M 4s\n     33600K .......... .......... .......... .......... .......... 34%  132M 4s\n     33650K .......... .......... .......... .......... .......... 34%  275M 4s\n     33700K .......... .......... .......... .......... .......... 34%  148M 4s\n     33750K .......... .......... .......... .......... .......... 34%  245M 4s\n     33800K .......... .......... .......... .......... .......... 34%  278M 4s\n     33850K .......... .......... .......... .......... .......... 34%  157M 4s\n     33900K .......... .......... .......... .......... .......... 34%  233M 4s\n     33950K .......... .......... .......... .......... .......... 34%  171M 4s\n     34000K .......... .......... .......... .......... .......... 34%  148M 4s\n     34050K .......... .......... .......... .......... .......... 34%  276M 4s\n     34100K .......... .......... .......... .......... .......... 34%  180M 4s\n     34150K .......... .......... .......... .......... .......... 34%  220M 4s\n     34200K .......... .......... .......... .......... .......... 34%  221M 4s\n     34250K .......... .......... .......... .......... .......... 34%  178M 4s\n     34300K .......... .......... .......... .......... .......... 34%  123M 4s\n     34350K .......... .......... .......... .......... .......... 34%  260M 4s\n     34400K .......... .......... .......... .......... .......... 34%  268M 4s\n     34450K .......... .......... .......... .......... .......... 35%  193M 4s\n     34500K .......... .......... .......... .......... .......... 35% 32.2M 4s\n     34550K .......... .......... .......... .......... .......... 35% 35.3M 4s\n     34600K .......... .......... .......... .......... .......... 35% 21.3M 4s\n     34650K .......... .......... .......... .......... .......... 35% 27.0M 4s\n     34700K .......... .......... .......... .......... .......... 35% 22.4M 4s\n     34750K .......... .......... .......... .......... .......... 35% 24.9M 4s\n     34800K .......... .......... .......... .......... .......... 35% 22.6M 4s\n     34850K .......... .......... .......... .......... .......... 35% 24.0M 4s\n     34900K .......... .......... .......... .......... .......... 35% 34.9M 4s\n     34950K .......... .......... .......... .......... .......... 35% 22.5M 4s\n     35000K .......... .......... .......... .......... .......... 35% 5.82M 4s\n     35050K .......... .......... .......... .......... .......... 35% 23.5M 4s\n     35100K .......... .......... .......... .......... .......... 35% 24.2M 4s\n     35150K .......... .......... .......... .......... .......... 35% 20.2M 4s\n     35200K .......... .......... .......... .......... .......... 35% 23.8M 4s\n     35250K .......... .......... .......... .......... .......... 35% 25.4M 4s\n     35300K .......... .......... .......... .......... .......... 35% 22.0M 4s\n     35350K .......... .......... .......... .......... .......... 35% 8.90M 4s\n     35400K .......... .......... .......... .......... .......... 35% 15.7M 4s\n     35450K .......... .......... .......... .......... .......... 36% 13.1M 4s\n     35500K .......... .......... .......... .......... .......... 36% 10.6M 4s\n     35550K .......... .......... .......... .......... .......... 36% 12.7M 4s\n     35600K .......... .......... .......... .......... .......... 36% 12.9M 4s\n     35650K .......... .......... .......... .......... .......... 36% 12.9M 4s\n     35700K .......... .......... .......... .......... .......... 36% 13.0M 4s\n     35750K .......... .......... .......... .......... .......... 36% 13.2M 4s\n     35800K .......... .......... .......... .......... .......... 36% 12.7M 4s\n     35850K .......... .......... .......... .......... .......... 36% 12.9M 4s\n     35900K .......... .......... .......... .......... .......... 36% 12.9M 4s\n     35950K .......... .......... .......... .......... .......... 36% 13.0M 4s\n     36000K .......... .......... .......... .......... .......... 36% 10.4M 4s\n     36050K .......... .......... .......... .......... .......... 36% 13.1M 4s\n     36100K .......... .......... .......... .......... .......... 36% 13.1M 4s\n     36150K .......... .......... .......... .......... .......... 36% 12.6M 4s\n     36200K .......... .......... .......... .......... .......... 36% 13.0M 4s\n     36250K .......... .......... .......... .......... .......... 36% 12.9M 4s\n     36300K .......... .......... .......... .......... .......... 36% 13.0M 4s\n     36350K .......... .......... .......... .......... .......... 36% 13.2M 4s\n     36400K .......... .......... .......... .......... .......... 36% 12.5M 4s\n     36450K .......... .......... .......... .......... .......... 37% 13.0M 4s\n     36500K .......... .......... .......... .......... .......... 37% 10.3M 4s\n     36550K .......... .......... .......... .......... .......... 37% 13.2M 4s\n     36600K .......... .......... .......... .......... .......... 37% 13.1M 4s\n     36650K .......... .......... .......... .......... .......... 37% 13.1M 4s\n     36700K .......... .......... .......... .......... .......... 37% 12.7M 4s\n     36750K .......... .......... .......... .......... .......... 37% 12.7M 4s\n     36800K .......... .......... .......... .......... .......... 37% 13.1M 4s\n     36850K .......... .......... .......... .......... .......... 37% 13.1M 4s\n     36900K .......... .......... .......... .......... .......... 37% 13.0M 4s\n     36950K .......... .......... .......... .......... .......... 37% 10.8M 4s\n     37000K .......... .......... .......... .......... .......... 37% 12.4M 4s\n     37050K .......... .......... .......... .......... .......... 37% 13.0M 4s\n     37100K .......... .......... .......... .......... .......... 37% 13.1M 4s\n     37150K .......... .......... .......... .......... .......... 37% 13.0M 4s\n     37200K .......... .......... .......... .......... .......... 37% 12.9M 4s\n     37250K .......... .......... .......... .......... .......... 37% 12.8M 4s\n     37300K .......... .......... .......... .......... .......... 37% 12.8M 4s\n     37350K .......... .......... .......... .......... .......... 37% 13.0M 4s\n     37400K .......... .......... .......... .......... .......... 38% 13.1M 4s\n     37450K .......... .......... .......... .......... .......... 38% 10.5M 4s\n     37500K .......... .......... .......... .......... .......... 38% 12.9M 4s\n     37550K .......... .......... .......... .......... .......... 38% 12.7M 4s\n     37600K .......... .......... .......... .......... .......... 38% 13.1M 4s\n     37650K .......... .......... .......... .......... .......... 38% 13.0M 4s\n     37700K .......... .......... .......... .......... .......... 38% 13.2M 4s\n     37750K .......... .......... .......... .......... .......... 38% 12.7M 4s\n     37800K .......... .......... .......... .......... .......... 38% 12.8M 4s\n     37850K .......... .......... .......... .......... .......... 38% 12.9M 4s\n     37900K .......... .......... .......... .......... .......... 38% 13.0M 4s\n     37950K .......... .......... .......... .......... .......... 38% 10.7M 4s\n     38000K .......... .......... .......... .......... .......... 38% 12.9M 4s\n     38050K .......... .......... .......... .......... .......... 38% 13.1M 4s\n     38100K .......... .......... .......... .......... .......... 38% 12.5M 4s\n     38150K .......... .......... .......... .......... .......... 38% 13.2M 4s\n     38200K .......... .......... .......... .......... .......... 38% 13.0M 4s\n     38250K .......... .......... .......... .......... .......... 38% 12.6M 4s\n     38300K .......... .......... .......... .......... .......... 38% 13.2M 4s\n     38350K .......... .......... .......... .......... .......... 38% 12.7M 4s\n     38400K .......... .......... .......... .......... .......... 39% 12.9M 4s\n     38450K .......... .......... .......... .......... .......... 39% 10.5M 4s\n     38500K .......... .......... .......... .......... .......... 39% 13.1M 4s\n     38550K .......... .......... .......... .......... .......... 39% 13.0M 4s\n     38600K .......... .......... .......... .......... .......... 39% 12.8M 4s\n     38650K .......... .......... .......... .......... .......... 39% 13.0M 4s\n     38700K .......... .......... .......... .......... .......... 39% 12.8M 4s\n     38750K .......... .......... .......... .......... .......... 39% 13.0M 4s\n     38800K .......... .......... .......... .......... .......... 39% 13.0M 4s\n     38850K .......... .......... .......... .......... .......... 39% 13.1M 4s\n     38900K .......... .......... .......... .......... .......... 39% 10.4M 4s\n     38950K .......... .......... .......... .......... .......... 39% 12.8M 4s\n     39000K .......... .......... .......... .......... .......... 39% 12.8M 4s\n     39050K .......... .......... .......... .......... .......... 39% 13.3M 4s\n     39100K .......... .......... .......... .......... .......... 39% 13.0M 4s\n     39150K .......... .......... .......... .......... .......... 39% 12.9M 4s\n     39200K .......... .......... .......... .......... .......... 39% 12.5M 4s\n     39250K .......... .......... .......... .......... .......... 39% 12.9M 4s\n     39300K .......... .......... .......... .......... .......... 39% 13.1M 4s\n     39350K .......... .......... .......... .......... .......... 39% 13.1M 4s\n     39400K .......... .......... .......... .......... .......... 40% 10.3M 4s\n     39450K .......... .......... .......... .......... .......... 40% 13.3M 4s\n     39500K .......... .......... .......... .......... .......... 40% 12.7M 4s\n     39550K .......... .......... .......... .......... .......... 40% 13.0M 4s\n     39600K .......... .......... .......... .......... .......... 40% 13.1M 4s\n     39650K .......... .......... .......... .......... .......... 40% 12.9M 4s\n     39700K .......... .......... .......... .......... .......... 40% 13.1M 4s\n     39750K .......... .......... .......... .......... .......... 40% 12.7M 4s\n     39800K .......... .......... .......... .......... .......... 40% 12.7M 4s\n     39850K .......... .......... .......... .......... .......... 40% 10.4M 4s\n     39900K .......... .......... .......... .......... .......... 40% 13.3M 4s\n     39950K .......... .......... .......... .......... .......... 40% 13.1M 4s\n     40000K .......... .......... .......... .......... .......... 40% 12.9M 4s\n     40050K .......... .......... .......... .......... .......... 40% 12.5M 4s\n     40100K .......... .......... .......... .......... .......... 40% 13.1M 4s\n     40150K .......... .......... .......... .......... .......... 40% 13.1M 4s\n     40200K .......... .......... .......... .......... .......... 40% 12.8M 4s\n     40250K .......... .......... .......... .......... .......... 40% 13.0M 4s\n     40300K .......... .......... .......... .......... .......... 40% 10.4M 4s\n     40350K .......... .......... .......... .......... .......... 41% 13.0M 4s\n     40400K .......... .......... .......... .......... .......... 41% 12.9M 4s\n     40450K .......... .......... .......... .......... .......... 41% 13.0M 4s\n     40500K .......... .......... .......... .......... .......... 41% 12.7M 4s\n     40550K .......... .......... .......... .......... .......... 41% 13.1M 4s\n     40600K .......... .......... .......... .......... .......... 41% 12.9M 4s\n     40650K .......... .......... .......... .......... .......... 41% 12.7M 4s\n     40700K .......... .......... .......... .......... .......... 41% 13.3M 4s\n     40750K .......... .......... .......... .......... .......... 41% 13.1M 4s\n     40800K .......... .......... .......... .......... .......... 41% 10.5M 4s\n     40850K .......... .......... .......... .......... .......... 41% 13.2M 4s\n     40900K .......... .......... .......... .......... .......... 41% 12.4M 4s\n     40950K .......... .......... .......... .......... .......... 41% 12.9M 4s\n     41000K .......... .......... .......... .......... .......... 41% 13.3M 4s\n     41050K .......... .......... .......... .......... .......... 41% 12.9M 4s\n     41100K .......... .......... .......... .......... .......... 41% 13.1M 4s\n     41150K .......... .......... .......... .......... .......... 41% 12.5M 4s\n     41200K .......... .......... .......... .......... .......... 41% 12.6M 4s\n     41250K .......... .......... .......... .......... .......... 41%  708K 4s\n     41300K .......... .......... .......... .......... .......... 41% 11.0M 4s\n     41350K .......... .......... .......... .......... .......... 42% 17.2M 4s\n     41400K .......... .......... .......... .......... .......... 42% 13.4M 4s\n     41450K .......... .......... .......... .......... .......... 42% 16.4M 4s\n     41500K .......... .......... .......... .......... .......... 42% 6.95M 4s\n     41550K .......... .......... .......... .......... .......... 42% 15.2M 4s\n     41600K .......... .......... .......... .......... .......... 42% 14.7M 4s\n     41650K .......... .......... .......... .......... .......... 42% 13.8M 4s\n     41700K .......... .......... .......... .......... .......... 42% 13.0M 4s\n     41750K .......... .......... .......... .......... .......... 42% 24.6M 4s\n     41800K .......... .......... .......... .......... .......... 42% 6.83M 4s\n     41850K .......... .......... .......... .......... .......... 42% 13.3M 4s\n     41900K .......... .......... .......... .......... .......... 42% 17.2M 4s\n     41950K .......... .......... .......... .......... .......... 42% 12.8M 4s\n     42000K .......... .......... .......... .......... .......... 42% 17.9M 4s\n     42050K .......... .......... .......... .......... .......... 42% 15.6M 4s\n     42100K .......... .......... .......... .......... .......... 42% 11.4M 4s\n     42150K .......... .......... .......... .......... .......... 42%  161M 4s\n     42200K .......... .......... .......... .......... .......... 42% 5.30M 4s\n     42250K .......... .......... .......... .......... .......... 42%  259M 4s\n     42300K .......... .......... .......... .......... .......... 42% 5.96M 4s\n     42350K .......... .......... .......... .......... .......... 43%  166M 4s\n     42400K .......... .......... .......... .......... .......... 43% 6.09M 4s\n     42450K .......... .......... .......... .......... .......... 43%  232M 4s\n     42500K .......... .......... .......... .......... .......... 43% 6.76M 4s\n     42550K .......... .......... .......... .......... .......... 43%  192M 4s\n     42600K .......... .......... .......... .......... .......... 43%  231M 4s\n     42650K .......... .......... .......... .......... .......... 43% 3.93M 4s\n     42700K .......... .......... .......... .......... .......... 43%  134M 4s\n     42750K .......... .......... .......... .......... .......... 43%  201M 4s\n     42800K .......... .......... .......... .......... .......... 43% 4.28M 4s\n     42850K .......... .......... .......... .......... .......... 43%  230M 4s\n     42900K .......... .......... .......... .......... .......... 43% 6.56M 4s\n     42950K .......... .......... .......... .......... .......... 43% 10.7M 4s\n     43000K .......... .......... .......... .......... .......... 43%  138M 4s\n     43050K .......... .......... .......... .......... .......... 43%  209M 4s\n     43100K .......... .......... .......... .......... .......... 43% 4.17M 4s\n     43150K .......... .......... .......... .......... .......... 43% 11.8M 4s\n     43200K .......... .......... .......... .......... .......... 43%  155M 4s\n     43250K .......... .......... .......... .......... .......... 43%  288M 4s\n     43300K .......... .......... .......... .......... .......... 44%  332M 4s\n     43350K .......... .......... .......... .......... .......... 44% 4.09M 4s\n     43400K .......... .......... .......... .......... .......... 44%  186M 4s\n     43450K .......... .......... .......... .......... .......... 44%  344M 4s\n     43500K .......... .......... .......... .......... .......... 44% 4.26M 4s\n     43550K .......... .......... .......... .......... .......... 44% 8.67M 4s\n     43600K .......... .......... .......... .......... .......... 44% 10.4M 4s\n     43650K .......... .......... .......... .......... .......... 44%  215M 4s\n     43700K .......... .......... .......... .......... .......... 44%  371M 4s\n     43750K .......... .......... .......... .......... .......... 44% 4.85M 4s\n     43800K .......... .......... .......... .......... .......... 44% 7.21M 4s\n     43850K .......... .......... .......... .......... .......... 44% 10.2M 4s\n     43900K .......... .......... .......... .......... .......... 44%  196M 4s\n     43950K .......... .......... .......... .......... .......... 44% 10.4M 4s\n     44000K .......... .......... .......... .......... .......... 44%  156M 4s\n     44050K .......... .......... .......... .......... .......... 44% 4.80M 4s\n     44100K .......... .......... .......... .......... .......... 44% 12.2M 4s\n     44150K .......... .......... .......... .......... .......... 44%  147M 4s\n     44200K .......... .......... .......... .......... .......... 44% 5.29M 4s\n     44250K .......... .......... .......... .......... .......... 44% 23.1M 4s\n     44300K .......... .......... .......... .......... .......... 45%  201M 4s\n     44350K .......... .......... .......... .......... .......... 45%  319M 3s\n     44400K .......... .......... .......... .......... .......... 45% 4.02M 4s\n     44450K .......... .......... .......... .......... .......... 45% 10.5M 4s\n     44500K .......... .......... .......... .......... .......... 45% 9.97M 3s\n     44550K .......... .......... .......... .......... .......... 45% 10.2M 3s\n     44600K .......... .......... .......... .......... .......... 45% 23.5M 3s\n     44650K .......... .......... .......... .......... .......... 45% 16.7M 3s\n     44700K .......... .......... .......... .......... .......... 45% 23.5M 3s\n     44750K .......... .......... .......... .......... .......... 45% 16.0M 3s\n     44800K .......... .......... .......... .......... .......... 45% 10.0M 3s\n     44850K .......... .......... .......... .......... .......... 45% 7.41M 3s\n     44900K .......... .......... .......... .......... .......... 45% 16.9M 3s\n     44950K .......... .......... .......... .......... .......... 45% 9.65M 3s\n     45000K .......... .......... .......... .......... .......... 45% 26.4M 3s\n     45050K .......... .......... .......... .......... .......... 45% 15.8M 3s\n     45100K .......... .......... .......... .......... .......... 45% 9.99M 3s\n     45150K .......... .......... .......... .......... .......... 45% 10.3M 3s\n     45200K .......... .......... .......... .......... .......... 45% 10.4M 3s\n     45250K .......... .......... .......... .......... .......... 45% 10.1M 3s\n     45300K .......... .......... .......... .......... .......... 46% 22.3M 3s\n     45350K .......... .......... .......... .......... .......... 46% 10.5M 3s\n     45400K .......... .......... .......... .......... .......... 46% 16.5M 3s\n     45450K .......... .......... .......... .......... .......... 46% 10.1M 3s\n     45500K .......... .......... .......... .......... .......... 46% 22.9M 3s\n     45550K .......... .......... .......... .......... .......... 46% 10.2M 3s\n     45600K .......... .......... .......... .......... .......... 46% 16.3M 3s\n     45650K .......... .......... .......... .......... .......... 46% 7.34M 3s\n     45700K .......... .......... .......... .......... .......... 46% 16.6M 3s\n     45750K .......... .......... .......... .......... .......... 46% 10.1M 3s\n     45800K .......... .......... .......... .......... .......... 46% 22.9M 3s\n     45850K .......... .......... .......... .......... .......... 46% 10.1M 3s\n     45900K .......... .......... .......... .......... .......... 46% 16.4M 3s\n     45950K .......... .......... .......... .......... .......... 46% 10.5M 3s\n     46000K .......... .......... .......... .......... .......... 46% 22.4M 3s\n     46050K .......... .......... .......... .......... .......... 46% 10.0M 3s\n     46100K .......... .......... .......... .......... .......... 46% 6.58M 3s\n     46150K .......... .......... .......... .......... .......... 46% 23.4M 3s\n     46200K .......... .......... .......... .......... .......... 46% 12.4M 3s\n     46250K .......... .......... .......... .......... .......... 46% 12.2M 3s\n     46300K .......... .......... .......... .......... .......... 47% 10.5M 3s\n     46350K .......... .......... .......... .......... .......... 47% 23.5M 3s\n     46400K .......... .......... .......... .......... .......... 47% 16.0M 3s\n     46450K .......... .......... .......... .......... .......... 47% 7.41M 3s\n     46500K .......... .......... .......... .......... .......... 47%  174M 3s\n     46550K .......... .......... .......... .......... .......... 47% 6.42M 3s\n     46600K .......... .......... .......... .......... .......... 47% 10.4M 3s\n     46650K .......... .......... .......... .......... .......... 47% 22.5M 3s\n     46700K .......... .......... .......... .......... .......... 47% 16.8M 3s\n     46750K .......... .......... .......... .......... .......... 47% 9.41M 3s\n     46800K .......... .......... .......... .......... .......... 47% 11.0M 3s\n     46850K .......... .......... .......... .......... .......... 47% 10.2M 3s\n     46900K .......... .......... .......... .......... .......... 47% 10.1M 3s\n     46950K .......... .......... .......... .......... .......... 47%  237M 3s\n     47000K .......... .......... .......... .......... .......... 47% 7.27M 3s\n     47050K .......... .......... .......... .......... .......... 47% 16.1M 3s\n     47100K .......... .......... .......... .......... .......... 47% 10.2M 3s\n     47150K .......... .......... .......... .......... .......... 47% 23.3M 3s\n     47200K .......... .......... .......... .......... .......... 47% 10.0M 3s\n     47250K .......... .......... .......... .......... .......... 48% 13.0M 3s\n     47300K .......... .......... .......... .......... .......... 48% 8.61M 3s\n     47350K .......... .......... .......... .......... .......... 48% 12.7M 3s\n     47400K .......... .......... .......... .......... .......... 48% 11.9M 3s\n     47450K .......... .......... .......... .......... .......... 48% 10.5M 3s\n     47500K .......... .......... .......... .......... .......... 48%  157M 3s\n     47550K .......... .......... .......... .......... .......... 48% 10.2M 3s\n     47600K .......... .......... .......... .......... .......... 48% 7.48M 3s\n     47650K .......... .......... .......... .......... .......... 48% 40.8M 3s\n     47700K .......... .......... .......... .......... .......... 48% 7.38M 3s\n     47750K .......... .......... .......... .......... .......... 48% 35.2M 3s\n     47800K .......... .......... .......... .......... .......... 48% 8.59M 3s\n     47850K .......... .......... .......... .......... .......... 48% 13.1M 3s\n     47900K .......... .......... .......... .......... .......... 48% 12.2M 3s\n     47950K .......... .......... .......... .......... .......... 48% 10.3M 3s\n     48000K .......... .......... .......... .......... .......... 48% 22.8M 3s\n     48050K .......... .......... .......... .......... .......... 48% 6.49M 3s\n     48100K .......... .......... .......... .......... .......... 48%  129M 3s\n     48150K .......... .......... .......... .......... .......... 48% 7.49M 3s\n     48200K .......... .......... .......... .......... .......... 48% 12.6M 3s\n     48250K .......... .......... .......... .......... .......... 49% 12.2M 3s\n     48300K .......... .......... .......... .......... .......... 49% 10.5M 3s\n     48350K .......... .......... .......... .......... .......... 49% 23.1M 3s\n     48400K .......... .......... .......... .......... .......... 49% 15.7M 3s\n     48450K .......... .......... .......... .......... .......... 49% 10.1M 3s\n     48500K .......... .......... .......... .......... .......... 49% 23.9M 3s\n     48550K .......... .......... .......... .......... .......... 49% 6.56M 3s\n     48600K .......... .......... .......... .......... .......... 49% 10.1M 3s\n     48650K .......... .......... .......... .......... .......... 49% 22.5M 3s\n     48700K .......... .......... .......... .......... .......... 49% 16.4M 3s\n     48750K .......... .......... .......... .......... .......... 49% 10.4M 3s\n     48800K .......... .......... .......... .......... .......... 49% 22.2M 3s\n     48850K .......... .......... .......... .......... .......... 49% 10.3M 3s\n     48900K .......... .......... .......... .......... .......... 49% 16.4M 3s\n     48950K .......... .......... .......... .......... .......... 49% 7.28M 3s\n     49000K .......... .......... .......... .......... .......... 49%  234M 3s\n     49050K .......... .......... .......... .......... .......... 49% 6.50M 3s\n     49100K .......... .......... .......... .......... .......... 49% 10.2M 3s\n     49150K .......... .......... .......... .......... .......... 49% 23.1M 3s\n     49200K .......... .......... .......... .......... .......... 49% 12.7M 3s\n     49250K .......... .......... .......... .......... .......... 50% 12.2M 3s\n     49300K .......... .......... .......... .......... .......... 50% 10.1M 3s\n     49350K .......... .......... .......... .......... .......... 50% 10.4M 3s\n     49400K .......... .......... .......... .......... .......... 50% 10.1M 3s\n     49450K .......... .......... .......... .......... .......... 50% 22.1M 3s\n     49500K .......... .......... .......... .......... .......... 50% 10.5M 3s\n     49550K .......... .......... .......... .......... .......... 50% 15.7M 3s\n     49600K .......... .......... .......... .......... .......... 50% 10.7M 3s\n     49650K .......... .......... .......... .......... .......... 50% 21.1M 3s\n     49700K .......... .......... .......... .......... .......... 50% 6.59M 3s\n     49750K .......... .......... .......... .......... .......... 50%  139M 3s\n     49800K .......... .......... .......... .......... .......... 50% 7.49M 3s\n     49850K .......... .......... .......... .......... .......... 50% 12.5M 3s\n     49900K .......... .......... .......... .......... .......... 50% 12.1M 3s\n     49950K .......... .......... .......... .......... .......... 50% 23.2M 3s\n     50000K .......... .......... .......... .......... .......... 50% 10.3M 3s\n     50050K .......... .......... .......... .......... .......... 50% 16.5M 3s\n     50100K .......... .......... .......... .......... .......... 50% 7.41M 3s\n     50150K .......... .......... .......... .......... .......... 50%  144M 3s\n     50200K .......... .......... .......... .......... .......... 51% 6.51M 3s\n     50250K .......... .......... .......... .......... .......... 51% 10.4M 3s\n     50300K .......... .......... .......... .......... .......... 51% 22.1M 3s\n     50350K .......... .......... .......... .......... .......... 51% 13.0M 3s\n     50400K .......... .......... .......... .......... .......... 51% 12.0M 3s\n     50450K .......... .......... .......... .......... .......... 51% 10.3M 3s\n     50500K .......... .......... .......... .......... .......... 51% 10.4M 3s\n     50550K .......... .......... .......... .......... .......... 51% 10.1M 3s\n     50600K .......... .......... .......... .......... .......... 51%  129M 3s\n     50650K .......... .......... .......... .......... .......... 51% 7.53M 3s\n     50700K .......... .......... .......... .......... .......... 51% 15.9M 3s\n     50750K .......... .......... .......... .......... .......... 51% 10.2M 3s\n     50800K .......... .......... .......... .......... .......... 51% 22.0M 3s\n     50850K .......... .......... .......... .......... .......... 51% 12.6M 3s\n     50900K .......... .......... .......... .......... .......... 51% 13.0M 3s\n     50950K .......... .......... .......... .......... .......... 51% 9.90M 3s\n     51000K .......... .......... .......... .......... .......... 51% 8.70M 3s\n     51050K .......... .......... .......... .......... .......... 51% 12.6M 3s\n     51100K .......... .......... .......... .......... .......... 51% 10.2M 3s\n     51150K .......... .......... .......... .......... .......... 51% 23.0M 3s\n     51200K .......... .......... .......... .......... .......... 52% 16.3M 3s\n     51250K .......... .......... .......... .......... .......... 52% 9.90M 3s\n     51300K .......... .......... .......... .......... .......... 52% 24.4M 3s\n     51350K .......... .......... .......... .......... .......... 52% 10.0M 3s\n     51400K .......... .......... .......... .......... .......... 52% 16.8M 3s\n     51450K .......... .......... .......... .......... .......... 52% 7.25M 3s\n     51500K .......... .......... .......... .......... .......... 52% 10.6M 3s\n     51550K .......... .......... .......... .......... .......... 52% 16.6M 3s\n     51600K .......... .......... .......... .......... .......... 52% 9.61M 3s\n     51650K .......... .......... .......... .......... .......... 52% 24.4M 3s\n     51700K .......... .......... .......... .......... .......... 52% 16.4M 3s\n     51750K .......... .......... .......... .......... .......... 52% 7.44M 3s\n     51800K .......... .......... .......... .......... .......... 52%  108M 3s\n     51850K .......... .......... .......... .......... .......... 52% 6.53M 3s\n     51900K .......... .......... .......... .......... .......... 52%  179M 3s\n     51950K .......... .......... .......... .......... .......... 52% 7.45M 3s\n     52000K .......... .......... .......... .......... .......... 52% 12.5M 3s\n     52050K .......... .......... .......... .......... .......... 52% 12.7M 3s\n     52100K .......... .......... .......... .......... .......... 52% 10.0M 3s\n     52150K .......... .......... .......... .......... .......... 52% 22.6M 3s\n     52200K .......... .......... .......... .......... .......... 53% 6.68M 3s\n     52250K .......... .......... .......... .......... .......... 53%  103M 3s\n     52300K .......... .......... .......... .......... .......... 53% 7.60M 3s\n     52350K .......... .......... .......... .......... .......... 53% 15.8M 3s\n     52400K .......... .......... .......... .......... .......... 53% 10.1M 3s\n     52450K .......... .......... .......... .......... .......... 53% 23.6M 3s\n     52500K .......... .......... .......... .......... .......... 53% 10.2M 3s\n     52550K .......... .......... .......... .......... .......... 53% 16.6M 3s\n     52600K .......... .......... .......... .......... .......... 53% 7.48M 3s\n     52650K .......... .......... .......... .......... .......... 53% 12.6M 3s\n     52700K .......... .......... .......... .......... .......... 53% 12.0M 3s\n     52750K .......... .......... .......... .......... .......... 53% 10.6M 3s\n     52800K .......... .......... .......... .......... .......... 53% 21.8M 3s\n     52850K .......... .......... .......... .......... .......... 53% 16.5M 3s\n     52900K .......... .......... .......... .......... .......... 53% 9.90M 3s\n     52950K .......... .......... .......... .......... .......... 53% 10.4M 3s\n     53000K .......... .......... .......... .......... .......... 53% 10.4M 3s\n     53050K .......... .......... .......... .......... .......... 53% 10.1M 3s\n     53100K .......... .......... .......... .......... .......... 53% 21.7M 3s\n     53150K .......... .......... .......... .......... .......... 53% 10.6M 3s\n     53200K .......... .......... .......... .......... .......... 54% 15.5M 3s\n     53250K .......... .......... .......... .......... .......... 54% 10.1M 3s\n     53300K .......... .......... .......... .......... .......... 54% 23.2M 3s\n     53350K .......... .......... .......... .......... .......... 54% 6.54M 3s\n     53400K .......... .......... .......... .......... .......... 54%  134M 3s\n     53450K .......... .......... .......... .......... .......... 54% 7.57M 3s\n     53500K .......... .......... .......... .......... .......... 54% 16.2M 3s\n     53550K .......... .......... .......... .......... .......... 54% 10.3M 3s\n     53600K .......... .......... .......... .......... .......... 54% 21.1M 3s\n     53650K .......... .......... .......... .......... .......... 54% 10.7M 3s\n     53700K .......... .......... .......... .......... .......... 54% 15.3M 3s\n     53750K .......... .......... .......... .......... .......... 54% 10.3M 3s\n     53800K .......... .......... .......... .......... .......... 54% 24.1M 3s\n     53850K .......... .......... .......... .......... .......... 54% 6.55M 3s\n     53900K .......... .......... .......... .......... .......... 54% 10.2M 3s\n     53950K .......... .......... .......... .......... .......... 54% 23.8M 3s\n     54000K .......... .......... .......... .......... .......... 54% 15.6M 3s\n     54050K .......... .......... .......... .......... .......... 54% 10.0M 3s\n     54100K .......... .......... .......... .......... .......... 54% 10.3M 3s\n     54150K .......... .......... .......... .......... .......... 55% 22.4M 3s\n     54200K .......... .......... .......... .......... .......... 55% 17.2M 3s\n     54250K .......... .......... .......... .......... .......... 55% 7.15M 3s\n     54300K .......... .......... .......... .......... .......... 55%  173M 3s\n     54350K .......... .......... .......... .......... .......... 55% 6.82M 3s\n     54400K .......... .......... .......... .......... .......... 55% 10.1M 3s\n     54450K .......... .......... .......... .......... .......... 55% 21.1M 3s\n     54500K .......... .......... .......... .......... .......... 55% 12.8M 3s\n     54550K .......... .......... .......... .......... .......... 55% 12.3M 3s\n     54600K .......... .......... .......... .......... .......... 55% 10.2M 3s\n     54650K .......... .......... .......... .......... .......... 55% 10.1M 3s\n     54700K .......... .......... .......... .......... .......... 55% 10.5M 3s\n     54750K .......... .......... .......... .......... .......... 55% 22.2M 3s\n     54800K .......... .......... .......... .......... .......... 55% 10.6M 3s\n     54850K .......... .......... .......... .......... .......... 55% 13.9M 3s\n     54900K .......... .......... .......... .......... .......... 55% 10.9M 3s\n     54950K .......... .......... .......... .......... .......... 55% 23.3M 3s\n     55000K .......... .......... .......... .......... .......... 55% 6.61M 3s\n     55050K .......... .......... .......... .......... .......... 55%  111M 3s\n     55100K .......... .......... .......... .......... .......... 55% 7.67M 3s\n     55150K .......... .......... .......... .......... .......... 56% 11.7M 3s\n     55200K .......... .......... .......... .......... .......... 56% 13.4M 3s\n     55250K .......... .......... .......... .......... .......... 56% 10.2M 3s\n     55300K .......... .......... .......... .......... .......... 56% 21.9M 3s\n     55350K .......... .......... .......... .......... .......... 56% 16.4M 3s\n     55400K .......... .......... .......... .......... .......... 56% 10.1M 3s\n     55450K .......... .......... .......... .......... .......... 56% 10.2M 3s\n     55500K .......... .......... .......... .......... .......... 56% 10.5M 3s\n     55550K .......... .......... .......... .......... .......... 56% 10.1M 3s\n     55600K .......... .......... .......... .......... .......... 56% 21.6M 3s\n     55650K .......... .......... .......... .......... .......... 56% 12.7M 3s\n     55700K .......... .......... .......... .......... .......... 56% 12.4M 3s\n     55750K .......... .......... .......... .......... .......... 56% 22.9M 3s\n     55800K .......... .......... .......... .......... .......... 56% 6.69M 3s\n     55850K .......... .......... .......... .......... .......... 56% 10.1M 3s\n     55900K .......... .......... .......... .......... .......... 56% 21.9M 3s\n     55950K .......... .......... .......... .......... .......... 56% 10.6M 3s\n     56000K .......... .......... .......... .......... .......... 56% 15.6M 3s\n     56050K .......... .......... .......... .......... .......... 56% 10.1M 3s\n     56100K .......... .......... .......... .......... .......... 56% 24.2M 3s\n     56150K .......... .......... .......... .......... .......... 57% 10.1M 3s\n     56200K .......... .......... .......... .......... .......... 57% 16.8M 3s\n     56250K .......... .......... .......... .......... .......... 57% 10.4M 3s\n     56300K .......... .......... .......... .......... .......... 57% 10.0M 3s\n     56350K .......... .......... .......... .......... .......... 57% 10.1M 3s\n     56400K .......... .......... .......... .......... .......... 57% 10.2M 3s\n     56450K .......... .......... .......... .......... .......... 57% 22.7M 3s\n     56500K .......... .......... .......... .......... .......... 57% 16.0M 3s\n     56550K .......... .......... .......... .......... .......... 57% 10.1M 3s\n     56600K .......... .......... .......... .......... .......... 57% 10.3M 3s\n     56650K .......... .......... .......... .......... .......... 57% 9.93M 3s\n     56700K .......... .......... .......... .......... .......... 57%  261M 3s\n     56750K .......... .......... .......... .......... .......... 57% 7.48M 3s\n     56800K .......... .......... .......... .......... .......... 57% 12.4M 3s\n     56850K .......... .......... .......... .......... .......... 57% 12.3M 3s\n     56900K .......... .......... .......... .......... .......... 57% 10.5M 3s\n     56950K .......... .......... .......... .......... .......... 57% 22.7M 3s\n     57000K .......... .......... .......... .......... .......... 57% 16.8M 3s\n     57050K .......... .......... .......... .......... .......... 57% 10.1M 3s\n     57100K .......... .......... .......... .......... .......... 58% 10.1M 3s\n     57150K .......... .......... .......... .......... .......... 58% 10.2M 3s\n     57200K .......... .......... .......... .......... .......... 58% 9.88M 3s\n     57250K .......... .......... .......... .......... .......... 58% 10.5M 3s\n     57300K .......... .......... .......... .......... .......... 58% 22.3M 3s\n     57350K .......... .......... .......... .......... .......... 58% 10.0M 3s\n     57400K .......... .......... .......... .......... .......... 58%  191M 3s\n     57450K .......... .......... .......... .......... .......... 58% 6.60M 3s\n     57500K .......... .......... .......... .......... .......... 58% 10.2M 3s\n     57550K .......... .......... .......... .......... .......... 58%  136M 3s\n     57600K .......... .......... .......... .......... .......... 58% 7.50M 3s\n     57650K .......... .......... .......... .......... .......... 58% 12.6M 3s\n     57700K .......... .......... .......... .......... .......... 58% 12.6M 3s\n     57750K .......... .......... .......... .......... .......... 58% 20.9M 3s\n     57800K .......... .......... .......... .......... .......... 58% 10.4M 3s\n     57850K .......... .......... .......... .......... .......... 58% 12.8M 3s\n     57900K .......... .......... .......... .......... .......... 58% 8.59M 3s\n     57950K .......... .......... .......... .......... .......... 58%  138M 3s\n     58000K .......... .......... .......... .......... .......... 58% 8.89M 3s\n     58050K .......... .......... .......... .......... .......... 58% 7.23M 3s\n     58100K .......... .......... .......... .......... .......... 59% 24.1M 3s\n     58150K .......... .......... .......... .......... .......... 59% 16.0M 3s\n     58200K .......... .......... .......... .......... .......... 59% 10.1M 3s\n     58250K .......... .......... .......... .......... .......... 59% 7.34M 3s\n     58300K .......... .......... .......... .......... .......... 59% 16.8M 3s\n     58350K .......... .......... .......... .......... .......... 59% 10.2M 3s\n     58400K .......... .......... .......... .......... .......... 59%  137M 3s\n     58450K .......... .......... .......... .......... .......... 59% 7.53M 3s\n     58500K .......... .......... .......... .......... .......... 59% 16.3M 3s\n     58550K .......... .......... .......... .......... .......... 59% 10.2M 3s\n     58600K .......... .......... .......... .......... .......... 59% 22.6M 3s\n     58650K .......... .......... .......... .......... .......... 59% 6.48M 3s\n     58700K .......... .......... .......... .......... .......... 59%  150M 3s\n     58750K .......... .......... .......... .......... .......... 59% 7.46M 3s\n     58800K .......... .......... .......... .......... .......... 59% 16.7M 3s\n     58850K .......... .......... .......... .......... .......... 59% 8.50M 3s\n     58900K .......... .......... .......... .......... .......... 59% 12.0M 3s\n     58950K .......... .......... .......... .......... .......... 59% 25.2M 3s\n     59000K .......... .......... .......... .......... .......... 59% 16.0M 3s\n     59050K .......... .......... .......... .......... .......... 59% 10.1M 3s\n     59100K .......... .......... .......... .......... .......... 60% 23.3M 3s\n     59150K .......... .......... .......... .......... .......... 60% 10.1M 3s\n     59200K .......... .......... .......... .......... .......... 60% 6.55M 3s\n     59250K .......... .......... .......... .......... .......... 60% 23.2M 3s\n     59300K .......... .......... .......... .......... .......... 60% 17.1M 3s\n     59350K .......... .......... .......... .......... .......... 60% 9.81M 3s\n     59400K .......... .......... .......... .......... .......... 60% 9.95M 3s\n     59450K .......... .......... .......... .......... .......... 60% 25.7M 3s\n     59500K .......... .......... .......... .......... .......... 60% 10.1M 3s\n     59550K .......... .......... .......... .......... .......... 60% 10.2M 3s\n     59600K .......... .......... .......... .......... .......... 60%  189M 3s\n     59650K .......... .......... .......... .......... .......... 60% 6.55M 3s\n     59700K .......... .......... .......... .......... .......... 60% 9.72M 3s\n     59750K .......... .......... .......... .......... .......... 60% 10.5M 3s\n     59800K .......... .......... .......... .......... .......... 60% 77.2M 3s\n     59850K .......... .......... .......... .......... .......... 60% 10.6M 3s\n     59900K .......... .......... .......... .......... .......... 60% 10.2M 3s\n     59950K .......... .......... .......... .......... .......... 60% 10.4M 3s\n     60000K .......... .......... .......... .......... .......... 60% 10.1M 3s\n     60050K .......... .......... .......... .......... .......... 61%  177M 3s\n     60100K .......... .......... .......... .......... .......... 61% 7.24M 3s\n     60150K .......... .......... .......... .......... .......... 61% 12.5M 3s\n     60200K .......... .......... .......... .......... .......... 61% 12.6M 3s\n     60250K .......... .......... .......... .......... .......... 61% 23.6M 3s\n     60300K .......... .......... .......... .......... .......... 61% 10.3M 3s\n     60350K .......... .......... .......... .......... .......... 61% 15.8M 3s\n     60400K .......... .......... .......... .......... .......... 61% 7.54M 3s\n     60450K .......... .......... .......... .......... .......... 61% 10.2M 3s\n     60500K .......... .......... .......... .......... .......... 61% 15.8M 3s\n     60550K .......... .......... .......... .......... .......... 61% 10.3M 3s\n     60600K .......... .......... .......... .......... .......... 61% 23.3M 3s\n     60650K .......... .......... .......... .......... .......... 61% 16.2M 3s\n     60700K .......... .......... .......... .......... .......... 61% 7.41M 3s\n     60750K .......... .......... .......... .......... .......... 61%  145M 3s\n     60800K .......... .......... .......... .......... .......... 61% 6.65M 3s\n     60850K .......... .......... .......... .......... .......... 61% 10.1M 3s\n     60900K .......... .......... .......... .......... .......... 61% 21.6M 3s\n     60950K .......... .......... .......... .......... .......... 61% 12.7M 3s\n     61000K .......... .......... .......... .......... .......... 61% 12.7M 3s\n     61050K .......... .......... .......... .......... .......... 62% 22.9M 3s\n     61100K .......... .......... .......... .......... .......... 62% 10.1M 3s\n     61150K .......... .......... .......... .......... .......... 62% 6.61M 3s\n     61200K .......... .......... .......... .......... .......... 62% 36.7M 3s\n     61250K .......... .......... .......... .......... .......... 62% 8.81M 3s\n     61300K .......... .......... .......... .......... .......... 62% 16.3M 3s\n     61350K .......... .......... .......... .......... .......... 62% 9.90M 3s\n     61400K .......... .......... .......... .......... .......... 62% 22.9M 3s\n     61450K .......... .......... .......... .......... .......... 62% 10.3M 3s\n     61500K .......... .......... .......... .......... .......... 62% 16.4M 3s\n     61550K .......... .......... .......... .......... .......... 62% 10.4M 3s\n     61600K .......... .......... .......... .......... .......... 62% 8.57M 3s\n     61650K .......... .......... .......... .......... .......... 62% 12.5M 3s\n     61700K .......... .......... .......... .......... .......... 62% 21.9M 3s\n     61750K .......... .......... .......... .......... .......... 62% 10.6M 3s\n     61800K .......... .......... .......... .......... .......... 62% 16.2M 3s\n     61850K .......... .......... .......... .......... .......... 62% 10.1M 3s\n     61900K .......... .......... .......... .......... .......... 62% 10.1M 3s\n     61950K .......... .......... .......... .......... .......... 62% 22.1M 3s\n     62000K .......... .......... .......... .......... .......... 62% 17.1M 3s\n     62050K .......... .......... .......... .......... .......... 63% 10.0M 3s\n     62100K .......... .......... .......... .......... .......... 63% 8.74M 3s\n     62150K .......... .......... .......... .......... .......... 63% 12.1M 3s\n     62200K .......... .......... .......... .......... .......... 63% 10.5M 3s\n     62250K .......... .......... .......... .......... .......... 63% 22.0M 3s\n     62300K .......... .......... .......... .......... .......... 63% 16.6M 2s\n     62350K .......... .......... .......... .......... .......... 63% 10.1M 2s\n     62400K .......... .......... .......... .......... .......... 63% 10.0M 2s\n     62450K .......... .......... .......... .......... .......... 63% 10.4M 2s\n     62500K .......... .......... .......... .......... .......... 63% 10.2M 2s\n     62550K .......... .......... .......... .......... .......... 63% 23.9M 2s\n     62600K .......... .......... .......... .......... .......... 63% 10.0M 2s\n     62650K .......... .......... .......... .......... .......... 63% 16.0M 2s\n     62700K .......... .......... .......... .......... .......... 63% 10.2M 2s\n     62750K .......... .......... .......... .......... .......... 63% 24.3M 2s\n     62800K .......... .......... .......... .......... .......... 63% 9.88M 2s\n     62850K .......... .......... .......... .......... .......... 63% 16.9M 2s\n     62900K .......... .......... .......... .......... .......... 63% 7.42M 2s\n     62950K .......... .......... .......... .......... .......... 63% 16.2M 2s\n     63000K .......... .......... .......... .......... .......... 63% 9.90M 2s\n     63050K .......... .......... .......... .......... .......... 64% 10.2M 2s\n     63100K .......... .......... .......... .......... .......... 64% 23.3M 2s\n     63150K .......... .......... .......... .......... .......... 64% 17.0M 2s\n     63200K .......... .......... .......... .......... .......... 64% 7.33M 2s\n     63250K .......... .......... .......... .......... .......... 64% 12.7M 2s\n     63300K .......... .......... .......... .......... .......... 64% 12.6M 2s\n     63350K .......... .......... .......... .......... .......... 64% 10.1M 2s\n     63400K .......... .......... .......... .......... .......... 64% 23.6M 2s\n     63450K .......... .......... .......... .......... .......... 64% 15.8M 2s\n     63500K .......... .......... .......... .......... .......... 64% 10.1M 2s\n     63550K .......... .......... .......... .......... .......... 64% 9.92M 2s\n     63600K .......... .......... .......... .......... .......... 64% 25.5M 2s\n     63650K .......... .......... .......... .......... .......... 64% 6.49M 2s\n     63700K .......... .......... .......... .......... .......... 64% 22.9M 2s\n     63750K .......... .......... .......... .......... .......... 64% 10.4M 2s\n     63800K .......... .......... .......... .......... .......... 64% 16.4M 2s\n     63850K .......... .......... .......... .......... .......... 64% 10.1M 2s\n     63900K .......... .......... .......... .......... .......... 64% 21.9M 2s\n     63950K .......... .......... .......... .......... .......... 64% 12.7M 2s\n     64000K .......... .......... .......... .......... .......... 65% 12.6M 2s\n     64050K .......... .......... .......... .......... .......... 65% 10.2M 2s\n     64100K .......... .......... .......... .......... .......... 65% 8.64M 2s\n     64150K .......... .......... .......... .......... .......... 65% 11.8M 2s\n     64200K .......... .......... .......... .......... .......... 65% 4.40M 2s\n     64250K .......... .......... .......... .......... .......... 65%  213M 2s\n     64300K .......... .......... .......... .......... .......... 65%  129M 2s\n     64350K .......... .......... .......... .......... .......... 65% 9.77M 2s\n     64400K .......... .......... .......... .......... .......... 65%  204M 2s\n     64450K .......... .......... .......... .......... .......... 65% 10.9M 2s\n     64500K .......... .......... .......... .......... .......... 65% 6.55M 2s\n     64550K .......... .......... .......... .......... .......... 65% 23.5M 2s\n     64600K .......... .......... .......... .......... .......... 65% 15.9M 2s\n     64650K .......... .......... .......... .......... .......... 65% 10.4M 2s\n     64700K .......... .......... .......... .......... .......... 65% 10.1M 2s\n     64750K .......... .......... .......... .......... .......... 65% 22.9M 2s\n     64800K .......... .......... .......... .......... .......... 65% 16.2M 2s\n     64850K .......... .......... .......... .......... .......... 65% 9.97M 2s\n     64900K .......... .......... .......... .......... .......... 65% 10.4M 2s\n     64950K .......... .......... .......... .......... .......... 65% 10.2M 2s\n     65000K .......... .......... .......... .......... .......... 66% 9.71M 2s\n     65050K .......... .......... .......... .......... .......... 66% 25.3M 2s\n     65100K .......... .......... .......... .......... .......... 66% 12.3M 2s\n     65150K .......... .......... .......... .......... .......... 66% 12.6M 2s\n     65200K .......... .......... .......... .......... .......... 66% 10.1M 2s\n     65250K .......... .......... .......... .......... .......... 66% 25.5M 2s\n     65300K .......... .......... .......... .......... .......... 66% 6.44M 2s\n     65350K .......... .......... .......... .......... .......... 66%  141M 2s\n     65400K .......... .......... .......... .......... .......... 66% 7.46M 2s\n     65450K .......... .......... .......... .......... .......... 66% 16.6M 2s\n     65500K .......... .......... .......... .......... .......... 66% 10.0M 2s\n     65550K .......... .......... .......... .......... .......... 66% 10.2M 2s\n     65600K .......... .......... .......... .......... .......... 66% 22.5M 2s\n     65650K .......... .......... .......... .......... .......... 66% 16.4M 2s\n     65700K .......... .......... .......... .......... .......... 66% 7.35M 2s\n     65750K .......... .......... .......... .......... .......... 66% 16.8M 2s\n     65800K .......... .......... .......... .......... .......... 66% 9.67M 2s\n     65850K .......... .......... .......... .......... .......... 66% 10.5M 2s\n     65900K .......... .......... .......... .......... .......... 66% 24.9M 2s\n     65950K .......... .......... .......... .......... .......... 66% 16.4M 2s\n     66000K .......... .......... .......... .......... .......... 67% 9.69M 2s\n     66050K .......... .......... .......... .......... .......... 67% 10.7M 2s\n     66100K .......... .......... .......... .......... .......... 67% 10.0M 2s\n     66150K .......... .......... .......... .......... .......... 67% 10.1M 2s\n     66200K .......... .......... .......... .......... .......... 67% 22.9M 2s\n     66250K .......... .......... .......... .......... .......... 67% 10.2M 2s\n     66300K .......... .......... .......... .......... .......... 67% 15.9M 2s\n     66350K .......... .......... .......... .......... .......... 67% 10.4M 2s\n     66400K .......... .......... .......... .......... .......... 67% 22.4M 2s\n     66450K .......... .......... .......... .......... .......... 67% 6.48M 2s\n     66500K .......... .......... .......... .......... .......... 67% 24.3M 2s\n     66550K .......... .......... .......... .......... .......... 67% 10.2M 2s\n     66600K .......... .......... .......... .......... .......... 67% 16.6M 2s\n     66650K .......... .......... .......... .......... .......... 67% 10.1M 2s\n     66700K .......... .......... .......... .......... .......... 67% 10.3M 2s\n     66750K .......... .......... .......... .......... .......... 67% 23.2M 2s\n     66800K .......... .......... .......... .......... .......... 67% 15.3M 2s\n     66850K .......... .......... .......... .......... .......... 67% 10.3M 2s\n     66900K .......... .......... .......... .......... .......... 67% 24.8M 2s\n     66950K .......... .......... .......... .......... .......... 68% 6.44M 2s\n     67000K .......... .......... .......... .......... .......... 68% 10.3M 2s\n     67050K .......... .......... .......... .......... .......... 68% 23.7M 2s\n     67100K .......... .......... .......... .......... .......... 68% 12.6M 2s\n     67150K .......... .......... .......... .......... .......... 68% 12.5M 2s\n     67200K .......... .......... .......... .......... .......... 68% 9.89M 2s\n     67250K .......... .......... .......... .......... .......... 68% 25.0M 2s\n     67300K .......... .......... .......... .......... .......... 68% 15.9M 2s\n     67350K .......... .......... .......... .......... .......... 68% 7.33M 2s\n     67400K .......... .......... .......... .......... .......... 68% 10.1M 2s\n     67450K .......... .......... .......... .......... .......... 68% 17.4M 2s\n     67500K .......... .......... .......... .......... .......... 68%  196M 2s\n     67550K .......... .......... .......... .......... .......... 68% 6.58M 2s\n     67600K .......... .......... .......... .......... .......... 68% 15.5M 2s\n     67650K .......... .......... .......... .......... .......... 68% 10.1M 2s\n     67700K .......... .......... .......... .......... .......... 68% 12.6M 2s\n     67750K .......... .......... .......... .......... .......... 68% 10.1M 2s\n     67800K .......... .......... .......... .......... .......... 68%  173M 2s\n     67850K .......... .......... .......... .......... .......... 68% 10.6M 2s\n     67900K .......... .......... .......... .......... .......... 68% 7.42M 2s\n     67950K .......... .......... .......... .......... .......... 69% 12.2M 2s\n     68000K .......... .......... .......... .......... .......... 69% 12.9M 2s\n     68050K .......... .......... .......... .......... .......... 69% 24.5M 2s\n     68100K .......... .......... .......... .......... .......... 69% 10.4M 2s\n     68150K .......... .......... .......... .......... .......... 69% 12.7M 2s\n     68200K .......... .......... .......... .......... .......... 69% 5.79M 2s\n     68250K .......... .......... .......... .......... .......... 69% 38.8M 2s\n     68300K .......... .......... .......... .......... .......... 69% 12.7M 2s\n     68350K .......... .......... .......... .......... .......... 69% 10.4M 2s\n     68400K .......... .......... .......... .......... .......... 69% 10.0M 2s\n     68450K .......... .......... .......... .......... .......... 69% 50.0M 2s\n     68500K .......... .......... .......... .......... .......... 69% 12.6M 2s\n     68550K .......... .......... .......... .......... .......... 69% 10.4M 2s\n     68600K .......... .......... .......... .......... .......... 69% 8.76M 2s\n     68650K .......... .......... .......... .......... .......... 69% 12.4M 2s\n     68700K .......... .......... .......... .......... .......... 69% 9.96M 2s\n     68750K .......... .......... .......... .......... .......... 69% 10.4M 2s\n     68800K .......... .......... .......... .......... .......... 69% 38.2M 2s\n     68850K .......... .......... .......... .......... .......... 69% 13.2M 2s\n     68900K .......... .......... .......... .......... .......... 69% 7.26M 2s\n     68950K .......... .......... .......... .......... .......... 70% 40.2M 2s\n     69000K .......... .......... .......... .......... .......... 70% 16.1M 2s\n     69050K .......... .......... .......... .......... .......... 70% 8.72M 2s\n     69100K .......... .......... .......... .......... .......... 70% 24.8M 2s\n     69150K .......... .......... .......... .......... .......... 70% 9.88M 2s\n     69200K .......... .......... .......... .......... .......... 70% 15.9M 2s\n     69250K .......... .......... .......... .......... .......... 70% 24.7M 2s\n     69300K .......... .......... .......... .......... .......... 70% 10.1M 2s\n     69350K .......... .......... .......... .......... .......... 70% 6.55M 2s\n     69400K .......... .......... .......... .......... .......... 70% 24.6M 2s\n     69450K .......... .......... .......... .......... .......... 70% 8.65M 2s\n     69500K .......... .......... .......... .......... .......... 70% 12.8M 2s\n     69550K .......... .......... .......... .......... .......... 70% 15.4M 2s\n     69600K .......... .......... .......... .......... .......... 70% 16.9M 2s\n     69650K .......... .......... .......... .......... .......... 70% 12.6M 2s\n     69700K .......... .......... .......... .......... .......... 70% 8.81M 2s\n     69750K .......... .......... .......... .......... .......... 70% 12.8M 2s\n     69800K .......... .......... .......... .......... .......... 70% 15.6M 2s\n     69850K .......... .......... .......... .......... .......... 70%  148M 2s\n     69900K .......... .......... .......... .......... .......... 71% 9.72M 2s\n     69950K .......... .......... .......... .......... .......... 71% 7.53M 2s\n     70000K .......... .......... .......... .......... .......... 71% 12.9M 2s\n     70050K .......... .......... .......... .......... .......... 71% 21.2M 2s\n     70100K .......... .......... .......... .......... .......... 71% 16.1M 2s\n     70150K .......... .......... .......... .......... .......... 71% 15.5M 2s\n     70200K .......... .......... .......... .......... .......... 71% 14.4M 2s\n     70250K .......... .......... .......... .......... .......... 71% 17.0M 2s\n     70300K .......... .......... .......... .......... .......... 71% 14.6M 2s\n     70350K .......... .......... .......... .......... .......... 71% 15.3M 2s\n     70400K .......... .......... .......... .......... .......... 71% 15.6M 2s\n     70450K .......... .......... .......... .......... .......... 71% 16.3M 2s\n     70500K .......... .......... .......... .......... .......... 71% 17.1M 2s\n     70550K .......... .......... .......... .......... .......... 71% 12.6M 2s\n     70600K .......... .......... .......... .......... .......... 71% 20.2M 2s\n     70650K .......... .......... .......... .......... .......... 71% 37.7M 2s\n     70700K .......... .......... .......... .......... .......... 71% 14.7M 2s\n     70750K .......... .......... .......... .......... .......... 71% 12.9M 2s\n     70800K .......... .......... .......... .......... .......... 71% 20.2M 2s\n     70850K .......... .......... .......... .......... .......... 71%  211M 2s\n     70900K .......... .......... .......... .......... .......... 72%  157M 2s\n     70950K .......... .......... .......... .......... .......... 72%  275M 2s\n     71000K .......... .......... .......... .......... .......... 72%  135M 2s\n     71050K .......... .......... .......... .......... .......... 72%  255M 2s\n     71100K .......... .......... .......... .......... .......... 72%  171M 2s\n     71150K .......... .......... .......... .......... .......... 72%  205M 2s\n     71200K .......... .......... .......... .......... .......... 72%  202M 2s\n     71250K .......... .......... .......... .......... .......... 72%  199M 2s\n     71300K .......... .......... .......... .......... .......... 72% 34.4M 2s\n     71350K .......... .......... .......... .......... .......... 72% 12.4M 2s\n     71400K .......... .......... .......... .......... .......... 72% 12.5M 2s\n     71450K .......... .......... .......... .......... .......... 72% 12.3M 2s\n     71500K .......... .......... .......... .......... .......... 72% 12.8M 2s\n     71550K .......... .......... .......... .......... .......... 72% 12.1M 2s\n     71600K .......... .......... .......... .......... .......... 72% 13.7M 2s\n     71650K .......... .......... .......... .......... .......... 72% 11.7M 2s\n     71700K .......... .......... .......... .......... .......... 72% 14.0M 2s\n     71750K .......... .......... .......... .......... .......... 72%  257K 2s\n     71800K .......... .......... .......... .......... .......... 72%  203M 2s\n     71850K .......... .......... .......... .......... .......... 72%  312M 2s\n     71900K .......... .......... .......... .......... .......... 73%  298M 2s\n     71950K .......... .......... .......... .......... .......... 73%  356M 2s\n     72000K .......... .......... .......... .......... .......... 73%  178M 2s\n     72050K .......... .......... .......... .......... .......... 73%  322M 2s\n     72100K .......... .......... .......... .......... .......... 73%  339M 2s\n     72150K .......... .......... .......... .......... .......... 73%  335M 2s\n     72200K .......... .......... .......... .......... .......... 73%  279M 2s\n     72250K .......... .......... .......... .......... .......... 73%  228M 2s\n     72300K .......... .......... .......... .......... .......... 73%  322M 2s\n     72350K .......... .......... .......... .......... .......... 73%  189M 2s\n     72400K .......... .......... .......... .......... .......... 73%  242M 2s\n     72450K .......... .......... .......... .......... .......... 73%  322M 2s\n     72500K .......... .......... .......... .......... .......... 73%  237M 2s\n     72550K .......... .......... .......... .......... .......... 73%  269M 2s\n     72600K .......... .......... .......... .......... .......... 73%  180M 2s\n     72650K .......... .......... .......... .......... .......... 73%  302M 2s\n     72700K .......... .......... .......... .......... .......... 73%  289M 2s\n     72750K .......... .......... .......... .......... .......... 73%  315M 2s\n     72800K .......... .......... .......... .......... .......... 73%  282M 2s\n     72850K .......... .......... .......... .......... .......... 73%  339M 2s\n     72900K .......... .......... .......... .......... .......... 74%  181M 2s\n     72950K .......... .......... .......... .......... .......... 74%  319M 2s\n     73000K .......... .......... .......... .......... .......... 74%  294M 2s\n     73050K .......... .......... .......... .......... .......... 74%  334M 2s\n     73100K .......... .......... .......... .......... .......... 74%  334M 2s\n     73150K .......... .......... .......... .......... .......... 74%  181M 2s\n     73200K .......... .......... .......... .......... .......... 74%  272M 2s\n     73250K .......... .......... .......... .......... .......... 74%  302M 2s\n     73300K .......... .......... .......... .......... .......... 74%  342M 2s\n     73350K .......... .......... .......... .......... .......... 74%  328M 2s\n     73400K .......... .......... .......... .......... .......... 74%  284M 2s\n     73450K .......... .......... .......... .......... .......... 74%  182M 2s\n     73500K .......... .......... .......... .......... .......... 74%  157M 2s\n     73550K .......... .......... .......... .......... .......... 74%  313M 2s\n     73600K .......... .......... .......... .......... .......... 74% 6.84M 2s\n     73650K .......... .......... .......... .......... .......... 74% 12.7M 2s\n     73700K .......... .......... .......... .......... .......... 74% 15.4M 2s\n     73750K .......... .......... .......... .......... .......... 74% 12.7M 2s\n     73800K .......... .......... .......... .......... .......... 74% 12.3M 2s\n     73850K .......... .......... .......... .......... .......... 75% 16.0M 2s\n     73900K .......... .......... .......... .......... .......... 75% 12.0M 2s\n     73950K .......... .......... .......... .......... .......... 75% 11.3M 2s\n     74000K .......... .......... .......... .......... .......... 75% 24.1M 2s\n     74050K .......... .......... .......... .......... .......... 75% 11.5M 2s\n     74100K .......... .......... .......... .......... .......... 75% 14.9M 2s\n     74150K .......... .......... .......... .......... .......... 75% 14.2M 2s\n     74200K .......... .......... .......... .......... .......... 75% 11.3M 2s\n     74250K .......... .......... .......... .......... .......... 75% 11.5M 2s\n     74300K .......... .......... .......... .......... .......... 75% 24.0M 2s\n     74350K .......... .......... .......... .......... .......... 75% 14.5M 2s\n     74400K .......... .......... .......... .......... .......... 75% 21.8M 2s\n     74450K .......... .......... .......... .......... .......... 75% 9.32M 2s\n     74500K .......... .......... .......... .......... .......... 75% 12.3M 2s\n     74550K .......... .......... .......... .......... .......... 75% 17.3M 2s\n     74600K .......... .......... .......... .......... .......... 75% 8.11M 2s\n     74650K .......... .......... .......... .......... .......... 75% 12.9M 2s\n     74700K .......... .......... .......... .......... .......... 75% 35.3M 2s\n     74750K .......... .......... .......... .......... .......... 75% 7.05M 2s\n     74800K .......... .......... .......... .......... .......... 75%  168M 2s\n     74850K .......... .......... .......... .......... .......... 76% 6.15M 2s\n     74900K .......... .......... .......... .......... .......... 76%  260M 2s\n     74950K .......... .......... .......... .......... .......... 76%  273M 2s\n     75000K .......... .......... .......... .......... .......... 76%  160M 2s\n     75050K .......... .......... .......... .......... .......... 76%  244M 2s\n     75100K .......... .......... .......... .......... .......... 76%  282M 2s\n     75150K .......... .......... .......... .......... .......... 76% 2.02M 2s\n     75200K .......... .......... .......... .......... .......... 76% 8.22M 2s\n     75250K .......... .......... .......... .......... .......... 76% 14.7M 2s\n     75300K .......... .......... .......... .......... .......... 76% 29.6M 2s\n     75350K .......... .......... .......... .......... .......... 76% 13.4M 2s\n     75400K .......... .......... .......... .......... .......... 76% 20.2M 2s\n     75450K .......... .......... .......... .......... .......... 76% 17.1M 2s\n     75500K .......... .......... .......... .......... .......... 76% 31.0M 2s\n     75550K .......... .......... .......... .......... .......... 76% 9.87M 2s\n     75600K .......... .......... .......... .......... .......... 76% 34.5M 2s\n     75650K .......... .......... .......... .......... .......... 76% 15.3M 2s\n     75700K .......... .......... .......... .......... .......... 76% 14.7M 2s\n     75750K .......... .......... .......... .......... .......... 76% 13.7M 2s\n     75800K .......... .......... .......... .......... .......... 76% 41.9M 2s\n     75850K .......... .......... .......... .......... .......... 77% 21.4M 2s\n     75900K .......... .......... .......... .......... .......... 77% 18.4M 2s\n     75950K .......... .......... .......... .......... .......... 77% 19.5M 2s\n     76000K .......... .......... .......... .......... .......... 77% 25.6M 2s\n     76050K .......... .......... .......... .......... .......... 77%  282M 2s\n     76100K .......... .......... .......... .......... .......... 77%  146M 2s\n     76150K .......... .......... .......... .......... .......... 77%  193M 2s\n     76200K .......... .......... .......... .......... .......... 77%  229M 2s\n     76250K .......... .......... .......... .......... .......... 77%  148M 2s\n     76300K .......... .......... .......... .......... .......... 77%  167M 2s\n     76350K .......... .......... .......... .......... .......... 77%  164M 2s\n     76400K .......... .......... .......... .......... .......... 77%  241M 2s\n     76450K .......... .......... .......... .......... .......... 77% 54.0M 2s\n     76500K .......... .......... .......... .......... .......... 77% 10.1M 2s\n     76550K .......... .......... .......... .......... .......... 77% 12.6M 2s\n     76600K .......... .......... .......... .......... .......... 77% 13.9M 2s\n     76650K .......... .......... .......... .......... .......... 77% 13.4M 2s\n     76700K .......... .......... .......... .......... .......... 77% 13.7M 2s\n     76750K .......... .......... .......... .......... .......... 77% 13.4M 2s\n     76800K .......... .......... .......... .......... .......... 78% 13.6M 2s\n     76850K .......... .......... .......... .......... .......... 78% 14.1M 2s\n     76900K .......... .......... .......... .......... .......... 78% 14.2M 2s\n     76950K .......... .......... .......... .......... .......... 78% 13.6M 2s\n     77000K .......... .......... .......... .......... .......... 78% 14.9M 2s\n     77050K .......... .......... .......... .......... .......... 78% 14.2M 2s\n     77100K .......... .......... .......... .......... .......... 78% 12.5M 2s\n     77150K .......... .......... .......... .......... .......... 78% 14.5M 1s\n     77200K .......... .......... .......... .......... .......... 78% 14.4M 1s\n     77250K .......... .......... .......... .......... .......... 78% 14.9M 1s\n     77300K .......... .......... .......... .......... .......... 78% 11.5M 1s\n     77350K .......... .......... .......... .......... .......... 78% 14.3M 1s\n     77400K .......... .......... .......... .......... .......... 78% 16.4M 1s\n     77450K .......... .......... .......... .......... .......... 78% 11.0M 1s\n     77500K .......... .......... .......... .......... .......... 78% 15.3M 1s\n     77550K .......... .......... .......... .......... .......... 78% 14.5M 1s\n     77600K .......... .......... .......... .......... .......... 78% 14.5M 1s\n     77650K .......... .......... .......... .......... .......... 78% 11.8M 1s\n     77700K .......... .......... .......... .......... .......... 78%  709K 1s\n     77750K .......... .......... .......... .......... .......... 78% 13.6M 1s\n     77800K .......... .......... .......... .......... .......... 79% 13.2M 1s\n     77850K .......... .......... .......... .......... .......... 79% 13.6M 1s\n     77900K .......... .......... .......... .......... .......... 79% 29.9M 1s\n     77950K .......... .......... .......... .......... .......... 79% 12.1M 1s\n     78000K .......... .......... .......... .......... .......... 79% 15.2M 1s\n     78050K .......... .......... .......... .......... .......... 79% 14.2M 1s\n     78100K .......... .......... .......... .......... .......... 79% 12.2M 1s\n     78150K .......... .......... .......... .......... .......... 79% 11.8M 1s\n     78200K .......... .......... .......... .......... .......... 79% 23.5M 1s\n     78250K .......... .......... .......... .......... .......... 79% 12.4M 1s\n     78300K .......... .......... .......... .......... .......... 79% 15.6M 1s\n     78350K .......... .......... .......... .......... .......... 79% 13.9M 1s\n     78400K .......... .......... .......... .......... .......... 79% 12.0M 1s\n     78450K .......... .......... .......... .......... .......... 79% 27.0M 1s\n     78500K .......... .......... .......... .......... .......... 79% 13.3M 1s\n     78550K .......... .......... .......... .......... .......... 79% 15.2M 1s\n     78600K .......... .......... .......... .......... .......... 79% 17.9M 1s\n     78650K .......... .......... .......... .......... .......... 79%  707K 1s\n     78700K .......... .......... .......... .......... .......... 79%  236M 1s\n     78750K .......... .......... .......... .......... .......... 79%  338M 1s\n     78800K .......... .......... .......... .......... .......... 80%  326M 1s\n     78850K .......... .......... .......... .......... .......... 80%  301M 1s\n     78900K .......... .......... .......... .......... .......... 80%  206M 1s\n     78950K .......... .......... .......... .......... .......... 80%  204M 1s\n     79000K .......... .......... .......... .......... .......... 80%  261M 1s\n     79050K .......... .......... .......... .......... .......... 80%  156M 1s\n     79100K .......... .......... .......... .......... .......... 80%  239M 1s\n     79150K .......... .......... .......... .......... .......... 80%  285M 1s\n     79200K .......... .......... .......... .......... .......... 80%  275M 1s\n     79250K .......... .......... .......... .......... .......... 80%  165M 1s\n     79300K .......... .......... .......... .......... .......... 80%  242M 1s\n     79350K .......... .......... .......... .......... .......... 80%  234M 1s\n     79400K .......... .......... .......... .......... .......... 80%  111M 1s\n     79450K .......... .......... .......... .......... .......... 80% 10.5M 1s\n     79500K .......... .......... .......... .......... .......... 80% 10.4M 1s\n     79550K .......... .......... .......... .......... .......... 80% 13.4M 1s\n     79600K .......... .......... .......... .......... .......... 80% 13.2M 1s\n     79650K .......... .......... .......... .......... .......... 80% 17.3M 1s\n     79700K .......... .......... .......... .......... .......... 80% 29.4M 1s\n     79750K .......... .......... .......... .......... .......... 80% 13.0M 1s\n     79800K .......... .......... .......... .......... .......... 81% 13.5M 1s\n     79850K .......... .......... .......... .......... .......... 81% 16.7M 1s\n     79900K .......... .......... .......... .......... .......... 81% 10.6M 1s\n     79950K .......... .......... .......... .......... .......... 81% 13.2M 1s\n     80000K .......... .......... .......... .......... .......... 81% 17.4M 1s\n     80050K .......... .......... .......... .......... .......... 81% 13.3M 1s\n     80100K .......... .......... .......... .......... .......... 81% 13.1M 1s\n     80150K .......... .......... .......... .......... .......... 81% 13.1M 1s\n     80200K .......... .......... .......... .......... .......... 81% 30.1M 1s\n     80250K .......... .......... .......... .......... .......... 81% 18.0M 1s\n     80300K .......... .......... .......... .......... .......... 81% 11.6M 1s\n     80350K .......... .......... .......... .......... .......... 81% 10.6M 1s\n     80400K .......... .......... .......... .......... .......... 81% 24.8M 1s\n     80450K .......... .......... .......... .......... .......... 81% 12.3M 1s\n     80500K .......... .......... .......... .......... .......... 81% 15.5M 1s\n     80550K .......... .......... .......... .......... .......... 81% 17.3M 1s\n     80600K .......... .......... .......... .......... .......... 81% 16.8M 1s\n     80650K .......... .......... .......... .......... .......... 81% 13.8M 1s\n     80700K .......... .......... .......... .......... .......... 81% 24.7M 1s\n     80750K .......... .......... .......... .......... .......... 82% 10.4M 1s\n     80800K .......... .......... .......... .......... .......... 82% 17.2M 1s\n     80850K .......... .......... .......... .......... .......... 82% 14.1M 1s\n     80900K .......... .......... .......... .......... .......... 82% 14.2M 1s\n     80950K .......... .......... .......... .......... .......... 82% 21.5M 1s\n     81000K .......... .......... .......... .......... .......... 82% 23.6M 1s\n     81050K .......... .......... .......... .......... .......... 82% 11.7M 1s\n     81100K .......... .......... .......... .......... .......... 82% 36.1M 1s\n     81150K .......... .......... .......... .......... .......... 82% 11.8M 1s\n     81200K .......... .......... .......... .......... .......... 82% 15.1M 1s\n     81250K .......... .......... .......... .......... .......... 82% 25.9M 1s\n     81300K .......... .......... .......... .......... .......... 82%  143M 1s\n     81350K .......... .......... .......... .......... .......... 82%  206M 1s\n     81400K .......... .......... .......... .......... .......... 82%  142M 1s\n     81450K .......... .......... .......... .......... .......... 82%  193M 1s\n     81500K .......... .......... .......... .......... .......... 82%  174M 1s\n     81550K .......... .......... .......... .......... .......... 82%  157M 1s\n     81600K .......... .......... .......... .......... .......... 82%  265M 1s\n     81650K .......... .......... .......... .......... .......... 82%  298M 1s\n     81700K .......... .......... .......... .......... .......... 82%  339M 1s\n     81750K .......... .......... .......... .......... .......... 83% 24.0M 1s\n     81800K .......... .......... .......... .......... .......... 83% 14.0M 1s\n     81850K .......... .......... .......... .......... .......... 83% 14.0M 1s\n     81900K .......... .......... .......... .......... .......... 83% 15.8M 1s\n     81950K .......... .......... .......... .......... .......... 83%  698K 1s\n     82000K .......... .......... .......... .......... .......... 83% 10.8M 1s\n     82050K .......... .......... .......... .......... .......... 83% 13.6M 1s\n     82100K .......... .......... .......... .......... .......... 83% 30.4M 1s\n     82150K .......... .......... .......... .......... .......... 83% 13.7M 1s\n     82200K .......... .......... .......... .......... .......... 83% 12.5M 1s\n     82250K .......... .......... .......... .......... .......... 83% 19.7M 1s\n     82300K .......... .......... .......... .......... .......... 83% 10.8M 1s\n     82350K .......... .......... .......... .......... .......... 83% 16.6M 1s\n     82400K .......... .......... .......... .......... .......... 83% 13.5M 1s\n     82450K .......... .......... .......... .......... .......... 83% 14.5M 1s\n     82500K .......... .......... .......... .......... .......... 83% 13.7M 1s\n     82550K .......... .......... .......... .......... .......... 83% 13.2M 1s\n     82600K .......... .......... .......... .......... .......... 83% 10.3M 1s\n     82650K .......... .......... .......... .......... .......... 83%  108M 1s\n     82700K .......... .......... .......... .......... .......... 83% 13.0M 1s\n     82750K .......... .......... .......... .......... .......... 84% 14.5M 1s\n     82800K .......... .......... .......... .......... .......... 84% 13.2M 1s\n     82850K .......... .......... .......... .......... .......... 84% 10.7M 1s\n     82900K .......... .......... .......... .......... .......... 84% 79.4M 1s\n     82950K .......... .......... .......... .......... .......... 84% 9.13M 1s\n     83000K .......... .......... .......... .......... .......... 84% 12.4M 1s\n     83050K .......... .......... .......... .......... .......... 84% 18.3M 1s\n     83100K .......... .......... .......... .......... .......... 84% 10.2M 1s\n     83150K .......... .......... .......... .......... .......... 84% 11.3M 1s\n     83200K .......... .......... .......... .......... .......... 84% 15.3M 1s\n     83250K .......... .......... .......... .......... .......... 84% 9.99M 1s\n     83300K .......... .......... .......... .......... .......... 84% 10.9M 1s\n     83350K .......... .......... .......... .......... .......... 84% 18.6M 1s\n     83400K .......... .......... .......... .......... .......... 84% 9.75M 1s\n     83450K .......... .......... .......... .......... .......... 84% 12.4M 1s\n     83500K .......... .......... .......... .......... .......... 84% 17.2M 1s\n     83550K .......... .......... .......... .......... .......... 84% 12.2M 1s\n     83600K .......... .......... .......... .......... .......... 84% 11.7M 1s\n     83650K .......... .......... .......... .......... .......... 84% 13.3M 1s\n     83700K .......... .......... .......... .......... .......... 85% 8.62M 1s\n     83750K .......... .......... .......... .......... .......... 85% 12.8M 1s\n     83800K .......... .......... .......... .......... .......... 85% 15.7M 1s\n     83850K .......... .......... .......... .......... .......... 85% 13.3M 1s\n     83900K .......... .......... .......... .......... .......... 85% 11.9M 1s\n     83950K .......... .......... .......... .......... .......... 85% 10.1M 1s\n     84000K .......... .......... .......... .......... .......... 85% 19.6M 1s\n     84050K .......... .......... .......... .......... .......... 85% 8.45M 1s\n     84100K .......... .......... .......... .......... .......... 85% 19.6M 1s\n     84150K .......... .......... .......... .......... .......... 85% 7.19M 1s\n     84200K .......... .......... .......... .......... .......... 85% 10.2M 1s\n     84250K .......... .......... .......... .......... .......... 85% 26.0M 1s\n     84300K .......... .......... .......... .......... .......... 85% 8.57M 1s\n     84350K .......... .......... .......... .......... .......... 85% 19.5M 1s\n     84400K .......... .......... .......... .......... .......... 85% 10.5M 1s\n     84450K .......... .......... .......... .......... .......... 85% 12.5M 1s\n     84500K .......... .......... .......... .......... .......... 85% 10.9M 1s\n     84550K .......... .......... .......... .......... .......... 85% 12.6M 1s\n     84600K .......... .......... .......... .......... .......... 85% 20.6M 1s\n     84650K .......... .......... .......... .......... .......... 85% 12.4M 1s\n     84700K .......... .......... .......... .......... .......... 86% 11.2M 1s\n     84750K .......... .......... .......... .......... .......... 86% 12.6M 1s\n     84800K .......... .......... .......... .......... .......... 86% 9.39M 1s\n     84850K .......... .......... .......... .......... .......... 86% 13.3M 1s\n     84900K .......... .......... .......... .......... .......... 86% 13.0M 1s\n     84950K .......... .......... .......... .......... .......... 86% 12.1M 1s\n     85000K .......... .......... .......... .......... .......... 86% 17.5M 1s\n     85050K .......... .......... .......... .......... .......... 86% 9.45M 1s\n     85100K .......... .......... .......... .......... .......... 86% 13.8M 1s\n     85150K .......... .......... .......... .......... .......... 86% 11.9M 1s\n     85200K .......... .......... .......... .......... .......... 86% 13.1M 1s\n     85250K .......... .......... .......... .......... .......... 86% 12.6M 1s\n     85300K .......... .......... .......... .......... .......... 86% 12.6M 1s\n     85350K .......... .......... .......... .......... .......... 86% 13.2M 1s\n     85400K .......... .......... .......... .......... .......... 86% 11.2M 1s\n     85450K .......... .......... .......... .......... .......... 86% 30.5M 1s\n     85500K .......... .......... .......... .......... .......... 86% 10.4M 1s\n     85550K .......... .......... .......... .......... .......... 86% 12.4M 1s\n     85600K .......... .......... .......... .......... .......... 86% 14.5M 1s\n     85650K .......... .......... .......... .......... .......... 86% 8.80M 1s\n     85700K .......... .......... .......... .......... .......... 87% 41.2M 1s\n     85750K .......... .......... .......... .......... .......... 87% 18.9M 1s\n     85800K .......... .......... .......... .......... .......... 87% 13.5M 1s\n     85850K .......... .......... .......... .......... .......... 87% 14.7M 1s\n     85900K .......... .......... .......... .......... .......... 87% 13.4M 1s\n     85950K .......... .......... .......... .......... .......... 87% 11.9M 1s\n     86000K .......... .......... .......... .......... .......... 87% 32.9M 1s\n     86050K .......... .......... .......... .......... .......... 87% 15.4M 1s\n     86100K .......... .......... .......... .......... .......... 87% 14.2M 1s\n     86150K .......... .......... .......... .......... .......... 87% 16.3M 1s\n     86200K .......... .......... .......... .......... .......... 87% 10.9M 1s\n     86250K .......... .......... .......... .......... .......... 87% 24.0M 1s\n     86300K .......... .......... .......... .......... .......... 87% 17.3M 1s\n     86350K .......... .......... .......... .......... .......... 87% 32.8M 1s\n     86400K .......... .......... .......... .......... .......... 87% 14.9M 1s\n     86450K .......... .......... .......... .......... .......... 87% 31.7M 1s\n     86500K .......... .......... .......... .......... .......... 87%  185M 1s\n     86550K .......... .......... .......... .......... .......... 87%  162M 1s\n     86600K .......... .......... .......... .......... .......... 87%  182M 1s\n     86650K .......... .......... .......... .......... .......... 88%  178M 1s\n     86700K .......... .......... .......... .......... .......... 88%  211M 1s\n     86750K .......... .......... .......... .......... .......... 88%  157M 1s\n     86800K .......... .......... .......... .......... .......... 88%  159M 1s\n     86850K .......... .......... .......... .......... .......... 88%  224M 1s\n     86900K .......... .......... .......... .......... .......... 88%  145M 1s\n     86950K .......... .......... .......... .......... .......... 88% 13.5M 1s\n     87000K .......... .......... .......... .......... .......... 88% 14.1M 1s\n     87050K .......... .......... .......... .......... .......... 88% 13.6M 1s\n     87100K .......... .......... .......... .......... .......... 88% 15.2M 1s\n     87150K .......... .......... .......... .......... .......... 88% 15.2M 1s\n     87200K .......... .......... .......... .......... .......... 88% 14.0M 1s\n     87250K .......... .......... .......... .......... .......... 88%  696K 1s\n     87300K .......... .......... .......... .......... .......... 88% 17.1M 1s\n     87350K .......... .......... .......... .......... .......... 88% 13.5M 1s\n     87400K .......... .......... .......... .......... .......... 88% 26.4M 1s\n     87450K .......... .......... .......... .......... .......... 88% 15.2M 1s\n     87500K .......... .......... .......... .......... .......... 88% 11.9M 1s\n     87550K .......... .......... .......... .......... .......... 88% 13.4M 1s\n     87600K .......... .......... .......... .......... .......... 88% 12.1M 1s\n     87650K .......... .......... .......... .......... .......... 89% 26.7M 1s\n     87700K .......... .......... .......... .......... .......... 89% 11.3M 1s\n     87750K .......... .......... .......... .......... .......... 89% 15.5M 1s\n     87800K .......... .......... .......... .......... .......... 89% 13.7M 1s\n     87850K .......... .......... .......... .......... .......... 89% 11.0M 1s\n     87900K .......... .......... .......... .......... .......... 89% 12.9M 1s\n     87950K .......... .......... .......... .......... .......... 89% 25.4M 1s\n     88000K .......... .......... .......... .......... .......... 89% 15.2M 1s\n     88050K .......... .......... .......... .......... .......... 89% 19.0M 1s\n     88100K .......... .......... .......... .......... .......... 89% 13.9M 1s\n     88150K .......... .......... .......... .......... .......... 89% 12.3M 1s\n     88200K .......... .......... .......... .......... .......... 89% 18.6M 1s\n     88250K .......... .......... .......... .......... .......... 89% 8.47M 1s\n     88300K .......... .......... .......... .......... .......... 89% 13.4M 1s\n     88350K .......... .......... .......... .......... .......... 89% 14.9M 1s\n     88400K .......... .......... .......... .......... .......... 89%  109M 1s\n     88450K .......... .......... .......... .......... .......... 89% 5.05M 1s\n     88500K .......... .......... .......... .......... .......... 89%  129M 1s\n     88550K .......... .......... .......... .......... .......... 89%  224M 1s\n     88600K .......... .......... .......... .......... .......... 89%  183M 1s\n     88650K .......... .......... .......... .......... .......... 90%  160M 1s\n     88700K .......... .......... .......... .......... .......... 90%  269M 1s\n     88750K .......... .......... .......... .......... .......... 90%  252M 1s\n     88800K .......... .......... .......... .......... .......... 90% 1.65M 1s\n     88850K .......... .......... .......... .......... .......... 90% 5.92M 1s\n     88900K .......... .......... .......... .......... .......... 90% 16.5M 1s\n     88950K .......... .......... .......... .......... .......... 90%  686K 1s\n     89000K .......... .......... .......... .......... .......... 90%  145M 1s\n     89050K .......... .......... .......... .......... .......... 90%  189M 1s\n     89100K .......... .......... .......... .......... .......... 90%  192M 1s\n     89150K .......... .......... .......... .......... .......... 90% 91.0M 1s\n     89200K .......... .......... .......... .......... .......... 90%  133M 1s\n     89250K .......... .......... .......... .......... .......... 90%  181M 1s\n     89300K .......... .......... .......... .......... .......... 90%  246M 1s\n     89350K .......... .......... .......... .......... .......... 90%  213M 1s\n     89400K .......... .......... .......... .......... .......... 90%  115M 1s\n     89450K .......... .......... .......... .......... .......... 90%  199M 1s\n     89500K .......... .......... .......... .......... .......... 90%  255M 1s\n     89550K .......... .......... .......... .......... .......... 90%  159M 1s\n     89600K .......... .......... .......... .......... .......... 90%  223M 1s\n     89650K .......... .......... .......... .......... .......... 91%  178M 1s\n     89700K .......... .......... .......... .......... .......... 91%  235M 1s\n     89750K .......... .......... .......... .......... .......... 91%  140M 1s\n     89800K .......... .......... .......... .......... .......... 91%  126M 1s\n     89850K .......... .......... .......... .......... .......... 91%  224M 1s\n     89900K .......... .......... .......... .......... .......... 91%  190M 1s\n     89950K .......... .......... .......... .......... .......... 91% 32.2M 1s\n     90000K .......... .......... .......... .......... .......... 91% 7.07M 1s\n     90050K .......... .......... .......... .......... .......... 91%  109M 1s\n     90100K .......... .......... .......... .......... .......... 91% 11.5M 1s\n     90150K .......... .......... .......... .......... .......... 91% 11.7M 1s\n     90200K .......... .......... .......... .......... .......... 91% 12.8M 1s\n     90250K .......... .......... .......... .......... .......... 91% 12.3M 1s\n     90300K .......... .......... .......... .......... .......... 91% 13.3M 1s\n     90350K .......... .......... .......... .......... .......... 91% 13.3M 1s\n     90400K .......... .......... .......... .......... .......... 91% 12.8M 1s\n     90450K .......... .......... .......... .......... .......... 91% 12.7M 1s\n     90500K .......... .......... .......... .......... .......... 91% 16.7M 1s\n     90550K .......... .......... .......... .......... .......... 91% 13.3M 1s\n     90600K .......... .......... .......... .......... .......... 92% 11.6M 1s\n     90650K .......... .......... .......... .......... .......... 92% 11.7M 1s\n     90700K .......... .......... .......... .......... .......... 92% 11.3M 1s\n     90750K .......... .......... .......... .......... .......... 92% 19.8M 1s\n     90800K .......... .......... .......... .......... .......... 92% 13.3M 1s\n     90850K .......... .......... .......... .......... .......... 92% 13.2M 1s\n     90900K .......... .......... .......... .......... .......... 92% 17.9M 1s\n     90950K .......... .......... .......... .......... .......... 92% 13.4M 1s\n     91000K .......... .......... .......... .......... .......... 92% 44.1M 1s\n     91050K .......... .......... .......... .......... .......... 92% 20.1M 1s\n     91100K .......... .......... .......... .......... .......... 92% 12.2M 1s\n     91150K .......... .......... .......... .......... .......... 92% 11.5M 1s\n     91200K .......... .......... .......... .......... .......... 92% 14.0M 1s\n     91250K .......... .......... .......... .......... .......... 92% 15.3M 1s\n     91300K .......... .......... .......... .......... .......... 92% 37.9M 1s\n     91350K .......... .......... .......... .......... .......... 92% 22.5M 1s\n     91400K .......... .......... .......... .......... .......... 92% 12.3M 1s\n     91450K .......... .......... .......... .......... .......... 92% 25.0M 1s\n     91500K .......... .......... .......... .......... .......... 92% 15.8M 1s\n     91550K .......... .......... .......... .......... .......... 92% 93.1M 0s\n     91600K .......... .......... .......... .......... .......... 93%  110M 0s\n     91650K .......... .......... .......... .......... .......... 93%  118M 0s\n     91700K .......... .......... .......... .......... .......... 93%  130M 0s\n     91750K .......... .......... .......... .......... .......... 93%  137M 0s\n     91800K .......... .......... .......... .......... .......... 93%  116M 0s\n     91850K .......... .......... .......... .......... .......... 93%  103M 0s\n     91900K .......... .......... .......... .......... .......... 93%  121M 0s\n     91950K .......... .......... .......... .......... .......... 93%  108M 0s\n     92000K .......... .......... .......... .......... .......... 93% 12.5M 0s\n     92050K .......... .......... .......... .......... .......... 93% 12.8M 0s\n     92100K .......... .......... .......... .......... .......... 93% 13.8M 0s\n     92150K .......... .......... .......... .......... .......... 93% 14.0M 0s\n     92200K .......... .......... .......... .......... .......... 93% 15.1M 0s\n     92250K .......... .......... .......... .......... .......... 93% 13.5M 0s\n     92300K .......... .......... .......... .......... .......... 93% 15.1M 0s\n     92350K .......... .......... .......... .......... .......... 93% 15.8M 0s\n     92400K .......... .......... .......... .......... .......... 93% 12.1M 0s\n     92450K .......... .......... .......... .......... .......... 93% 14.6M 0s\n     92500K .......... .......... .......... .......... .......... 93% 17.2M 0s\n     92550K .......... .......... .......... .......... .......... 93% 11.7M 0s\n     92600K .......... .......... .......... .......... .......... 94% 15.2M 0s\n     92650K .......... .......... .......... .......... .......... 94% 14.9M 0s\n     92700K .......... .......... .......... .......... .......... 94% 14.9M 0s\n     92750K .......... .......... .......... .......... .......... 94% 13.5M 0s\n     92800K .......... .......... .......... .......... .......... 94% 14.7M 0s\n     92850K .......... .......... .......... .......... .......... 94%  699K 0s\n     92900K .......... .......... .......... .......... .......... 94% 14.1M 0s\n     92950K .......... .......... .......... .......... .......... 94% 14.5M 0s\n     93000K .......... .......... .......... .......... .......... 94% 28.6M 0s\n     93050K .......... .......... .......... .......... .......... 94% 14.1M 0s\n     93100K .......... .......... .......... .......... .......... 94% 16.2M 0s\n     93150K .......... .......... .......... .......... .......... 94% 14.1M 0s\n     93200K .......... .......... .......... .......... .......... 94% 11.8M 0s\n     93250K .......... .......... .......... .......... .......... 94% 12.0M 0s\n     93300K .......... .......... .......... .......... .......... 94% 25.7M 0s\n     93350K .......... .......... .......... .......... .......... 94% 12.3M 0s\n     93400K .......... .......... .......... .......... .......... 94% 14.9M 0s\n     93450K .......... .......... .......... .......... .......... 94% 15.0M 0s\n     93500K .......... .......... .......... .......... .......... 94% 12.6M 0s\n     93550K .......... .......... .......... .......... .......... 95% 24.9M 0s\n     93600K .......... .......... .......... .......... .......... 95% 11.5M 0s\n     93650K .......... .......... .......... .......... .......... 95% 12.8M 0s\n     93700K .......... .......... .......... .......... .......... 95% 19.8M 0s\n     93750K .......... .......... .......... .......... .......... 95% 11.8M 0s\n     93800K .......... .......... .......... .......... .......... 95% 16.1M 0s\n     93850K .......... .......... .......... .......... .......... 95% 18.9M 0s\n     93900K .......... .......... .......... .......... .......... 95%  672K 0s\n     93950K .......... .......... .......... .......... .......... 95% 13.3M 0s\n     94000K .......... .......... .......... .......... .......... 95%  226M 0s\n     94050K .......... .......... .......... .......... .......... 95%  210M 0s\n     94100K .......... .......... .......... .......... .......... 95%  241M 0s\n     94150K .......... .......... .......... .......... .......... 95%  184M 0s\n     94200K .......... .......... .......... .......... .......... 95%  201M 0s\n     94250K .......... .......... .......... .......... .......... 95%  216M 0s\n     94300K .......... .......... .......... .......... .......... 95%  150M 0s\n     94350K .......... .......... .......... .......... .......... 95%  250M 0s\n     94400K .......... .......... .......... .......... .......... 95%  254M 0s\n     94450K .......... .......... .......... .......... .......... 95%  225M 0s\n     94500K .......... .......... .......... .......... .......... 95%  175M 0s\n     94550K .......... .......... .......... .......... .......... 96%  281M 0s\n     94600K .......... .......... .......... .......... .......... 96%  267M 0s\n     94650K .......... .......... .......... .......... .......... 96%  237M 0s\n     94700K .......... .......... .......... .......... .......... 96%  290M 0s\n     94750K .......... .......... .......... .......... .......... 96% 20.9M 0s\n     94800K .......... .......... .......... .......... .......... 96% 31.3M 0s\n     94850K .......... .......... .......... .......... .......... 96% 8.66M 0s\n     94900K .......... .......... .......... .......... .......... 96% 23.0M 0s\n     94950K .......... .......... .......... .......... .......... 96% 8.60M 0s\n     95000K .......... .......... .......... .......... .......... 96% 14.1M 0s\n     95050K .......... .......... .......... .......... .......... 96% 40.6M 0s\n     95100K .......... .......... .......... .......... .......... 96% 12.4M 0s\n     95150K .......... .......... .......... .......... .......... 96% 12.4M 0s\n     95200K .......... .......... .......... .......... .......... 96% 13.1M 0s\n     95250K .......... .......... .......... .......... .......... 96%  113M 0s\n     95300K .......... .......... .......... .......... .......... 96% 13.3M 0s\n     95350K .......... .......... .......... .......... .......... 96% 8.16M 0s\n     95400K .......... .......... .......... .......... .......... 96% 95.9M 0s\n     95450K .......... .......... .......... .......... .......... 96% 10.7M 0s\n     95500K .......... .......... .......... .......... .......... 96% 51.1M 0s\n     95550K .......... .......... .......... .......... .......... 97% 7.12M 0s\n     95600K .......... .......... .......... .......... .......... 97% 12.8M 0s\n     95650K .......... .......... .......... .......... .......... 97% 17.2M 0s\n     95700K .......... .......... .......... .......... .......... 97% 17.2M 0s\n     95750K .......... .......... .......... .......... .......... 97% 11.6M 0s\n     95800K .......... .......... .......... .......... .......... 97% 20.6M 0s\n     95850K .......... .......... .......... .......... .......... 97% 9.50M 0s\n     95900K .......... .......... .......... .......... .......... 97% 15.9M 0s\n     95950K .......... .......... .......... .......... .......... 97% 14.7M 0s\n     96000K .......... .......... .......... .......... .......... 97% 14.8M 0s\n     96050K .......... .......... .......... .......... .......... 97% 19.7M 0s\n     96100K .......... .......... .......... .......... .......... 97% 16.9M 0s\n     96150K .......... .......... .......... .......... .......... 97% 13.4M 0s\n     96200K .......... .......... .......... .......... .......... 97% 11.2M 0s\n     96250K .......... .......... .......... .......... .......... 97% 24.8M 0s\n     96300K .......... .......... .......... .......... .......... 97% 34.6M 0s\n     96350K .......... .......... .......... .......... .......... 97% 15.7M 0s\n     96400K .......... .......... .......... .......... .......... 97% 13.2M 0s\n     96450K .......... .......... .......... .......... .......... 97% 13.2M 0s\n     96500K .......... .......... .......... .......... .......... 97% 12.1M 0s\n     96550K .......... .......... .......... .......... .......... 98% 24.7M 0s\n     96600K .......... .......... .......... .......... .......... 98% 21.7M 0s\n     96650K .......... .......... .......... .......... .......... 98% 31.4M 0s\n     96700K .......... .......... .......... .......... .......... 98% 15.2M 0s\n     96750K .......... .......... .......... .......... .......... 98%  189M 0s\n     96800K .......... .......... .......... .......... .......... 98%  196M 0s\n     96850K .......... .......... .......... .......... .......... 98%  329M 0s\n     96900K .......... .......... .......... .......... .......... 98%  218M 0s\n     96950K .......... .......... .......... .......... .......... 98%  192M 0s\n     97000K .......... .......... .......... .......... .......... 98%  251M 0s\n     97050K .......... .......... .......... .......... .......... 98%  316M 0s\n     97100K .......... .......... .......... .......... .......... 98%  225M 0s\n     97150K .......... .......... .......... .......... .......... 98% 77.3M 0s\n     97200K .......... .......... .......... .......... .......... 98% 13.7M 0s\n     97250K .......... .......... .......... .......... .......... 98% 14.0M 0s\n     97300K .......... .......... .......... .......... .......... 98%  698K 0s\n     97350K .......... .......... .......... .......... .......... 98% 12.1M 0s\n     97400K .......... .......... .......... .......... .......... 98% 24.0M 0s\n     97450K .......... .......... .......... .......... .......... 98% 14.3M 0s\n     97500K .......... .......... .......... .......... .......... 99%  752K 0s\n     97550K .......... .......... .......... .......... .......... 99%  334M 0s\n     97600K .......... .......... .......... .......... .......... 99%  289M 0s\n     97650K .......... .......... .......... .......... .......... 99%  352M 0s\n     97700K .......... .......... .......... .......... .......... 99%  309M 0s\n     97750K .......... .......... .......... .......... .......... 99%  356M 0s\n     97800K .......... .......... .......... .......... .......... 99%  308M 0s\n     97850K .......... .......... .......... .......... .......... 99%  346M 0s\n     97900K .......... .......... .......... .......... .......... 99%  374M 0s\n     97950K .......... .......... .......... .......... .......... 99%  327M 0s\n     98000K .......... .......... .......... .......... .......... 99%  297M 0s\n     98050K .......... .......... .......... .......... .......... 99%  370M 0s\n     98100K .......... .......... .......... .......... .......... 99%  334M 0s\n     98150K .......... .......... .......... .......... .......... 99%  376M 0s\n     98200K .......... .......... .......... .......... .......... 99%  153M 0s\n     98250K .......... .......... .......... .......... .......... 99%  340M 0s\n     98300K .......... .......... .......... .......... .......... 99%  313M 0s\n     98350K .......... .......... .......... .......... .......... 99%  328M 0s\n     98400K .......... .......... .......... .......... .......... 99%  270M 0s\n     98450K .......... .......... .......... .......... .......... 99%  331M 0s\n     98500K .......... ..........                                 100%  312M=7.2s\n    \n    2019-03-15 06:35:12 (13.4 MB/s) - 'inception_v3_2016_08_28.tar.gz' saved [100885009/100885009]\n    \n    \n\n\n```bash\n%%bash\n! home=\"./models/research/\"\n\n# Directory containing preprocessed MSCOCO data.\n! MSCOCO_DIR=\"${home}/im2txt/data/mscoco\"\n\n# Inception v3 checkpoint file.\n! INCEPTION_CHECKPOINT=\"${home}/im2txt/data/inception_v3.ckpt\"\n\n# 保存模型位置\n! MODEL_DIR=\"${home}/im2txt/model\"\n\n# 创建模型\n! cd $home/im2txt\n! bazel build -c opt //im2txt/...\n\n# 训练模型\n! bazel-bin/im2txt/train \\\n  --input_file_pattern=\"${MSCOCO_DIR}/train-?????-of-00256\" \\\n  --inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\" \\\n  --train_dir=\"${MODEL_DIR}/train\" \\\n  --train_inception=false \\\n  --number_of_steps=1000000\n```\n\n    Starting local Bazel server and connecting to it...\n    Loading: \n    Loading: 0 packages loaded\n    Analyzing: 17 targets (3 packages loaded)\n    Analyzing: 17 targets (3 packages loaded, 0 targets configured)\n    Analyzing: 17 targets (10 packages loaded, 45 targets configured)\n    Analyzing: 17 targets (18 packages loaded, 90 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    Analyzing: 17 targets (20 packages loaded, 189 targets configured)\n    INFO: Analysed 17 targets (21 packages loaded, 312 targets configured).\n    INFO: Found 17 targets...\n    [0 / 4] [-----] Expanding template im2txt/inference_utils/caption_generator_test\n    INFO: Elapsed time: 49.943s, Critical Path: 0.05s\n    INFO: 0 processes.\n    INFO: Build completed successfully, 25 total actions\n    INFO: Build completed successfully, 25 total actions\n    Traceback (most recent call last):\n      File \"/notebooks/Visual_Text_Task--Image_Caption_and_VQA/Chapter1_Image_Caption/models/research/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py\", line 22, in <module>\n        import tensorflow as tf\n    ImportError: No module named tensorflow\n    \n\n\n```bash\n%%bash\n! home=\"./models/research/\"\n# 使用模型来进行预测 \n! CHECKPOINT_PATH=\"${home}/im2txt/model/train\"\n\n# 词表位置\n! VOCAB_FILE=\"${home}/im2txt/data/mscoco/word_counts.txt\"\n\n# 使用某一张JPEG图片来做演示\n! IMAGE_FILE=\"${home}/im2txt/data/mscoco/raw-data/val2014/COCO_val2014_000000224477.jpg\"\n\n# 创建inference的二进制代码\n! cd $home/im2txt\n! bazel build -c opt //im2txt:run_inference\n\n! export CUDA_VISIBLE_DEVICES=0\n\n# 生成caption\n! bazel-bin/im2txt/run_inference \\\n  --checkpoint_path=${CHECKPOINT_PATH} \\\n  --vocab_file=${VOCAB_FILE} \\\n  --input_files=${IMAGE_FILE}\n\n\n```\n\n```bash\nCaptions for image COCO_val2014_000000224477.jpg:\n  0) a man riding a wave on top of a surfboard . (p=0.040413)\n  1) a person riding a surf board on a wave (p=0.017452)\n  2) a man riding a wave on a surfboard in the ocean . (p=0.005743)\n```\n\n![](./img/example_img.jpg)\n\n## 1.3 注意力机制与beam search优化\n\n```python\nclass ShowAndTellModel(object):\n  def __init__(self, config, mode, train_inception=False):\n    \"\"\"Basic setup.\n    Args:\n      config: Object containing configuration parameters.\n      mode: \"train\", \"eval\" or \"inference\".\n      train_inception: Whether the inception submodel variables are trainable.\n    \"\"\"\n    ...\n    # A float32 Tensor with shape [batch_size, height, width, channels].\n    self.images = None\n\n    # An int32 Tensor with shape [batch_size, padded_length].\n    self.input_seqs = None\n\n    # An int32 Tensor with shape [batch_size, padded_length].\n    self.target_seqs = None\n\n    # An int32 0/1 Tensor with shape [batch_size, padded_length].\n    self.input_mask = None\n\n    # A float32 Tensor with shape [batch_size, embedding_size].\n    self.image_embeddings = None\n\n    # A float32 Tensor with shape [batch_size, padded_length, embedding_size].\n    self.seq_embeddings = None\n\n    # A float32 scalar Tensor; the total loss for the trainer to optimize.\n    self.total_loss = None\n\n    # A float32 Tensor with shape [batch_size * padded_length].\n    self.target_cross_entropy_losses = None\n\n    # A float32 Tensor with shape [batch_size * padded_length].\n    self.target_cross_entropy_loss_weights = None\n\n    # Collection of variables from the inception submodel.\n    self.inception_variables = []\n\n    # Function to restore the inception submodel from checkpoint.\n    self.init_fn = None\n    \n  def build_inputs(self):\n    \"\"\"预处理输入数据\n    Outputs:\n      self.images\n      self.input_seqs\n      self.target_seqs (training and eval only)\n      self.input_mask (training and eval only)\n    \"\"\"\n    if self.mode == \"inference\":\n      # In inference mode, images and inputs are fed via placeholders.\n      image_feed = tf.placeholder(dtype=tf.string, shape=[], name=\"image_feed\")\n      input_feed = tf.placeholder(dtype=tf.int64,\n                                  shape=[None],  # batch_size\n                                  name=\"input_feed\")\n\n      # Process image and insert batch dimensions.\n      images = tf.expand_dims(self.process_image(image_feed), 0)\n      input_seqs = tf.expand_dims(input_feed, 1)\n\n      # No target sequences or input mask in inference mode.\n      target_seqs = None\n      input_mask = None\n    else:\n      # Prefetch serialized SequenceExample protos.\n      input_queue = input_ops.prefetch_input_data(\n          self.reader,\n          self.config.input_file_pattern,\n          is_training=self.is_training(),\n          batch_size=self.config.batch_size,\n          values_per_shard=self.config.values_per_input_shard,\n          input_queue_capacity_factor=self.config.input_queue_capacity_factor,\n          num_reader_threads=self.config.num_input_reader_threads)\n\n      # Image processing and random distortion. Split across multiple threads\n      # with each thread applying a slightly different distortion.\n      assert self.config.num_preprocess_threads % 2 == 0\n      images_and_captions = []\n      for thread_id in range(self.config.num_preprocess_threads):\n        serialized_sequence_example = input_queue.dequeue()\n        encoded_image, caption = input_ops.parse_sequence_example(\n            serialized_sequence_example,\n            image_feature=self.config.image_feature_name,\n            caption_feature=self.config.caption_feature_name)\n        image = self.process_image(encoded_image, thread_id=thread_id)\n        images_and_captions.append([image, caption])\n\n      # Batch inputs.\n      queue_capacity = (2 * self.config.num_preprocess_threads *\n                        self.config.batch_size)\n      images, input_seqs, target_seqs, input_mask = (\n          input_ops.batch_with_dynamic_pad(images_and_captions,\n                                           batch_size=self.config.batch_size,\n                                           queue_capacity=queue_capacity))\n\n    self.images = images\n    self.input_seqs = input_seqs\n    self.target_seqs = target_seqs\n    self.input_mask = input_mask\n\n  def build_image_embeddings(self):\n    \"\"\"CNN 将图片encode到图像空间\n    Inputs:\n      self.images\n    Outputs:\n      self.image_embeddings\n    \"\"\"\n    inception_output = image_embedding.inception_v3(\n        self.images,\n        trainable=self.train_inception,\n        is_training=self.is_training())\n    self.inception_variables = tf.get_collection(\n        tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\")\n\n    # Map inception output into embedding space.\n    with tf.variable_scope(\"image_embedding\") as scope:\n      image_embeddings = tf.contrib.layers.fully_connected(\n          inputs=inception_output,\n          num_outputs=self.config.embedding_size,\n          activation_fn=None,\n          weights_initializer=self.initializer,\n          biases_initializer=None,\n          scope=scope)\n\n    # Save the embedding size in the graph.\n    tf.constant(self.config.embedding_size, name=\"embedding_size\")\n\n    self.image_embeddings = image_embeddings\n\n  def build_seq_embeddings(self):\n    \"\"\"将输入文字映射到词向量空间\n    Inputs:\n      self.input_seqs\n    Outputs:\n      self.seq_embeddings\n    \"\"\"\n    with tf.variable_scope(\"seq_embedding\"), tf.device(\"/cpu:0\"):\n      embedding_map = tf.get_variable(\n          name=\"map\",\n          shape=[self.config.vocab_size, self.config.embedding_size],\n          initializer=self.initializer)\n      seq_embeddings = tf.nn.embedding_lookup(embedding_map, self.input_seqs)\n\n    self.seq_embeddings = seq_embeddings\n\n  def build_model(self):\n    \"\"\"创建 CNN-encoder 与RNN-decoder，计算loss \n    Inputs:\n      self.image_embeddings\n      self.seq_embeddings\n      self.target_seqs (training and eval only)\n      self.input_mask (training and eval only)\n    Outputs:\n      self.total_loss (training and eval only)\n      self.target_cross_entropy_losses (training and eval only)\n      self.target_cross_entropy_loss_weights (training and eval only)\n    \"\"\"\n    # This LSTM cell has biases and outputs tanh(new_c) * sigmoid(o), but the\n    # modified LSTM in the \"Show and Tell\" paper has no biases and outputs\n    # new_c * sigmoid(o).\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == \"train\":\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n      _, initial_state = lstm_cell(self.image_embeddings, zero_state)\n\n      # Allow the LSTM variables to be reused.\n      lstm_scope.reuse_variables()\n\n      if self.mode == \"inference\":\n        # In inference mode, use concatenated states for convenient feeding and\n        # fetching.\n        tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n\n        # Placeholder for feeding a batch of concatenated states.\n        state_feed = tf.placeholder(dtype=tf.float32,\n                                    shape=[None, sum(lstm_cell.state_size)],\n                                    name=\"state_feed\")\n        state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n\n        # Run a single LSTM step.\n        lstm_outputs, state_tuple = lstm_cell(\n            inputs=tf.squeeze(self.seq_embeddings, axis=[1]),\n            state=state_tuple)\n\n        # Concatentate the resulting state.\n        tf.concat(axis=1, values=state_tuple, name=\"state\")\n      else:\n        # Run the batch of sequence embeddings through the LSTM.\n        sequence_length = tf.reduce_sum(self.input_mask, 1)\n        lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n                                            inputs=self.seq_embeddings,\n                                            sequence_length=sequence_length,\n                                            initial_state=initial_state,\n                                            dtype=tf.float32,\n                                            scope=lstm_scope)\n\n    # Stack batches vertically.\n    lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n\n    with tf.variable_scope(\"logits\") as logits_scope:\n      logits = tf.contrib.layers.fully_connected(\n          inputs=lstm_outputs,\n          num_outputs=self.config.vocab_size,\n          activation_fn=None,\n          weights_initializer=self.initializer,\n          scope=logits_scope)\n\n    if self.mode == \"inference\":\n      tf.nn.softmax(logits, name=\"softmax\")\n    else:\n      targets = tf.reshape(self.target_seqs, [-1])\n      weights = tf.to_float(tf.reshape(self.input_mask, [-1]))\n\n      # Compute losses.\n      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n                                                              logits=logits)\n      batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n                          tf.reduce_sum(weights),\n                          name=\"batch_loss\")\n      tf.losses.add_loss(batch_loss)\n      total_loss = tf.losses.get_total_loss()\n\n      # Add summaries.\n      tf.summary.scalar(\"losses/batch_loss\", batch_loss)\n      tf.summary.scalar(\"losses/total_loss\", total_loss)\n      for var in tf.trainable_variables():\n        tf.summary.histogram(\"parameters/\" + var.op.name, var)\n\n      self.total_loss = total_loss\n      self.target_cross_entropy_losses = losses  # Used in evaluation.\n      self.target_cross_entropy_loss_weights = weights  # Used in evaluation.\n```\n\n## 1.5 【实战】基于attention model的“看图说话”实现\n\n```bash\n# 下载代码\ngit clone https://github.com/yunjey/show-attend-and-tell-tensorflow.git\ngit clone https://github.com/tylin/coco-caption.git\n\n# 下载数据\n# 所有图片存放在 image/ 这个文件夹下，VGGNet19模型文件会存放在 data/ 文件夹下\ncd show-attend-and-tell-tensorflow\npip install -r requirements.txt\nchmod +x ./download.sh\n./download.sh\n\n# 预处理数据\n# 把所有图片都resize成224x224维度, 存放在image/train2014_resized/， image/val2014_resized/\npython resize.py\n# 把图片和文字都预处理成向量\npython prepro.py\n\n# 训练模型\npython train.py\n```\n\n## 1.5 【实战】基于attention model的“看图说话”实现\n\n```python\nclass CaptionGenerator(object):\n    def __init__(self, word_to_idx, dim_feature=[196, 512], dim_embed=512, dim_hidden=1024, n_time_step=16,\n                  prev2out=True, ctx2out=True, alpha_c=0.0, selector=True, dropout=True):\n        \"\"\"\n        Args:\n            word_to_idx: word-to-index mapping dictionary.\n            dim_feature: (optional) Dimension of vggnet19 conv5_3 feature vectors.\n            dim_embed: (optional) Dimension of word embedding.\n            dim_hidden: (optional) Dimension of all hidden state.\n            n_time_step: (optional) Time step size of LSTM.\n            prev2out: (optional) previously generated word to hidden state. (see Eq (7) for explanation)\n            ctx2out: (optional) context to hidden state (see Eq (7) for explanation)\n            alpha_c: (optional) Doubly stochastic regularization coefficient. (see Section (4.2.1) for explanation)\n            selector: (optional) gating scalar for context vector. (see Section (4.2.1) for explanation)\n            dropout: (optional) If true then dropout layer is added.\n        \"\"\"\n\n        self.word_to_idx = word_to_idx\n        self.idx_to_word = {i: w for w, i in word_to_idx.iteritems()}\n        self.prev2out = prev2out\n        self.ctx2out = ctx2out\n        self.alpha_c = alpha_c\n        self.selector = selector\n        self.dropout = dropout\n        self.V = len(word_to_idx)\n        self.L = dim_feature[0]\n        self.D = dim_feature[1]\n        self.M = dim_embed\n        self.H = dim_hidden\n        self.T = n_time_step\n        self._start = word_to_idx['<START>']\n        self._null = word_to_idx['<NULL>']\n\n        self.weight_initializer = tf.contrib.layers.xavier_initializer()\n        self.const_initializer = tf.constant_initializer(0.0)\n        self.emb_initializer = tf.random_uniform_initializer(minval=-1.0, maxval=1.0)\n\n        # Place holder for features and captions\n        self.features = tf.placeholder(tf.float32, [None, self.L, self.D])\n        self.captions = tf.placeholder(tf.int32, [None, self.T + 1])\n\n\n    def build_model(self):\n        features = self.features\n        captions = self.captions\n        batch_size = tf.shape(features)[0]\n\n        captions_in = captions[:, :self.T]\n        captions_out = captions[:, 1:]\n        mask = tf.to_float(tf.not_equal(captions_out, self._null))\n\n\n        # batch normalize feature vectors\n        features = self._batch_norm(features, mode='train', name='conv_features')\n\n        c, h = self._get_initial_lstm(features=features)\n        x = self._word_embedding(inputs=captions_in)\n        features_proj = self._project_features(features=features)\n\n        loss = 0.0\n        alpha_list = []\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.H)\n\n        for t in range(self.T):\n            context, alpha = self._attention_layer(features, features_proj, h, reuse=(t!=0))\n            alpha_list.append(alpha)\n\n            if self.selector:\n                context, beta = self._selector(context, h, reuse=(t!=0))\n\n            with tf.variable_scope('lstm', reuse=(t!=0)):\n                _, (c, h) = lstm_cell(inputs=tf.concat( [x[:,t,:], context],1), state=[c, h])\n\n            logits = self._decode_lstm(x[:,t,:], h, context, dropout=self.dropout, reuse=(t!=0))\n\n            loss += tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=captions_out[:, t],logits=logits)*mask[:, t] )\n\n        if self.alpha_c > 0:\n            alphas = tf.transpose(tf.stack(alpha_list), (1, 0, 2))     # (N, T, L)\n            alphas_all = tf.reduce_sum(alphas, 1)      # (N, L)\n            alpha_reg = self.alpha_c * tf.reduce_sum((16./196 - alphas_all) ** 2)\n            loss += alpha_reg\n\n        return loss / tf.to_float(batch_size)\n   \n```\n\n## 1.5 【实战】基于attention model的“看图说话”实现\n\n- 测试生成caption\n\n\n```python\nimport matplotlib.pyplot as plt\n#import cPickle as pickle\nimport tensorflow as tf\nfrom core.solver import CaptioningSolver\nfrom core.model import CaptionGenerator\nfrom core.utils import load_coco_data\nfrom core.bleu import evaluate\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8.0, 6.0)  # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n```\n\n\n    ---------------------------------------------------------------------------\n\n    ImportError                               Traceback (most recent call last)\n\n    <ipython-input-2-62b36cc50a3e> in <module>()\n          2 #import cPickle as pickle\n          3 import tensorflow as tf\n    ----> 4 from core.solver import CaptioningSolver\n          5 from core.model import CaptionGenerator\n          6 from core.utils import load_coco_data\n    \n\n    ImportError: No module named 'core'\n\n\n\n```python\nimport matplotlib.pyplot as plt\nimport cPickle as pickle\nimport tensorflow as tf\nfrom core.solver import CaptioningSolver\nfrom core.model import CaptionGenerator\nfrom core.utils import load_coco_data\nfrom core.bleu import evaluate\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8.0, 6.0)  # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n```\n\n\n```python\ndata = load_coco_data(data_path='./data', split='val')\ntest = load_coco_data(data_path='./data', split='test')\nwith open('./data/train/word_to_idx.pkl', 'rb') as f:\n    word_to_idx = pickle.load(f)\n```\n\n\n```python\nmodel = CaptionGenerator(word_to_idx, dim_feature=[196, 512], dim_embed=512,\n                        dim_hidden=1500, n_time_step=16, prev2out=True, \n                        ctx2out=True, alpha_c=1.0, selector=True, dropout=True)\n```\n\n\n```python\nsolver = CaptioningSolver(model, data, data, n_epochs=15, batch_size=128, update_rule='adam',\n                        learning_rate=0.0025, print_every=2000, save_every=1, image_path='./image/val2014_resized',\n                        pretrained_model=None, model_path='./model/lstm', test_model='./model/lstm3/model-18',\n                        print_bleu=False, log_path='./log/')\ntf.get_variable_scope().reuse_variables()\nsolver.test(test, split='test')\n```\n\n```bash\nSampled Caption: a sign that is on a pole in front of a building .\n```\n![](./img/test1.png)\n\n## 本章小结\n    1.1 “看图说话”问题介绍\n    1.2 简易CNN+RNN编码解码模型完成图片短文本描述原理\n    1.3 注意力模型与“看图说话”优化\n    1.4 【实战】基于CNN+RNN的编解码“看图说话”与beam-search优化\n    1.5 【实战】基于attention model的“看图说话”实现\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/1NLP_intro/1.language_model_calculation/","content":"\n## 统计语言模型\n\n\n>以下内容摘自和修改自吴军《数学之美》\n\n自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递方式。因此让计算机处理自然语言，一个基本问题就是**为自然语言这种上下文相关的特性建立数学模型**，这个数学模型就是在自然语言处理中常说的**统计语言模型(Statistical Language Model)**。它是今天所有自然语言处理的基础，并且广泛应用于机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询。\n\n### 1. 用数学的方法描述语言规律\n\n统计语言模型产生的初衷是为了解决语音识别问题。在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解并且有意义的句子，然后显示或打印给使用者。\n\n比如：\n\n> 美联储主席本·伯南克昨天告诉媒体 7000 亿美元的救助资金将借给上百家银行、保险公司和汽车公司。\n\n这句话就很通顺，意义也很明白。\n\n如果改变一些词的顺序，或者替换掉一些词，将这句话变成：\n\n> 本·伯南克美联储主席昨天 7000 亿美元的救助资金告诉媒体将借给银行、保险公司和汽车公司上百家。\n\n意思就含混了，虽然多少还能猜到一点。\n\n但如果再换成：\n\n> 联主美储席本·伯诉体南将借天的救克告媒昨助资金 70 元亿 00 美给上百百百家银保行、汽车险公司公司和。\n\n基本上读者就不知所云了。\n\n第一个句子合乎语法，词义清晰。第二个句子虽不合乎语法，但是词义还算清晰。而第三个句子则连词义都不清晰了。上世纪 70 年代以前，科学家们也是这样想的，他们试图判断这个文字序列是否合乎文法、含义是否正确等。但是语言的结构千变万化，要通过制定规则来覆盖所有的文法根本是不可能的。而弗里德里克·贾里尼克(Frederick Jelinek)换了一个角度，用一个简单的统计模型就很漂亮地搞定了这个问题。\n\n#### 贾里尼克的想法\n\n贾里尼克的出发点很简单：**一个句子是否合理，就看它的可能性大小如何。**上面的例子中，第一个句子出现的概率大致是$10^{−20}$，第二个句子出现的概率是 $10^{−25}$，第三个句子出现的概率是 $10^{−70}$。因此第一个句子出现的可能性最大，是第二个句子的 10万倍，是第三个句子的一百亿亿亿亿亿亿倍。\n\n用更普遍而严格的描述是：\n\n假定 SS 是一个有意义的句子，由一连串特定顺序排列的词 $w_1,w_2,⋯,w_n$组成，n为句子的长度。那么 S 在文本中出现的可能性就是 S 的概率 P(S)。于是可以把 P(S) 展开表示为：\n\n$P(S)=P(w_1,w_2,⋯,w_n)$\n\n利用条件概率公式，SS 这个序列出现的概率等于每一个词出现的条件概率相乘，于是：\n\n$P(w_{1},w_{2},⋯,w_{n})=P(w_{1})⋅P(w_{2}∣w_{1})⋅P(w_{3}∣w_{1},w_{2})⋯P(w_{n}∣w_{1},w_{2},⋯,w_{n−1})P(w_{1},w_{2},⋯,w_{n})=P(w_{1})⋅P(w_{2}∣w_{1})⋅P(w_{3}∣w_{1},w_{2})⋯P(w_{n}∣w_{1},w_{2},⋯,w_{n−1})$\n\n其中 $P(w_{1})$ 表示句子第一个词为 $w_1$ 的概率；$P(w_{2}∣w_{1})$ 是在已知第一个词的前提下，第二个词出现的概率；以此类推。不难看出，词 $w_n$ 的出现概率取决于它前面所有的词。\n\n> $P(w_{1})$ 更准确的描述是 $P(w_{1}∣BOS)$ 即这个词在句子开头出现的概率。\n\n从计算上来看，第一个词的条件概率 $P(w_{1})$ 很容易算，第二个词的条件概率 $P(w_{2}∣w_{1})$ 也还不太麻烦，但是从第三个词的条件概率 $P(w_{3}∣w_{1},w_{2})$ 开始就非常难算了，因为它涉及到三个变量 $w_{1},w_{2},w_3$，而每个变量的可能性都是一种语言字典的大小。到了最后一个词 $w_n$，条件概率 $P(w_{n}∣w_{1},w_{2},⋯,w_{n−1})$ 的可能性太多，根本无法估算。\n\n#### 二元模型与 N 元模型\n\n从 19 世纪到 20 世纪初，俄国有个数学家叫马尔可夫(Andrey Markov)，他提出了一种偷懒但还颇为有效的方法：假设任意一个词语 wiwi 出现的概率只同它前面的词 wi−1 有关。于是问题就变得很简单了，这种假设在数学上称为马尔可夫假设。\n\n> 马尔可夫在 1906 年首先做出了这类过程，而将此一般化到可数无限状态空间是由柯尔莫果洛夫在 1936 年给出的。\n\n现在，S 出现的概率就变得简单了：\n\n$P(S)=P(w_{1})⋅P(w_{2}∣w_{1})⋅P(w_{3}∣w_{2})⋯P(w_{i}∣w_{i−1})⋯P(w_{n}∣w_{n−1})$\n\n上面的公式对应的统计语言模型是二元模型(Bigram Model)。当然，也可以假设一个词由前面的 N−1 个词决定，对应的模型稍微复杂些，被称为 N 元模型。\n\n接下来的问题就是如何估计条件概率 $P(w_{i}∣w_{i−1})$。根据它的定义：\n\n$P(w_{i}∣w_{i−1})=P(w_{i−1},w_{i})P(w_{i−1})P(w_{i}∣w_{i−1})=P(w_{i−1},w_{i})P(w_{i−1})$\n\n而估计联合概率 $P(w_{i−1},w_{i})$ 和边缘概率 $P(w_{i−1})$ 是很简单的。根据大数定理，只要统计量足够，相对频度就等于概率，因而只需在语料库(Corpus)的文本中统计一下 $w_{i−1},w_i$ 这对词前后相邻出现了多少次 $N(w_{i−1},w_{i})$，以及 $w_{i−1}$ 出现了多少次 $N(w_{i−1})$，然后用两个数分别处以语料库的大小 N，即可得到这些词或者二元组的概率：\n\n$$P(w_{i-1},w_i)=f(w_{i-1},w_i)=\\frac{N(w_{i-1},w_i)}{N} \\\\ P(w_{i-1})=f(w_{i-1})=\\frac{N(w_{i-1})}{N}$$\n$$P(w_{i-1},w_i)=f(w_{i-1},w_i)=\\frac{N(w_{i-1},w_i)}{N} \\\\ P(w_{i-1})=f(w_{i-1})=\\frac{N(w_{i-1})}{N}$$\n\n于是，\n\n$$P(w_i\\mid w_{i-1})\\approx\\frac{N(w_{i-1},w_i)}{N(w_{i-1})}$$\n\n这似乎有点难以置信，用这么简单的数学模型就能解决复杂的语音识别、机器翻译等问题，而用很复杂的文法规则和人工智能却做不到。其实很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法更有效。\n\n### 2. 高阶语言模型\n\n在基于一阶马尔可夫假设的二元模型中，句子中每个词只和前面一个词有关，这似乎过于简化了，或者说近似地过头了。比如说在句子“美丽的花朵”中，“花朵”其实是和“美丽”有关，也就是说是与前面的第二个词有关。因此，更普遍的假设是某个词和前面的若干个词有关。\n\n正如之前介绍的那样，N 元模型假设每个词 $w_i$ 和前面的 N−1 个词有关，而与更前面的词无关，这样词 $w_i$ 的概率只取决于前面的 N−1 个词 $w_{i−N+1},w_{i−N+2},⋯,w_{i−1}$。因此：\n\n$$P(w_i\\mid w_1,w_2,\\cdots,w_{i-1})=P(w_i\\mid w_{i-N+1},w_{i-N+2},\\cdots,w_{i-1})$$\n\n这种假设被称为 N−1 阶马尔可夫假设，对应的语言模型称为 N 元模型(N-Gram Model)。N=2时就是之前介绍的二元模型，而 N=1 的一元模型实际上是一个上下文无关模型，即假定当前词的出现概率与前面的词无关。在实际中应用最多的就是 N=3 的三元模型，更高阶的模型就很少使用了。\n\n> **为什么 N 取值这么小？**\n>\n> 首先，N 元模型的大小（空间复杂度）几乎是 N 的指数函数，即 $O(|V|^N)$，这里 |V|是一种语言词典的词汇量，一般在几万到几十万个。其次，使用 N 元模型的速度（时间复杂度）也几乎是一个指数函数，即 $O(|V|^{N−1})$。因此，N 不能很大。\n>\n> 当 N 从 1 到 2，再从 2 到 3 时，模型的效果上升显著。而当模型从 3 到 4 时，效果的提升就不是很显著了，而资源的耗费却增加地非常快。所以，除非是为了做到极致不惜资源，很少有人会使用四元以上的模型。\n\n还有一个问题，三元、四元或更高阶的模型也并不能覆盖所有的语言现象。在自然语言处理中，上下文之间的相关性可能跨度非常大，甚至可以从一个段落跨到另一个段落。因此，即便再怎么提高模型的阶数，对这种情况也无可奈何，这就是马尔可夫模型的局限性，这是就需要采用其他一些长程的依赖性(Long Distance Dependency)来解决这个问题了。\n\n#### 3. 模型的训练、零概率问题和平滑方法\n\n语言模型中所有的条件概率称为模型的参数，通过对语料的统计，得到这些参数的过程称为模型的训练。前面提到的二元模型训练方法似乎非常简单，只需计算一下 $w_{i−1},w_i$ 前后相邻出现的次数 $N(w_{i−1},w_{i})$ 和 $w_{i−1}$ 单独出现的次数 $N(w_{i−1})$ 的比值即可。但是如果同现的次数 $N(w_{i−1},w_{i})=0$ 怎么办，是否意味着条件概率 $P(w_{i}∣w_{i−1})=0$？反之，如果 $N(w_{i−1},w_{i})$ 和 $N(w_{i−1})$ 都只出现一次，能否得出 $P(w_{i}∣w_{i−1})=1$ 这样非常绝对的结论？\n\n这就涉及到统计的可靠性问题了。在数理统计中，我们之所以敢用对采样数据进行观察的结果来预测概率，是因为有大数定理(Law of Large Number)在背后做支持，它的要求是有足够的观察值。但是在估计语言模型的概率时，很多人恰恰忘了这个道理，因此训练出来的语言模型“不管用”，然后回过头来怀疑这个方法是否有效。那么如何正确地训练一个语言模型呢？\n\n一个直接的办法就是增加数据量，但是即使如此，仍会遇到零概率或者统计量不足的问题。假定要训练一个汉语的语言模型，汉语的词汇量大致是 20 万这个数量级，训练一个三元模型就有 $200,000^3=8×10^{15} $个不同参数。假设抓取 100 亿个有意义的中文网页，每个网页平均 1000 词，全部用作训练也依然只有 $10^{13}$。因此，如果用直接的比值计算概率，大部分条件概率依然是零，这种模型我们称之为“不平滑”。\n\n训练统计语言模型的艺术就在于解决好统计样本不足时的概率估计问题。\n\n#### 古德-图灵估计\n\n1953 年古德(I.J.Good)在他的老板图灵(Alan Turing)的指导下，提出了在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法，同时将折扣出的那一小部分概率给予未看见的事件(Unseen Events)。古德和图灵还给出了一个很漂亮的重新估算概率的公式，这个公式后来被称为古德-图灵估计(Good-Turing Estimate)。\n\n古德-图灵估计的原理是：对于没看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量(Probability Mass)中，分配一个很小的比例给这些没有看见的事件。这样一来，看见了的事件的概率总和就小于 1了。因此，需要将所有看见了的事件概率调小一点，并且按照“越是不可信的统计折扣越多”的方法进行。\n\n以统计词典中每个词的概率为例：假定在语料库中出现 r 次的词有 $N_r$ 个，特别地，未出现的词数量为 $N_0$。语料库的大小为 N。那么，很显然\n\n$$N=\\sum_{r=1}^{\\infty}rN_r$$\n\n出现 r 次的词在整个语料库中的相对频度(Relative Frequency)则是 $r/N$，如果不做任何优化处理，就以这个相对频度作为这些词的概率估计。现在假定当 r 比较小时，它的统计可能不可靠，因此在计算那些出现 r 次的词的概率时，要使用一个更小一点的次数，是 $d_r$（而不直接使用r），古德-图灵估计按照下面的公式计算$d_r$：\n\n$$d_r = (r+1)\\cdot N_{r+1}/N_r$$\n\n显然\n\n$$\\sum_rd_r\\cdot N_r=N$$\n\n根据 $Zipf$ 定律，一般情况下 $N_{r+1}<N_r$，因而 $d_r<r$，而 $d_0>0$。这样就给未出现的词赋予了一个很小的非零值，从而解决了零概率的问题。同时下调了出现频率很低的词的概率。实际运用中，一般只对出现次数低于某个阈值的词下调频率，然后把下调得到的频率总和给未出现的词。\n\n> 一般来说，出现一次的词的数量比出现两次的多，出现两次的比出现三次的多，这种规律称为 Zipf定律(Zipf’s Law)，即 r 越大，词的数量 $N_r$ 越小。\n\n这样出现 r 次的词的概率估计为$d_r/N$。于是，对于频率超过一定阈值的词，它们的概率估计就是它们在语料库中的相对频度，对于频率小于阈值的词，它们的概率估计就小于它们的相对频度，并且出现次数越少，折扣越多。对于未看见的词，也给与了一个比较小的概率。这样所有词的概率估计都很平滑了。\n\n#### 卡茨退避法\n\n对于二元组 $(w_{i−1},w_{i})(w_{i−1},w_{i})$ 的条件概率估计 $P(w_{i}∣w_{i−1})$也可以做同样的处理。我们知道，通过前一个词 $w_{i−1}$ 预测后一个词 $w_i$ 时，所有的可能情况的条件概率总和应该为 1，即\n\n$$\\sum_{w_i\\in V}P(w_i\\mid w_{i-1})=1$$\n\n对于出现次数非常少的二元组 $(w_{i−1},w_{i})(w_{i−1},w_{i})$，需要按照古德-图灵的方法打折扣，这样 $\\sum_{w_{i-1},w_i\\text{ seen}}P(w_i\\mid w_{i-1})\\lt 1$，这意味着有一部分概率量没有分配出去，留给了没有看到的二元组 $(w_{i−1},w_{i})(w_{i−1},w_{i})$。基于这种思想，估计二元模型概率的公式为：\n\n$$P(w_i\\mid w_{i-1})=\\begin{cases}f(w_i\\mid w_{i-1})\\quad\\text{if }N(w_{i-1},w_i) \\ge T \\\\f_{gt}(w_i\\mid w_{i-1})\\quad\\text{if }0\\lt N(w_{i-1},w_i)\\lt T \\\\ Q(w_{i-1})\\cdot f(w_i)\\quad\\text{otherwise}\\end{cases}$$\n\n其中 T 是阈值，一般在 8−10 左右，函数 $f_{gt}()$ 表示经过古德-图灵估计后的相对频度，而\n\n$$Q(w_{i-1})=\\frac{1-\\sum_{w_i \\text{ seen}}P(w_i\\mid w_{i-1})}{\\sum_{w_i\\text{ unseen}}f(w_i)}$$\n\n这样可以保证所有的可能情况的条件概率总和为 11。\n\n这种平滑方法最早由前 IBM 科学家卡茨(S.M.Katz)提出，故称为卡茨退避法(Katz backoff)。类似地，对于三元模型，概率估计的公式如下：\n\n$$P(w_i\\mid w_{i-2},w_{i-1})=\\begin{cases}f(w_i\\mid w_{i-2},w_{i-1})\\quad\\text{if }N(w_{i-2,}w_{i-1},w_i) \\ge T \\\\f_{gt}(w_i\\mid w_{i-2,}w_{i-1})\\quad\\text{if }0\\lt N(w_{i-2},w_{i-1},w_i)\\lt T \\\\ Q(w_{i-2},w_{i-1})\\cdot P(w_i\\mid w_{i-1})\\quad\\text{otherwise}\\end{cases}$$\n\n对于一般情况的 N 元模型概率估计公式，以此类推。\n\n> 内伊(Herman Ney)等人在此基础上优化了卡茨退避法，原理大同小异。\n\n#### 线性插值\n\n因为一元组 $(w_{i})$ 出现的次数平均比二元组 $(w_{i−1},w_{i})$ 出现的次数要多很多，根据大数定律，它的相对频度更接近概率分布。类似地，二元组平均出现的次数比三元组要高，二元组的相对频度比三元组更接近概率分布。同时，低阶模型的零概率问题也比高阶模型轻微。因此，用低阶语言模型和高阶模型进行线性插值来达到平滑的目的，也是过去行业中经常使用的一种方法，这种方法称为删除插值(Deleted Interpolation)，详见下面的公式：\n\n$$P(w_i\\mid w_{i-2},w_{i-1})=\\lambda(w_{i-2},w_{i-1})\\cdot f(w_i\\mid w_{i-2},w_{i-1}) +\\lambda(w_{i-1})\\cdot f(w_i\\mid w_{i-1})+\\lambda f(w_i)$$\n\n其中，三个 λ 为插值权重，均为正数且和为 1。\n\n线性插值法的效果比卡茨退避法略差，故现在已经较少使用了。\n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/5Text_topic_extraction_and_representation_v2/0_Text_topic_extraction_and_representation/","content":"\n# 文本主题抽取与表示\n\n\n## 基于tf-idf与text-rank的主题词抽取\n### 基于 TF-IDF 算法的关键词抽取\n\nimport jieba.analyse\n\n* jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n    * sentence 为待提取的文本\n    * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n    * withWeight 为是否一并返回关键词权重值，默认值为 False\n    * allowPOS 仅包括指定词性的词，默认值为空，即不筛选\n\n### 基于 TextRank 算法的关键词抽取\n\n* jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n* jieba.analyse.TextRank() 新建自定义 TextRank 实例\n\n算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n\n基本思想:\n\n* 将待抽取关键词的文本进行分词\n* 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n* 计算图中节点的PageRank，注意是无向带权图\n\n## 监督学习与文本打主题标签\n\n\n* 如果不要求主题一定是文本中的词呢？\n * 如果有已经标注好的主题的文本，可以直接用文本分类的技术来识别文本的主题。\n * 如BOW/CNN/LSTM/BERT等\n\n <p align=\"center\">\n <img src=\"https://github.com/PaddlePaddle/book/blob/develop/05.recommender_system/image/text_cnn.png?raw=true\" width = \"80%\" align=\"center\"/><br/>\n 卷积神经网络文本分类模型\n </p>\n \n\n## 无监督学习与LDA主题模型\n### 什么是主题模型\n\n\n![](img/LDA1.png)\n\n### 汪峰歌词的例子\n\n有一位网友统计了汪峰老师在大陆发行的 9 张专辑中 117 首歌曲的歌词，同一词语在一首歌出现只算一次，形容词、名词和动词的前十名如下表所示（词语后面的数字是出现的次数）：\n\n![](img/LDA2.jpg)\n\n如果我们随便写一串数字，然后按照数位依次在形容词、名词和动词中取出一个词，连在一起会怎么样呢？\n例如取圆周率 3.1415926，对应的词语是：坚强，路，飞，自由，雨，埋，迷惘。稍微连接和润色一下：\n\n> 坚强的孩子，\n\n> 依然前行在路上，\n\n> 张开翅膀飞向自由，\n\n> 让雨水埋葬他的迷惘。\n\n比如某人的生日19820307：自由，桥，再见，迷惘，生命，死，孤独，鸟。润色一下：\n\n> 站在通向自由的桥上，\n\n> 再见了，迷惘的生命，\n\n> 犹如死亡般的孤独，\n\n> 将不再桎梏这只小鸟。\n\n有没有汪老师的感觉?找个数字试一试。\n\n上面根据汪老师的主题风格，我们创作了一首歌。其实我们更关心的是**如何得到这些主题，这就是 topic model 所要解决的问题**。\n\n### 基本脉络\n\n\n![](img/LDA3.png)\n\n### pLSA\n\n\n![](img/PLSA1.png)\n\n\n#### 我们看看PLSA的概率图形式：\n\n![](img/PLSA3.png)\n\n概率图可以很方便地写出所有变量的联合概率公式。\n* 扩展阅读：[《概率有向图模型》](https://blog.csdn.net/zb1165048017/article/details/60468659)。重点了解“条件局部独立性”，以及以此写出联合概率公式。\n\n![](img/probability.jpg)\n\n如果直接对这些自变量求偏导数，我们会发现由于自变量包含在对数和中，这个方程的求解很困难。因此对于这样的包含“隐含变量”或者“缺失数据”的概率模型参数估计问题，我们采用EM算法。\n\n#### *EM算法求解\n\n* 参考文章[](https://kexue.fm/archives/5239)\n![《从最大似然到EM算法：一致的理解方式》](img/EM1.png)\n\nEM算法的步骤本质上是一种交替最优化（二部坐标下降法）：\n\n(1)E步骤：求隐含变量Given当前估计的参数条件下的后验概率。\n\n(2)M步骤：最大化Complete data对数似然函数的期望，此时我们使用E步骤里计算的隐含变量的后验概率，得到新的参数值。\n\n两步迭代进行直到收敛。\n\n\n* E步：\n![](img/EM3.png)\n\n![](img/EM4.png)\n\n* M步：\n![](img/EM2.png)\n\n#### *EM算法求解PLSA\n\n* 已知量：w,d\n* 隐变量：z\n* 参数：P(w|z)，P(z|d)\n* E:直接写出\n* M:拉格朗日乘子法求解\n\n### LDA\n\n#### 我们看看pLSA与LDA的概率图形式的对比：\n![](img/LDA5.png)\n![](img/LDA6.png)\n\n参考文章[《共轭分布》](https://www.cnblogs.com/ooon/p/5845917.html),重点了解共轭分布的概念，和明确狄利克雷分布是多项分布的共轭分布。\n\n\n#### *EM算法求解LDA\n\n* 已知量：w\n* 隐变量：z，θ，φ\n* 参数：a，β\n* E:直接写不出，需要用变分法近似，或者吉布斯采样\n* M:坐标下降法求解，可以考虑牛顿法\n\n####  *变分法近似\n\n![](img/LDA7.png)\n注意：这里的φ不是隐变量φ，而是变分变量φ（字母不够用了）\n\n## 基于python的中文关键词抽取与可视化\n\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n## 基于LDA的新闻主题分析与可视化呈现\n\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/5Text_topic_extraction_and_representation_v2/2_LDA/","content":"\n## 基于LDA的新闻主题分析与可视化呈现\n\n\n\n```python\nimport jieba\n```\n\n\n```python\n# 创建停用词列表\ndef stopwordslist():\n    stopwords = [line.strip() for line in open('./data/stopwords.txt',encoding='UTF-8').readlines()]\n    return stopwords\n\n# 对句子进行中文分词\ndef seg_depart(sentence):\n     # 对文档中的每一行进行中文分词\n    # print(\"正在分词\")\n    sentence_depart = jieba.cut(sentence.strip())\n    # 创建一个停用词列表\n    stopwords = stopwordslist()\n    # 输出结果为outstr\n    outstr = ''\n    # 去停用词\n    for word in sentence_depart:\n        if word not in stopwords:\n            if word != '\\t':\n                 outstr += word\n                 outstr += \" \"\n    return outstr\n\n # 给出文档路径\nfilename = \"./data/cnews.train_preprocess.txt\"\noutfilename = \"./data/cnews.train_jieba.txt\"\ninputs = open(filename, 'r', encoding='UTF-8')\noutputs = open(outfilename, 'w', encoding='UTF-8')\n\n# 将输出结果写入ou.txt中\nfor line in inputs:\n    line_seg = seg_depart(line)\n    outputs.write(line_seg + '\\n')\n    \noutputs.close()\ninputs.close()\nprint(\"删除停用词和分词成功！！！\")\n```\n\n    Building prefix dict from the default dictionary ...\n    Loading model from cache /tmp/jieba.cache\n    Loading model cost 1.053 seconds.\n    Prefix dict has been built succesfully.\n    \n\n    删除停用词和分词成功！！！\n    \n\n\n```python\nfrom gensim import corpora, models, similarities\nfr = open('./data/cnews.train_jieba.txt', 'r',encoding='utf-8')\ntrain = []\nfor line in fr.readlines():\n    line = line.split(' ')\n    train.append(line)\n# 接下来就是模型构建的步骤了，首先构建词频矩阵\ndictionary = corpora.Dictionary(train)\ncorpus = [dictionary.doc2bow(text) for text in train]\nlda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=6)\ntopic_list = lda.print_topics(6)\nfor topic in topic_list:\n    print(topic)\n```\n\n    (0, '0.205*\"\" + 0.008*\"基金\" + 0.006*\"型基金\" + 0.005*\"像素\" + 0.004*\"净值\" + 0.004*\"佳能\" + 0.003*\"中\" + 0.003*\"公司\" + 0.002*\"投资\" + 0.002*\"证券\"')\n    (1, '0.268*\"\" + 0.009*\"对焦\" + 0.005*\"online\" + 0.003*\"中\" + 0.003*\"九州\" + 0.003*\"灵兽\" + 0.002*\"\\n\" + 0.002*\"尼康\" + 0.002*\"学生\" + 0.002*\"EOS\"')\n    (2, '0.388*\"\" + 0.003*\"\\n\" + 0.003*\"中\" + 0.002*\"游戏\" + 0.002*\"说\" + 0.002*\"学生\" + 0.002*\"移民\" + 0.002*\"做\" + 0.002*\"考试\" + 0.001*\"拍摄\"')\n    (3, '0.435*\"\" + 0.005*\"玩家\" + 0.004*\"中\" + 0.003*\"基金\" + 0.003*\"中国\" + 0.003*\"\\n\" + 0.002*\"游戏\" + 0.002*\"说\" + 0.002*\"学生\" + 0.002*\"机身\"')\n    (4, '0.406*\"\" + 0.003*\"中\" + 0.003*\"游戏\" + 0.003*\"功能\" + 0.003*\"\\n\" + 0.003*\"元\" + 0.002*\"英寸\" + 0.002*\"活动\" + 0.002*\"说\" + 0.002*\"相机\"')\n    (5, '0.389*\"\" + 0.024*\"基金\" + 0.006*\"市场\" + 0.005*\"投资\" + 0.005*\"公司\" + 0.004*\"中\" + 0.003*\"中国\" + 0.003*\"元\" + 0.003*\"产品\" + 0.002*\"新\"')\n    \n\n\n```python\n# coding=utf-8\nimport gensim\nfrom gensim.models import word2vec\ncut_file='./data/cnews.train_jieba.txt'\nsave_model_name = './data/train_word2vec.model.bin'\nsentences =word2vec.Text8Corpus(cut_file)  # 加载语料\nmodel =gensim.models.Word2Vec(sentences, size=200)  # 训练skip-gram模型; 默认window=5\nmodel.save(save_model_name)\nfrom gensim.models import word2vec\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmopdelfilePath = './data/train_word2vec.model.bin'\nmodel = word2vec.Word2Vec.load(mopdelfilePath)\nraw_word_vec = model.wv.vectors\n\ncent_word1 = \"新闻\"\ncent_word2 = \"娱乐\"\ncent_word3 = \"家具\"\ncent_word4 = \"房产\"\ncent_word5 = \"教育\"\ncent_word6 = \"时尚\"\n\n\nwordList1 = model.most_similar(cent_word1)\nwordList2 = model.most_similar(cent_word2)\nwordList3 = model.most_similar(cent_word3)\nwordList4 = model.most_similar(cent_word4)\nwordList5 = model.most_similar(cent_word5)\nwordList6 = model.most_similar(cent_word6)\n\n\nwordList1 = np.append([item[0] for item in wordList1], cent_word1)\nwordList2 = np.append([item[0] for item in wordList2], cent_word2)\nwordList3 = np.append([item[0] for item in wordList3], cent_word3)\nwordList4 = np.append([item[0] for item in wordList4], cent_word4)\nwordList5 = np.append([item[0] for item in wordList5], cent_word5)\nwordList6 = np.append([item[0] for item in wordList6], cent_word6)\n\n\ndef get_word_index(word):\n    index = model.wv.vocab[word].index\n    return index\n\nindex_list1 = map(get_word_index, wordList1)\nindex_list2 = map(get_word_index, wordList2)\nindex_list3 = map(get_word_index, wordList3)\nindex_list4 = map(get_word_index, wordList4)\nindex_list5 = map(get_word_index, wordList5)\nindex_list6 = map(get_word_index, wordList6)\n\n\nvec_reduced = PCA(n_components=2).fit_transform(raw_word_vec)\nzhfont = matplotlib.font_manager.FontProperties(fname='./data/msyh.ttf')\nx = np.arange(-10, 10, 0.1)\ny = x\nplt.plot(x, y)\n\nfor i in index_list1:\n    plt.text(vec_reduced[i][0], vec_reduced[i][1], model.wv.index2word[i], color='r', fontproperties=zhfont)\n\nfor i in index_list2:\n    plt.text(vec_reduced[i][0], vec_reduced[i][1], model.wv.index2word[i], color='b', fontproperties=zhfont)\n\nfor i in index_list3:\n    plt.text(vec_reduced[i][0], vec_reduced[i][1], model.wv.index2word[i], color='g', fontproperties=zhfont)\n\nfor i in index_list4:\n    plt.text(vec_reduced[i][0], vec_reduced[i][1], model.wv.index2word[i], color='k', fontproperties=zhfont)\n\nfor i in index_list5:\n    plt.text(vec_reduced[i][0], vec_reduced[i][1], model.wv.index2word[i], color='c', fontproperties=zhfont)\nfor i in index_list6:\n    plt.text(vec_reduced[i][0], vec_reduced[i][1], model.wv.index2word[i], color='c', fontproperties=zhfont)\n\nplt.show()\nplt.savefig(\"./img/title.png\")\n\n```\n\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n    /usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n      if np.issubdtype(vec.dtype, np.int):\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n    \n\n\n![png](2_LDA_files/2_LDA_5_1.png)\n\n\n\n    <Figure size 432x288 with 0 Axes>\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/6seq2seq_v2/1.seq2seq_visualization/","content":"\n# 图解seq2seq\n\n\n### 1.seq2seq（序列到序列模型）简介\n对于很多自然语言处理任务，比如**聊天机器人，机器翻译，自动文摘，智能问答**等，传统的解决方案都是**检索式(从候选集中选出答案)**，这对素材的完善程度要求很高，随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，它是目前自然语言处理技术中非常重要和流行的一个模型，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务的先河，并且被证实在各主流语言之间的相互翻译以及语音助手中人机短问快答的应用中有着非常好的表现，我们在这个notebook中主要给大家以动图的方式展示一下seq2seq模型的一些细节。\n\n参考资料:[Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n\n### 2.编码解码模型\n\n\n序列到序列的模型是非常有意思的NLP模型，我们的很多NLP任务，是文本到文本的映射(对应)，这个过程就像是下面图里展示的过程。当然seq2seq模型不仅仅是用在NLP中的模型，它的输入也可以是语音信号或者图像表示。\n\n![](./img/[1]_seq2seq_1.gif)\n\n更具体一点，在NLP的任务中，其实输入的是文本序列，输出的很多时候也是文本序列，下图所示的是一个典型的机器翻译任务中，输入的文本序列(源语言表述)到输出的文本序列(目标语言表述)之间的变换。\n\n![](./img/[2]_seq2seq_2.gif)\n\n更细节一点的结构是一个“编码解码器”结构，编码器处理输入序列中的每个元素(在这里可能是1个词)，将捕获的信息编译成向量（称为上下文内容向量）。在处理整个输入序列之后，编码器将上下文发送到解码器，解码器逐项开始产生输出序列。\n\n![](./img/[3]_seq2seq_3.gif)\n\n在机器翻译的场景下，是下面这样的。\n\n![](./img/[4]_seq2seq_4.gif)\n\n所谓的上下文向量其实就是\n\n![](./img/context.png)\n\n输入的数据(文本序列)中的每个元素(词)通常会被编码成一个稠密的向量，这个过程叫做word embedding，如下图所示\n\n![](./img/embedding_seq2seq.png)\n\n我们的encoder和decoder都会借助于循环神经网络(RNN)这类特殊的神经网络完成，循环神经网络会接受每个位置(时间点)上的输入，同时经过处理进行信息融合，并可能会在某些位置(时间点)上输出。如下图所示。\n\n![](./img/[5]_RNN_1.gif)\n\n所以动态地展示整个编码器和解码器，分拆的步骤过程大概是下面这个样子。\n\n![](./img/[6]_seq2seq_6.gif)\n\n更详细地展开，其实是这样的。\n\n![](./img/[7]_seq2seq_7.gif)\n\n在更多的时候，我们考虑到提升效果，不会寄希望于把所有的内容都放到一个上下文向量(context vector)中，而是会采用一个叫做**注意力模型**的模型来动态处理和解码，动态的图如下所示。\n\n![](./img/[8]_seq2seq_8.gif)\n\n所谓的注意力机制，可以粗略地理解为是一种对于输入的信息，根据重要程度进行不同权重的加权处理(通常加权的权重来源于softmax后的结果)的机制，如下图所示，是一个在解码阶段，简单地对编码器中的hidden states进行不同权重的加权处理的过程。\n\n![](./img/[9]_seq2seq_9.gif)\n\n更详细一点的**注意力解码**过程如下图所示。\n\n- 带注意力的解码器RNN接收<END>的嵌入(embedding)和一个初始的解码器隐藏状态(hidden state)。\n- RNN处理输入，产生输出和新的隐藏状态向量（h4），输出被摒弃不用。\n- attention的步骤：使用编码器隐藏状态(hidden state)和h4向量来计算该时间步长的上下文向量（C4）。\n- 把h4和C4拼接成一个向量。\n- 把拼接后的向量连接全连接层和softmax完成解码\n- 每个时间点上重复这个操作\n\n![](./img/attention_tensor_dance.gif)\n\n也可以把这个动态解码的过程展示成下述图所示的过程。\n\n![](./img/[11]_seq2seq_9.gif)\n\n注意力机制是一个很神奇地可以学习源语言和目标语言之间词和词对齐关系的方式。如下图所示。\n\n![](./img/attention_sentence.png)\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/6seq2seq_v2/2.seq2seq_details/","content":"\n# 图解seq2seq\n\n\n### 1.seq2seq（序列到序列模型）简介\n对于很多自然语言处理任务，比如**聊天机器人，机器翻译，自动文摘，智能问答**等，传统的解决方案都是**检索式(从候选集中选出答案)**，这对素材的完善程度要求很高，随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，它是目前自然语言处理技术中非常重要和流行的一个模型，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务的先河，并且被证实在各主流语言之间的相互翻译以及语音助手中人机短问快答的应用中有着非常好的表现，我们在这个notebook中主要给大家以动图的方式展示一下seq2seq模型的一些细节。\n\n参考资料:[图解seq2seq](https://zhuanlan.zhihu.com/p/40920384)\n\n### seq2seq模型\n\n\nseq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。\n\n![](./img/pic1-edit.jpg)\n\n输入： $x = (x_1,...,x_{T_x})$\n\n输出： $y = (y_1,...,y_{T_y})$\n\n(1) $h_t = RNN_{enc}(x_t, h_{t-1})$ , Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。\n\n(2) $s_t = RNN_{dec}(\\hat{y_{t-1}},s_{t-1})$ ， Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。\n\n(3) $c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$ , context vector是一个对于encoder输出的hidden states的一个加权平均。\n\n(4) $\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$ , 每一个encoder的hidden states对应的权重。\n\n(5) $e_{ij} = score(s_i, h_j)$ , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)\n\n(6) $\\hat{s_t} = tanh(W_c[c_t;s_t])$, 将context vector 和 decoder的hidden states 串起来。\n\n(7) $p(y_t|y_{<t},x) = softmax(W_s\\hat{s_t})$ ，计算最后的输出概率。\n\n![](./img/pic2-edit.jpg)\n\n(1) $h_t = RNN_{enc}(x_t, h_{t-1})$ , Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。\n\n![](./img/pic3-edit.jpg)\n\n(2) $s_t = RNN_{dec}(\\hat{y_{t-1}},s_{t-1})$ ， Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。\n\n![](./img/pic4-edit.jpg)\n\n(3) $c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$ , context vector是一个对于encoder输出的hidden states的一个加权平均。\n\n(4) $\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$ , 每一个encoder的hidden states对应的权重。\n\n(5) $e_{ij} = score(s_i, h_j)$ , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)\n\n![](./img/pic5-edit.jpg)\n\n下一个时间点\n\n![](./img/pic6-edit.jpg)\n\n(6) $\\hat{s_t} = tanh(W_c[c_t;s_t])$, 将context vector 和 decoder的hidden states 串起来。\n\n(7) $p(y_t|y_{<t},x) = softmax(W_s\\hat{s_t})$ ，计算最后的输出概率。\n\n![](./img/pic7-edit.jpg)\n\n在luong中提到了三种score的计算方法。这里图解前两种：\n![](./img/score.png)\n\n#### 第1种\n输入是encoder的所有hidden states H: 大小为(hid dim, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim, 1）。\n\n第一步：旋转H为（sequence length, hid dim) 与s做点乘得到一个 大小为(sequence length, 1)的分数。\n\n第二步：对分数做softmax得到一个合为1的权重。\n\n第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。\n\n![](./img/pic8-edit.jpg)\n\n#### 第2种\n输入是encoder的所有hidden states H: 大小为(hid dim1, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim2, 1）。此处两个hidden state的纬度并不一样。\n\n第一步：旋转H为（sequence length, hid dim1) 与 Wa [大小为 hid dim1, hid dim 2)] 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数。\n\n第二步：对分数做softmax得到一个合为1的权重。\n\n第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。\n\n![](./img/pic9-edit.jpg)\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/8Machine_Translation/1.Statistical_machine_translation/Statistical_machine_translation/","content":"\n# moses统计翻译系统实战\n\n\n\n```bash\n%%bash\n\n# 安装 Moses\n# http://www.statmt.org/moses/?n=Development.GetStarted\n\n# 下载数据集\ncorpus=\"$PWD/corpus\"\nmkdir -p $corpus\ncd $corpus \nwget http://www.statmt.org/wmt13/training-parallel-nc-v8.tgz\ntar zxvf training-parallel-nc-v8.tgz\n```\n\n    training/news-commentary-v8.cs-en.cs\n    training/news-commentary-v8.cs-en.en\n    training/news-commentary-v8.de-en.de\n    training/news-commentary-v8.de-en.en\n    training/news-commentary-v8.es-en.en\n    training/news-commentary-v8.es-en.es\n    training/news-commentary-v8.fr-en.en\n    training/news-commentary-v8.fr-en.fr\n    training/news-commentary-v8.ru-en.en\n    training/news-commentary-v8.ru-en.ru\n    \n\n\n```bash\n%%bash\ncorpus=\"$PWD/corpus\"\nhead -n 5 $corpus/training/news-commentary-v8.fr-en.en\n```\n\n    SAN FRANCISCO – It has never been easy to have a rational conversation about the value of gold.\n    Lately, with gold prices up more than 300% over the last decade, it is harder than ever.\n    Just last December, fellow economists Martin Feldstein and Nouriel Roubini each penned op-eds bravely questioning bullish market sentiment, sensibly pointing out gold’s risks.\n    Wouldn’t you know it?\n    Since their articles appeared, the price of gold has moved up still further. Gold prices even hit a record-high $1,300 recently.\n    \n\n\n```bash\n%%bash\ncorpus=\"$PWD/corpus\"\nhead -n 5 $corpus/training/news-commentary-v8.fr-en.fr\n```\n\n    SAN FRANCISCO – Il n’a jamais été facile d’avoir une discussion rationnelle sur la valeur du métal jaune.\n    Et aujourd’hui, alors que le cours de l’or a augmenté de 300 pour cent au cours de la dernière décennie, c’est plus difficile que jamais.\n    En décembre dernier, mes collègues économistes Martin Feldstein et Nouriel Roubini ont chacun publié une tribune libre dans laquelle ils doutaient courageusement du marché haussier, soulignant de manière sensée les risques liés à l’or.\n    Mais devinez ce qui s’est passé ?\n    Depuis la parution de leurs articles, le cours de l’or a encore grimpé, pour atteindre récemment un plus haut historique de 1300 dollars l’once.\n    \n\n\n```bash\n%%bash\ncorpus=\"$PWD/corpus\"\nwc -l $corpus/training/news-commentary-v8.fr-en.{fr,en}\n```\n\n      157168 /Users/jjhu/remoteshare/MT/corpus/training/news-commentary-v8.fr-en.fr\n      157168 /Users/jjhu/remoteshare/MT/corpus/training/news-commentary-v8.fr-en.en\n      314336 total\n    \n\n\n```bash\n%%bash\n# tokenization: 对句子进行符号化\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\n$mosesdecoder/scripts/tokenizer/tokenizer.perl -l en \\\n   < $corpus/training/news-commentary-v8.fr-en.en \\\n   > $corpus/news-commentary-v8.fr-en.tok.en\n\n$mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr \\\n   < $corpus/training/news-commentary-v8.fr-en.fr \\\n   > $corpus/news-commentary-v8.fr-en.tok.fr\n\n```\n\n    Tokenizer Version 1.1\n    Language: en\n    Number of threads: 1\n    Tokenizer Version 1.1\n    Language: fr\n    Number of threads: 1\n    \n\n\n```bash\n%%bash \n\n# 训练 truecaser: 将句子第一个字母变成小写\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\n$mosesdecoder/scripts/recaser/train-truecaser.perl \\\n     --model $corpus/truecase-model.en --corpus     \\\n     $corpus/news-commentary-v8.fr-en.tok.en\n $mosesdecoder/scripts/recaser/train-truecaser.perl \\\n     --model $corpus/truecase-model.fr --corpus     \\\n     $corpus/news-commentary-v8.fr-en.tok.fr\n```\n\n\n```bash\n%%bash \n\n# 将句子第一个字母变成小写\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\n $mosesdecoder/scripts/recaser/truecase.perl \\\n   --model $corpus/truecase-model.en         \\\n   < $corpus/news-commentary-v8.fr-en.tok.en \\\n   > $corpus/news-commentary-v8.fr-en.true.en\n $mosesdecoder/scripts/recaser/truecase.perl \\\n   --model $corpus/truecase-model.fr         \\\n   < $corpus/news-commentary-v8.fr-en.tok.fr \\\n   > $corpus/news-commentary-v8.fr-en.true.fr\n```\n\n\n```bash\n%%bash \n\n# 将双语语料库中句子单词个数多于80的句子去除\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\n$mosesdecoder/scripts/training/clean-corpus-n.perl \\\n    $corpus/news-commentary-v8.fr-en.true fr en \\\n    $corpus/news-commentary-v8.fr-en.clean 1 80\n```\n\n    clean-corpus.perl: processing /home/jjhu/MT/corpus/news-commentary-v8.fr-en.true.fr & .en to /home/jjhu/MT/corpus/news-commentary-v8.fr-en.clean, cutoff 1-80, ratio 9\n    ..........(100000).....\n    Input sentences: 157168  Output sentences:  155362\n    \n\n\n```bash\n%%bash\n\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\n# 训练语言模型\nlm=\"$PWD/lm\"\nmkdir $lm\ncd $lm\n$mosesdecoder/bin/lmplz -o 3 < $corpus/news-commentary-v8.fr-en.true.en > $lm/news-commentary-v8.fr-en.arpa.en\n$mosesdecoder/bin/build_binary $lm/news-commentary-v8.fr-en.arpa.en $lm/news-commentary-v8.fr-en.blm.en\n```\n\n    === 1/5 Counting and sorting n-grams ===\n    Reading /home/jjhu/MT/corpus/news-commentary-v8.fr-en.true.en\n    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n    ****************************************************************************************************\n    Unigram tokens 4066728 types 62719\n    === 2/5 Calculating and sorting adjusted counts ===\n    Chain sizes: 1:752628 2:9359020032 3:17548163072\n    Statistics:\n    1 62719 D1=0.622525 D2=0.981811 D3+=1.36293\n    2 906706 D1=0.742501 D2=1.07915 D3+=1.38352\n    3 2389081 D1=0.835943 D2=1.1625 D3+=1.34981\n    Memory estimate for binary LM:\n    type    MB\n    probing 63 assuming -p 1.5\n    probing 68 assuming -r models -p 1.5\n    trie    25 without quantization\n    trie    14 assuming -q 8 -b 8 quantization \n    trie    24 assuming -a 22 array pointer compression\n    trie    12 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n    === 3/5 Calculating and sorting initial probabilities ===\n    Chain sizes: 1:752628 2:14507296 3:47781620\n    === 4/5 Calculating and writing order-interpolated probabilities ===\n    Chain sizes: 1:752628 2:14507296 3:47781620\n    === 5/5 Writing ARPA model ===\n    Name:lmplz\tVmPeak:26452684 kB\tVmRSS:20124 kB\tRSSMax:6136000 kB\tuser:4.572\tsys:0.976\tCPU:5.548\treal:4.68449\n    Reading /home/jjhu/MT/lm/news-commentary-v8.fr-en.arpa.en\n    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n    ****************************************************************************************************\n    SUCCESS\n    \n\n\n```bash\n%%bash \n\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\nworking=\"$PWD/working\"\nlm=\"$PWD/lm\"\n# 训练翻译模型\nmkdir $working\ncd $working\nnohup nice $mosesdecoder/scripts/training/train-model.perl -root-dir train \\\n -corpus $corpus/news-commentary-v8.fr-en.clean                             \\\n -f fr -e en -alignment grow-diag-final-and -reordering msd-bidirectional-fe \\\n -lm 0:3:$lm/news-commentary-v8.fr-en.blm.en:8                          \\\n -external-bin-dir $mosesdecoder/tools >& training.out &\n```\n\n\n```bash\n%%bash \ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\nworking=\"$PWD/working\"\nlm=\"$PWD/lm\"\n\n# 利用dev 数据调整模型参数\ncd $corpus\nwget http://www.statmt.org/wmt12/dev.tgz\ntar zxvf dev.tgz\n\n# 处理dev 数据， tokenization + truecase\n$mosesdecoder/scripts/tokenizer/tokenizer.perl -l en \\\n   < $corpus/dev/news-test2008.en > $corpus/news-test2008.tok.en\n$mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr \\\n   < $corpus/dev/news-test2008.fr > $corpus/news-test2008.tok.fr\n$mosesdecoder/scripts/recaser/truecase.perl --model truecase-model.en \\\n   < $corpus/news-test2008.tok.en > $corpus/news-test2008.true.en\n$mosesdecoder/scripts/recaser/truecase.perl --model truecase-model.fr \\\n   < $corpus/news-test2008.tok.fr > $corpus/news-test2008.true.fr\n```\n\n    dev/\n    dev/newstest2009-src.fr.sgm\n    dev/news-test2008-src.hu.sgm\n    dev/newssyscomb2009-ref.cs.sgm\n    dev/newstest2009-ref.en.sgm\n    dev/newstest2009-ref.hu.sgm\n    dev/newstest2010.cs\n    dev/newssyscomb2009-ref.hu.sgm\n    dev/newstest2010-src.es.sgm\n    dev/newstest2011-src.es.sgm\n    dev/newstest2010-src.fr.sgm\n    dev/newssyscomb2009-src.fr.sgm\n    dev/newstest2011-ref.de.sgm\n    dev/news-test2008-ref.es.sgm\n    dev/newssyscomb2009-ref.de.sgm\n    dev/newstest2011.fr\n    dev/news-test2008.cs\n    dev/news-test2008-ref.fr.sgm\n    dev/newstest2009-ref.cz.sgm\n    dev/newstest2011-src.de.sgm\n    dev/newstest2011-ref.cs.sgm\n    dev/raw/\n    dev/raw/newstest2011-src.fr.raw.sgm\n    dev/raw/newstest2011-src.es.raw.sgm\n    dev/raw/newstest2011-src.cs.raw.sgm\n    dev/raw/newstest2011-ref.fr.raw.sgm\n    dev/raw/newstest2011-ref.es.raw.sgm\n    dev/raw/newstest2011-ref.en.raw.sgm\n    dev/raw/newstest2011-src.de.raw.sgm\n    dev/raw/newstest2011-src.en.raw.sgm\n    dev/raw/newstest2011-ref.de.raw.sgm\n    dev/raw/newstest2011-ref.cs.raw.sgm\n    dev/news-test2008-src.fr.sgm\n    dev/newssyscomb2009-src.it.sgm\n    dev/newstest2009.fr\n    dev/news-test2008-ref.hu.sgm\n    dev/newstest2009-src.it.sgm\n    dev/newstest2009.cs\n    dev/news-test2008-src.de.sgm\n    dev/newstest2009-src.es.sgm\n    dev/newstest2009.cz\n    dev/newstest2010-ref.en.sgm\n    dev/newstest2009-ref.it.sgm\n    dev/newstest2011-ref.es.sgm\n    dev/news-test2008.es\n    dev/news-test2008.cz\n    dev/newstest2010-src.en.sgm\n    dev/newstest2011.en\n    dev/news-test2008-ref.cz.sgm\n    dev/newstest2009-src.hu.sgm\n    dev/news-test2008.fr\n    dev/newstest2009.en\n    dev/newstest2009-ref.cs.sgm\n    dev/newssyscomb2009-src.cs.sgm\n    dev/news-test2008-ref.de.sgm\n    dev/newssyscomb2009-src.cz.sgm\n    dev/newssyscomb2009-ref.en.sgm\n    dev/newstest2011-ref.fr.sgm\n    dev/news-test2008.en\n    dev/newstest2009-ref.de.sgm\n    dev/newstest2009-src.en.sgm\n    dev/newstest2011-src.fr.sgm\n    dev/newstest2011-ref.en.sgm\n    dev/newstest2010-ref.cz.sgm\n    dev/newstest2010-src.cs.sgm\n    dev/newssyscomb2009.de\n    dev/newstest2010-ref.cs.sgm\n    dev/newssyscomb2009-ref.it.sgm\n    dev/newstest2009.de\n    dev/newssyscomb2009.cs\n    dev/newstest2009-src.de.sgm\n    dev/newstest2009-src.xx.sgm\n    dev/newssyscomb2009.fr\n    dev/news-test2008-ref.cs.sgm\n    dev/newstest2010.en\n    dev/newstest2010-src.cz.sgm\n    dev/newstest2011-src.en.sgm\n    dev/newssyscomb2009-src.en.sgm\n    dev/newssyscomb2009-src.de.sgm\n    dev/news-test2008-ref.en.sgm\n    dev/newstest2011.de\n    dev/newssyscomb2009.en\n    dev/newstest2011.es\n    dev/newstest2009-src.cz.sgm\n    dev/newssyscomb2009-ref.fr.sgm\n    dev/newstest2010.de\n    dev/newstest2010.es\n    dev/newstest2010-src.de.sgm\n    dev/newstest2009.es\n    dev/newstest2009-ref.es.sgm\n    dev/news-test2008-src.en.sgm\n    dev/newstest2009-ref.fr.sgm\n    dev/newssyscomb2009-src.hu.sgm\n    dev/newssyscomb2009-src.es.sgm\n    dev/news-test2008-src.es.sgm\n    dev/news-test2008.de\n    dev/newstest2011.cs\n    dev/newstest2010-ref.es.sgm\n    dev/news-test2008-src.cs.sgm\n    dev/newstest2010.fr\n    dev/newstest2009-src.cs.sgm\n    dev/newstest2010.cz\n    dev/newssyscomb2009-ref.cz.sgm\n    dev/newstest2010-ref.de.sgm\n    dev/newssyscomb2009.es\n    dev/newstest2010-ref.fr.sgm\n    dev/news-test2008-src.cz.sgm\n    dev/newstest2011-src.cs.sgm\n    dev/newssyscomb2009-ref.es.sgm\n    \n\n    wget: /home/jjhu/Software/anaconda2/envs/py36/lib/libcrypto.so.1.0.0: no version information available (required by wget)\n    wget: /home/jjhu/Software/anaconda2/envs/py36/lib/libuuid.so.1: no version information available (required by wget)\n    wget: /home/jjhu/Software/anaconda2/envs/py36/lib/libssl.so.1.0.0: no version information available (required by wget)\n    --2019-01-20 01:52:41--  http://www.statmt.org/wmt12/dev.tgz\n    Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n    Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 13260990 (13M) [application/x-gzip]\n    Saving to: ‘dev.tgz’\n    \n         0K .......... .......... .......... .......... ..........  0%  252K 51s\n        50K .......... .......... .......... .......... ..........  0%  479K 39s\n       100K .......... .......... .......... .......... ..........  1% 28.9M 26s\n       150K .......... .......... .......... .......... ..........  1%  102M 19s\n       200K .......... .......... .......... .......... ..........  1%  508K 20s\n       250K .......... .......... .......... .......... ..........  2% 48.9M 17s\n       300K .......... .......... .......... .......... ..........  2% 62.3M 15s\n       350K .......... .......... .......... .......... ..........  3% 51.4M 13s\n       400K .......... .......... .......... .......... ..........  3%  378K 15s\n       450K .......... .......... .......... .......... ..........  3% 65.5M 13s\n       500K .......... .......... .......... .......... ..........  4% 70.5M 12s\n       550K .......... .......... .......... .......... ..........  4%  109M 11s\n       600K .......... .......... .......... .......... ..........  5%  108M 10s\n       650K .......... .......... .......... .......... ..........  5% 31.5M 9s\n       700K .......... .......... .......... .......... ..........  5%  518K 10s\n       750K .......... .......... .......... .......... ..........  6% 35.3M 10s\n       800K .......... .......... .......... .......... ..........  6% 43.2M 9s\n       850K .......... .......... .......... .......... ..........  6% 43.7M 9s\n       900K .......... .......... .......... .......... ..........  7% 47.2M 8s\n       950K .......... .......... .......... .......... ..........  7%  532K 9s\n      1000K .......... .......... .......... .......... ..........  8% 39.8M 8s\n      1050K .......... .......... .......... .......... ..........  8% 36.3M 8s\n      1100K .......... .......... .......... .......... ..........  8% 35.5M 8s\n      1150K .......... .......... .......... .......... ..........  9% 50.1M 7s\n      1200K .......... .......... .......... .......... ..........  9% 44.5M 7s\n      1250K .......... .......... .......... .......... .......... 10%  536K 7s\n      1300K .......... .......... .......... .......... .......... 10% 29.8M 7s\n      1350K .......... .......... .......... .......... .......... 10% 34.8M 7s\n      1400K .......... .......... .......... .......... .......... 11% 54.8M 7s\n      1450K .......... .......... .......... .......... .......... 11% 30.8M 6s\n      1500K .......... .......... .......... .......... .......... 11% 51.6M 6s\n      1550K .......... .......... .......... .......... .......... 12%  537K 7s\n      1600K .......... .......... .......... .......... .......... 12% 45.9M 6s\n      1650K .......... .......... .......... .......... .......... 13% 31.1M 6s\n      1700K .......... .......... .......... .......... .......... 13% 45.0M 6s\n      1750K .......... .......... .......... .......... .......... 13% 34.0M 6s\n      1800K .......... .......... .......... .......... .......... 14% 46.0M 6s\n      1850K .......... .......... .......... .......... .......... 14%  540K 6s\n      1900K .......... .......... .......... .......... .......... 15% 31.4M 6s\n      1950K .......... .......... .......... .......... .......... 15% 38.2M 6s\n      2000K .......... .......... .......... .......... .......... 15% 36.4M 6s\n      2050K .......... .......... .......... .......... .......... 16% 44.8M 5s\n      2100K .......... .......... .......... .......... .......... 16% 39.5M 5s\n      2150K .......... .......... .......... .......... .......... 16% 42.1M 5s\n      2200K .......... .......... .......... .......... .......... 17%  543K 5s\n      2250K .......... .......... .......... .......... .......... 17% 32.1M 5s\n      2300K .......... .......... .......... .......... .......... 18% 32.8M 5s\n      2350K .......... .......... .......... .......... .......... 18% 44.0M 5s\n      2400K .......... .......... .......... .......... .......... 18% 37.8M 5s\n      2450K .......... .......... .......... .......... .......... 19% 39.1M 5s\n      2500K .......... .......... .......... .......... .......... 19% 37.7M 5s\n      2550K .......... .......... .......... .......... .......... 20%  548K 5s\n      2600K .......... .......... .......... .......... .......... 20% 26.9M 5s\n      2650K .......... .......... .......... .......... .......... 20% 38.8M 5s\n      2700K .......... .......... .......... .......... .......... 21% 37.2M 5s\n      2750K .......... .......... .......... .......... .......... 21% 42.0M 4s\n      2800K .......... .......... .......... .......... .......... 22% 43.6M 4s\n      2850K .......... .......... .......... .......... .......... 22% 43.6M 4s\n      2900K .......... .......... .......... .......... .......... 22%  550K 5s\n      2950K .......... .......... .......... .......... .......... 23% 26.6M 4s\n      3000K .......... .......... .......... .......... .......... 23% 42.0M 4s\n      3050K .......... .......... .......... .......... .......... 23% 32.5M 4s\n      3100K .......... .......... .......... .......... .......... 24% 50.7M 4s\n      3150K .......... .......... .......... .......... .......... 24% 40.0M 4s\n      3200K .......... .......... .......... .......... .......... 25% 44.2M 4s\n      3250K .......... .......... .......... .......... .......... 25%  552K 4s\n      3300K .......... .......... .......... .......... .......... 25% 29.1M 4s\n      3350K .......... .......... .......... .......... .......... 26% 35.5M 4s\n      3400K .......... .......... .......... .......... .......... 26% 38.2M 4s\n      3450K .......... .......... .......... .......... .......... 27% 41.3M 4s\n      3500K .......... .......... .......... .......... .......... 27% 42.6M 4s\n      3550K .......... .......... .......... .......... .......... 27% 48.0M 4s\n      3600K .......... .......... .......... .......... .......... 28% 43.4M 4s\n      3650K .......... .......... .......... .......... .......... 28%  553K 4s\n      3700K .......... .......... .......... .......... .......... 28% 26.4M 4s\n      3750K .......... .......... .......... .......... .......... 29% 40.6M 4s\n      3800K .......... .......... .......... .......... .......... 29% 42.8M 4s\n      3850K .......... .......... .......... .......... .......... 30% 43.0M 4s\n      3900K .......... .......... .......... .......... .......... 30% 39.1M 4s\n      3950K .......... .......... .......... .......... .......... 30% 63.1M 3s\n      4000K .......... .......... .......... .......... .......... 31% 43.2M 3s\n      4050K .......... .......... .......... .......... .......... 31% 44.4M 3s\n      4100K .......... .......... .......... .......... .......... 32%  551K 3s\n      4150K .......... .......... .......... .......... .......... 32% 30.2M 3s\n      4200K .......... .......... .......... .......... .......... 32% 59.7M 3s\n      4250K .......... .......... .......... .......... .......... 33% 41.1M 3s\n      4300K .......... .......... .......... .......... .......... 33% 38.5M 3s\n      4350K .......... .......... .......... .......... .......... 33% 54.1M 3s\n      4400K .......... .......... .......... .......... .......... 34% 75.4M 3s\n      4450K .......... .......... .......... .......... .......... 34% 35.0M 3s\n      4500K .......... .......... .......... .......... .......... 35%  555K 3s\n      4550K .......... .......... .......... .......... .......... 35% 33.4M 3s\n      4600K .......... .......... .......... .......... .......... 35% 30.5M 3s\n      4650K .......... .......... .......... .......... .......... 36% 59.5M 3s\n      4700K .......... .......... .......... .......... .......... 36% 52.1M 3s\n      4750K .......... .......... .......... .......... .......... 37% 42.4M 3s\n      4800K .......... .......... .......... .......... .......... 37% 43.8M 3s\n      4850K .......... .......... .......... .......... .......... 37% 73.0M 3s\n      4900K .......... .......... .......... .......... .......... 38% 38.4M 3s\n      4950K .......... .......... .......... .......... .......... 38%  556K 3s\n      5000K .......... .......... .......... .......... .......... 38% 28.6M 3s\n      5050K .......... .......... .......... .......... .......... 39% 42.3M 3s\n      5100K .......... .......... .......... .......... .......... 39% 39.7M 3s\n      5150K .......... .......... .......... .......... .......... 40% 45.0M 3s\n      5200K .......... .......... .......... .......... .......... 40% 58.1M 3s\n      5250K .......... .......... .......... .......... .......... 40% 49.0M 3s\n      5300K .......... .......... .......... .......... .......... 41% 52.7M 3s\n      5350K .......... .......... .......... .......... .......... 41% 64.7M 3s\n      5400K .......... .......... .......... .......... .......... 42% 37.4M 3s\n      5450K .......... .......... .......... .......... .......... 42%  557K 3s\n      5500K .......... .......... .......... .......... .......... 42% 35.3M 3s\n      5550K .......... .......... .......... .......... .......... 43% 38.1M 3s\n      5600K .......... .......... .......... .......... .......... 43% 37.6M 2s\n      5650K .......... .......... .......... .......... .......... 44% 86.6M 2s\n      5700K .......... .......... .......... .......... .......... 44% 41.4M 2s\n      5750K .......... .......... .......... .......... .......... 44% 47.1M 2s\n      5800K .......... .......... .......... .......... .......... 45% 63.8M 2s\n      5850K .......... .......... .......... .......... .......... 45% 64.4M 2s\n      5900K .......... .......... .......... .......... .......... 45% 40.8M 2s\n      5950K .......... .......... .......... .......... .......... 46%  557K 2s\n      6000K .......... .......... .......... .......... .......... 46% 42.3M 2s\n      6050K .......... .......... .......... .......... .......... 47% 37.8M 2s\n      6100K .......... .......... .......... .......... .......... 47% 39.9M 2s\n      6150K .......... .......... .......... .......... .......... 47% 49.7M 2s\n      6200K .......... .......... .......... .......... .......... 48% 58.0M 2s\n      6250K .......... .......... .......... .......... .......... 48% 56.6M 2s\n      6300K .......... .......... .......... .......... .......... 49% 81.2M 2s\n      6350K .......... .......... .......... .......... .......... 49% 65.4M 2s\n      6400K .......... .......... .......... .......... .......... 49% 47.2M 2s\n      6450K .......... .......... .......... .......... .......... 50% 39.2M 2s\n      6500K .......... .......... .......... .......... .......... 50%  558K 2s\n      6550K .......... .......... .......... .......... .......... 50% 41.7M 2s\n      6600K .......... .......... .......... .......... .......... 51% 39.9M 2s\n      6650K .......... .......... .......... .......... .......... 51% 39.9M 2s\n      6700K .......... .......... .......... .......... .......... 52% 63.8M 2s\n      6750K .......... .......... .......... .......... .......... 52% 53.9M 2s\n      6800K .......... .......... .......... .......... .......... 52% 52.8M 2s\n      6850K .......... .......... .......... .......... .......... 53% 74.6M 2s\n      6900K .......... .......... .......... .......... .......... 53% 71.5M 2s\n      6950K .......... .......... .......... .......... .......... 54% 57.7M 2s\n      7000K .......... .......... .......... .......... .......... 54% 48.3M 2s\n      7050K .......... .......... .......... .......... .......... 54%  560K 2s\n      7100K .......... .......... .......... .......... .......... 55% 36.4M 2s\n      7150K .......... .......... .......... .......... .......... 55% 37.6M 2s\n      7200K .......... .......... .......... .......... .......... 55% 49.8M 2s\n      7250K .......... .......... .......... .......... .......... 56% 57.2M 2s\n      7300K .......... .......... .......... .......... .......... 56% 43.0M 2s\n      7350K .......... .......... .......... .......... .......... 57% 70.5M 2s\n      7400K .......... .......... .......... .......... .......... 57% 59.3M 2s\n      7450K .......... .......... .......... .......... .......... 57% 68.4M 2s\n      7500K .......... .......... .......... .......... .......... 58% 66.2M 2s\n      7550K .......... .......... .......... .......... .......... 58% 60.8M 2s\n      7600K .......... .......... .......... .......... .......... 59% 40.0M 2s\n      7650K .......... .......... .......... .......... .......... 59%  559K 2s\n      7700K .......... .......... .......... .......... .......... 59% 49.4M 2s\n      7750K .......... .......... .......... .......... .......... 60% 47.7M 2s\n      7800K .......... .......... .......... .......... .......... 60% 48.4M 2s\n      7850K .......... .......... .......... .......... .......... 61% 58.5M 1s\n      7900K .......... .......... .......... .......... .......... 61% 48.4M 1s\n      7950K .......... .......... .......... .......... .......... 61% 76.9M 1s\n      8000K .......... .......... .......... .......... .......... 62% 59.8M 1s\n      8050K .......... .......... .......... .......... .......... 62% 62.8M 1s\n      8100K .......... .......... .......... .......... .......... 62% 70.7M 1s\n      8150K .......... .......... .......... .......... .......... 63% 88.3M 1s\n      8200K .......... .......... .......... .......... .......... 63% 63.6M 1s\n      8250K .......... .......... .......... .......... .......... 64%  562K 1s\n      8300K .......... .......... .......... .......... .......... 64% 31.1M 1s\n      8350K .......... .......... .......... .......... .......... 64% 49.1M 1s\n      8400K .......... .......... .......... .......... .......... 65% 34.9M 1s\n      8450K .......... .......... .......... .......... .......... 65% 69.6M 1s\n      8500K .......... .......... .......... .......... .......... 66% 74.9M 1s\n      8550K .......... .......... .......... .......... .......... 66% 51.2M 1s\n      8600K .......... .......... .......... .......... .......... 66% 87.6M 1s\n      8650K .......... .......... .......... .......... .......... 67% 56.2M 1s\n      8700K .......... .......... .......... .......... .......... 67% 61.1M 1s\n      8750K .......... .......... .......... .......... .......... 67% 89.1M 1s\n      8800K .......... .......... .......... .......... .......... 68% 69.5M 1s\n      8850K .......... .......... .......... .......... .......... 68% 86.4M 1s\n      8900K .......... .......... .......... .......... .......... 69%  559K 1s\n      8950K .......... .......... .......... .......... .......... 69% 37.1M 1s\n      9000K .......... .......... .......... .......... .......... 69% 62.9M 1s\n      9050K .......... .......... .......... .......... .......... 70% 39.3M 1s\n      9100K .......... .......... .......... .......... .......... 70% 48.6M 1s\n      9150K .......... .......... .......... .......... .......... 71% 96.6M 1s\n      9200K .......... .......... .......... .......... .......... 71% 78.9M 1s\n      9250K .......... .......... .......... .......... .......... 71% 63.6M 1s\n      9300K .......... .......... .......... .......... .......... 72% 55.4M 1s\n      9350K .......... .......... .......... .......... .......... 72% 76.0M 1s\n      9400K .......... .......... .......... .......... .......... 72% 71.1M 1s\n      9450K .......... .......... .......... .......... .......... 73%  109M 1s\n      9500K .......... .......... .......... .......... .......... 73% 69.9M 1s\n      9550K .......... .......... .......... .......... .......... 74%  562K 1s\n      9600K .......... .......... .......... .......... .......... 74% 36.6M 1s\n      9650K .......... .......... .......... .......... .......... 74% 47.9M 1s\n      9700K .......... .......... .......... .......... .......... 75% 45.8M 1s\n      9750K .......... .......... .......... .......... .......... 75% 71.4M 1s\n      9800K .......... .......... .......... .......... .......... 76% 47.0M 1s\n      9850K .......... .......... .......... .......... .......... 76% 66.5M 1s\n      9900K .......... .......... .......... .......... .......... 76% 45.5M 1s\n      9950K .......... .......... .......... .......... .......... 77% 86.8M 1s\n     10000K .......... .......... .......... .......... .......... 77% 65.4M 1s\n     10050K .......... .......... .......... .......... .......... 77%  117M 1s\n     10100K .......... .......... .......... .......... .......... 78% 70.6M 1s\n     10150K .......... .......... .......... .......... .......... 78% 94.6M 1s\n     10200K .......... .......... .......... .......... .......... 79% 87.3M 1s\n     10250K .......... .......... .......... .......... .......... 79% 1.37M 1s\n     10300K .......... .......... .......... .......... .......... 79%  921K 1s\n     10350K .......... .......... .......... .......... .......... 80% 45.0M 1s\n     10400K .......... .......... .......... .......... .......... 80% 76.7M 1s\n     10450K .......... .......... .......... .......... .......... 81% 48.6M 1s\n     10500K .......... .......... .......... .......... .......... 81% 55.7M 1s\n     10550K .......... .......... .......... .......... .......... 81% 61.9M 1s\n     10600K .......... .......... .......... .......... .......... 82% 60.3M 1s\n     10650K .......... .......... .......... .......... .......... 82% 54.6M 1s\n     10700K .......... .......... .......... .......... .......... 83% 7.18M 1s\n     10750K .......... .......... .......... .......... .......... 83% 4.46M 1s\n     10800K .......... .......... .......... .......... .......... 83% 3.80M 1s\n     10850K .......... .......... .......... .......... .......... 84% 4.46M 1s\n     10900K .......... .......... .......... .......... .......... 84% 4.37M 1s\n     10950K .......... .......... .......... .......... .......... 84% 4.47M 0s\n     11000K .......... .......... .......... .......... .......... 85% 1.70M 0s\n     11050K .......... .......... .......... .......... .......... 85% 34.6M 0s\n     11100K .......... .......... .......... .......... .......... 86% 12.7M 0s\n     11150K .......... .......... .......... .......... .......... 86% 4.79M 0s\n     11200K .......... .......... .......... .......... .......... 86% 3.88M 0s\n     11250K .......... .......... .......... .......... .......... 87% 4.29M 0s\n     11300K .......... .......... .......... .......... .......... 87% 4.59M 0s\n     11350K .......... .......... .......... .......... .......... 88% 4.53M 0s\n     11400K .......... .......... .......... .......... .......... 88% 3.94M 0s\n     11450K .......... .......... .......... .......... .......... 88% 4.56M 0s\n     11500K .......... .......... .......... .......... .......... 89% 4.43M 0s\n     11550K .......... .......... .......... .......... .......... 89% 4.54M 0s\n     11600K .......... .......... .......... .......... .......... 89% 3.92M 0s\n     11650K .......... .......... .......... .......... .......... 90% 4.53M 0s\n     11700K .......... .......... .......... .......... .......... 90% 4.49M 0s\n     11750K .......... .......... .......... .......... .......... 91% 4.48M 0s\n     11800K .......... .......... .......... .......... .......... 91% 3.94M 0s\n     11850K .......... .......... .......... .......... .......... 91% 4.49M 0s\n     11900K .......... .......... .......... .......... .......... 92% 4.39M 0s\n     11950K .......... .......... .......... .......... .......... 92% 4.58M 0s\n     12000K .......... .......... .......... .......... .......... 93% 3.87M 0s\n     12050K .......... .......... .......... .......... .......... 93% 4.64M 0s\n     12100K .......... .......... .......... .......... .......... 93% 4.47M 0s\n     12150K .......... .......... .......... .......... .......... 94% 4.47M 0s\n     12200K .......... .......... .......... .......... .......... 94% 3.90M 0s\n     12250K .......... .......... .......... .......... .......... 94% 4.47M 0s\n     12300K .......... .......... .......... .......... .......... 95% 4.42M 0s\n     12350K .......... .......... .......... .......... .......... 95% 4.55M 0s\n     12400K .......... .......... .......... .......... .......... 96% 3.94M 0s\n     12450K .......... .......... .......... .......... .......... 96% 4.44M 0s\n     12500K .......... .......... .......... .......... .......... 96% 4.58M 0s\n     12550K .......... .......... .......... .......... .......... 97% 4.43M 0s\n     12600K .......... .......... .......... .......... .......... 97% 3.89M 0s\n     12650K .......... .......... .......... .......... .......... 98% 4.60M 0s\n     12700K .......... .......... .......... .......... .......... 98% 4.40M 0s\n     12750K .......... .......... .......... .......... .......... 98% 4.60M 0s\n     12800K .......... .......... .......... .......... .......... 99% 3.89M 0s\n     12850K .......... .......... .......... .......... .......... 99% 4.49M 0s\n     12900K .......... .......... .......... .......... .......... 99% 4.44M 0s\n     12950K                                                       100%  354G=3.2s\n    \n    2019-01-20 01:52:44 (3.91 MB/s) - ‘dev.tgz’ saved [13260990/13260990]\n    \n    Tokenizer Version 1.1\n    Language: en\n    Number of threads: 1\n    Tokenizer Version 1.1\n    Language: fr\n    Number of threads: 1\n    \n\n\n```bash\n%%bash \ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\nworking=\"$PWD/working\"\nlm=\"$PWD/lm\"\n\n# 调节参数\ncd $working\nnohup nice $mosesdecoder/scripts/training/mert-moses.pl \\\n  $corpus/news-test2008.true.fr $corpus/news-test2008.true.en \\\n  $mosesdecoder/bin/moses train/model/moses.ini --mertdir $mosesdecoder/bin/ \\\n  &> mert.out &\n```\n\n\n```bash\n%%bash\ncorpus=\"$PWD/corpus\"\nmosesdecoder=\"$PWD/mosesdecoder\"\nworking=\"$PWD/working\"\nlm=\"$PWD/lm\"\n\n# 处理test 数据\ncd $corpus\n$mosesdecoder/scripts/tokenizer/tokenizer.perl -l en \\\n   < $corpus/dev/newstest2011.en > $corpus/newstest2011.tok.en\n$mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr \\\n   < $corpus/dev/newstest2011.fr > $corpus/newstest2011.tok.fr\n$mosesdecoder/scripts/recaser/truecase.perl --model $corpus/truecase-model.en \\\n   < $corpus/newstest2011.tok.en > $corpus/newstest2011.true.en\n$mosesdecoder/scripts/recaser/truecase.perl --model $corpus/truecase-model.fr \\\n   < $corpus/newstest2011.tok.fr > $corpus/newstest2011.true.fr\n   \n# 翻译test 数据\nnohup nice $mosesdecoder/bin/moses            \\\n   -f $working/filtered-newstest2011/moses.ini   \\\n   < $corpus/newstest2011.true.fr                \\\n   > $working/newstest2011.translated.en         \\\n   2> $working/newstest2011.out \n\n# 自动评估：BLEU\n$mosesdecoder/scripts/generic/multi-bleu.perl \\\n   -lc $corpus/newstest2011.true.en              \\\n   < $working/newstest2011.translated.en\n```\n\n### 资料补充\n\n* Moses相关介绍: http://www.52nlp.cn/moses-introduction\n* Moses基本框架: http://www.52nlp.cn/moses-basic-framework\n* Moses的一些新变化: http://www.52nlp.cn/moses%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B0%E5%8F%98%E5%8C%96\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/1.retrieval_based_chatbot/2.chatbot_retrieval_based_pytorch/","content":"\n# 构建于Ubuntu对话数据集上的基于检索的聊天机器人\n\n**提示：如果大家觉得计算资源有限，欢迎大家在”科学上网“后免费试用[google的colab](https://colab.research.google.com)，有免费的K80 GPU供大家使用，大家只需要把课程的notebook上传即可运行**\n\n和上一个notebook一样，这是一个基于检索的对话系统，我们会对候选集中的回答和问题进行匹配打分，根据分数的高低进行排序并给出我们选择的最佳回复。\n\n\n完整的数据可以在Google Drive文件夹中找到：https://drive.google.com/open?id=1RIIbsS-vxR7Dlo2_v6FWHDFE7q1XPPgj\n\n **要复现文档中的代码，需要执行以下操作：**\n\n1) **下载** 以下文件:\n\n    - glove.6B.50d.txt (Subfolder GloVe)\n    - training_10000.csv (Subfolder MAIN FILES)\n    - validation_1000.csv (Subfolder MAIN FILES)\n    - testing_same_structure_1000.csv (Subfolder MAIN FILES)\n    - testing_different_structure_100.csv (Subfolder MAIN FILES)\n    - saved_model_10000_gpu.pt (Subfolder SAVED MODELS)\n\n2) **调整变量大小** ：对于代码中出现的 *num_training_examples*, *num_validation_examples*, *embedding_dim*, *test_dataframe_same_structure*, *test_dataframe_different_structure* 和*saved model file name* 可以根据数据量的大小进行调整\n\n3) **调整超参数设置**：具体模型的参数大家可以自己调整，也可以参考SAVED MODELS文件夹下的内容，你可以找到**模型截图**，做和它一样的设定，大家也可以复现本notebook的结果。\n\n==========================================================================================================\n\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n```\n\n    Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n    \n    Enter your authorization code:\n    ··········\n    Mounted at /content/gdrive\n    \n\n\n```python\n!ls /content/gdrive/My\\ Drive/Dialogue\\ Files\\ \n```\n\n     GloVe\t'MAIN FILES'  'Original Files'\t'SAVED MODELS'\n    \n\n\n```python\n!pip install torch torchvision\n```\n\n    Collecting torch\n    \u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n    \u001b[K    100% |████████████████████████████████| 591.8MB 24kB/s \n    tcmalloc: large alloc 1073750016 bytes == 0x6211c000 @  0x7effd0bd42a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n    \u001b[?25hCollecting torchvision\n    \u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n    \u001b[K    100% |████████████████████████████████| 61kB 26.8MB/s \n    \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n    Collecting pillow>=4.1.1 (from torchvision)\n    \u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n    \u001b[K    100% |████████████████████████████████| 2.0MB 7.2MB/s \n    \u001b[?25hInstalling collected packages: torch, pillow, torchvision\n      Found existing installation: Pillow 4.0.0\n        Uninstalling Pillow-4.0.0:\n          Successfully uninstalled Pillow-4.0.0\n    Successfully installed pillow-5.4.1 torch-1.0.0 torchvision-0.2.1\n    \n\n\n```python\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch\nimport torch.autograd as autograd\nfrom torch.nn import init\nimport torch.nn.utils.rnn \nimport datetime\nimport operator\n\nnp.random.seed(0)\n```\n\n## 定义helper函数以构建训练和验证过程中的变量\n\n\n```python\ndef create_dataframe(csvfile):\n    dataframe = pd.read_csv(csvfile)\n    return dataframe\n\ndef shuffle_dataframe(dataframe):\n    dataframe.reindex(np.random.permutation(dataframe.index))\n\ndef create_vocab(dataframe):\n    vocab = []\n    word_freq = {}\n    \n    for index, row in dataframe.iterrows():\n        \n        context_cell = row[\"Context\"]\n        response_cell = row[\"Utterance\"]\n        \n        train_words = str(context_cell).split() + str(response_cell).split()\n        \n        for word in train_words:\n          \n            if word.lower() not in vocab:\n                vocab.append(word.lower())         \n                       \n            if word.lower() not in word_freq:\n                word_freq[word.lower()] = 1\n            else:\n                word_freq[word] += 1\n    \n    word_freq_sorted = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n    vocab = [\"<UNK>\"] + [pair[0] for pair in word_freq_sorted]\n    \n    return vocab\n\n\ndef create_word_to_id(vocab):             \n    word_to_id = {word: id for id, word in enumerate(vocab)}\n    \n    return word_to_id\n\n\ndef create_id_to_vec(word_to_id, glovefile): \n    lines = open(glovefile, 'r').readlines()\n    id_to_vec = {}\n    vector = None\n    \n    for line in lines:\n        word = line.split()[0]\n        vector = np.array(line.split()[1:], dtype='float32') #32\n        \n        if word in word_to_id:\n            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(vector))\n            \n    for word, id in word_to_id.items(): \n        if word_to_id[word] not in id_to_vec:\n            v = np.zeros(*vector.shape, dtype='float32')\n            v[:] = np.random.randn(*v.shape)*0.01\n            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(v))\n            \n    embedding_dim = id_to_vec[0].shape[0]\n    \n    return id_to_vec, embedding_dim\n\n\ndef load_ids_and_labels(row, word_to_id):\n    context_ids = []\n    response_ids = []\n\n    context_cell = row['Context']\n    response_cell = row['Utterance']\n    label_cell = row['Label']\n\n    max_context_len = 160\n    \n    context_words = context_cell.split()\n    if len(context_words) > max_context_len:\n        context_words = context_words[:max_context_len]\n    for word in context_words:\n        if word in word_to_id:\n            context_ids.append(word_to_id[word])\n        else: \n            context_ids.append(0) #UNK\n    \n    response_words = response_cell.split()\n    for word in response_words:\n        if word in word_to_id:\n            response_ids.append(word_to_id[word])\n        else: \n            response_ids.append(0)\n    \n    label = np.array(label_cell).astype(np.float32)\n\n    return context_ids, response_ids, label\n```\n\n## 模型定义\n\n\n```python\nclass Encoder(nn.Module):\n\n    def __init__(self, \n            emb_size, \n            hidden_size, \n            vocab_size, \n            p_dropout): \n    \n            super(Encoder, self).__init__()\n             \n            self.emb_size = emb_size\n            self.hidden_size = hidden_size\n            self.vocab_size = vocab_size\n            self.p_dropout = p_dropout\n       \n            self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n            self.lstm = nn.LSTM(self.emb_size, self.hidden_size)\n            self.dropout_layer = nn.Dropout(self.p_dropout) \n\n            self.init_weights()\n             \n    def init_weights(self):\n        init.uniform(self.lstm.weight_ih_l0, a = -0.01, b = 0.01)\n        init.orthogonal(self.lstm.weight_hh_l0)\n        self.lstm.weight_ih_l0.requires_grad = True\n        self.lstm.weight_hh_l0.requires_grad = True\n        \n        embedding_weights = torch.FloatTensor(self.vocab_size, self.emb_size)\n            \n        for id, vec in id_to_vec.items():\n            embedding_weights[id] = vec\n        \n        self.embedding.weight = nn.Parameter(embedding_weights, requires_grad = True)\n            \n    def forward(self, inputs):\n        embeddings = self.embedding(inputs)\n        _, (last_hidden, _) = self.lstm(embeddings) #dimensions: (num_layers * num_directions x batch_size x hidden_size)\n        last_hidden = self.dropout_layer(last_hidden[-1])#access last lstm layer, dimensions: (batch_size x hidden_size)\n\n        return last_hidden\n\n    \nclass DualEncoder(nn.Module):\n     \n    def __init__(self, encoder):\n        super(DualEncoder, self).__init__()\n        self.encoder = encoder\n        self.hidden_size = self.encoder.hidden_size\n        M = torch.FloatTensor(self.hidden_size, self.hidden_size)     \n        init.xavier_normal(M)\n        self.M = nn.Parameter(M, requires_grad = True)\n\n    def forward(self, context_tensor, response_tensor):\n        \n        context_last_hidden = self.encoder(context_tensor) #dimensions: (batch_size x hidden_size)\n        response_last_hidden = self.encoder(response_tensor) #dimensions: (batch_size x hidden_size)\n        \n        #context = context_last_hidden.mm(self.M).cuda()\n        context = context_last_hidden.mm(self.M) #dimensions: (batch_size x hidden_size)\n        context = context.view(-1, 1, self.hidden_size) #dimensions: (batch_size x 1 x hidden_size)\n        \n        response = response_last_hidden.view(-1, self.hidden_size, 1) #dimensions: (batch_size x hidden_size x 1)\n        \n        #score = torch.bmm(context, response).view(-1, 1).cuda()\n        score = torch.bmm(context, response).view(-1, 1) #dimensions: (batch_size x 1 x 1) and lastly --> (batch_size x 1)\n\n        return score\n```\n\n## 数据与变量构建\n**定义函数去调用所有的helper函数，以便完成各种数据和变量初始化，以及部分的预训练词向量加载等**\n\n\n```python\ndef creating_variables(num_training_examples, num_validation_examples, embedding_dim):\n\n    print(str(datetime.datetime.now()).split('.')[0], \"Creating variables for training and validation...\")\n\n    training_dataframe = create_dataframe('training_%d.csv' %num_training_examples)\n    vocab = create_vocab(training_dataframe)\n    word_to_id = create_word_to_id(vocab)\n    id_to_vec, emb_dim = create_id_to_vec(word_to_id, 'glove.6B.%dd.txt' %embedding_dim)\n\n    validation_dataframe = create_dataframe('validation_%d.csv' %num_validation_examples)\n\n    print(str(datetime.datetime.now()).split('.')[0], \"Variables created.\\n\")\n    \n    return training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe\n     \n```\n\n## 模型构建\n**调用Encoder和DualEncoder去构建模型**\n\n\n```python\ndef creating_model(hidden_size, p_dropout):\n\n    print(str(datetime.datetime.now()).split('.')[0], \"Calling model...\")\n\n    encoder = Encoder(\n            emb_size = emb_dim,\n            hidden_size = hidden_size,\n            vocab_size = len(vocab),\n            p_dropout = p_dropout)\n\n    dual_encoder = DualEncoder(encoder)\n\n    print(str(datetime.datetime.now()).split('.')[0], \"Model created.\\n\")\n    print(dual_encoder)\n    \n    return encoder, dual_encoder\n```\n\n**训练集和验证集准确率计算**\n\n\n```python\ndef increase_count(correct_count, score, label):\n    if ((score.data[0][0] >= 0.5) and (label.data[0][0] == 1.0)) or ((score.data[0][0] < 0.5) and (label.data[0][0]  == 0.0)):\n       correct_count +=1  \n   \n    return correct_count\n\ndef get_accuracy(correct_count, dataframe):\n    accuracy = correct_count/(len(dataframe))\n        \n    return accuracy\n        \n```\n\n## 模型训练\n构建模型训练函数\n\n\n```python\ndef train_model(learning_rate, l2_penalty, epochs): \n    print(str(datetime.datetime.now()).split('.')[0], \"Starting training and validation...\\n\")\n    print(\"====================Data and Hyperparameter Overview====================\\n\")\n    print(\"Number of training examples: %d, Number of validation examples: %d\" %(len(training_dataframe), len(validation_dataframe)))\n    print(\"Learning rate: %.5f, Embedding Dimension: %d, Hidden Size: %d, Dropout: %.2f, L2:%.10f\\n\" %(learning_rate, emb_dim, encoder.hidden_size, encoder.p_dropout, l2_penalty))\n    print(\"================================Results...==============================\\n\")\n\n    optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n       \n    loss_func = torch.nn.BCEWithLogitsLoss()\n    #loss_func.cuda()\n     \n    best_validation_accuracy = 0.0\n     \n    for epoch in range(epochs):\n                     \n            shuffle_dataframe(training_dataframe)\n                        \n            sum_loss_training = 0.0\n            \n            training_correct_count = 0\n            \n            dual_encoder.train()\n\n            for index, row in training_dataframe.iterrows():            \n            \n                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n                \n                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False) #.cuda()\n                \n                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) #.cuda()\n                                \n                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1))), requires_grad = False) #.cuda()\n                             \n                score = dual_encoder(context, response)\n        \n                loss = loss_func(score, label)\n                \n                sum_loss_training += loss.data[0]\n                \n                loss.backward()\n        \n                optimizer.step()\n               \n                optimizer.zero_grad()\n                \n                training_correct_count = increase_count(training_correct_count, score, label)\n                                                    \n            training_accuracy = get_accuracy(training_correct_count, training_dataframe)\n            \n            #plt.plot(epoch, training_accuracy)\n                \n            shuffle_dataframe(validation_dataframe)\n            \n            validation_correct_count = 0\n\n            sum_loss_validation = 0.0\n\n            dual_encoder.eval()\n\n            for index, row in validation_dataframe.iterrows():\n                \n                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n                \n                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) #.cuda()\n                \n                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) #.cuda()\n                                \n                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) #.cuda()\n                \n                score = dual_encoder(context, response)\n                \n                loss = loss_func(score, label)\n                \n                sum_loss_validation += loss.data[0]\n                \n                validation_correct_count = increase_count(validation_correct_count, score, label)\n                    \n            validation_accuracy = get_accuracy(validation_correct_count, validation_dataframe)\n                        \n            print(str(datetime.datetime.now()).split('.')[0], \n                  \"Epoch: %d/%d\" %(epoch,epochs),  \n                  \"TrainLoss: %.3f\" %(sum_loss_training/len(training_dataframe)), \n                  \"TrainAccuracy: %.3f\" %(training_accuracy), \n                  \"ValLoss: %.3f\" %(sum_loss_validation/len(validation_dataframe)), \n                  \"ValAccuracy: %.3f\" %(validation_accuracy))\n            \n            if validation_accuracy > best_validation_accuracy:\n                best_validation_accuracy = validation_accuracy\n                torch.save(dual_encoder.state_dict(), 'saved_model_%d_examples.pt' %(len(training_dataframe)))\n                print(\"New best found and saved.\")\n                \n    print(str(datetime.datetime.now()).split('.')[0], \"Training and validation epochs finished.\")\n\n```\n\n\n```python\ntraining_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe = creating_variables(num_training_examples = 10000, \n                                                                                                     embedding_dim = 50, \n                                                                                                     num_validation_examples = 1000)\n```\n\n**设定hidden size和dropout概率，构建模型**\n\n\n```python\nencoder, dual_encoder = creating_model(hidden_size = 50, \n                                       p_dropout = 0.85)\n\n#encoder.cuda()\n#dual_encoder.cuda\n\nfor name, param in dual_encoder.named_parameters():\n    if param.requires_grad:\n        print(name)\n```\n\n**设定学习率，迭代轮数，l2正则化强度，开始训练**\n\n\n```python\ntrain_model(learning_rate = 0.0001, \n            l2_penalty = 0.0001,\n            epochs = 100)\n```\n\n**加载训练好的模型进行测试**\n\n\n```python\ndual_encoder.load_state_dict(torch.load('saved_model_10000_examples.pt'))\n\ndual_encoder.eval()\n```\n\n**第1种测试方式:**\n\n*测试数据集和训练还有验证数据集有着一样的数据组织格式 (context, response, label)*\n\n*测试评判指标：准确率*\n\nLoading data:\n\n\n```python\ntest_dataframe_same_structure = pd.read_csv('testing_same_structure_1000.csv')\n```\n\n构建测试函数\n\n\n```python\ndef testing_same_structure():\n    \n    test_correct_count = 0\n\n    for index, row in test_dataframe_same_structure.iterrows():\n\n        context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n\n        context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) #.cuda()\n\n        response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) #.cuda()\n\n        label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) #.cuda()\n\n        score = dual_encoder(context, response)\n\n        test_correct_count = increase_count(test_correct_count, score, label)\n\n    test_accuracy = get_accuracy(test_correct_count, test_dataframe_same_structure)\n    \n    return test_accuracy\n```\n\n准确率\n\n\n```python\ntest_accuracy = testing_same_structure()\nprint(\"Test accuracy for %d training examples and %d test examples: %.2f\" %(len(training_dataframe),len(test_dataframe_same_structure),test_accuracy))\n```\n\n**第2种测试方式**\n\n*测试数据集和训练/验证集格式不一样 (1个问题，1个标准答案，9个干扰项错误答案)*\n\n*测试评估指标：recall(召回)*\n\n加载数据\n\n\n```python\ntest_dataframe_different_structure = pd.read_csv('testing_different_structure_100.csv')\n```\n\n以字典形态存储对话word ids\n\n*Outer dictionary \"ids_per_example_and_candidate\": keys = examples, values = inner dictionaries*\n\n*Inner dictionaries \"ids_per_candidate\": keys = candidate names, values = list of word IDs*\n\n\n```python\ndef load_ids(test_dataframe_different_structure, word_to_id):\n    \n    print(str(datetime.datetime.now()).split('.')[0], \"Loading test IDs...\")\n\n    max_context_len = 160\n    \n    ids_per_example_and_candidate = {}\n    \n    for i, example in test_dataframe_different_structure.iterrows():\n        \n        ids_per_candidate = {}\n      \n        for column_name, cell in  example.iteritems():\n            \n                id_list = []\n            \n                words = str(cell).split()\n                if len(words) > max_context_len:\n                    words = words[:max_context_len]\n    \n                for word in words:\n                    if word in word_to_id:\n                        id_list.append(word_to_id[word])\n                    else: \n                        id_list.append(0) #UNK  \n                    \n                ids_per_candidate[column_name] = id_list\n    \n        ids_per_example_and_candidate[i] = ids_per_candidate\n    \n    print(str(datetime.datetime.now()).split('.')[0], \"Test IDs loaded.\")\n    \n    return ids_per_example_and_candidate\n```\n\n\n```python\nids_per_example_and_candidate = load_ids(test_dataframe_different_structure, word_to_id)\n```\n\n以字典形态存储得分score\n\n*Outer dictionary \"scores_per_example_and_candidate\": keys = examples, values = inner dictionaries*\n\n*Inner dictionaries \"scores_per_candidate\": keys = candidate names, values = score*\n\n\n```python\ndef load_scores(): \n    print(str(datetime.datetime.now()).split('.')[0], \"Computing test scores...\")\n    \n    scores_per_example_and_candidate = {}\n                 \n    for example, utterance_ids_dict in sorted(ids_per_example_and_candidate.items()): \n        \n        score_per_candidate = {}\n\n        for utterance_name, ids_list in sorted(utterance_ids_dict.items()):\n        \n            context = autograd.Variable(torch.LongTensor(utterance_ids_dict['Context']).view(-1,1))#.cuda()\n            \n            if utterance_name != 'Context':\n\n                candidate_response = autograd.Variable(torch.LongTensor(utterance_ids_dict[utterance_name]).view(-1, 1))#.cuda()\n                        \n                score = torch.sigmoid(dual_encoder(context, candidate_response))\n                \n                score_per_candidate[\"Score with \" + utterance_name] = score.data[0][0]\n    \n        scores_per_example_and_candidate[example] = score_per_candidate\n\n    print(str(datetime.datetime.now()).split('.')[0], \"Test scores computed.\")\n    \n    return scores_per_example_and_candidate\n\n```\n\n\n```python\nscores_per_example_and_candidate = load_scores()\n```\n\n定义计算召回结果的方法： \n\n这里计算的是recall@k这个评估指标。\n\n\n```python\ndef get_recall_at_k(k):\n    count_true_hits = 0\n    \n    for example, score_per_candidate_dict in sorted(scores_per_example_and_candidate.items()): \n    \n        top_k = dict(sorted(score_per_candidate_dict.items(), key=operator.itemgetter(1), reverse=True)[:k])\n        \n        if 'Score with Ground Truth Utterance' in top_k:\n            count_true_hits += 1\n    \n    number_of_examples = len(scores_per_example_and_candidate)\n    \n    recall_at_k = count_true_hits/number_of_examples\n    \n    return recall_at_k\n\n```\n\n\n```python\nprint(\"recall_at_5 =\",get_recall_at_k(k = 5)) #Baseline expectation: 5/10 = 0.5 for random guess\nprint(\"recall_at_2 =\",get_recall_at_k(k = 2)) #Baseline expectation: 2/10 = 0.2 for random guess\nprint(\"recall_at_1 =\",get_recall_at_k(k = 1)) #Baseline expectation: 1/10 = 0.1 for random guess\n```\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/1.retrieval_based_chatbot/1.chatbot_retrieval_based_tensorflow/","content":"\n# 基于内容检索式的聊天机器人\n\n**提示：如果大家觉得计算资源有限，欢迎大家在”科学上网“后免费试用[google的colab](https://colab.research.google.com)，有免费的K80 GPU供大家使用，大家只需要把课程的notebook上传即可运行**\n\n![](./img/retrieval_chatbot.png)\n\n以下内容会介绍到基于检索的聊天机器人原理，并实现一个基于检索的模型，使用了双层Decoder的LSTM模型，通过这个模型可以实现聊天机器人。\n\n本部分英文原文见[deep-learning-for-chatbots-2-retrieval-based-model-tensorflow](http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/)，本文涉及到的数据和代码见[Github仓库地址](https://github.com/dennybritz/chatbot-retrieval/)。\n\n------\n\n## 基于检索模型的聊天机器人\n\n本文我们将介绍和实现一个基于检索模型的聊天机器人。检索模型所使用的回复数据通常是**预先存储且知道（或定义）的数据**，而不像生成式模型那样可以创造出崭新的、未知的回复内容（模型没有见过）。准确来讲，检索式模型的输入是**一段上下文内容 C (会话到目前未知的内容信息)** 和**一个可能作为回复的候选答案**；模型的输出是对这个候选答案的打分。寻找最合适的回复内容的过程是：先对一堆候选答案进行打分及排序，最后选出分值最高的那个最为回复。\n\n也许你会质疑为什么不直接使用生成式模型，生成式模型不需要预先存储且定义好的数据，比起检索模型更加的灵活多变。原因在于目前生成式模型的效果并不佳，由于生成式模型的约束条件少，过于多变的模型导致生成的response中出现一些语法错误和语义无关的内容。生成式模型需要海量的训练数据，且难以优化。目前工业界常用的模型还是基于检索的模型，或者以生成式模型作为补充的两者结合，谷歌的[Smart Reply](http://arxiv.org/abs/1606.04870)就是一个例子。尽管目前生成式模型是学术界的研究热点，但在实践中使用检索式模型是更加合适的选择。\n\n## Ubuntu对话数据集\n\n这篇博客我们将使用Ubuntu对话数据集（[论文来源](http://arxiv.org/abs/1506.08909) [github地址](https://github.com/rkadlec/ubuntu-ranking-dataset-creator)）。这个数据集（Ubuntu Dialog Corpus, UDC）是目前最大的公开对话数据集之一，它是来自Ubuntu的IRC网络上的对话日志。[这篇论文](http://arxiv.org/abs/1506.08909)介绍了该数据集生成的具体细节。下面简单介绍一下数据的格式。\n\n训练数据有1,000,000条实例，其中一半是正例（label为1），一半是负例（label为0，负例为随机生成）。每条实例包括一段上下文信息（context），即Query；和一段可能的回复内容，即Response；Label为1表示该Response确实是Query的回复，Label为0则表示不是。下面是数据示例：\n\n[![img](http://i.imgur.com/tlKSbnT.png)](http://i.imgur.com/tlKSbnT.png)\n\n数据集的生成使用了[NLTK工具](http://www.nltk.org/)，包括分词、stemmed、lemmatized等文本预处理步骤；同时还使用了NER技术，将文本中的实体，如姓名、地点、组织、URL等替换成特殊字符。这些文本预处理并不是必须的，但是能够提升一些模型的性能。据统计，query的平均长度为86个word，而response的平均长度为17个word，更多的数据统计信息见[Jupyter notebook](https://github.com/dennybritz/chatbot-retrieval/blob/master/notebooks/Data%20Exploration.ipynb)。\n\n数据集也包括了测试和验证集，但这两部分的数据和训练数据在格式上不太一样。在测试集和验证集中，对于每一条实例，有一个正例和九个负例数据（也称为干扰数据）。模型的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。下面是数据示例：\n\n[![img](http://i.imgur.com/EoEK6vy.png)](http://i.imgur.com/EoEK6vy.png)\n\n模型的评测方式有很多种。其中最常用到的是**recall@k**，即经模型对候选的response排序后，前k个候选中存在正例数据（正确的那个）的占比；显然k值越大，该指标会越高，因为这对模型性能的要求越松。\n\n在Ubuntu数据集中，负例数据都是随机生成的；然而在现实中，想要从全部的数据中随机生成负例是不可能的。谷歌的Smart Reply则使用了[聚类技术](http://arxiv.org/abs/1606.04870)，然后将每个类的中取一些作为负例，这样生成负例的方式显得更加合理（考虑了负例数据的多样性，同时减少时间开销）。\n\n## BASELINE\n\n在使用NN模型之前，先设立一些简单的baseline模型，以方便后续的效果对比。使用如下的函数来计算**recall@k**:\n\n```python\ndef evaluate_recall(y, y_test, k=1):\n    num_examples = float(len(y))\n    num_correct = 0\n    for predictions, label in zip(y, y_test):\n        if label in predictions[:k]:\n            num_correct += 1\n    return num_correct/num_examples\n```\n\n其中，`y`是所预测的以降序排列的模型预测分值，`y_test`是实际的label值。举个例子，假设`y`的值为[0,3,1,2,5,6,4,7,8,9]，这说明第0号的候选的预测分值最高、作为回复的可能性最高，而9号则最低。这里的第0号同时也是正确的那个，即正例数据，标号为1-9的为随机生成的负例数据。\n\n理论上，最base的随机模型（Random Predictor）的recall@1的值为10%，recall@2的值为20%。相应的代码如下：\n\n```python\n# Random Predictor\ndef predict_random(context, utterances):\n    return np.random.choice(len(utterances), 10, replace=False)\n\n# Evaluate Random predictor\ny_random = [predict_random(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]\ny_test = np.zeros(len(y_random))\nfor n in [1, 2, 5, 10]:\n    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y_random, y_test, n)))\n```\n\n实际的模型结果如下：\n\n```python\nRecall @ (1, 10): 0.0937632\nRecall @ (2, 10): 0.194503\nRecall @ (5, 10): 0.49297\nRecall @ (10, 10): 1\n```\n\n这与理论预期相符，但这不是我们所追求的结果。\n\n另外一个baseline的模型为**tfidf predictor**。tfidf表示词频（term frequency）和逆文档词频（inverse document frequency），它衡量了一个词在一篇文档中的重要程度（基于整个语料库）。直观上，两篇文档对应的tfidf向量越接近，两篇文章的内容也越相似。同样的，对于一个QR pair，它们语义上接近的词共现的越多，也将越可能是一个正确的QR pair（**这句话存疑，原因在于QR之间也有可能不存在语义上的相似，一个Q对应的R是多样的。**）。tfidf predictor对应的代码如下（利用[scikit-learn工具](http://scikit-learn.org/)能够轻易实现）：\n\n```python\nclass TFIDFPredictor:\n    def __init__(self):\n        self.vectorizer = TfidfVectorizer()\n\n    def train(self, data):\n        self.vectorizer.fit(np.append(data.Context.values,data.Utterance.values))\n\n    def predict(self, context, utterances):\n        # Convert context and utterances into tfidf vector\n        vector_context = self.vectorizer.transform([context])\n        vector_doc = self.vectorizer.transform(utterances)\n        # The dot product measures the similarity of the resulting vectors\n        result = np.dot(vector_doc, vector_context.T).todense()\n        result = np.asarray(result).flatten()\n        # Sort by top results and return the indices in descending order\n        return np.argsort(result, axis=0)[::-1]\n\n\n# Evaluate TFIDF predictor\npred = TFIDFPredictor()\npred.train(train_df)\ny = [pred.predict(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]\nfor n in [1, 2, 5, 10]:\n    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))\n```\n\n模型结果如下：\n\n```\nRecall @ (1, 10): 0.495032\nRecall @ (2, 10): 0.596882\nRecall @ (5, 10): 0.766121\nRecall @ (10, 10): 1\n```\n\n显然这比Random的模型要好得多，但这还不够。之前的假设并不完美，首先query和response之间并不一定要是语义上的相近；其次tfidf模型忽略了词序这一重要的信息。使用NN模型我们能做得更好一些。\n\n## LSTM\n\n这篇博文将建立的NN模型为两层Encoder的LSTM模型（Dual Encoder LSTM Network），这种形式的网络被广泛应用在chatbot中（尽管可能效果并不是最佳的那个，你可以尽可能地尝试其他的NN模型）。[seq2seq模型](https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html)常用于机器翻译领域，并取得了较大的效果。使用Dual LSTM模型的原因在于这个模型被证明在这个数据集有较好的效果（[详情见这里](http://arxiv.org/abs/1510.03753)）,这可以作为我们后续模型效果的验证。\n\n两层Encoder的LSTM模型的结构图如下（[论文来源](http://arxiv.org/abs/1506.08909)）：\n\n[![img](http://i.imgur.com/qpFDJWM.png)](http://i.imgur.com/qpFDJWM.png)\n\n大致的流程如下：\n\n(1) Query和Response都是经过分词的，分词后每个词embedded为向量形式。初始的词向量使用[GloVe vectors](http://nlp.stanford.edu/projects/glove/)，之后词向量随着模型的训练会进行fine-tuned（实验发现，初始的词向量使用GloVe并没有在性能上带来显著的提升）。\n\n(2) 分词且向量化的Query和Response经过相同的RNN（word by word）。RNN最终生成一个向量表示，捕捉了Query和Response之间的[语义联系]（图中的c和r）；这个向量的维度是可以指定的，这里指定为256维。\n\n(3) 将向量c与一个矩阵M相乘，来预测一个可能的回复r’。如果c为一个256维的向量，M维256*256的矩阵，两者相乘的结果为另一个256维的向量，我们可以将其解释为[一个生成式的回复向量]。矩阵M是需要训练的参数。\n\n(4) 通过点乘的方式来预测生成的回复r’和候选的回复r之间的相似程度，点乘结果越大表示候选回复作为回复的可信度越高；之后通过sigmoid函数归一化，转成概率形式。图中把第(3)步和第(4)步结合在一起了。\n\n为了训练模型，我们还需要一个损失函数（loss function）。这里使用二元的交叉熵（binary cross-entropy）作为损失函数。我们已知实例的真实label `y`，值为0或1；通过上面的第(4)步可以得到一个概率值 `y'`；因此，交叉熵损失值为`L = -y * ln(y') - (1 - y) * ln(1 - y')`。这个公式的意义是直观的，即当`y=1`时，`L = -ln(y')`，我们期望`y'`尽量地接近1使得损失函数的值越小；反之亦然。\n\n实现过程中使用了[numpy](http://www.numpy.org/)、[pandas](http://pandas.pydata.org/)、[TensorFlow](http://www.tensorflow.org/)和[TF Learn](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn)等工具。\n\n### 数据预处理\n\n[数据集](https://github.com/rkadlec/ubuntu-ranking-dataset-creator)的原始格式为csv格式，我们需要先将其转为[TensorFlow专有的格式](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto)，这种格式的好处在于能够直接从输入文件中load tensors，并让TensorFlow来处理洗牌(shuffling)、批量(batching)和队列化(queuing)等操作。预处理中还包括创建一个字典库，将词进行标号，TFRecord文件将直接存储这些词的标号。\n\n每个实例包括如下几个字段：\n\n- Query：表示为一串词标号的序列，如[231, 2190, 737, 0, 912]；\n- Query的长度；\n- Response：同样是一串词标号的序列；\n- Response的长度；\n- Label；\n- Distractor_[N]：表示负例干扰数据，仅在验证集和测试集中有，N的取值为0-8；\n- Distractor_[N]的长度；\n\n数据预处理的Python脚本见[这里](https://github.com/dennybritz/chatbot-retrieval/blob/master/scripts/prepare_data.py)，生成了3个文件：train.tfrecords, validation.tfrecords 和 test.tfrecords。你可以尝试自己运行程序，或者直接下载和使用[预处理后的数据](https://drive.google.com/open?id=0B_bZck-ksdkpVEtVc1R6Y01HMWM)。\n\n### 创建输入函数\n\n为了使用TensoFlow内置的训练和评测模块，我们需要创建一个输入函数：这个函数返回输入数据的batch。因为训练数据和测试数据的格式不同，我们需要创建不同的输入函数。输入函数需要返回批量(batch)的特征和标签值(如果有的话)。类似于如下：\n\n```python\ndef input_fn():\n  # TODO Load and preprocess data here\n  return batched_features, labels\n```\n\n因为我们需要在模型训练和评测过程中使用不同的输入函数，为了防止重复书写代码，我们创建一个包装器(wrapper)，名称为`create_input_fn`，针对不同的mode使用相应的code，如下：\n\n```python\ndef create_input_fn(mode, input_files, batch_size, num_epochs=None):\n  def input_fn():\n    # TODO Load and preprocess data here\n    return batched_features, labels\n  return input_fn\n```\n\n完整的code见[udc_inputs.py](https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_inputs.py)。整体上，这个函数做了如下的事情：\n\n(1) 定义了示例文件中的feature字段；\n(2) 使用`tf.TFRecordReader`来读取`input_files`中的数据；\n(3) 根据feature字段的定义对数据进行解析；\n(4) 提取训练数据的标签；\n(5) 产生批量化的训练数据；\n(6) 返回批量的特征数据及对应标签；\n\n### 定义评测指标\n\n之前已经提到用`recall@k`这个指标来评测模型，TensorFlow中已经实现了许多标准指标（包括`recall@k`）。为了使用这些指标，需要创建一个字典，key为指标名称，value为对应的计算函数。如下：\n\n```python\ndef create_evaluation_metrics():\n  eval_metrics = {}\n  for k in [1, 2, 5, 10]:\n    eval_metrics[\"recall_at_%d\" % k] = functools.partial(\n        tf.contrib.metrics.streaming_sparse_recall_at_k,\n        k=k)\n  return eval_metrics\n```\n\n如上，我们使用了[functools.partial](https://docs.python.org/2/library/functools.html#functools.partial)函数，这个函数的输入参数有两个。不要被`streaming_sparse_recall_at_k`所困惑，其中的`streaming`的含义是表示指标的计算是增量式的。\n\n训练和测试所使用的评测方式是不一样的，训练过程中我们对每个case可能作为正确回复的概率进行预测，而测试过程中我们对每组数据（包含10个case，其中1个是正确的，另外9个是生成的负例/噪音数据）中的case进行逐条概率预测，得到例如`[0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11]`这样格式的输出，这些输出值的和并不要求为1（因为是逐条预测的，有单独的预测概率值，在0到1之间）；而对于这组数据而言，因为`数据index=0`对应的为正确答案，这里`recall@1`为0，因为`0.34`是其中第二大的值，所以`recall@2`是1（表示这组数据中预测概率值在前二的中有一个是正确的）。\n\n### 训练程序样例\n\n首先，给一个模型训练和测试的程序样例，这之后你可以参照程序中所用到的标准函数，来快速切换和使用其他的网络模型。假设我们有一个函数`model_fn`，函数的输入参数有`batched features`，`label`和`mode(train/evaluation)`，函数的输出为预测值。程序样例如下：\n\n```python\nestimator = tf.contrib.learn.Estimator(\nmodel_fn=model_fn,\nmodel_dir=MODEL_DIR,\nconfig=tf.contrib.learn.RunConfig())\n\ninput_fn_train = udc_inputs.create_input_fn(\nmode=tf.contrib.learn.ModeKeys.TRAIN,\ninput_files=[TRAIN_FILE],\nbatch_size=hparams.batch_size)\n\ninput_fn_eval = udc_inputs.create_input_fn(\nmode=tf.contrib.learn.ModeKeys.EVAL,\ninput_files=[VALIDATION_FILE],\nbatch_size=hparams.eval_batch_size,\nnum_epochs=1)\n\neval_metrics = udc_metrics.create_evaluation_metrics()\n\n# We need to subclass theis manually for now. The next TF version will\n# have support ValidationMonitors with metrics built-in.\n# It's already on the master branch.\nclass EvaluationMonitor(tf.contrib.learn.monitors.EveryN):\ndef every_n_step_end(self, step, outputs):\n  self._estimator.evaluate(\n    input_fn=input_fn_eval,\n    metrics=eval_metrics,\n    steps=None)\n\neval_monitor = EvaluationMonitor(every_n_steps=FLAGS.eval_every)\nestimator.fit(input_fn=input_fn_train, steps=None, monitors=[eval_monitor])\n```\n\n这里创建了一个`model_fn`的`estimator`(评估函数)；两个输入函数，`input_fn_train`和`input_fn_eval`，以及计算评测指标的函数；\n\n### 创建模型\n\n到目前为止，我们创建了模型的输入、解析、评测和训练的样例程序。现在我们来写LSTM的程序，[create_model_fn](https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_model.py)函数用以处理不同格式的训练和测试数据；它的输入参数为`model_impl`，这个函数表示实际作出预测的模型，这里就是用的LSTM，当然你可以替换成任意的其他模型。程序如下：\n\n```python\ndef dual_encoder_model(\n    hparams,\n    mode,\n    context,\n    context_len,\n    utterance,\n    utterance_len,\n    targets):\n\n  # Initialize embedidngs randomly or with pre-trained vectors if available\n  embeddings_W = get_embeddings(hparams)\n\n  # Embed the context and the utterance\n  context_embedded = tf.nn.embedding_lookup(\n      embeddings_W, context, name=\"embed_context\")\n  utterance_embedded = tf.nn.embedding_lookup(\n      embeddings_W, utterance, name=\"embed_utterance\")\n\n\n  # Build the RNN\n  with tf.variable_scope(\"rnn\") as vs:\n    # We use an LSTM Cell\n    cell = tf.nn.rnn_cell.LSTMCell(\n        hparams.rnn_dim,\n        forget_bias=2.0,\n        use_peepholes=True,\n        state_is_tuple=True)\n\n    # Run the utterance and context through the RNN\n    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n        cell,\n        tf.concat(0, [context_embedded, utterance_embedded]),\n        sequence_length=tf.concat(0, [context_len, utterance_len]),\n        dtype=tf.float32)\n    encoding_context, encoding_utterance = tf.split(0, 2, rnn_states.h)\n\n  with tf.variable_scope(\"prediction\") as vs:\n    M = tf.get_variable(\"M\",\n      shape=[hparams.rnn_dim, hparams.rnn_dim],\n      initializer=tf.truncated_normal_initializer())\n\n    # \"Predict\" a  response: c * M\n    generated_response = tf.matmul(encoding_context, M)\n    generated_response = tf.expand_dims(generated_response, 2)\n    encoding_utterance = tf.expand_dims(encoding_utterance, 2)\n\n    # Dot product between generated response and actual response\n    # (c * M) * r\n    logits = tf.batch_matmul(generated_response, encoding_utterance, True)\n    logits = tf.squeeze(logits, [2])\n\n    # Apply sigmoid to convert logits to probabilities\n    probs = tf.sigmoid(logits)\n\n    # Calculate the binary cross-entropy loss\n    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets))\n\n  # Mean loss across the batch of examples\n  mean_loss = tf.reduce_mean(losses, name=\"mean_loss\")\n  return probs, mean_loss\n```\n\n完整的程序见[dual_encoder.py](https://github.com/dennybritz/chatbot-retrieval/blob/master/models/dual_encoder.py)。基于这个，我们能够实例化model函数在我们之前定义的[udc_train.py](https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py)，如下：\n\n```python\nmodel_fn = udc_model.create_model_fn(\n  hparams=hparams,\n  model_impl=dual_encoder_model)\n```\n\n这样我们就可以直接运行`udc_train.py`文件，来开始模型的训练和评测了，你可以设定`--eval_every`参数来控制模型在验证集上的评测频率。更多的命令行参数信息可见`tf.flags`和`hparams`，你也可以运行`python udc_train.py --help`来查看。\n\n运行程序的效果如下：\n\n```\nINFO:tensorflow:training step 20200, loss = 0.36895 (0.330 sec/batch).\nINFO:tensorflow:Step 20201: mean_loss:0 = 0.385877\nINFO:tensorflow:training step 20300, loss = 0.25251 (0.338 sec/batch).\nINFO:tensorflow:Step 20301: mean_loss:0 = 0.405653\n...\nINFO:tensorflow:Results after 270 steps (0.248 sec/batch): recall_at_1 = 0.507581018519, recall_at_2 = 0.689699074074, recall_at_5 = 0.913020833333, recall_at_10 = 1.0, loss = 0.5383\n...\n```\n\n### 模型的评测\n\n在训练完模型后，你可以将其应用在测试集上，使用：\n\n```python\npython udc_test.py --model_dir=$MODEL_DIR_FROM_TRAINING    \n```\n\n例如：\n\n```\npython udc_test.py --model_dir=~/github/chatbot-retrieval/runs/1467389151\n```\n\n这将得到模型在测试集上的`recall@k`的结果，注意在使用`udc_test.py`文件时，需要使用与训练时相同的参数。\n\n在训练模型的次数大约2w次时(在GPU上大约花费1小时)，模型在测试集上得到如下的结果：\n\n```\nrecall_at_1 = 0.507581018519\nrecall_at_2 = 0.689699074074\nrecall_at_5 = 0.913020833333\n```\n\n其中，`recall@1`的值与tfidf模型的差不多，但是`recall@2`和`recall@5`的值则比tfidf模型的结果好太多。原论文中的结果依次是0.55,0.72和0.92，可能通过模型调参或者预处理能够达到这个结果。\n\n### 使用模型进行预测\n\n对于新的数据，你可以使用[udc_predict.py](https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_predict.py)来进行预测；例如：\n\n```\npython udc_predict.py --model_dir=./runs/1467576365/\n```\n\n结果如下：\n\n```\nContext: Example context\nResponse 1: 0.44806\nResponse 2: 0.481638\n```\n\n你可以从候选的回复中，选择预测分值最高的那个作为回复。\n\n### 总结\n\n以上，我们实现了一个基于检索的NN模型，它能够对候选的回复进行预测和打分，通过输出分值最高（或者满足一定阈值）的候选回复已完成聊天的过程。后续可以尝试其他更好的模型，或者通过调参来取得更好的实验结果。\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson2/2.English_text_analysis_and_processing-spaCy/","content":"\n# 英文文本处理与[spaCy](https://spacy.io/)\n\n\n[spaCy](https://spacy.io/)是Python和Cython中的高级自然语言处理库，它建立在最新的研究基础之上，从一开始就设计用于实际产品。spaCy 带有预先训练的统计模型和单词向量，目前支持 20 多种语言的标记。它具有世界上速度最快的句法分析器，用于标签的卷积神经网络模型，解析和命名实体识别以及与深度学习整合。\n\n![](../img/L2_spaCy.png)\n\n### 0.英文Tokenization(标记化/分词)\n\n>文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。在spaCY中同样可以很方便地完成Tokenization。\n\n\n```python\nimport spacy\nnlp = spacy.load('en')\ndoc = nlp('Hello World! My name is pastor')\nfor token in doc:\n    print('\"' + token.text + '\"')\n```\n\n    \"Hello\"\n    \"World\"\n    \"!\"\n    \"My\"\n    \"name\"\n    \"is\"\n    \"pastor\"\n    \n\n每个token对象有着非常丰富的属性，如下的方式可以取出其中的部分属性。\n\n\n```python\ndoc = nlp(\"Next week I'll   be in Shanghai.\")\nfor token in doc:\n    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n        token.text,\n        token.idx,\n        token.lemma_,\n        token.is_punct,\n        token.is_space,\n        token.shape_,\n        token.pos_,\n        token.tag_\n    ))\n```\n\n    Next\t0\tnext\tFalse\tFalse\tXxxx\tADJ\tJJ\n    week\t5\tweek\tFalse\tFalse\txxxx\tNOUN\tNN\n    I\t10\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n    'll\t11\twill\tFalse\tFalse\t'xx\tVERB\tMD\n      \t15\t  \tFalse\tTrue\t  \tSPACE\t_SP\n    be\t17\tbe\tFalse\tFalse\txx\tVERB\tVB\n    in\t20\tin\tFalse\tFalse\txx\tADP\tIN\n    Shanghai\t23\tshanghai\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n    .\t31\t.\tTrue\tFalse\t.\tPUNCT\t.\n    \n\n断句功能在spaCy中也有体现，如下\n\n\n```python\n# 断句\ndoc = nlp(\"Hello World! My name is pastor\")\nfor sent in doc.sents:\n    print(sent)\n```\n\n    Hello World!\n    My name is pastor\n    \n\n### 1.词性标注\n\n>词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n\n>词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。\n\n>词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。\n>词性标注的最简单做法是选取最高频词性，主流的做法可以分为基于规则和基于统计的方法，包括：\n* 基于最大熵的词性标注\n* 基于统计最大概率输出词性\n* 基于HMM的词性标注\n\n\n```python\n# 词性标注\ndoc = nlp(\"Next week I'll be in Shanghai.\")\nprint([(token.text, token.tag_) for token in doc])\n```\n\n    [('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('in', 'IN'), ('Shanghai', 'NNP'), ('.', '.')]\n    \n\n具体的词性标注编码和含义见如下对应表：\n\n| POS Tag | Description | Example |\n| --- | --- | --- |\n| CC | coordinating conjunction | and |\n| CD | cardinal number | 1, third |\n| DT | determiner | the |\n| EX | existential there | there, is |\n| FW | foreign word | d’hoevre |\n| IN | preposition or subordinating conjunction | in, of, like |\n| JJ | adjective | big |\n| JJR | adjective, comparative | bigger |\n| JJS | adjective, superlative | biggest |\n| LS | list marker | 1) |\n| MD | modal | could, will |\n| NN | noun, singular or mass | door |\n| NNS | noun plural | doors |\n| NNP | proper noun, singular | John |\n| NNPS | proper noun, plural | Vikings |\n| PDT | predeterminer | both the boys |\n| POS | possessive ending | friend‘s |\n| PRP | personal pronoun | I, he, it |\n| PRP$ | possessive pronoun | my, his |\n| RB | adverb | however, usually, naturally, here, good |\n| RBR | adverb, comparative | better |\n| RBS | adverb, superlative | best |\n| RP | particle | give up |\n| TO | to | to go, to him |\n| UH | interjection | uhhuhhuhh |\n| VB | verb, base form | take |\n| VBD | verb, past tense | took |\n| VBG | verb, gerund or present participle | taking |\n| VBN | verb, past participle | taken |\n| VBP | verb, sing. present, non-3d | take |\n| VBZ | verb, 3rd person sing. present | takes |\n| WDT | wh-determiner | which |\n| WP | wh-pronoun | who, what |\n| WP\\$ | possessive wh-pronoun | whose |\n| WRB | wh-abverb | where, when |\n\n### 2.命名实体识别\n\n命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。通常包括两部分：1) 实体边界识别；2) 确定实体类别（人名、地名、机构名或其他）。\n\n\n```python\ndoc = nlp(\"Next week I'll be in Shanghai.\")\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\n    Next week DATE\n    Shanghai GPE\n    \n\n\n```python\nfrom nltk.chunk import conlltags2tree\n\ndoc = nlp(\"Next week I'll be in Shanghai.\")\niob_tagged = [\n    (\n        token.text, \n        token.tag_, \n        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n    ) for token in doc\n]\n \nprint(iob_tagged)\n# 按照nltk.Tree的格式显示\nprint(conlltags2tree(iob_tagged))\n```\n\n    [('Next', 'JJ', 'B-DATE'), ('week', 'NN', 'I-DATE'), ('I', 'PRP', 'O'), (\"'ll\", 'MD', 'O'), ('be', 'VB', 'O'), ('in', 'IN', 'O'), ('Shanghai', 'NNP', 'B-GPE'), ('.', '.', 'O')]\n    (S\n      (DATE Next/JJ week/NN)\n      I/PRP\n      'll/MD\n      be/VB\n      in/IN\n      (GPE Shanghai/NNP)\n      ./.)\n    \n\nspaCy中包含的命名实体非常丰富，如下例所示：\n\n\n```python\ndoc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```\n\n    2 CARDINAL\n    9 a.m. TIME\n    30% PERCENT\n    just 2 days DATE\n    WSJ ORG\n    \n\n还可以用非常漂亮的可视化做显示：\n\n\n```python\nfrom spacy import displacy\n \ndoc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\ndisplacy.render(doc, style='ent', jupyter=True)\n```\n\n\n<div class=\"entities\" style=\"line-height: 2.5\">I just bought \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    2\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n shares at \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    9 a.m.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n</mark>\n because the stock went up \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    30%\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n</mark>\n in \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    just 2 days\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n</mark>\n according to the \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n    WSJ\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n</div>\n\n\n### 3.chunking/组块分析\n\nspaCy可以自动检测名词短语，并输出根(root)词，比如下面的\"Journal\",\"piece\",\"currencies\"\n\n\n```python\ndoc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\nfor chunk in doc.noun_chunks:\n    print(chunk.text, chunk.label_, chunk.root.text)\n```\n\n    Wall Street Journal NP Journal\n    an interesting piece NP piece\n    crypto currencies NP currencies\n    \n\n### 4.句法依存解析\n\nspaCy有着非常强大的句法依存解析功能，可以试试对句子进行解析。\n\n\n```python\ndoc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n \nfor token in doc:\n    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n```\n\n    Wall/NNP <--compound-- Street/NNP\n    Street/NNP <--compound-- Journal/NNP\n    Journal/NNP <--nsubj-- published/VBD\n    just/RB <--advmod-- published/VBD\n    published/VBD <--ROOT-- published/VBD\n    an/DT <--det-- piece/NN\n    interesting/JJ <--amod-- piece/NN\n    piece/NN <--dobj-- published/VBD\n    on/IN <--prep-- piece/NN\n    crypto/JJ <--compound-- currencies/NNS\n    currencies/NNS <--pobj-- on/IN\n    \n\n\n```python\nfrom spacy import displacy\n \ndoc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\ndisplacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n```\n\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"675-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wall</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">Street</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">Journal</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">just</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">published</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">an</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">interesting</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">piece</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">on</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">crypto</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">currencies</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,47.0 675.0,47.0 675.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-5\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M610,139.0 L602,127.0 618,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-6\" stroke-width=\"2px\" d=\"M430,137.0 C430,2.0 680.0,2.0 680.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M680.0,139.0 L688.0,127.0 672.0,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-7\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M760.0,139.0 L768.0,127.0 752.0,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-675-0-9\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-675-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M945.0,139.0 L953.0,127.0 937.0,127.0\" fill=\"currentColor\"/>\n</g>\n</svg>\n\n\n### 5.词向量使用\n\nNLP中有一个非常强大的文本表示学习方法叫做word2vec，通过词的上下文学习到词语的稠密向量化表示，同时在这个表示形态下，语义相关的词在向量空间中会比较接近。也有类似`v(爷爷)-v(奶奶) ≈ v(男人)-v(女人)`的关系。\n\n如果大家要使用英文的词向量，需要先下载预先训练好的结果。\n\n命令：!python3 -m spacy download en_core_web_lg\n\n*注：实验平台已预先下载好，可直接调用*\n\n\n```python\nnlp = spacy.load('en_core_web_lg')\nprint(nlp.vocab['banana'].vector)\n```\n\n    [ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n     -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n     -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n      5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n      1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n     -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n      1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n     -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n      2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n      6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n     -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n     -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n      1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n      5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n     -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n      4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n     -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n     -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n      4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n      1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n      5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n      5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n      8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n     -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n     -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n      3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n      4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n      4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n      2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n     -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n      2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n     -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n      2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n     -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n      3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n      3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n     -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n      8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n      2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n     -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n     -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n      2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n     -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n      1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n     -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n     -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n      5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n      4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n      1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n     -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n    \n\n\n```python\nfrom scipy import spatial\n\n# 余弦相似度计算\ncosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n\n# 男人、女人、国王、女王 的词向量\nman = nlp.vocab['man'].vector\nwoman = nlp.vocab['woman'].vector\nqueen = nlp.vocab['queen'].vector\nking = nlp.vocab['king'].vector\n \n# 我们对向量做一个简单的计算，\"man\" - \"woman\" + \"queen\"\nmaybe_king = man - woman + queen\ncomputed_similarities = []\n\n# 扫描整个词库的词向量做比对，召回最接近的词向量\nfor word in nlp.vocab:\n    if not word.has_vector:\n        continue\n \n    similarity = cosine_similarity(maybe_king, word.vector)\n    computed_similarities.append((word, similarity))\n\n# 排序与最接近结果展示\ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint([w[0].text for w in computed_similarities[:10]])\n```\n\n    ['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'Kings', 'KINGS', 'kings']\n    \n\n### 6.词汇与文本相似度\n\n在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法，下面的例子是它的使用方法。\n\n\n```python\n# 词汇语义相似度(关联性)\nbanana = nlp.vocab['banana']\ndog = nlp.vocab['dog']\nfruit = nlp.vocab['fruit']\nanimal = nlp.vocab['animal']\n \nprint(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845\nprint(banana.similarity(fruit), banana.similarity(animal)) # 0.67148364 0.2427285\n```\n\n    0.66185343 0.23552851\n    0.67148364 0.24272855\n    \n\n\n```python\n# 文本语义相似度(关联性)\ntarget = nlp(\"Cats are beautiful animals.\")\n \ndoc1 = nlp(\"Dogs are awesome.\")\ndoc2 = nlp(\"Some gorgeous creatures are felines.\")\ndoc3 = nlp(\"Dolphins are swimming mammals.\")\n \nprint(target.similarity(doc1))  # 0.8901765218466683\nprint(target.similarity(doc2))  # 0.9115828449161616\nprint(target.similarity(doc3))  # 0.7822956752876101\n```\n\n    0.8901766262114666\n    0.9115828449161616\n    0.7822956256736615\n    \n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/1.retrieval_based_chatbot/0.chatbot/","content":"\n# 聊天机器人概述\n\n![](./img/chatbot.png)\n\n资料原文[《Deep Learning For Chatbots, Part 1 - Introduction》](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/) 的翻译，供自己学习及他人参考。\n\n以下内容主要概述了目前聊天机器人主要用到的技术，从宏观上进行介绍，不涉及具体的技术细节。\n\n------\n\n聊天机器人 (Chatbot)，也被称为对话引擎或者对话系统，大家在智能客服和语音智能助手等场景下可以看到它的身影，它是目前的热点之一。在以下内容当中我们将重温一些被用于聊天机器人中的深度学习技术，披露出目前技术能够解决或者可能解决的问题以及几乎难以解决的问题。\n\n## 模型分类\n\n### 基于检索技术的模型 VS 生成式模型\n\n**基于检索技术的模型**较为简单，主要是根据用户的输入和上下文内容，使用了知识库（存储了事先定义好的回复内容）和一些启发式方法来得到一个合适的回复。启发式方法简单的有基于规则的表达式匹配，复杂的有一些机器学习里的分类器。这些系统不能够生成任何新的内容，只是从一个固定的数据集中找到合适的内容作为回复。\n\n**生成式模型**则更加复杂，它不依赖于预定义好的回复内容，而是利用生成式的方法逐词(字)生成新的回复内容。生成式模型典型的有**基于机器翻译模型**的，与传统机器翻译模型不同的是，生成式模型的任务不是将一句话翻译成其他语言的一句话，而是将**用户的输入[翻译]为一个回答(response)**\n\n[![img](http://i.imgur.com/P03eaBZ.png)](http://i.imgur.com/P03eaBZ.png)\n\n**总结**\n\n以上两种模型均有优缺点。对于基于检索技术的模型，由于使用了知识库且数据为预先定义好的，因此进行回复的内容语法上较为通顺，较少出现语法错误；但是基于检索技术的模型中没有会话概念，不能结合上下文给出更加[智能]的回复。而生成式模型则更加[智能]一些，它能够更加有效地利用上下文信息从而知道你在讨论的东西是什么；然而生成式模型比较难以训练，并且输出的内容经常存在一些语法错误（尤其对于长句子而言），以及模型训练需要大规模的数据。\n\n深度学习技术都能够用于基于检索技术的模型和生成式模型中，但是目前的研究热点在生成式模型上。深度学习框架例如**Sequence to Sequence**非常适合用来生成文本，非常多的研究者希望能够在这个领域取得成功。然而目前这一块的研究还在初期阶段，工业界的产品更多的还是使用基于检索计算的模型。\n\n### 短对话 VS 长对话\n\n直观上处理长对话内容将更加困难，这是因为你需要在当前对话的情境下知道之前的对话说过什么。如果是一问一答的形式，技术上这将简单的多。通常对于客服对话而言，长对话更加常见，一次对话中往往会伴随着多个关联问题。\n\n### 开放域 VS 特定领域\n\n面向开放域的聊天机器人技术面临更多困难，这是因为会话可能涉及的面太广，没有一个清晰的目标和意图。在一些社交网站例如Twitter和Reddit上的会话是属于开放域的，会话涉及的主题多种多样，需要的知识量也将非常巨大。\n\n面向特定领域的相关技术则相对简单一些，这是因为特定领域给会话的主题进行了限制，目标和意图也更加清晰，典型的例子有客服系统助手和购物助手。这些系统通常是为了完成某些特定任务，尽管用户在该系统中也能够问些其他方面的东西，但是系统并不会给出相应的回复。\n\n## 面临的挑战\n\n下面介绍一下聊天机器人技术所面临的挑战。\n\n### 如何结合上下文信息\n\n为了产生质量更高的回复，聊天机器人系统通常需要利用一些上下文信息(Context)，这里的上下文信息包括了**对话过程中的语言上下文信息**和**用户的身份信息**等。在长对话中人们关注的是之前说了什么内容以及产生了什么内容的交换，这是语言上下文信息的典型。常见的方法是将一个会话转化为向量形式，但这对长会话而言是困难的。论文[Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](http://arxiv.org/abs/1507.04808)和[Attention with Intention for a Neural Network Conversation Model](http://arxiv.org/abs/1510.08565)中的实验结果表明了这一点。另外，可以结合的上下文信息还包括会话进行时的日期地点信息、用户信息等。\n\n### 语义一致性\n\n理论上来说，机器人面对相同语义而不同形式的问题应该给予一致的回复，例如这两个问题[How old are you?]和[What’s your age?]。这理解起来是简单的，但却是学术界目前的难题之一（如下图）。许多系统都试图对相同语义而不同形式的问题给予语义上合理的回复，但却没有考虑一致性，最大的原因在于训练模型的数据来源于大量不同的用户，这导致机器人失去了固定统一的人格。论文[A Persona-Based Neural Conversation Model](http://arxiv.org/abs/1603.06155)中提及的模型旨在创建具有固定统一人格的机器人。\n\n[![img](http://i.imgur.com/JGrn8LK.png)](http://i.imgur.com/JGrn8LK.png)\n\n### 对话模型的评测\n\n评价一个对话模型的好坏在于它是否很好地完成了某项任务，例如在对话中解决了客户的问题。这样的训练数据需要人工标注和评测，所以获取上需要一定人力代价。有时在开放域中的对话系统也没有一个清晰的优化目标。用于机器翻译的评测指标[BLEU](https://en.wikipedia.org/wiki/BLEU)不能适用于此，是因为它的计算基础是语言表面上的匹配程度，而对话中的回答可以是完全不同词型但语义通顺的语句。论文[How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation](http://arxiv.org/abs/1603.08023)中给出结论，目前常用的评测指标均与人工评测无关。\n\n### 意图和回复多样性\n\n生成式模型中的一个普遍问题是，它们都想要生成一些通用的回答，例如[That’s great!]和[I don’t know]这样的可以应付许多的用户询问。早期的Google智能回复基本上以[I love you]回复所有的东西[链接](http://googleresearch.blogspot.com/2015/11/computer-respond-to-this-email.html)，这是一些模型最终训练出来的结果，原因在于训练数据和训练的优化目标。因此，有些研究学者开始关注[如何提升机器人的回复的多样性](http://arxiv.org/abs/1510.03055)，然而人们在对话过程中的回复与询问有一定特定关系，是有一定意图的，而许多面向开放域的机器人不具备特定的意图。\n\n## 实际效果\n\n以目前的研究水平所制造的机器人能够取得的效果如何？使用基于检索技术的显然无法制作出面向开放域的机器人，这是因为你不能编写覆盖所有领域的语料；而生成式的面向开放域的机器人还属于**通用人工智能(Artifical General Intelligence, AGI)**水平，距离理想状态还相距甚远，但相关研究学者还在致力于此。\n\n对于特定领域的机器人，基于检索的技术和生成式模型都能够利用。但是对于长对话的情境，也面临许多困难。\n\n在最近对[Andrew NG的采访](http://www.seattletimes.com/business/baidu-research-chief-andrew-ng-fixed-on-self-taught-computers-self-driving-cars/)中，NG提到：\n\n> 目前深度学习的价值主要体现在能够获取大量数据的特定领域。目前一个无法做的事情是产生一个有意义的对话。\n\n许多创业公司声称只要有足够多的数据，就能够产生自动智能的对话系统。然而，目前的水平生产出面向一个特定的子领域的对话应用（如利用Uber打车），而对于一个稍微开放点的领域就难以实现了（如自动销售）。但是，帮助用户提供自动回复建议以及语法纠正还是可行的。\n\n使用基于检索技术的对话系统更加可控和稳定，给出的回复出现语法错误的几率更低。而使用生成式模型的风险在于回复不可控，且容易出现一些风险，例如[微软的Tay](http://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3)。\n\n\n\n\n```python\n\n```\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/8Machine_Translation/4.Transformer_Model_from_Google/Transformer_Model_from_Google/","content":"\n# 来自Google的Transformer模型\n\n\n## 本章概述\n\n- Google的Transformer模型\n    - 编码器，解码器\n    - 传统的注意力机制及Multi-head attention\n    - 基于位置的单词编码，及词向量，输出层\n    - 可视化multi-head attention\n    - Transformer与RNN和CNN神经翻译模型的对比\n- Google模型的训练细节\n    - 优化器选择\n    - 正则化\n    - label smoothing\n- 实战演示\n    - 介绍encoder，decoder类及model类\n    - 介绍如何训练模型\n    - 运用OpenNMT开源工具来实战演示\n\n## 1.来自Google的Transformer模型\n\n- 序列计算中，传统的RNN在预测下一个符号（token）的时候，会对以往的历史信息有很强的依赖，使得难以充分地并行化，也无法很好地加深网络的层级结构。而对于传统的基于CNN的神经机器翻译模型，两个任意输入与输出位置的信号关联所需要的运算数量与它们的位置距离成正比，Facebook提出的CNN NMT 为线性增长。这两种常见的结构使得学习较远位置的依赖关系（long-term dependency）非常困难。\n\n- 在 Transformer 中，两个任意输入的信号关联的开销会减少到一个固定的运算数量，使用 Multi-Head Attention 注意力机制可以完全脱离RNN及CNN的结构，只使用自注意机制（self-attention)，使得Transformer可以高效地并行化，并堆叠非常多层的深层网络。\n\n- 自注意力（Self-attention），是一种涉及单序列不同位置的注意力机制，并能计算序列的表征。自注意力在多种任务中都有非常成功的应用，例如阅读理解、摘要概括、文字蕴含和语句表征等。自注意力这种在序列内部执行 Attention 的方法可以视为搜索序列内部的隐藏关系，这种内部关系对于翻译以及序列任务的性能非常重要。\n\n\n### 1.1 编码器 encoder\n\n\n- 编码器encoder由6层结构一样的网络层组成，每一层有2个子层：\n    - 第一个子层是multi-head self-attention Layer\n    - 第二个子层是一个基于位置编码的全连接网络层（position-wise fully connected feed-forward network）\n    - 我们会使用残差连接的方式，分别对每个子层的输入加到这个子层的输出上，然后我们再接一个Layer normalization的归一化层。\n    - 所有的embedding及hidden state的维度都是512 \n    $$ \\text{LayerNorm}(x+\\text{Sublayer}(x)) $$\n\n\n### 1.2 解码器 decoder\n\n\n- 解码器decoder由6层结构一样的网络层组成，每一层除了跟encode人一样有2个子层以外，还有第3个子层\n    - 第一个子层是multi-head self-attention Layer\n    - 第二个子层是一个基于位置编码的全连接网络层（position-wise fully connected feed-forward network）\n    - <font color='red'>第三个子层用于对encoder的输出向量进行multi-head attention</font>\n    - 同样的，我们会使用残差连接的方式，分别对每个子层的输入加到该子层的输出上，然后我们再接一个Layer normalization的归一化层。\n    $$ \\text{LayerNorm}(x+\\text{Sublayer}(x))\\\\ $$\n    - decoder还需要将还没有生成的后续序列掩盖掉（masking），这样做是为了防止decoder在做self-attention的时候关注到后续还未生成的单词上去。\n\n\n### 1.3 注意力机制\n\n\n- 传统的注意力机制，也称为scaled Dot-Product Attention，可以看成是有一个询问的词（query），去跟一堆哈希表中的键值对（key-value pair）进行匹配，找到最相关的键（key），之后返回该键所对应的值（value）。通常的，如果我们只返回一个key所对应的value，我们称之为hard attention。如果我们对所有的key都计算一个相关系数，（也称之为attention weight），我们可以将所有key对应的value进行加权求和（weighted sum）这样的操作我们称之为soft attention。\n$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left({QK^T \\over \\sqrt{d_k}}\\right)V$$\n- 其中所有的query和key都是维度为$d_k$的向量，我们将这些向量分别叠在一起形成 $Q\\in\\mathbb{R}^{|Q|\\times d_k}, K\\in\\mathbb{R}^{|K|\\times d_k}$的矩阵。\n- 所有的value都是维度为$d_v$的向量，我们将这些向量叠在一起形成$V\\in\\mathbb{R}^{|V|\\times d_k}$\n- 这里如果维度$d_k$很大的时候，两个向量的乘积会变得很大，使得softmax会得到非常小的数值，所以我们会在这里除以$\\sqrt{d_k}$来抵消这个影响\n![](./img/attention.png)\n\n### 1.4 Multi-Head Attention\n\n\n- 这里我们假设$Q,~K,~V\\in \\mathbb{R}^{d_\\text{model}}$都在一个${d_\\text{model}}$维度的空间中\n- 我们使用h个不一样权重的线性映射函数$(QW^Q_i, KW^K_i, VW^V_i)$将Q, K, V分别映射到$d_k,~d_k,~d_v$空间中\n- 我们对映射之后的Q, K, V 做h次attention，并将h个attention head连接在一起形成一个新的向量\n- 最后再将这个向量映射到$d_\\text{model}$空间，作为下一层的输入\n\n$$ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\cdots, \\text{head}_h) W^O \\\\ \\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i) $$\n- 其中$W^Q_i\\in \\mathbb{R}^{d_\\text{model}\\times d_k}, W^K_i\\in \\mathbb{R}^{d_\\text{model}\\times d_k}, W^V_i\\in \\mathbb{R}^{d_\\text{model}\\times d_v}, W^O\\in \\mathbb{R}^{hd_v\\times d_\\text{model}}$, 常见的我们设置$h=8,d_k=d_v=d_\\text{model}/h=64$\n- 模型图\n![](./img/mha.png)\n\n### 1.4 Multi-Head Attention\n\n\n- 应用\n    - 将decoder上一个时刻的hidden state 作为query，将encoder的最顶层的所有输出的hidden state作为key和value，这样可以类似传统的attention机制一样去发现源语言单词与目标语言单词之间的联系\n    - encoder本身会对源语言单词进行multi-head self attention，其中query，key，value都是一样的，都是上一层中输出的单词的hidden state，每一个时刻计算出来的context vector都会作为该层输出的新的单词的hidden state，并作为下一层的输入。\n    - decoder本身也会类似encoder一样去做self attention，不同的是，decoder只对左边已经生成的序列进行attention，对还没有生成的（右边的）序列掩盖掉（masking）\n\n- 完整的模型图\n![](./img/transformer.png)\n\n### 1.5 基于位置的前向神经网络（Position-wise Feed-Forward Networks）\n\n\n- 对于encoder和decoder的每个attention层之后，我们还会在连接一个全连接的前向神经网络。这个网络包含了两个线性转换和中间加一个ReLU的激活函数\n$$FFN(x) =\\max(0, xW_1+b_1) W_2+b_2$$\n- 这里每一层，我们都用不同的$W_1,W_2,b_1,b_2$。\n\n![](./img/cnn-encoder.png)\n\n### 1.6 词向量矩阵及Softmax层\n\n\n- 这里我们使用常见的词向量矩阵，并encoder会把词向量映射到$d_\\text{model}$空间上，作为第一层的输入\n- 在做预测的时候，我们会将输出向量映射到一个词表大小的概率空间中，并使用softmax来归一化到一个$[0,1]$之间的概率值。\n\n\n### 1.7 位置编码（position embeddings）\n\n- 因为模型没有recurrence及convolution的操作，所以为了让模型能够分辨不同位置的单词，我们需要对单词的位置进行编码。\n\n$$PE(pos, 2i)=\\sin(pos/10000^{2i/d_\\text{model}}) \\\\ PE(pos, 2i+1)=\\cos(pos/10000^{2i/d_\\text{model}})$$\n\n- pos是这个单词在句子中的位置，i是这个位置向量的第i个维度的编号。这样的波长形成了一个从$2\\pi$到$1000\\cdot 2\\pi$的几何级数。这样会使得模型更容易学到相对距离，因为$PE_{pos+k}$可以表示为$PE_{pos}$的一个线性变化。\n\n    \n\n\n### 1.8 Transformer 对比RNN及CNN\n\n\n- 我们发现RNN需要进行$O(n)$个序列操作，而Transformer和CNN只需要$O(1)$个\n- CNN会形成一个层级结构，类似树状，所以任意两个单词到达的最大路径长度是$O(\\log_k(n))$\n- 如果self-attention只对该单词周围r个单词进行attention操作，我们可以得到restricted版本的self-attention， 这样可以减少每一层的计算复杂度，但为增加两个任意词之间到达的最长路径\n![](./img/compare.png)\n\n\n\n### 1.9 可视化attention\n\n\n- 一个例子发现attention能找到长距离的依赖关系“making ... more difficult”\n![](./img/att-1.png)\n\n\n\n### 1.9 可视化attention\n\n\n- 一个例子发现attention能找到名词的对应关系\"Law\" 和 \"application\" 都对应于 \"its\"\n![](./img/att-2.png)\n\n\n\n### 1.9 可视化attention\n\n\n- 一个例子发现attention能找到句子中的结构关系，比如某一些head能发现单词的依赖关系\n![](./img/att-3.png)\n\n\n\n## 2. Transformer模型的训练细节\n\n\n- 优化方法\n- 正则化 （regularization） \n- label smoothing\n\n### 2.1 优化方法\n\n\n- Adam 优化方法，$\\beta_1=0.9, \\beta_2=0.98, \\epsilon=10^{-9}$\n- learning rate是随着训练的过程中，通过以下一个函数进行变化。一开始在前 warmup_steps个训练迭代中learning rate是线性增长的，往后随着步长的增加而下降。 一般会设置 warmup_steps = 4000\n$$lr = d_\\text{model}^{-0.5} \\cdot \\min(\\text{step_num}^{-0.5},\\text{step_num}\\cdot \\text{warmup_steps}^{-1.5}) $$\n    \n\n\n### 2.2 正则化 Regularization\n\n\n- 对每一个子层的输出，在该子层的输出加上该子层的输入之前进行dropout\n- 对encoder及decoder，词向量和位置向量求和之后都进行dropout\n\n\n### 2.3 Label Smoothing\n\n- 对于正确的标注label，在其one-hot表达上，加上一个均匀分布的向量，这个smoothing的数值是$\\epsilon_{ls}=0.1$\n\n## 3 Tranformer源码解析\n\n\n我们来看一下基于OpenNMT中Transformer的实现\n\n- [官方代码](https://github.com/tensorflow/models/tree/master/official/transformer)\n- [作者代码](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)\n- [哈佛NLP组pytorch实现](https://github.com/OpenNMT/OpenNMT-py/tree/master/onmt)\n\n### 3.1 编码器encoder\n\n\n```python\nclass TransformerEncoderLayer(nn.Module):\n    \"\"\"编码器中的一个层 \"\"\"\n    def __init__(self, d_model, heads, d_ff, dropout, max_relative_positions=0):\n        self.self_attn = MultiHeadedAttention(\n            heads, d_model, dropout=dropout,\n            max_relative_positions=max_relative_positions)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, inputs, mask):\n        # 1. LayerNorm 2. self-attention 3. dropout 4. Feed-Forward\n        input_norm = self.layer_norm(inputs)\n        context, _ = self.self_attn(input_norm, input_norm, input_norm, mask=mask, type=\"self\")\n        out = self.dropout(context) + inputs\n        return self.feed_forward(out)\n\nclass TransformerEncoder(EncoderBase):\n    def __init__(self, num_layers, d_model, heads, d_ff, dropout, embeddings, max_relative_positions):\n        ...\n        self.embeddings = embeddings\n        self.transformer = nn.ModuleList([TransformerEncoderLayer(...) for i in range(num_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, src, lengths=None):\n        ...\n        # 单词的编码\n        emb = self.embeddings(src)\n\n        out = emb.transpose(0, 1).contiguous()\n        words = src[:, :, 0].transpose(0, 1)\n        w_batch, w_len = words.size()\n        padding_idx = self.embeddings.word_padding_idx\n        mask = words.data.eq(padding_idx).unsqueeze(1)  # [B, 1, T]\n        # 遍历多层网络\n        for layer in self.transformer:\n            out = layer(out, mask)\n        out = self.layer_norm(out)\n        return emb, out.transpose(0, 1).contiguous(), lengths\n```\n\n### 3.2 解码 decoder\n\n\n```python\nclass TransformerDecoderLayer(nn.Module):\n    '''解码器中一个单层'''\n    def __init__(self, d_model, heads, d_ff, dropout,\n                 self_attn_type=\"scaled-dot\", max_relative_positions=0):\n        \n        # 定义self-attention类型\n        if self_attn_type == \"scaled-dot\":\n            self.self_attn = MultiHeadedAttention(heads, d_model, dropout, max_relative_positions)\n        elif self_attn_type == \"average\":\n            self.self_attn = AverageAttention(d_model, dropout=dropout)\n        # 定义对encoder所有输出单词的context attention\n        self.context_attn = MultiHeadedAttention(heads, d_model, dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.layer_norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n        self.layer_norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, inputs, memory_bank, src_pad_mask, tgt_pad_mask, layer_cache=None, step=None):\n        # 对还未生成的单词做masking\n        dec_mask = None\n        if step is None:\n            tgt_len = tgt_pad_mask.size(-1)\n            future_mask = torch.ones(...)\n            future_mask = future_mask.triu_(1).view(1, tgt_len, tgt_len)\n            dec_mask = torch.gt(tgt_pad_mask + future_mask, 0)\n        \n        input_norm = self.layer_norm_1(inputs)\n        if isinstance(self.self_attn, MultiHeadedAttention):\n            query, attn = self.self_attn(input_norm, input_norm, input_norm, ..., type=\"self\")\n        elif isinstance(self.self_attn, AverageAttention):\n            query, attn = self.self_attn(input_norm, mask=dec_mask, layer_cache=layer_cache, step=step)\n        # 对encoder输出做context attention\n        query = self.drop(query) + inputs\n        query_norm = self.layer_norm_2(query)\n        mid, attn = self.context_attn(memory_bank, memory_bank, query_norm, ..., type=\"context\")\n        output = self.feed_forward(self.drop(mid) + query)\n        return output, attn\n        \nclass TransformerDecoder(DecoderBase):\n    def __init__(self, num_layers, d_model, heads, d_ff, attn_type,\n                 copy_attn, self_attn_type, dropout, embeddings,\n                 max_relative_positions):\n        self.embeddings = embeddings\n\n        # Decoder State\n        self.state = {}\n        self.transformer_layers = nn.ModuleList([TransformerDecoderLayer(...) for i in range(num_layers)])\n\n        self._copy = copy_attn\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, tgt, memory_bank, step=None, **kwargs):\n        ...\n        src = self.state[\"src\"]\n        src_words = src[:, :, 0].transpose(0, 1)\n        tgt_words = tgt[:, :, 0].transpose(0, 1)\n        src_batch, src_len = src_words.size()\n        tgt_batch, tgt_len = tgt_words.size()\n\n        emb = self.embeddings(tgt, step=step) # len x batch x embedding_dim\n\n        output = emb.transpose(0, 1).contiguous()\n        src_memory_bank = memory_bank.transpose(0, 1).contiguous()\n        \n        # Masking\n        pad_idx = self.embeddings.word_padding_idx\n        src_pad_mask = src_words.data.eq(pad_idx).unsqueeze(1)  # [B, 1, T_src]\n        tgt_pad_mask = tgt_words.data.eq(pad_idx).unsqueeze(1)  # [B, 1, T_tgt]\n\n        for i, layer in enumerate(self.transformer_layers):\n            ...\n            # 多层网络\n            output, attn = layer(output, src_memory_bank, src_pad_mask, tgt_pad_mask, layer_cache, step)\n        output = self.layer_norm(output)\n        dec_outs = output.transpose(0, 1).contiguous()\n        attn = attn.transpose(0, 1).contiguous()\n        attns = {\"std\": attn}\n        return dec_outs, attns\n```\n\n\n### 3.3 Transformer模型\n\n\n```python\nclass NMTModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(NMTModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, tgt, lengths, bptt=False):\n        tgt = tgt[:-1]  # 从输入中去掉最后一个单词\n        # 编码\n        enc_state, memory_bank, lengths = self.encoder(src, lengths)\n        if bptt is False:\n            self.decoder.init_state(src, memory_bank, enc_state)\n        # 解码\n        dec_out, attns = self.decoder(tgt, memory_bank, memory_lengths=lengths)\n        return dec_out, attns\n```\n\n\n### 3.4 训练\n\n\n```python\nclass Trainer(object):\n    # 定义训练的类\n    def __init__(self, model, train_loss, valid_loss, optim,\n                 trunc_size=0, shard_size=32,\n                 norm_method=\"sents\", grad_accum_count=1, n_gpu=1, gpu_rank=1,\n                 gpu_verbose_level=0, report_manager=None, model_saver=None,\n                 average_decay=0, average_every=1):\n        ...\n        # 把模型设置成训练状态\n        self.model.train()\n\n    def train(self, train_iter, train_steps, save_checkpoint_steps=5000, valid_iter=None, valid_steps=10000):\n        ...\n        # 开始遍历每个mini-batch\n        for i, (batches, normalization) in enumerate(\n                self._accum_batches(train_iter)):\n            step = self.optim.training_step\n            ...\n            # 累积多个mini-batch算出来的gradient\n            self._gradient_accumulation(\n                batches, normalization, total_stats,\n                report_stats)\n            # 把多个gradient求平均值并更新模型参数\n            if self.average_decay > 0 and i % self.average_every == 0:\n                self._update_average(step)\n            ...\n            # 做validation，可用于判断是否需要终止训练\n            if valid_iter is not None and step % valid_steps == 0:\n                valid_stats = self.validate(valid_iter, moving_average=self.moving_average)\n                ...\n                \n            # 保存model\n            if (self.model_saver is not None\n                and (save_checkpoint_steps != 0\n                     and step % save_checkpoint_steps == 0)):\n                self.model_saver.save(step, moving_average=self.moving_average)\n\n            if train_steps > 0 and step >= train_steps:\n                break\n        return total_stats\n```\n\n\n### 3.5 实战例子\n\n\n- 运行预处理句子，\n```bash\npython preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\n\n # 会得到以下三个文件\ndemo.train.pt: serialized PyTorch file containing training data\ndemo.valid.pt: serialized PyTorch file containing validation data\ndemo.vocab.pt\n\n (2) 训练\npython train.py -data data/demo -save_model demo-model \\\n        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\\n        -encoder_type transformer -decoder_type transformer -position_encoding \\\n        -train_steps 200000  -max_generator_batches 2 -dropout 0.1 \\\n        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\\n        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\\n        -max_grad_norm 0 -param_init 0  -param_init_glorot \\\n        -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 \\\n        -world_size 4 -gpu_ranks 0 1 2 3 \n\n (3) 翻译\npython translate.py -model demo-model_acc_XX.XX_ppl_XXX.XX_eX.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose\n```\n\n\n## 本章小结\n\n- Google的Transformer模型\n    - 编码器，解码器\n    - 传统的注意力机制及Multi-head attention\n    - 基于位置的单词编码，及词向量，输出层\n    - 可视化multi-head attention\n    - Transformer与RNN和CNN神经翻译模型的对比\n- Google模型的训练细节\n    - 优化器选择\n    - 正则化\n    - label smoothing\n- 实战演示\n    - 介绍encoder，decoder类及model类\n    - 介绍如何训练模型\n    - 运用OpenNMT开源工具来实战演示\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/7text_generation_v2/poetry_generator/poetry_generator/","content":"\n# 文本生成-诗词生成案例\n\n\n## 0.文本生成问题\n文本生成是自然语言处理中一个重要的研究领域，具有广阔的应用前景。国内外已经有诸如Automated Insights、Narrative Science以及“小南”机器人、“小明”机器人、“运动报道机器人”等文本生成系统投入使用。这些系统根据格式化数据或自然语言文本生成新闻、财报或者其他解释性文本。例如，Automated Insights的WordSmith技术已经被美联社等机构使用，帮助美联社报道大学橄榄球赛事、公司财报等新闻。这使得美联社不仅新闻更新速度更快，而且在人力资源不变的情况下扩大了其在公司财报方面报道的覆盖面。\n\n解决这个问题的深度学习模型，最常见的是借助于语言模型，或者seq2seq这种encoder-decoder模型。我们这里使用最经典的语言模型，借助LSTM构建一个AI文本生成器。\n\n## 0.文本生成原理\n我们简单回顾一下我们在语言模型课程中讲到的内容，基于RNN(LSTM)的语言模型可以根据上下文去推断下一个位置有更高的概率出现哪个词，比如一个人说：\"我是中国人，我的母语是___ \"。 对于在“__”中需要填写的内容，RNN通过前文的“母语”知道需要是一种语言，通过“中国”知道这个语言需要是“中文”。而这种方式可以用来做文本生成，我们在给定词的头部一些字后，可以根据大量的诗歌文本总训练得到的语言模型，预估下一个位置合适填入的字词。\n![](../img/L2_rnnLM.png)\n\n## 0.关于数据源\n本项目更丰富的数据源可以在[诗词github](https://github.com/chinese-poetry/chinese-poetry)取到，感谢github作者的分享。\n\n## 1.诗词生成器\n我们这里使用keras工具库完成一个诗词生成器，实现方案是最简单的基于语言模型的实现方式。\n\n![](poem.png)\n\n\n```python\n# 引入需要的工具库\nimport numpy as np\nimport random\nimport os\nfrom keras.layers import LSTM, Dropout, Dense\nfrom keras.models import Input, Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LambdaCallback,ModelCheckpoint\n```\n\n    Using TensorFlow backend.\n    \n\n\n```python\n!head -5 dataset/poetry.txt\n```\n\n    首春:寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n\n    初晴落景:晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n\n    初夏:一朝春夏改，隔夜鸟花迁。阴阳深浅叶，晓夕重轻烟。哢莺犹响殿，横丝正网天。珮高兰影接，绶细草纹连。碧鳞惊棹侧，玄燕舞檐前。何必汾阳处，始复有山泉。\n\n    度秋:夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n\n    仪鸾殿早秋:寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n\n\n\n\n```python\n# 定义配置类\nclass ModelConfig(object):\n    poetry_file = 'dataset/poetry.txt'\n    weight_file = 'model/poetry_model.h5'\n    max_len = 6\n    batch_size = 32\n    learning_rate = 0.003\n\n# 定义文件读取函数\ndef preprocess_data(ModelConfig):\n    # 语料文本内容\n    files_content = ''\n    with open(ModelConfig.poetry_file, 'r',encoding='UTF-8') as f:\n        for line in f:\n            x = line.strip() + \"]\"\n            # 取出具体诗的内容\n            x = x.split(\":\")[1]\n            # 根据长度过滤脏数据\n            if len(x) <= 5 :\n                continue\n            # 过滤出五言绝句\n            if x[5] == '，':\n                files_content += x\n            \n    # 字频统计\n    words = sorted(list(files_content))\n    counted_words = {}\n    for word in words:\n        if word in counted_words:\n            counted_words[word] += 1\n        else:\n            counted_words[word] = 1\n\n    # 低频字过滤\n    delete_words = []\n    for key in counted_words:\n        if counted_words[key] <= 2:\n            delete_words.append(key)\n    for key in delete_words:\n        del counted_words[key]\n    wordPairs = sorted(counted_words.items(), key=lambda x: -x[1])\n\n    words, _ = zip(*wordPairs)\n    words += (\" \",)\n    \n    # 构建 字到id的映射字典 与 id到字的映射字典\n    word2idx = dict((c, i) for i, c in enumerate(words))\n    idx2word = dict((i, c) for i, c in enumerate(words))\n    word2idx_dic = lambda x: word2idx.get(x, len(words) - 1)\n    return word2idx_dic, idx2word, words, files_content\n```\n\n\n```python\nclass LSTMPoetryModel(object):\n    def __init__(self, config):\n        self.model = None\n        self.do_train = True\n        self.loaded_model = True\n        self.config = config\n\n        # 诗歌训练文件预处理\n        self.word2idx_dic, self.idx2word, self.words, self.files_content = preprocess_data(self.config)\n        \n        # 诗列表\n        self.poems = self.files_content.split(']')\n        # 诗的总数量\n        self.poems_num = len(self.poems)\n        \n        # 如果有预训练好的模型文件，则直接加载模型，否则开始训练\n        if os.path.exists(self.config.weight_file) and self.loaded_model:\n            self.model = load_model(self.config.weight_file)\n        else:\n            self.train()\n\n    def build_model(self):\n        '''LSTM模型构建'''\n        print('模型构建中...')\n\n        # 输入的维度\n        input_tensor = Input(shape=(self.config.max_len, len(self.words)))\n        lstm = LSTM(512, return_sequences=True)(input_tensor)\n        dropout = Dropout(0.6)(lstm)\n        lstm = LSTM(256)(dropout)\n        dropout = Dropout(0.6)(lstm)\n        dense = Dense(len(self.words), activation='softmax')(dropout)\n        self.model = Model(inputs=input_tensor, outputs=dense)\n        optimizer = Adam(lr=self.config.learning_rate)\n        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    def sample(self, preds, temperature=1.0):\n        '''\n        temperature可以控制生成诗的创作自由约束度\n        当temperature<1.0时，模型会做一些随机探索，输出相对比较新的内容\n        当temperature>1.0时，模型预估方式偏保守\n        在训练的过程中可以看到temperature不同，结果也不同\n        就是一个概率分布变换的问题，保守的时候概率大的值变得更大，选择的可能性也更大\n        '''\n        preds = np.asarray(preds).astype('float64')\n        exp_preds = np.power(preds,1./temperature)\n        preds = exp_preds / np.sum(exp_preds)\n        prob = np.random.choice(range(len(preds)),1,p=preds)\n        return int(prob.squeeze())\n    \n    def generate_sample_result(self, epoch, logs):\n        '''训练过程中，每5个epoch打印出当前的学习情况'''\n        if epoch % 5 != 0:\n            return\n        \n        # 追加模式添加内容\n        with open('out/out.txt', 'a',encoding='utf-8') as f:\n            f.write('==================第{}轮=====================\\n'.format(epoch))\n                \n        print(\"\\n==================第{}轮=====================\".format(epoch))\n        for diversity in [0.7, 1.0, 1.3]:\n            print(\"------------设定诗词创作自由度约束参数为{}--------------\".format(diversity))\n            generate = self.predict_random(temperature=diversity)\n            print(generate)\n            \n            # 训练时的预测结果写入txt\n            with open('out/out.txt', 'a',encoding='utf-8') as f:\n                f.write(generate+'\\n')\n    \n    def predict_random(self,temperature = 1):\n        '''预估模式1：随机从库中选取一句开头的诗句，生成五言绝句'''\n        if not self.model:\n            print('没有预训练模型可用于加载！')\n            return\n        \n        index = random.randint(0, self.poems_num)\n        sentence = self.poems[index][: self.config.max_len]\n        generate = self.predict_sen(sentence,temperature=temperature)\n        return generate\n    \n    def predict_first(self, char,temperature =1):\n        '''预估模式2：根据给出的首个文字，生成五言绝句'''\n        if not self.model:\n            print('没有预训练模型可用于加载！')\n            return\n        \n        index = random.randint(0, self.poems_num)\n        # 选取随机一首诗的最后max_len个字+给出的首个文字作为初始输入\n        sentence = self.poems[index][1-self.config.max_len:] + char\n        generate = str(char)\n        # 预测后面23个字\n        generate += self._preds(sentence,length=23,temperature=temperature)\n        return generate\n    \n    def predict_sen(self, text,temperature =1):\n        '''预估模式3：根据给出的前max_len个字，生成诗句'''\n        '''此例中，即根据给出的第一句诗句（含逗号），来生成古诗'''\n        if not self.model:\n            return\n        max_len = self.config.max_len\n        if len(text)<max_len:\n            print('给出的初始字数不低于 ',max_len)\n            return\n\n        sentence = text[-max_len:]\n        print('第一行为:',sentence)\n        generate = str(sentence)\n        generate += self._preds(sentence,length = 24-max_len,temperature=temperature)\n        return generate\n    \n    def predict_hide(self, text,temperature = 1):\n        '''预估模式4：根据给4个字，生成藏头诗五言绝句'''\n        if not self.model:\n            print('没有预训练模型可用于加载！')\n            return\n        if len(text)!=4:\n            print('藏头诗的输入必须是4个字！')\n            return\n        \n        index = random.randint(0, self.poems_num)\n        # 选取随机一首诗的最后max_len个字+给出的首个文字作为初始输入\n        sentence = self.poems[index][1-self.config.max_len:] + text[0]\n        generate = str(text[0])\n        print('第一行为 ',sentence)\n        \n        for i in range(5):\n            next_char = self._pred(sentence,temperature)           \n            sentence = sentence[1:] + next_char\n            generate+= next_char\n        \n        for i in range(3):\n            generate += text[i+1]\n            sentence = sentence[1:] + text[i+1]\n            for i in range(5):\n                next_char = self._pred(sentence,temperature)           \n                sentence = sentence[1:] + next_char\n                generate+= next_char\n\n        return generate\n    \n    \n    def _preds(self,sentence,length = 23,temperature =1):\n        '''\n        供类内部调用的预估函数，输入max_len长度字符串，返回length长度的预测值字符串\n        sentence:预测输入值\n        lenth:预测出的字符串长度\n        '''\n        sentence = sentence[:self.config.max_len]\n        generate = ''\n        for i in range(length):\n            pred = self._pred(sentence,temperature)\n            generate += pred\n            sentence = sentence[1:]+pred\n        return generate\n        \n        \n    def _pred(self,sentence,temperature =1):\n        '''供类内部调用的预估函数，根据一串输入，返回单个预测字符'''\n        if len(sentence) < self.config.max_len:\n            print('in def _pred,length error ')\n            return\n        \n        sentence = sentence[-self.config.max_len:]\n        x_pred = np.zeros((1, self.config.max_len, len(self.words)))\n        for t, char in enumerate(sentence):\n            x_pred[0, t, self.word2idx_dic(char)] = 1.\n        preds = self.model.predict(x_pred, verbose=0)[0]\n        next_index = self.sample(preds,temperature=temperature)\n        next_char = self.idx2word[next_index]\n        \n        return next_char\n\n    def data_generator(self):\n        '''生成器生成数据'''\n        i = 0\n        while 1:\n            x = self.files_content[i: i + self.config.max_len]\n            y = self.files_content[i + self.config.max_len]\n\n            if ']' in x or ']' in y:\n                i += 1\n                continue\n\n            y_vec = np.zeros(\n                shape=(1, len(self.words)),\n                dtype=np.bool\n            )\n            y_vec[0, self.word2idx_dic(y)] = 1.0\n\n            x_vec = np.zeros(\n                shape=(1, self.config.max_len, len(self.words)),\n                dtype=np.bool\n            )\n\n            for t, char in enumerate(x):\n                x_vec[0, t, self.word2idx_dic(char)] = 1.0\n\n            yield x_vec, y_vec\n            i += 1\n\n    def train(self):\n        '''训练模型'''\n        print('开始训练...')\n        number_of_epoch = len(self.files_content)-(self.config.max_len + 1)*self.poems_num\n        number_of_epoch /= self.config.batch_size \n        number_of_epoch = int(number_of_epoch / 1.5)\n        print('总迭代轮次为 ',number_of_epoch)\n        print('总诗词数量为 ',self.poems_num)\n        print('文件内容的长度为 ',len(self.files_content))\n\n        if not self.model:\n            self.build_model()\n\n        self.model.fit_generator(\n            generator=self.data_generator(),\n            verbose=True,\n            steps_per_epoch=self.config.batch_size,\n            epochs=number_of_epoch,\n            callbacks=[\n                ModelCheckpoint(self.config.weight_file, save_weights_only=False),\n                LambdaCallback(on_epoch_end=self.generate_sample_result)\n            ]\n        )\n```\n\n\n```python\nmodel = LSTMPoetryModel(ModelConfig)\n\nprint('预训练模型加载成功！')\n```\n\n    开始训练...\n    总迭代轮次为  50000\n    总诗词数量为  24027\n    文件内容的长度为  1841397\n    模型构建中...\n    Epoch 1/50000\n    32/32 [==============================] - 13s 406ms/step - loss: 8.4915 - acc: 0.0000e+00\n    \n    ==================第0轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 远客夜衣薄，\n    远客夜衣薄，敖懔籥醐靺家箦这筜累癸泼帷、觿蝀慊臭\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 受脤清边服，\n    受脤清边服，聋抽味忏晚踏缱刃变委杏调缘间，赦雷晃\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 百鸟有啼时，\n    百鸟有啼时，卖斥人飓玃跻踠自撩辖偕砰裟容聊猷婿沱\n    Epoch 2/50000\n    32/32 [==============================] - 11s 347ms/step - loss: 7.5517 - acc: 0.0938\n    Epoch 3/50000\n    32/32 [==============================] - 11s 346ms/step - loss: 7.7621 - acc: 0.1250\n    Epoch 4/50000\n    32/32 [==============================] - 11s 352ms/step - loss: 8.4055 - acc: 0.0625\n    Epoch 5/50000\n    32/32 [==============================] - 12s 362ms/step - loss: 8.0757 - acc: 0.0312\n    Epoch 6/50000\n    32/32 [==============================] - 11s 341ms/step - loss: 8.1178 - acc: 0.0938\n    \n    ==================第5轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 朝亦有所思，\n    朝亦有所思，郑，。，，，筑，。。，。，。，茗。，\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 漂泊来千里，\n    漂泊来千里，，撩。讽，苗熇剌，翠虹娉。崷喟苎。，\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 飘飖经远道，\n    飘飖经远道，跄鶗胼琵攲旰级音硖健臆房璆遗敏触旎孕\n    Epoch 7/50000\n    32/32 [==============================] - 12s 361ms/step - loss: 7.6054 - acc: 0.0312\n    Epoch 8/50000\n    32/32 [==============================] - 11s 339ms/step - loss: 7.9582 - acc: 0.0625\n    Epoch 9/50000\n    32/32 [==============================] - 12s 378ms/step - loss: 7.8895 - acc: 0.0625\n    Epoch 10/50000\n    32/32 [==============================] - 10s 327ms/step - loss: 8.8176 - acc: 0.0312\n    Epoch 11/50000\n    32/32 [==============================] - 10s 322ms/step - loss: 7.6722 - acc: 0.0938\n    \n    ==================第10轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 吴娥声绝天，\n    吴娥声绝天，翰风尊竹。。。。新。。，。，。。。。\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 往年吟月社，\n    往年吟月社，诰蓁燥露避。声菊花腐坏，月桢路柢隍。\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 五陵射雕客，\n    五陵射雕客，崩松。诣逦，蟊慧，田汉梅硗冷植山樛古\n    Epoch 12/50000\n    32/32 [==============================] - 10s 322ms/step - loss: 7.0762 - acc: 0.1562\n    Epoch 13/50000\n    32/32 [==============================] - 11s 356ms/step - loss: 7.5219 - acc: 0.0938\n    Epoch 14/50000\n    32/32 [==============================] - 12s 365ms/step - loss: 8.0725 - acc: 0.0625\n    Epoch 15/50000\n    32/32 [==============================] - 11s 332ms/step - loss: 7.3090 - acc: 0.1875\n    Epoch 16/50000\n    32/32 [==============================] - 11s 344ms/step - loss: 7.7346 - acc: 0.1562\n    \n    ==================第15轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 开国维东井，\n    开国维东井，风花风酋飞。澎捣梅壶飞，掬肢羝沼风。\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 丽句传人口，\n    丽句传人口，琨避碧起隐。林栴日箭睫，摹榆混跹擐，\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 吾师晋阳宝，\n    吾师晋阳宝，岸噞府奄庵。釜铛綍颾绡，林榈臂绿睛。\n    Epoch 17/50000\n    32/32 [==============================] - 12s 372ms/step - loss: 7.6869 - acc: 0.1250\n    Epoch 18/50000\n    32/32 [==============================] - 13s 396ms/step - loss: 7.8961 - acc: 0.0625\n    Epoch 19/50000\n    32/32 [==============================] - 11s 331ms/step - loss: 7.5612 - acc: 0.1250\n    Epoch 20/50000\n    32/32 [==============================] - 11s 336ms/step - loss: 8.4879 - acc: 0.0312\n    Epoch 21/50000\n    32/32 [==============================] - 11s 347ms/step - loss: 7.9418 - acc: 0.0000e+00\n    \n    ==================第20轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 谁家洛浦神，\n    谁家洛浦神，，烟花初。初，月日风日，日。。，花云\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 秋馆烟雨合，\n    秋馆烟雨合，。涤牒沼。，。棹林云，，。。日阴。日\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 清香闻晓莲，\n    清香闻晓莲，艺罥飕缟，痗。匄断影。霭涴，薪我，麦\n    Epoch 22/50000\n    32/32 [==============================] - 11s 343ms/step - loss: 8.3587 - acc: 0.1250\n    Epoch 23/50000\n    32/32 [==============================] - 11s 342ms/step - loss: 8.8217 - acc: 0.0625\n    Epoch 24/50000\n    32/32 [==============================] - 11s 333ms/step - loss: 8.5927 - acc: 0.0938\n    Epoch 25/50000\n    32/32 [==============================] - 11s 336ms/step - loss: 9.2647 - acc: 0.0312\n    Epoch 26/50000\n    32/32 [==============================] - 11s 330ms/step - loss: 9.0967 - acc: 0.0625\n    \n    ==================第25轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 涪右众山内，\n    涪右众山内，，，。。，，。。，，，，，，，。。，\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 忆昨征还日，\n    忆昨征还日，獒，。，，初，。。。，魔病离，，，，\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 白须芸阁吏，\n    白须芸阁吏，刘芙咫起去弧保览翠起仿毵。棹醴晴雳赏\n    Epoch 27/50000\n    32/32 [==============================] - 11s 331ms/step - loss: 9.0581 - acc: 0.0625\n    Epoch 28/50000\n    32/32 [==============================] - 11s 346ms/step - loss: 9.3762 - acc: 0.0938\n    Epoch 29/50000\n    32/32 [==============================] - 11s 334ms/step - loss: 8.7210 - acc: 0.0312\n    Epoch 30/50000\n    32/32 [==============================] - 11s 339ms/step - loss: 9.3932 - acc: 0.1250\n    Epoch 31/50000\n    32/32 [==============================] - 11s 358ms/step - loss: 9.9746 - acc: 0.0625\n    \n    ==================第30轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 云戟曙沈沈，\n    云戟曙沈沈，，，菊，，。。风阴。。，，。，，，。\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 秋尽初移幕，\n    秋尽初移幕，，功赏。雪。。乐房，。头，月烟。。稍\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 川上风雨来，\n    川上风雨来，巫巩。绮阴许饰风，。竹戊，妤语，弦形\n    Epoch 32/50000\n    32/32 [==============================] - 11s 353ms/step - loss: 9.2413 - acc: 0.0625\n    Epoch 33/50000\n    32/32 [==============================] - 11s 334ms/step - loss: 9.2510 - acc: 0.1250\n    Epoch 34/50000\n    32/32 [==============================] - 11s 349ms/step - loss: 9.0946 - acc: 0.1250\n    Epoch 35/50000\n    32/32 [==============================] - 11s 344ms/step - loss: 8.2386 - acc: 0.1250\n    Epoch 36/50000\n    32/32 [==============================] - 11s 330ms/step - loss: 7.6258 - acc: 0.1250\n    \n    ==================第35轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 好雪动高情，\n    好雪动高情，灶停前流封。何幸巧金。驻悠开兴鞚万争\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 杂英纷已积，\n    杂英纷已积，挥望凌珉配。说鸟珂审估涕峡屋可，雾龄\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 枳棘君尚栖，\n    枳棘君尚栖，粜濮巢举冈。裳蓁峒锽侃寄歈鸟缦徇菊幌\n    Epoch 37/50000\n    32/32 [==============================] - 11s 329ms/step - loss: 8.7009 - acc: 0.0625\n    Epoch 38/50000\n    32/32 [==============================] - 11s 349ms/step - loss: 8.1561 - acc: 0.1562\n    Epoch 39/50000\n    32/32 [==============================] - 11s 341ms/step - loss: 7.0176 - acc: 0.1875\n    Epoch 40/50000\n    32/32 [==============================] - 12s 361ms/step - loss: 7.9349 - acc: 0.1562\n    Epoch 41/50000\n    32/32 [==============================] - 11s 352ms/step - loss: 7.9580 - acc: 0.1562\n    \n    ==================第40轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 假邑非拙素，\n    假邑非拙素，，空阴百石。。日，接岩，，飞。阴池。\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 凌雾朝阊阖，\n    凌雾朝阊阖，松不的飘雁。摹轻篡酊出，雾突空新冷。\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 晓起磬房前，\n    晓起磬房前，赢醁音烟梅。色蛄虺空影，谁鸿飞务雾。\n    Epoch 42/50000\n    32/32 [==============================] - 11s 359ms/step - loss: 7.3957 - acc: 0.1875\n    Epoch 43/50000\n    32/32 [==============================] - 11s 343ms/step - loss: 8.3570 - acc: 0.1562\n    Epoch 44/50000\n    32/32 [==============================] - 12s 360ms/step - loss: 7.8394 - acc: 0.1562\n    Epoch 45/50000\n    32/32 [==============================] - 11s 336ms/step - loss: 7.9412 - acc: 0.1875\n    Epoch 46/50000\n    32/32 [==============================] - 11s 346ms/step - loss: 8.1780 - acc: 0.1562\n    \n    ==================第45轮=====================\n    ------------设定诗词创作自由度约束参数为0.7--------------\n    第一行为: 黄鹤悲歌绝，\n    黄鹤悲歌绝，，赡弥八。。。当雙荒，，，歌柳成。。\n    ------------设定诗词创作自由度约束参数为1.0--------------\n    第一行为: 惜君滞南楚，\n    惜君滞南楚，襁黼风岫思。雁鼯莩参胚，埙砚万孰忆。\n    ------------设定诗词创作自由度约束参数为1.3--------------\n    第一行为: 迢递双崤道，\n    迢递双崤道，砀，台烈檐。槌。憾无郁，羽，滕阆诚。\n    Epoch 47/50000\n    32/32 [==============================] - 11s 342ms/step - loss: 7.3162 - acc: 0.1562\n    Epoch 48/50000\n    12/32 [==========>...................] - ETA: 7s - loss: 7.5969 - acc: 0.1667\n\n\n```python\nfor i in range(3):\n    #藏头诗\n    sen = model.predict_hide('争云日夏')\n    print(sen)\n```\n\n    第一行为:  翁夜往还。争\n    争音常开台，云来清子恩。日天扉青家，夏作浮音为。\n    第一行为:  啄江海隅。争\n    争空谁上尽，云云中林翠。日落危西烟，夏更无长塞。\n    第一行为:  珠坠还结。争\n    争独望云落，云华北山山。日远仙入还，夏红游长无。\n    \n\n\n```python\nfor i in range(3):\n    #给出第一句话进行预测\n    sen = model.predict_sen('山为斜好几，')\n    print(sen)\n```\n\n    第一行为: 山为斜好几，\n    山为斜好几，风外风玉正。东云水赏叶，先松句断采。\n    第一行为: 山为斜好几，\n    山为斜好几，隐公帝碧自。开夜知孤满，下且露落鸟。\n    第一行为: 山为斜好几，\n    山为斜好几，六池如中田。阙露奇雪前，然十盛空不。\n    \n\n\n```python\nfor i in range(3):\n    #给出第一个字进行预测\n    sen = model.predict_first('山')\n    print(sen)\n```\n\n    山家光出观，隐黄戎识移。愿传兰重弦，飞方来凤为。\n    山迹几星道，寒行极幽直。方朝蝉家复，人经识子木。\n    山溪二屡正，归飞情尽宅。山未子华帝，花云新酒三。\n    \n\n\n```python\nfor temp in [0.5,1,1.5]:\n    #随机抽取第一句话进行预测\n    sen = model.predict_random(temperature=temp)\n    print(sen)\n```\n\n    第一行为: 十载别仙峰，\n    十载别仙峰，不春幽思入。山不春兰知，光三落台平。\n    第一行为: 已沐识坚贞，\n    已沐识坚贞，薄欢月坐终。旗国去向仙，采成赠金露。\n    第一行为: 水尔何如此，\n    水尔何如此，良不枝愿宁。中鹤四刺疑，境暮衣可独。\n    \n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter2_Chapter1_Text_Classification_based_on_Deep_Learning/3.Emotion_analysis_model_of_Movie_review_based_on_LSTM/","content":"\n## 情感分析背景介绍\n\n\n在自然语言处理中，情感分析一般是指判断一段文本所表达的情绪状态。其中，一段文本可以是一个句子，一个段落或一个文档。情绪状态可以是两类，如（正面，负面），（高兴，悲伤）；也可以是三类，如（积极，消极，中性）等等。情感分析的应用场景十分广泛，如把用户在购物网站（亚马逊、天猫、淘宝等）、旅游网站、电影评论网站上发表的评论分成正面评论和负面评论；或为了分析用户对于某一产品的整体使用感受，抓取产品的用户评论并进行情感分析等等。表格1展示了对电影评论进行情感分析的例子：\n\n| 电影评论       | 类别  |\n| --------     | -----  |\n| 在冯小刚这几年的电影里，算最好的一部的了| 正面 |\n| 很不好看，好像一个地方台的电视剧     | 负面 |\n| 圆方镜头全程炫技，色调背景美则美矣，但剧情拖沓，口音不伦不类，一直努力却始终无法入戏| 负面|\n|剧情四星。但是圆镜视角加上婺源的风景整个非常有中国写意山水画的感觉，看得实在太舒服了。。|正面|\n\n\n<p align=\"center\"> 表格 1 电影评论情感分析 </p>\n\n在自然语言处理中，情感分析属于典型的**文本分类**问题，即把需要进行情感分析的文本划分为其所属类别。文本分类涉及文本表示和分类方法两个问题。在深度学习的方法出现之前，主流的文本表示方法为词袋模型BOW(bag of words)，话题模型等等；分类方法有SVM(support vector machine), LR(logistic regression)等等。  \n\n对于一段文本，BOW表示会忽略其词顺序、语法和句法，将这段文本仅仅看做是一个词集合，因此BOW方法并不能充分表示文本的语义信息。例如，句子“这部电影糟糕透了”和“一个乏味，空洞，没有内涵的作品”在情感分析中具有很高的语义相似度，但是它们的BOW表示的相似度为0。又如，句子“一个空洞，没有内涵的作品”和“一个不空洞而且有内涵的作品”的BOW相似度很高，但实际上它们的意思很不一样。  \n\n\n```python\n# 数据预处理\nimport jieba\nimport pandas as pd\ndf_trainData = pd.read_csv(\"./data/labeledTrainData.tsv\", sep='\\t', encoding='utf-8')\ndf_trainData = df_trainData.dropna()\ntrainData = df_trainData.review.values.tolist()[1000:21000]\n```\n\n\n```python\n# 停用词\nstopwords=pd.read_csv(\"./data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\nstopwords=stopwords['stopword'].values\n```\n\n\n```python\n# 构建数据集\ndef preprocess_text(content_lines, sentences, category):\n    for line in content_lines:\n        try:\n            segs=jieba.lcut(line)\n            segs = filter(lambda x:len(x)>1, segs)\n            segs = filter(lambda x:x not in stopwords, segs)\n            sentences.append((\" \".join(segs), category))\n        except Exception as e:\n            print(line)\n            continue \n\n#生成训练数据\nsentences = []\n\npreprocess_text(trainData, sentences, 'trainData')\n```\n\n    Building prefix dict from the default dictionary ...\n    Loading model from cache C:\\Users\\vip\\AppData\\Local\\Temp\\jieba.cache\n    Loading model cost 0.673 seconds.\n    Prefix dict has been built succesfully.\n    \n\n\n```python\nfrom sklearn.model_selection import train_test_split\nx, y = zip(*sentences)\ntrain_data, test_data, train_target, test_target = train_test_split(x, y, random_state=1234)\n```\n\n\n```python\n\"\"\"\n基于卷积神经网络的中文文本分类\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport pandas\nfrom sklearn import metrics\nimport tensorflow as tf\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.3\nsession = tf.Session(config=config)\n\nlearn = tf.contrib.learn\n\nFLAGS = None\n\n#文档最长长度\nMAX_DOCUMENT_LENGTH = 100\n#最小词频数\nMIN_WORD_FREQUENCE = 2\n#词嵌入的维度\nEMBEDDING_SIZE = 20\n#filter个数\nN_FILTERS = 10\n#感知野大小\nWINDOW_SIZE = 20\n#filter的形状\nFILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]\nFILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS]\n#池化\nPOOLING_WINDOW = 4\nPOOLING_STRIDE = 2\nn_words = 0\n\n\ndef cnn_model(features, target):\n    \"\"\"\n    2层的卷积神经网络，用于短文本分类\n    \"\"\"\n    # 先把词转成词嵌入\n    # 我们得到一个形状为[n_words, EMBEDDING_SIZE]的词表映射矩阵\n    # 接着我们可以把一批文本映射成[batch_size, sequence_length, EMBEDDING_SIZE]的矩阵形式\n    target = tf.one_hot(target, 15, 1, 0)\n    word_vectors = tf.contrib.layers.embed_sequence(\n            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n    word_vectors = tf.expand_dims(word_vectors, 3)\n    with tf.variable_scope('CNN_Layer1'):\n        # 添加卷积层做滤波\n        conv1 = tf.contrib.layers.convolution2d(\n                word_vectors, N_FILTERS, FILTER_SHAPE1, padding='VALID')\n        # 添加RELU非线性\n        conv1 = tf.nn.relu(conv1)\n        # 最大池化\n        pool1 = tf.nn.max_pool(\n                conv1,\n                ksize=[1, POOLING_WINDOW, 1, 1],\n                strides=[1, POOLING_STRIDE, 1, 1],\n                padding='SAME')\n        # 对矩阵进行转置，以满足形状\n        pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n    with tf.variable_scope('CNN_Layer2'):\n        # 第2个卷积层\n        conv2 = tf.contrib.layers.convolution2d(\n                pool1, N_FILTERS, FILTER_SHAPE2, padding='VALID')\n        # 抽取特征\n        pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n\n    # 全连接层\n    logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None)\n    loss = tf.losses.softmax_cross_entropy(target, logits)\n\n    train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.contrib.framework.get_global_step(),\n            optimizer='Adam',\n            learning_rate=0.01)\n\n    return ({\n            'class': tf.argmax(logits, 1),\n            'prob': tf.nn.softmax(logits)\n    }, loss, train_op)\n```\n\n    D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n      from ._conv import register_converters as _register_converters\n    \n\n\n```python\n\"\"\"\n来看看tensorflow.preprocessing里的learn包含的VocabularyProcesser\n\"\"\"\ntmp = ['I am good', 'you are here', 'I am glad', 'it is great']\nvocab_processor = learn.preprocessing.VocabularyProcessor(10)\nlist(vocab_processor.fit_transform(tmp))\n```\n\n    WARNING:tensorflow:From <ipython-input-6-43c5bc39889f>:5: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    \n\n\n\n\n    [array([1, 2, 3, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n     array([4, 5, 6, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n     array([1, 2, 7, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n     array([ 8,  9, 10,  0,  0,  0,  0,  0,  0,  0], dtype=int64)]\n\n\n\n\n```python\nglobal n_words\n# 处理词汇\nvocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\nx_train = np.array(list(vocab_processor.fit_transform(train_data)))\nx_test = np.array(list(vocab_processor.transform(test_data)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n```\n\n    Total words: 34733\n    \n\n\n```python\ntrain_target = df_trainData.sentiment.values.tolist()[1000:16000]\ntest_target = df_trainData.sentiment.values.tolist()[16000:21000]\ny_train = pandas.Series(train_target)\ny_test = pandas.Series(test_target)\n```\n\n\n```python\n# 构建模型\nclassifier = learn.SKCompat(learn.Estimator(model_fn=cnn_model))\n\n# 训练和预测\nclassifier.fit(x_train, y_train, steps=1000)\ny_predicted = classifier.predict(x_test)['class']\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint('Accuracy: {0:f}'.format(score))\n```\n\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py:1180: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py:428: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n    INFO:tensorflow:Using default config.\n    WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1u86hp4c\n    INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023BEC65D898>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n      per_process_gpu_memory_fraction: 1.0\n    }\n    , '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\vip\\\\AppData\\\\Local\\\\Temp\\\\tmp1u86hp4c'}\n    WARNING:tensorflow:From <ipython-input-9-49860c40bfe1>:2: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please switch to the Estimator interface.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\data_feeder.py:96: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please feed input to tf.data to support dask.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\data_feeder.py:98: extract_dask_labels (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please feed input to tf.data to support dask.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please access pandas data directly.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please access pandas data directly.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please convert numpy dtypes explicitly.\n    WARNING:tensorflow:From <ipython-input-5-c2c13d88f68c>:70: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Use the `axis` argument instead\n    WARNING:tensorflow:From <ipython-input-5-c2c13d88f68c>:78: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please switch to tf.train.get_global_step\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py:1241: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n    INFO:tensorflow:Create CheckpointSaverHook.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1u86hp4c\\model.ckpt.\n    INFO:tensorflow:loss = 2.7084143, step = 1\n    INFO:tensorflow:global_step/sec: 21.9116\n    INFO:tensorflow:loss = 0.69469905, step = 101 (4.569 sec)\n    INFO:tensorflow:global_step/sec: 23.1404\n    INFO:tensorflow:loss = 0.7188001, step = 201 (4.317 sec)\n    INFO:tensorflow:global_step/sec: 22.3611\n    INFO:tensorflow:loss = 0.658976, step = 301 (4.474 sec)\n    INFO:tensorflow:global_step/sec: 23.1993\n    INFO:tensorflow:loss = 0.3258043, step = 401 (4.310 sec)\n    INFO:tensorflow:global_step/sec: 22.3462\n    INFO:tensorflow:loss = 0.16393386, step = 501 (4.476 sec)\n    INFO:tensorflow:global_step/sec: 21.9355\n    INFO:tensorflow:loss = 0.06501806, step = 601 (4.557 sec)\n    INFO:tensorflow:global_step/sec: 22.4663\n    INFO:tensorflow:loss = 0.029251155, step = 701 (4.451 sec)\n    INFO:tensorflow:global_step/sec: 21.8447\n    INFO:tensorflow:loss = 0.0904543, step = 801 (4.578 sec)\n    INFO:tensorflow:global_step/sec: 23.1725\n    INFO:tensorflow:loss = 0.159938, step = 901 (4.314 sec)\n    INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1u86hp4c\\model.ckpt.\n    INFO:tensorflow:Loss for final step: 0.001890616.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Restoring parameters from C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1u86hp4c\\model.ckpt-1000\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    Accuracy: 0.507400\n    \n\n\n```python\n\"\"\"\n使用RNN完成文本分类\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport pandas\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import encoders\n\nlearn = tf.contrib.learn\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.3\nsession = tf.Session(config=config)\n\nFLAGS = None\n```\n\n\n```python\n# 词袋模型\n\nMAX_DOCUMENT_LENGTH = 15\nMIN_WORD_FREQUENCE = 1\nEMBEDDING_SIZE = 50\nglobal n_words\n# 处理词汇\nvocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\nx_train = np.array(list(vocab_processor.fit_transform(train_data)))\nx_test = np.array(list(vocab_processor.transform(test_data)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n\ndef bag_of_words_model(features, target):\n    \"\"\"先转成词袋模型\"\"\"\n    target = tf.one_hot(target, 15, 1, 0)\n    features = encoders.bow_encoder(\n            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n    logits = tf.contrib.layers.fully_connected(features, 15, activation_fn=None)\n    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n    train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.contrib.framework.get_global_step(),\n            optimizer='Adam',\n            learning_rate=0.01)\n    return ({\n            'class': tf.argmax(logits, 1),\n            'prob': tf.nn.softmax(logits)\n    }, loss, train_op)\n\n\nmodel_fn = bag_of_words_model\nclassifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n\n# Train and predict\nclassifier.fit(x_train, y_train, steps=1000)\ny_predicted = classifier.predict(x_test)['class']\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint('Accuracy: {0:f}'.format(score))\n```\n\n    Total words: 44791\n    INFO:tensorflow:Using default config.\n    WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\vip\\AppData\\Local\\Temp\\tmp_6oox54k\n    INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023BECB07D68>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n      per_process_gpu_memory_fraction: 1.0\n    }\n    , '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\vip\\\\AppData\\\\Local\\\\Temp\\\\tmp_6oox54k'}\n    WARNING:tensorflow:From <ipython-input-11-a6718fa18db5>:20: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n    Instructions for updating:\n    Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    \n    Future major versions of TensorFlow will allow gradients to flow\n    into the labels input on backprop by default.\n    \n    See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n    \n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:399: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n    Instructions for updating:\n    Use tf.losses.compute_weighted_loss instead.\n    WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:147: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n    Instructions for updating:\n    Use tf.losses.add_loss instead.\n    INFO:tensorflow:Create CheckpointSaverHook.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp_6oox54k\\model.ckpt.\n    INFO:tensorflow:loss = 2.7080827, step = 1\n    INFO:tensorflow:global_step/sec: 37.9513\n    INFO:tensorflow:loss = 0.70537966, step = 101 (2.636 sec)\n    INFO:tensorflow:global_step/sec: 40.3815\n    INFO:tensorflow:loss = 0.61258155, step = 201 (2.476 sec)\n    INFO:tensorflow:global_step/sec: 40.7094\n    INFO:tensorflow:loss = 0.44011787, step = 301 (2.456 sec)\n    INFO:tensorflow:global_step/sec: 40.8254\n    INFO:tensorflow:loss = 0.26835862, step = 401 (2.449 sec)\n    INFO:tensorflow:global_step/sec: 40.7756\n    INFO:tensorflow:loss = 0.19353409, step = 501 (2.451 sec)\n    INFO:tensorflow:global_step/sec: 41.3644\n    INFO:tensorflow:loss = 0.11005337, step = 601 (2.420 sec)\n    INFO:tensorflow:global_step/sec: 40.8088\n    INFO:tensorflow:loss = 0.21476589, step = 701 (2.450 sec)\n    INFO:tensorflow:global_step/sec: 38.4608\n    INFO:tensorflow:loss = 0.13205583, step = 801 (2.599 sec)\n    INFO:tensorflow:global_step/sec: 38.7132\n    INFO:tensorflow:loss = 0.16063747, step = 901 (2.586 sec)\n    INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp_6oox54k\\model.ckpt.\n    INFO:tensorflow:Loss for final step: 0.10237734.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Restoring parameters from C:\\Users\\vip\\AppData\\Local\\Temp\\tmp_6oox54k\\model.ckpt-1000\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    Accuracy: 0.509800\n    \n\n\n```python\n# GRU分类器\ndef rnn_model(features, target):\n    \"\"\"用RNN模型(这里用的是GRU)完成文本分类\"\"\"\n    # Convert indexes of words into embeddings.\n    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n    # maps word indexes of the sequence into [batch_size, sequence_length,\n    # EMBEDDING_SIZE].\n    word_vectors = tf.contrib.layers.embed_sequence(\n            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n\n    # Split into list of embedding per word, while removing doc length dim.\n    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n    word_list = tf.unstack(word_vectors, axis=1)\n\n    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n\n    # Create an unrolled Recurrent Neural Networks to length of\n    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n\n    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n    # neural network of last step) and pass it as features for logistic\n    # regression over output classes.\n    target = tf.one_hot(target, 15, 1, 0)\n    logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n\n    # Create a training op.\n    train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.contrib.framework.get_global_step(),\n            optimizer='Adam',\n            learning_rate=0.01)\n\n    return ({\n            'class': tf.argmax(logits, 1),\n            'prob': tf.nn.softmax(logits)\n    }, loss, train_op)\n```\n\n\n```python\nmodel_fn = rnn_model\nclassifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n\n# Train and predict\nclassifier.fit(x_train, y_train, steps=1000)\ny_predicted = classifier.predict(x_test)['class']\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint('Accuracy: {0:f}'.format(score))\n```\n\n    INFO:tensorflow:Using default config.\n    WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\vip\\AppData\\Local\\Temp\\tmp4cvz_d9s\n    INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023BEC82D748>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n      per_process_gpu_memory_fraction: 1.0\n    }\n    , '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\vip\\\\AppData\\\\Local\\\\Temp\\\\tmp4cvz_d9s'}\n    INFO:tensorflow:Create CheckpointSaverHook.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp4cvz_d9s\\model.ckpt.\n    INFO:tensorflow:loss = 2.7077389, step = 1\n    INFO:tensorflow:global_step/sec: 27.546\n    INFO:tensorflow:loss = 0.69382405, step = 101 (3.631 sec)\n    INFO:tensorflow:global_step/sec: 34.563\n    INFO:tensorflow:loss = 0.55330396, step = 201 (2.894 sec)\n    INFO:tensorflow:global_step/sec: 36.1324\n    INFO:tensorflow:loss = 0.3219909, step = 301 (2.767 sec)\n    INFO:tensorflow:global_step/sec: 36.3419\n    INFO:tensorflow:loss = 0.09328692, step = 401 (2.753 sec)\n    INFO:tensorflow:global_step/sec: 34.9242\n    INFO:tensorflow:loss = 0.054063544, step = 501 (2.862 sec)\n    INFO:tensorflow:global_step/sec: 34.8271\n    INFO:tensorflow:loss = 0.010423068, step = 601 (2.871 sec)\n    INFO:tensorflow:global_step/sec: 36.0674\n    INFO:tensorflow:loss = 0.026143463, step = 701 (2.774 sec)\n    INFO:tensorflow:global_step/sec: 35.9897\n    INFO:tensorflow:loss = 0.02321876, step = 801 (2.779 sec)\n    INFO:tensorflow:global_step/sec: 36.8088\n    INFO:tensorflow:loss = 0.066823944, step = 901 (2.716 sec)\n    INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp4cvz_d9s\\model.ckpt.\n    INFO:tensorflow:Loss for final step: 0.0048412527.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Restoring parameters from C:\\Users\\vip\\AppData\\Local\\Temp\\tmp4cvz_d9s\\model.ckpt-1000\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    Accuracy: 0.509400\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter2_Chapter1_Text_Classification_based_on_Deep_Learning/2.News_Classification_based_on_CNN/","content":"\n# 使用TensorFlow构建卷积神经网络完成新闻分类\n\n\n\n```python\n# 数据预处理\nimport jieba\nimport pandas as pd\ndf_cnews =pd.read_csv(\"./data/cnews.train.txt\",sep=\"\\t\",names=['category','cnews'], encoding='utf-8')\ndf_cnews = df_cnews.dropna()\n\nty = df_cnews[df_cnews.category=='体育'].cnews.values.tolist()\njj = df_cnews[df_cnews.category=='家居'].cnews.values.tolist()\nss = df_cnews[df_cnews.category=='时尚'].cnews.values.tolist()\nfc = df_cnews[df_cnews.category=='房产'].cnews.values.tolist()\njy = df_cnews[df_cnews.category=='教育'].cnews.values.tolist()\nsz = df_cnews[df_cnews.category=='时政'].cnews.values.tolist()\nyl = df_cnews[df_cnews.category=='娱乐'].cnews.values.tolist()\nyx = df_cnews[df_cnews.category=='游戏'].cnews.values.tolist()\n```\n\n\n```python\n# 停用词\nstopwords=pd.read_csv(\"./data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\nstopwords=stopwords['stopword'].values\n```\n\n\n```python\n# 构建数据集\ndef preprocess_text(content_lines, sentences, category):\n    for line in content_lines:\n        try:\n            segs=jieba.lcut(line)\n            segs = filter(lambda x:len(x)>1, segs)\n            segs = filter(lambda x:x not in stopwords, segs)\n            sentences.append((\" \".join(segs), category))\n        except Exception as e:\n            print(line)\n            continue \n\n#生成训练数据\nsentences = []\n\npreprocess_text(ty, sentences, 'ty')\npreprocess_text(jj, sentences, 'jj')\npreprocess_text(ss, sentences, 'ss')\npreprocess_text(fc, sentences, 'fc')\npreprocess_text(jy, sentences, 'jy')\npreprocess_text(sz, sentences, 'sz')\npreprocess_text(yl, sentences, 'yl')\npreprocess_text(yx, sentences, 'yx')\n\n```\n\n    Building prefix dict from the default dictionary ...\n    Dumping model to file cache /tmp/jieba.cache\n    Loading model cost 2.004 seconds.\n    Prefix dict has been built succesfully.\n    \n\n\n```python\nfrom sklearn.model_selection import train_test_split\nx, y = zip(*sentences)\ntrain_data, test_data, train_target, test_target = train_test_split(x, y, random_state=1234)\n```\n\n\n```python\n\"\"\"\n基于卷积神经网络的中文文本分类\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport pandas\nfrom sklearn import metrics\nimport tensorflow as tf\n\nlearn = tf.contrib.learn\n\nFLAGS = None\n\n#文档最长长度\nMAX_DOCUMENT_LENGTH = 100\n#最小词频数\nMIN_WORD_FREQUENCE = 2\n#词嵌入的维度\nEMBEDDING_SIZE = 20\n#filter个数\nN_FILTERS = 10\n#感知野大小\nWINDOW_SIZE = 20\n#filter的形状\nFILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]\nFILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS]\n#池化\nPOOLING_WINDOW = 4\nPOOLING_STRIDE = 2\nn_words = 0\n\n\ndef cnn_model(features, target):\n    \"\"\"\n    2层的卷积神经网络，用于短文本分类\n    \"\"\"\n    # 先把词转成词嵌入\n    # 我们得到一个形状为[n_words, EMBEDDING_SIZE]的词表映射矩阵\n    # 接着我们可以把一批文本映射成[batch_size, sequence_length, EMBEDDING_SIZE]的矩阵形式\n    target = tf.one_hot(target, 15, 1, 0)\n    word_vectors = tf.contrib.layers.embed_sequence(\n            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n    word_vectors = tf.expand_dims(word_vectors, 3)\n    with tf.variable_scope('CNN_Layer1'):\n        # 添加卷积层做滤波\n        conv1 = tf.contrib.layers.convolution2d(\n                word_vectors, N_FILTERS, FILTER_SHAPE1, padding='VALID')\n        # 添加RELU非线性\n        conv1 = tf.nn.relu(conv1)\n        # 最大池化\n        pool1 = tf.nn.max_pool(\n                conv1,\n                ksize=[1, POOLING_WINDOW, 1, 1],\n                strides=[1, POOLING_STRIDE, 1, 1],\n                padding='SAME')\n        # 对矩阵进行转置，以满足形状\n        pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n    with tf.variable_scope('CNN_Layer2'):\n        # 第2个卷积层\n        conv2 = tf.contrib.layers.convolution2d(\n                pool1, N_FILTERS, FILTER_SHAPE2, padding='VALID')\n        # 抽取特征\n        pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n\n    # 全连接层\n    logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None)\n    loss = tf.losses.softmax_cross_entropy(target, logits)\n\n    train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.contrib.framework.get_global_step(),\n            optimizer='Adam',\n            learning_rate=0.01)\n\n    return ({\n            'class': tf.argmax(logits, 1),\n            'prob': tf.nn.softmax(logits)\n    }, loss, train_op)\n```\n\n\n```python\nglobal n_words\n# 处理词汇\nvocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\nx_train = np.array(list(vocab_processor.fit_transform(train_data)))\nx_test = np.array(list(vocab_processor.transform(test_data)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n```\n\n    WARNING:tensorflow:From <ipython-input-6-101cbea574b0>:3: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    Total words: 48058\n    \n\n\n```python\ncate_dic = {'ty':1, 'jj':2, 'ss':3, 'fc':4, 'jy':5,'sz':6,'yl':7,'yx':8}\ntrain_target = map(lambda x:cate_dic[x], train_target)\ntest_target = map(lambda x:cate_dic[x], test_target)\ny_train = pandas.Series(train_target)\ny_test = pandas.Series(test_target)\n```\n\n\n```python\n# 构建模型\nclassifier = learn.SKCompat(learn.Estimator(model_fn=cnn_model))\n\n# 训练和预测\nclassifier.fit(x_train, y_train, steps=1000)\ny_predicted = classifier.predict(x_test)['class']\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint('Accuracy: {0:f}'.format(score))\n```\n\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1180: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:428: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n    INFO:tensorflow:Using default config.\n    WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp568rmfet\n    INFO:tensorflow:Using config: {'_train_distribute': None, '_is_chief': True, '_log_step_count_steps': 100, '_device_fn': None, '_save_summary_steps': 100, '_protocol': None, '_environment': 'local', '_task_id': 0, '_save_checkpoints_steps': None, '_task_type': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_session_config': None, '_model_dir': '/tmp/tmp568rmfet', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f469eaa6278>, '_tf_config': gpu_options {\n      per_process_gpu_memory_fraction: 1.0\n    }\n    , '_evaluation_master': '', '_num_worker_replicas': 0, '_eval_distribute': None, '_master': '', '_tf_random_seed': None, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000}\n    WARNING:tensorflow:From <ipython-input-8-49860c40bfe1>:2: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please switch to the Estimator interface.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:143: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please access pandas data directly.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please access pandas data directly.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please use tensorflow/transform or tf.data.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please convert numpy dtypes explicitly.\n    WARNING:tensorflow:From <ipython-input-5-c2c13d88f68c>:70: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Use the `axis` argument instead\n    WARNING:tensorflow:From <ipython-input-5-c2c13d88f68c>:78: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Please switch to tf.train.get_global_step\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1241: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n    INFO:tensorflow:Create CheckpointSaverHook.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp568rmfet/model.ckpt.\n    INFO:tensorflow:loss = 2.7079399, step = 0\n    INFO:tensorflow:global_step/sec: 98.5193\n    INFO:tensorflow:loss = 0.050897215, step = 100 (1.020 sec)\n    INFO:tensorflow:global_step/sec: 119.128\n    INFO:tensorflow:loss = 0.0014143699, step = 200 (0.837 sec)\n    INFO:tensorflow:global_step/sec: 108.514\n    INFO:tensorflow:loss = 0.000132761, step = 300 (0.924 sec)\n    INFO:tensorflow:global_step/sec: 117.55\n    INFO:tensorflow:loss = 0.0002916466, step = 400 (0.848 sec)\n    INFO:tensorflow:global_step/sec: 119.855\n    INFO:tensorflow:loss = 3.9822407e-05, step = 500 (0.834 sec)\n    INFO:tensorflow:global_step/sec: 119.059\n    INFO:tensorflow:loss = 3.786313e-05, step = 600 (0.840 sec)\n    INFO:tensorflow:global_step/sec: 110.227\n    INFO:tensorflow:loss = 0.0004890824, step = 700 (0.909 sec)\n    INFO:tensorflow:global_step/sec: 115.1\n    INFO:tensorflow:loss = 3.7943282e-05, step = 800 (0.868 sec)\n    INFO:tensorflow:global_step/sec: 117.432\n    INFO:tensorflow:loss = 1.45628455e-05, step = 900 (0.850 sec)\n    INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp568rmfet/model.ckpt.\n    INFO:tensorflow:Loss for final step: 1.8430579e-05.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Restoring parameters from /tmp/tmp568rmfet/model.ckpt-1000\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    Accuracy: 0.933500\n    \n\n\n```python\n\"\"\"\n使用RNN完成文本分类\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport pandas\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import encoders\n\nlearn = tf.contrib.learn\n\nFLAGS = None\n```\n\n\n```python\n# 词袋模型\n\nMAX_DOCUMENT_LENGTH = 15\nMIN_WORD_FREQUENCE = 1\nEMBEDDING_SIZE = 50\nglobal n_words\n# 处理词汇\nvocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\nx_train = np.array(list(vocab_processor.fit_transform(train_data)))\nx_test = np.array(list(vocab_processor.transform(test_data)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n\ndef bag_of_words_model(features, target):\n    \"\"\"先转成词袋模型\"\"\"\n    target = tf.one_hot(target, 15, 1, 0)\n    features = encoders.bow_encoder(\n            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n    logits = tf.contrib.layers.fully_connected(features, 15, activation_fn=None)\n    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n    train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.contrib.framework.get_global_step(),\n            optimizer='Adam',\n            learning_rate=0.01)\n    return ({\n            'class': tf.argmax(logits, 1),\n            'prob': tf.nn.softmax(logits)\n    }, loss, train_op)\n\n\nmodel_fn = bag_of_words_model\nclassifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n\n# Train and predict\nclassifier.fit(x_train, y_train, steps=1000)\ny_predicted = classifier.predict(x_test)['class']\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint('Accuracy: {0:f}'.format(score))\n```\n\n    Total words: 65350\n    INFO:tensorflow:Using default config.\n    WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_xsdnfqs\n    INFO:tensorflow:Using config: {'_train_distribute': None, '_is_chief': True, '_log_step_count_steps': 100, '_device_fn': None, '_save_summary_steps': 100, '_protocol': None, '_environment': 'local', '_task_id': 0, '_save_checkpoints_steps': None, '_task_type': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_session_config': None, '_model_dir': '/tmp/tmp_xsdnfqs', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f469e6b8cf8>, '_tf_config': gpu_options {\n      per_process_gpu_memory_fraction: 1.0\n    }\n    , '_evaluation_master': '', '_num_worker_replicas': 0, '_eval_distribute': None, '_master': '', '_tf_random_seed': None, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000}\n    WARNING:tensorflow:From <ipython-input-10-a6718fa18db5>:20: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n    Instructions for updating:\n    Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    \n    Future major versions of TensorFlow will allow gradients to flow\n    into the labels input on backprop by default.\n    \n    See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n    \n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:399: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n    Instructions for updating:\n    Use tf.losses.compute_weighted_loss instead.\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:147: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n    Instructions for updating:\n    Use tf.losses.add_loss instead.\n    INFO:tensorflow:Create CheckpointSaverHook.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp_xsdnfqs/model.ckpt.\n    INFO:tensorflow:loss = 2.7085168, step = 0\n    INFO:tensorflow:global_step/sec: 91.6219\n    INFO:tensorflow:loss = 0.044218887, step = 100 (1.094 sec)\n    INFO:tensorflow:global_step/sec: 102.446\n    INFO:tensorflow:loss = 0.0072543425, step = 200 (0.975 sec)\n    INFO:tensorflow:global_step/sec: 103.751\n    INFO:tensorflow:loss = 0.002886944, step = 300 (0.964 sec)\n    INFO:tensorflow:global_step/sec: 103.69\n    INFO:tensorflow:loss = 0.0020158489, step = 400 (0.965 sec)\n    INFO:tensorflow:global_step/sec: 102.387\n    INFO:tensorflow:loss = 0.0011120392, step = 500 (0.976 sec)\n    INFO:tensorflow:global_step/sec: 102.986\n    INFO:tensorflow:loss = 0.0008669663, step = 600 (0.971 sec)\n    INFO:tensorflow:global_step/sec: 102.535\n    INFO:tensorflow:loss = 0.000653078, step = 700 (0.975 sec)\n    INFO:tensorflow:global_step/sec: 102.798\n    INFO:tensorflow:loss = 0.0005366546, step = 800 (0.973 sec)\n    INFO:tensorflow:global_step/sec: 103.618\n    INFO:tensorflow:loss = 0.0005024085, step = 900 (0.965 sec)\n    INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp_xsdnfqs/model.ckpt.\n    INFO:tensorflow:Loss for final step: 0.00040020357.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Restoring parameters from /tmp/tmp_xsdnfqs/model.ckpt-1000\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    Accuracy: 0.957000\n    \n\n\n```python\n# GRU分类器\ndef rnn_model(features, target):\n    \"\"\"用RNN模型(这里用的是GRU)完成文本分类\"\"\"\n    # Convert indexes of words into embeddings.\n    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n    # maps word indexes of the sequence into [batch_size, sequence_length,\n    # EMBEDDING_SIZE].\n    word_vectors = tf.contrib.layers.embed_sequence(\n            features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n\n    # Split into list of embedding per word, while removing doc length dim.\n    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n    word_list = tf.unstack(word_vectors, axis=1)\n\n    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n\n    # Create an unrolled Recurrent Neural Networks to length of\n    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n    _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n\n    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n    # neural network of last step) and pass it as features for logistic\n    # regression over output classes.\n    target = tf.one_hot(target, 15, 1, 0)\n    logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n\n    # Create a training op.\n    train_op = tf.contrib.layers.optimize_loss(\n            loss,\n            tf.contrib.framework.get_global_step(),\n            optimizer='Adam',\n            learning_rate=0.01)\n\n    return ({\n            'class': tf.argmax(logits, 1),\n            'prob': tf.nn.softmax(logits)\n    }, loss, train_op)\n```\n\n\n```python\nmodel_fn = rnn_model\nclassifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n\n# Train and predict\nclassifier.fit(x_train, y_train, steps=1000)\ny_predicted = classifier.predict(x_test)['class']\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint('Accuracy: {0:f}'.format(score))\n```\n\n    INFO:tensorflow:Using default config.\n    WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1fdgvpeq\n    INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020B6B1DE5C0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n      per_process_gpu_memory_fraction: 1.0\n    }\n    , '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\vip\\\\AppData\\\\Local\\\\Temp\\\\tmp1fdgvpeq'}\n    INFO:tensorflow:Create CheckpointSaverHook.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1fdgvpeq\\model.ckpt.\n    INFO:tensorflow:loss = 2.7080488, step = 1\n    INFO:tensorflow:global_step/sec: 23.1885\n    INFO:tensorflow:loss = 0.051642288, step = 101 (4.314 sec)\n    INFO:tensorflow:global_step/sec: 26.0637\n    INFO:tensorflow:loss = 0.00094055594, step = 201 (3.837 sec)\n    INFO:tensorflow:global_step/sec: 25.6373\n    INFO:tensorflow:loss = 0.00044845277, step = 301 (3.901 sec)\n    INFO:tensorflow:global_step/sec: 26.2068\n    INFO:tensorflow:loss = 0.00029878103, step = 401 (3.816 sec)\n    INFO:tensorflow:global_step/sec: 25.8688\n    INFO:tensorflow:loss = 0.00020871352, step = 501 (3.865 sec)\n    INFO:tensorflow:global_step/sec: 24.9545\n    INFO:tensorflow:loss = 0.00015666419, step = 601 (4.008 sec)\n    INFO:tensorflow:global_step/sec: 25.2118\n    INFO:tensorflow:loss = 0.00012052382, step = 701 (3.966 sec)\n    INFO:tensorflow:global_step/sec: 22.598\n    INFO:tensorflow:loss = 0.000109548244, step = 801 (4.426 sec)\n    INFO:tensorflow:global_step/sec: 25.2499\n    INFO:tensorflow:loss = 8.7993896e-05, step = 901 (3.959 sec)\n    INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1fdgvpeq\\model.ckpt.\n    INFO:tensorflow:Loss for final step: 8.049416e-05.\n    INFO:tensorflow:Graph was finalized.\n    INFO:tensorflow:Restoring parameters from C:\\Users\\vip\\AppData\\Local\\Temp\\tmp1fdgvpeq\\model.ckpt-1000\n    INFO:tensorflow:Running local_init_op.\n    INFO:tensorflow:Done running local_init_op.\n    Accuracy: 0.950500\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/8Machine_Translation/3.Facebook_Machine_Translation_Model_based_on_CNN/Facebook_Machine_Translation_Model_based_on_CNN/","content":"\n# facebook基于CNN的机器翻译模型\n\n\n## 本章概述\n- 基于CNN的翻译系统模型架构\n    - Pooling Encoder\n    - Convolution Encoder\n    - Convolution NMT\n    - 对比CNN与RNN去构建的 encoder-decoder模型，分析CNN的优缺点\n- 使用CNN完成神经翻译系统的Trick\n    - 对模型某些部分做缩放（scaling）\n    - 对模型参数的初始化\n    - 对超参数的选择\n- 【实战】facebook CNN机器翻译系统代码解析\n    - 举例训练，及测试 CNN翻译系统\n    - 分析 FconvModel, FconvEncoder, FconvDecoder\n    - 分析 main 函数中训练模型部分\n\n## 1.基于CNN的翻译系统模型结构\n\n- 在自然语言处理中，大部分流行的seq2seq模型都是基于RNN结构去构建encoder和decoder，但是RNN对于下一个状态的预测需要依赖前面的所有历史状态，使得并行化操作难以充分进行，难以发挥完全发挥GPU并行的效率。相反CNN通过在固定窗口内的计算，使得计算的并行化变得更加简单，而且通过多层CNN网络可以构建层级结构(hierarchical structure)，可以达到利用更短的路径去覆盖更长范围内的信息。\n- Facebook提出了基于CNN的机器翻译模型，并开源了CNN的机器翻译工具[Fairseq](https://github.com/facebookresearch/fairseq)\n\n\n### 1.1 Pooling Encoder\n\n\n- 最简单的non-recurrent encoder就是把k个连续的单词的词向量求平均值，通过在句子左右两边都做添加额外的空单词(paddings)，可以使得encoder输出跟原来句子同等长度的hidden embeddings。\n    - 假设原来的句子的词向量（word embedding）表示为 $w=[w_1,\\cdots,w_m],~\\forall~w_j\\in R^f$\n    - absolute position embeddings用于编码位置信息　$p=[p_1,\\cdots,p_m],~\\forall~p_j\\in R^f$\n    $$e_j = w_j + p_j,~~ z_j = {1\\over k} \\sum_{t=-k/2}^{k/2}e_{j+t} $$\n    - 传统的attention 机制\n    $$ c_i = \\sum_{j=1}^m a_{ij} e_j$$\n\n\n### 1.2 卷积编码器　Convolutional Encoder NMT [Gehring et. al 2016](https://arxiv.org/pdf/1611.02344.pdf)\n\n\n- 卷积编码器在pooling encoder的基础上进行改进，使用一个CNN-a 卷积层来进一步编码源语言句子中的每个单词\n\n$$z_j = CNN-a(e)_j $$\n\n- 注意attention的时候，使用了另一个CNN-c　卷积层来编码源语言句子中的每个单词，并计算atttention weight，再进行加权求和\n$$c_i = \\sum_{j=1}^m a_{ij} CNN-c(e)_j$$\n\n### 1.2 卷积编码器　Convolutional Encoder NMT [Gehring et. al 2016](https://arxiv.org/pdf/1611.02344.pdf)\n\n\n- 该模型的encoder 采用的是CNN，但其decoder还是采用了传统的RNN模型\n\n![](./img/cnn-encoder.png)\n\n### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n\n\n- 该模型的encoder和decoder都采用的是卷积核CNN，动图演示\n![](./img/fairseq.gif)\n\n### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n\n\n- 卷积核结构\n    - 假设有1D的卷积核的窗口大小是k(比如k=5)，每个卷积核都可以用一个权重矩阵$W\\in \\mathbb{R}^{2d\\times kd}$和 bias $b_w\\in \\mathbb{R}^{2d}$。对于窗口内的词向量　$X\\in \\mathbb{R}^{k\\times d}$把每个单词都拼接成一个长向量　$X'\\in \\mathbb{R}^{kd}$.\n    $$Y=WX'+b_w = [A B] \\in \\mathbb{R}^{2d} \\\\ A,B\\in \\mathbb{R}^{d} $$\n    \n    - 接下来我们采用Gated Linear Unites(GLU)的方式来进行编码, $\\sigma()$是一个非线性的激活函数，　$\\otimes$是element-wise mulitiplication，指的是对两个向量中的每个维度上的数值分别求乘积　\n    $$v([A B] = A \\otimes \\sigma(B) \\in \\mathbb{R}^d$$\n    \n    - 残差连接　Residual Connection:　把上一层的输入也累加到下一层的输出\n    $$h_i^l = v(W^l [h_{(i-k)/2}^{l-1},\\cdots,h_{(i+k)/2}^{l-1}]+b_w^l)+h_i^{l-1}　\\in \\mathbb{R}^d$$\n    \n\n\n### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n\n\n- 编码器　Encoder:\n    - 假设原来的句子的词向量（word embedding）表示为 $w=[w_1,\\cdots,w_m],~\\forall~w_j\\in \\mathbb{R}^f$\n    - absolute position embeddings用于编码位置信息　$p=[p_1,\\cdots,p_m],~\\forall~p_j\\in \\mathbb{R}^f$\n    $$e_j = w_j + p_j \\\\ $$\n    \n    - encoder 先用一个线性函数$f:\\mathbb{R}^f\\rightarrow \\mathbb{R}^d$，把词向量映射到d维空间中  \n    - 接下来encoder会将词向量通过一层层卷积核，得到每一层的单词的隐式表达（hidden state）, 其中　$z_j^u$　代表的是第u层CNN中第j个单词的表达\n\n\n### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n\n- Multi-step Attention机制\n    - 假设已经翻译的单词的词表达是 $g=[g_1,\\cdots, g_n]$，跟源语言的词表达一样，这里也是word embeddings加上positional embeddings\n    －假设decoder的卷积核的hidden state $h_i^l$, 可以进一步计算decoder已经生成的单词的每一层的单词表达\n    $$d_i^l = W_d^l h_i^l + b_d^l + g_i $$\n    \n    －假设encoder 最顶层(假设是第u层)中，每个单词的表达是　$z_j^u$。我们可以计算decoder第l层中第i个已经生成的单词　$h_i^l$与源语言句子中最顶层（也即是第u层）的第j个单词 $z_j^u$的权重:    \n    $$a_{ij}^l = {\\exp(d_i^l \\cdot z_j^u) \\over \\sum_{t=1}^m \\exp(d_i^l \\cdot z_t^u) } $$\n    \n    －我们可以进一步计算在decoder第l层，在第i个时刻的上下文向量（也即是context vector）如以下公式，其中我们将encoder最顶层(第u层)的词向量$z_j^u$与最底层的词向量$e_j$相加。\n    \n    $$c_i^l = \\sum_{j=1}^m a_{ij}^l (z_j^u + e_j) $$\n    －一旦我们计算好$c_i^l$,我们将　$c_i^l$加到$h_i^l$中，作为decoder 的下一层的输入\n\n### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n\n- 解码器　decoder\n    - 把decoder最顶层的hidden state $h_i^L$　通过一个线性的函数映射到词表空间上$d\\rightarrow |V|$，之后在通过一个softmax函数　归一化成一个条件概率向量：\n    $$p(y_{i+1}|y_1,\\cdots, y_i, x)= softmax(W_o h_i^L + b_0) \\in \\mathbb{R}^{|V|} $$\n    \n\n\n### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n\n- 模型的结构图\n\n<img src=\"./img/cnn-nmt.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n\n### 1.4 全卷积神经翻译模型对比RNN神经翻译模型\n\n- 全卷积神经网络使用层级结构，可以充分地并行化\n- 对于一个窗口大小为$k$的CNN，编码一个特征向量可以总结一个窗口为n个单词的信息，只需要做$O(n/k)$个卷积核操作。对比RNN，RNN编码一个窗口为n个单词的信息，需要做$O(n)$个操作，跟句子的长度成正比\n- 对于一个CNN的输入，我们都进行了相同数量的卷积操作及非线性操作。对比RNN，第一个输入的单词进行了n词非线性操作，而最后一个输入的单词只进行了一次非线性操作。对于每个输入都进行相同数量的操作会有利于训练。\n- 训练CNN NMT需要非常小心地设置参数及调整网络中某些层的缩放。\n\n\n## 2 使用CNN完成神经机器翻译系统的tricks\n\n- 训练过程中，我们需要将网络中某些部分进行缩放(scaling)\n- 训练过程中，我们需要对权重初始化\n- 训练过程中，我们需要对超参数进行设置\n\n### 2.1 缩放操作（scaling）\n\n- 我们将残差层的输出乘以　$\\sqrt{0.5}$，　这样会减小一半的偏差variance\n- 对于attention机制产生的上下文向量　$c_{ij}^l$　乘以一个系数　$m\\sqrt{1/m}$, 其中m为源语言句子中单词个个数，这样做的好处也是能减小偏差。\n- 对于CNN decoder有multiple atttention的情况，我们将encoder 每一层的gradient乘以一个系数，该系数是我们使用的attention的数量。注意的是，我们只对encoder中除了源语言单词的词向量矩阵以外的参数进行放大他们的gradient，源语言的词向量矩阵的gradient不进行放大。在实验中，我们发现这样的操作会使得训练能更加稳定。\n\n\n### 2.2 参数初始化\n\n- 所有的词向量矩阵从一个以０为中心，标准差为0.1的高斯分布中随机初始化　$\\mathcal{N}(0, \\sqrt{n_l})$, 其中$n_l$为输入到这个神经元的输入个数，一般我们可以设置为0.1。这样能有助于保持一个正态分布的偏差。\n- 我们还需要对每一层的激活函数输出进行正则化(normalization)，　比如残差连接中，每一层层的输出向量需要先做正则化，再把这一层的输入加到输出的向量上。\n- 对于GLU，我们需要对其权重　$W$从一个正态分布$\\mathcal{N}(0, \\sqrt{4p\\over n_l})$中随机抽样，而其bias设置成０\n- 我们对每一层网络的输入向量都进行dropout处理\n\n\n\n\n### 2.3 超参数设置\n\n- encoder 和decoder都是用512维的hidden units，512维的word embeddings\n- 训练的时候使用Nesterov's accelerated gradient 的方法进行优化模型，momentum 设置成0.99\n- 如果gradient的norm超过0.1就把gradient 重新归一化到0.1以内。\n- 初始的learning rate设置成0.25，如果在每次进行valudation的时候dev数据集中的perplexity没有下降，我们就将learning rate乘以0.1,　一直持续到learning rate 降到$10^{-4}$以下我们停止训练\n- mini-batch　的大小设置成每次处理64句双语句子\n\n\n\n\n\n## 3. Facebook CNN 机器翻译系统代码解析\n\n- 相应的代码可以在github上找到　[fairseq](https://github.com/pytorch/fairseq)\n- 安装\n```bash\ngit clone https://github.com/pytorch/fairseq.git\ncd fairseq\npip install -r requirements.txt\npython setup.py build develop\n```\n\n### 3.1 实战例子\n\n\n```bash\n# 预处理数据\n$ bash prepare-wmt14en2de.sh --icml17\n\n$ cd examples/translation/\n$ bash prepare-wmt14en2de.sh\n$ cd ../..\n\n# 将数据处理成二进制形式，加速读写\n$ TEXT=examples/translation/wmt14_en_de\n$ python preprocess.py --source-lang en --target-lang de \\\n  --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n  --destdir data-bin/wmt14_en_de --thresholdtgt 0 --thresholdsrc 0\n\n# 训练模型\n# 如果显存不足，可以将--max-tokens设置成1500\n$ mkdir -p checkpoints/fconv_wmt_en_de\n$ python train.py data-bin/wmt14_en_de \\\n  --lr 0.5 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n  --lr-scheduler fixed --force-anneal 50 \\\n  --arch fconv_wmt_en_de --save-dir checkpoints/fconv_wmt_en_de\n\n# 测试，生成\n$ python generate.py data-bin/wmt14_en_de \\\n  --path checkpoints/fconv_wmt_en_de/checkpoint_best.pt --beam 5 --remove-bpe\n```\n\n### 3.2 使用预训练好的模型\n\n\n```bash\n# 下载模型及数据\n$ mkdir -p data-bin\n$ curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf - -C data-bin\n$ curl https://dl.fbaipublicfiles.com/fairseq/data/wmt14.v2.en-fr.newstest2014.tar.bz2 | tar xvjf - -C data-bin\n\n# 进行翻译生成\n$ python generate.py data-bin/wmt14.en-fr.newstest2014  \\\n  --path data-bin/wmt14.en-fr.fconv-py/model.pt \\\n  --beam 5 --batch-size 128 --remove-bpe | tee /tmp/gen.out\n...\n| Translated 3003 sentences (96311 tokens) in 166.0s (580.04 tokens/s)\n| Generate test with beam=5: BLEU4 = 40.83, 67.5/46.9/34.4/25.5 (BP=1.000, ratio=1.006, syslen=83262, reflen=82787)\n\n# 对翻译结果打分\n$ grep ^H /tmp/gen.out | cut -f3- > /tmp/gen.out.sys\n$ grep ^T /tmp/gen.out | cut -f2- > /tmp/gen.out.ref\n$ python score.py --sys /tmp/gen.out.sys --ref /tmp/gen.out.ref\nBLEU4 = 40.83, 67.5/46.9/34.4/25.5 (BP=1.000, ratio=1.006, syslen=83262, reflen=82787)\n```\n\n### 3.3 代码讲解\n\n- CNN NMT类 FConvModel\n```python\n@register_model('fconv')\nclass FConvModel(FairseqModel):\n    \"\"\"\n    Args:\n        encoder (FConvEncoder): the encoder\n        decoder (FConvDecoder): the decoder\n    \"\"\"\n\n    def __init__(self, encoder, decoder):\n        ...\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument('--dropout', type=float, metavar='D',\n                            help='dropout probability')\n        parser.add_argument('--encoder-embed-dim', type=int, metavar='N',\n                            help='encoder embedding dimension')\n        parser.add_argument('--encoder-embed-path', type=str, metavar='STR',\n                            help='path to pre-trained encoder embedding')\n        parser.add_argument('--encoder-layers', type=str, metavar='EXPR',\n                            help='encoder layers [(dim, kernel_size), ...]')\n        parser.add_argument('--decoder-embed-dim', type=int, metavar='N',\n                            help='decoder embedding dimension')\n        parser.add_argument('--decoder-embed-path', type=str, metavar='STR',\n                            help='path to pre-trained decoder embedding')\n        parser.add_argument('--decoder-layers', type=str, metavar='EXPR',\n                            help='decoder layers [(dim, kernel_size), ...]')\n        parser.add_argument('--decoder-out-embed-dim', type=int, metavar='N',\n                            help='decoder output embedding dimension')\n    @classmethod\n    def build_model(cls, args, task):\n        base_architecture(args)\n        ...\n        encoder = FConvEncoder(\n            dictionary=task.source_dictionary,\n            embed_dim=args.encoder_embed_dim,\n            embed_dict=encoder_embed_dict,\n            convolutions=eval(args.encoder_layers),\n            dropout=args.dropout,\n            max_positions=args.max_source_positions,\n        )\n        decoder = FConvDecoder(\n            dictionary=task.target_dictionary,\n            embed_dim=args.decoder_embed_dim,\n            embed_dict=decoder_embed_dict,\n            convolutions=eval(args.decoder_layers),\n            out_embed_dim=args.decoder_out_embed_dim,\n            attention=eval(args.decoder_attention),\n            dropout=args.dropout,\n            max_positions=args.max_target_positions,\n            share_embed=args.share_input_output_embed,\n        )\n        return FConvModel(encoder, decoder)\n```\n\n### 3.3 代码讲解\n\n- CNN encoder类 \n```python\nclass FConvEncoder(FairseqEncoder):\n    def __init__(\n            self, dictionary, embed_dim=512, embed_dict=None, max_positions=1024,\n            convolutions=((512, 3),) * 20, dropout=0.1, left_pad=True,\n    ):\n        ...\n        # 定义词向量矩阵及位置矩阵\n        self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)\n        self.embed_positions = PositionalEmbedding(\n            max_positions,\n            embed_dim,\n            self.padding_idx,\n            left_pad=self.left_pad,\n        )\n\n        convolutions = extend_conv_spec(convolutions)\n        in_channels = convolutions[0][0]\n        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n        self.projections = nn.ModuleList()\n        self.convolutions = nn.ModuleList()\n        self.residuals = []\n        \n        # 定义CNN层及残差层\n        layer_in_channels = [in_channels]\n        for _, (out_channels, kernel_size, residual) in enumerate(convolutions):\n            if residual == 0:\n                residual_dim = out_channels\n            else:\n                residual_dim = layer_in_channels[-residual]\n            self.projections.append(Linear(residual_dim, out_channels)\n                                    if residual_dim != out_channels else None)\n            if kernel_size % 2 == 1:\n                padding = kernel_size // 2\n            else:\n                padding = 0\n            self.convolutions.append(\n                ConvTBC(in_channels, out_channels * 2, kernel_size,\n                        dropout=dropout, padding=padding)\n            )\n            self.residuals.append(residual)\n            in_channels = out_channels\n            layer_in_channels.append(out_channels)\n        self.fc2 = Linear(in_channels, embed_dim)\n\n    def forward(self, src_tokens, src_lengths):\n        # 查找词向量及位置向量\n        x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        input_embedding = x\n\n        # 将词的表达映射到CNN的输入空间 fc1: R^f ->R^d\n        x = self.fc1(x)\n\n        # 在句子左右两边添加padding\n        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()  # -> T x B\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        # 转置：B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        residuals = [x]\n        # 多层的CNN 层叠起来\n        for proj, conv, res_layer in zip(self.projections, self.convolutions, self.residuals):\n            if res_layer > 0:\n                residual = residuals[-res_layer]\n                residual = residual if proj is None else proj(residual)\n            else:\n                residual = None\n\n            if encoder_padding_mask is not None:\n                x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            if conv.kernel_size[0] % 2 == 1:\n                # padding is implicit in the conv\n                x = conv(x)\n            else:\n                padding_l = (conv.kernel_size[0] - 1) // 2\n                padding_r = conv.kernel_size[0] // 2\n                x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r))\n                x = conv(x)\n            # GLU 层\n            x = F.glu(x, dim=2)\n            \n            # 残差层\n            if residual is not None:\n                x = (x + residual) * math.sqrt(0.5)\n            residuals.append(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(1, 0)\n\n        # 将x映射回词向量空间 R^d -> R^f\n        x = self.fc2(x)\n\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = encoder_padding_mask.t()  # -> B x T\n            x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n\n        # 将gradient放大\n        x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers))\n\n        # 把input embedding加到output中\n        y = (x + input_embedding) * math.sqrt(0.5)\n\n        return {\n            'encoder_out': (x, y),\n            'encoder_padding_mask': encoder_padding_mask,  # B x T\n        }\n```\n\n### 3.3 代码讲解\n\n- 解码器decoder\n\n```python\nclass FConvDecoder(FairseqIncrementalDecoder):\n    def __init__(self,...):\n\n        # 定义词向量矩阵及位置向量矩阵\n        self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n        self.embed_positions = PositionalEmbedding(\n            max_positions,\n            embed_dim,\n            padding_idx,\n            left_pad=self.left_pad,\n        ) if positional_embeddings else None\n        \n        convolutions = extend_conv_spec(convolutions)\n        in_channels = convolutions[0][0]\n        \n        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n        self.projections = nn.ModuleList()\n        self.convolutions = nn.ModuleList()\n        self.attention = nn.ModuleList()\n        self.residuals = []\n        \n        # 定义多层CNN\n        layer_in_channels = [in_channels]\n        for i, (out_channels, kernel_size, residual) in enumerate(convolutions):\n            if residual == 0:\n                residual_dim = out_channels\n            else:\n                residual_dim = layer_in_channels[-residual]\n            self.projections.append(Linear(residual_dim, out_channels)\n                                    if residual_dim != out_channels else None)\n            self.convolutions.append(\n                LinearizedConv1d(in_channels, out_channels * 2, kernel_size,\n                                 padding=(kernel_size - 1), dropout=dropout)\n            )\n            self.attention.append(AttentionLayer(out_channels, embed_dim)\n                                  if attention[i] else None)\n            self.residuals.append(residual)\n            in_channels = out_channels\n            layer_in_channels.append(out_channels)\n\n        self.adaptive_softmax = None\n        self.fc2 = self.fc3 = None\n\n    def forward(self, prev_output_tokens, encoder_out_dict=None, incremental_state=None):\n        ...\n        # 获得位置向量\n        if self.embed_positions is not None:\n            pos_embed = self.embed_positions(prev_output_tokens, incremental_state)\n        else:\n            pos_embed = 0\n\n        # 获得上一个生成的单词的词向量\n        x = self._embed_tokens(prev_output_tokens, incremental_state)\n\n        # 将词向量加上位置向量作为当前时刻的输入\n        x += pos_embed\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        target_embedding = x\n\n        # 将输入从词向量空间映射到CNN输入空间\n        x = self.fc1(x)\n\n        # 转置：B x T x C -> T x B x C\n        x = self._transpose_if_training(x, incremental_state)\n\n        # 多层的CNN 堆叠\n        avg_attn_scores = None\n        num_attn_layers = len(self.attention)\n        residuals = [x]\n        for proj, conv, attention, res_layer in zip(self.projections, self.convolutions, self.attention,\n                                                    self.residuals):\n            if res_layer > 0:\n                residual = residuals[-res_layer]\n                residual = residual if proj is None else proj(residual)\n            else:\n                residual = None\n\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = conv(x, incremental_state)\n            x = F.glu(x, dim=2)\n\n            # 注意力机制\n            if attention is not None:\n                x = self._transpose_if_training(x, incremental_state)\n\n                x, attn_scores = attention(x, target_embedding, (encoder_a, encoder_b), encoder_padding_mask)\n\n                if not self.training and self.need_attn:\n                    attn_scores = attn_scores / num_attn_layers\n                    if avg_attn_scores is None:\n                        avg_attn_scores = attn_scores\n                    else:\n                        avg_attn_scores.add_(attn_scores)\n\n                x = self._transpose_if_training(x, incremental_state)\n\n            # 残差连接\n            if residual is not None:\n                x = (x + residual) * math.sqrt(0.5)\n            residuals.append(x)\n\n        # 转置：T x B x C -> B x T x C\n        x = self._transpose_if_training(x, incremental_state)\n\n        # fc2:将输入映射到词表大小空间，可进行预测\n        if self.fc2 is not None and self.fc3 is not None:\n            x = self.fc2(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = self.fc3(x)\n\n        return x, avg_attn_scores\n```\n\n### 3.3 代码讲解\n\n```python\ndef main(args, init_distributed=False):\n    ...\n\n    # 载入数据\n    load_dataset_splits(task, ['train', 'valid'])\n\n    # 构建模型及优化函数\n    model = task.build_model(args)\n    criterion = task.build_criterion(args)\n\n    # 构建训练器 trainer\n    trainer = Trainer(args, task, model, criterion, dummy_batch, oom_batch)\n\n    # 初始化dataloader\n    epoch_itr = task.get_batch_iterator(...)\n\n    # 训练一直到learning rate太小就停止\n    max_epoch = args.max_epoch or math.inf\n    max_update = args.max_update or math.inf\n    lr = trainer.get_lr()\n    train_meter = StopwatchMeter()\n    train_meter.start()\n    while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n        # 训练一个epoch\n        train(args, trainer, task, epoch_itr)\n\n        if epoch_itr.epoch % args.validate_interval == 0:\n            valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n\n        # 只用第一个validation loss去更新learning rate\n        lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n\n        # 保存模型\n        if epoch_itr.epoch % args.save_interval == 0:\n            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n    train_meter.stop()\n```\n\n## 本节小结\n- 基于CNN的翻译系统模型架构\n    - Pooling Encoder\n    - Convolution Encoder\n    - Convolution NMT\n    - 对比CNN与RNN去构建的 encoder-decoder模型，分析CNN的优缺点\n- 使用CNN完成神经翻译系统的Trick\n    - 对模型某些部分做缩放（scaling）\n    - 对模型参数的初始化\n    - 对超参数的选择\n- 【实战】facebook CNN机器翻译系统代码解析\n    - 举例训练，及测试 CNN翻译系统\n    - 分析了 FconvModel, FconvEncoder, FconvDecoder\n    - 分析了 main 函数中训练模型部分\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson2/3.Emotion Detection/","content":"\n# 简易情感分析\n\n我们来结合一下sklearn，pandas和刚讲过的工具库，来构建一个简易情感分析模型。\n\n\n```python\nimport numpy as np\nimport pandas as pd\n```\n\n## 加载数据\n\n\n```python\ndata = pd.read_csv(\"./data/emotion_data.csv\")\n```\n\n\n```python\ndata.shape\n```\n\n\n\n\n    (40000, 4)\n\n\n\n\n```python\ndata.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>author</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>xoshayzers</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>wannamama</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>coolfunky</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>czareaquino</td>\n      <td>wants to hang out with friends SOON!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>xkilljoyx</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 不同的情感种类\ndata.sentiment.unique()\n```\n\n\n\n\n    array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n           'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n          dtype=object)\n\n\n\n## 数据预处理\n\n\n```python\n# 去掉无关列\ndata = data.drop(data.columns[[0,2]], axis=1)\n```\n\n\n```python\ndata.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>empty</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sadness</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sadness</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>enthusiasm</td>\n      <td>wants to hang out with friends SOON!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ndataset = data.as_matrix()\n```\n\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n      \"\"\"Entry point for launching an IPython kernel.\n    \n\n\n```python\ndataset.shape\n```\n\n\n\n\n    (40000, 2)\n\n\n\n\n```python\nfeatures = dataset[:,1]\n```\n\n\n```python\nfeatures[123]\n```\n\n\n\n\n    '@poinktoinkdoink He died.  Wait, what about Magic Jack? I just read it.'\n\n\n\n\n```python\ntarget = dataset[:,0]\n```\n\n\n```python\n# 使用LabelEncoder对不同的情感target进行编码\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntarget_processed = le.fit_transform(target)\n```\n\n\n```python\nle.classes_\n```\n\n\n\n\n    array(['anger', 'boredom', 'empty', 'enthusiasm', 'fun', 'happiness',\n           'hate', 'love', 'neutral', 'relief', 'sadness', 'surprise',\n           'worry'], dtype=object)\n\n\n\n\n```python\n# 对输入的文本进行特征抽取和表示(这里用到的tf-idf特征在后面的课程中会讲到)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nX_processed = tfidf.fit_transform(features)\n```\n\n\n```python\nX_processed\n```\n\n\n\n\n    <40000x48212 sparse matrix of type '<class 'numpy.float64'>'\n    \twith 475946 stored elements in Compressed Sparse Row format>\n\n\n\n\n```python\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_processed, target_processed, test_size=0.5, random_state=42)\n```\n\n\n```python\ny_train\n```\n\n\n\n\n    array([ 3,  5, 10, ...,  4,  6,  7])\n\n\n\n## 模型训练\n\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n```\n\n\n\n\n    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n              verbose=0, warm_start=False)\n\n\n\n\n```python\n# 模型评估\nlr.score(X_test, y_test)\n```\n\n\n\n\n    0.3489\n\n\n\n\n```python\n# 模型预测\ntest_ex = [\"It is so horrible\"]\ntext_ex_processed = tfidf.transform(test_ex)\nlr.predict(text_ex_processed)\n```\n\n\n\n\n    array([12])\n\n\n\n![](../img/xiniu_neteasy.png)\n\n\n```python\n\n```\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson2/1.English_text_analysis_and_processing-nltk/","content":"\n# 英文文本处理与[NLTK](https://www.nltk.org/)\n\n\n[NLTK](https://www.nltk.org/)，全称Natural Language Toolkit，自然语言处理工具包，是NLP研究领域常用的一个Python库，由宾夕法尼亚大学的Steven Bird和Edward Loper在Python的基础上开发的一个模块，至今已有超过十万行的代码。这是一个开源项目，包含数据集、Python模块、教程等；NLTK是最常用的英文自然语言处理python基础库之一。\n\n![](../img/L2_NLTK.png)\n\n### 1.英文Tokenization(标记化/分词)\n\n>文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。在NLTK中提供了2种不同方式的tokenization，sentence tokenization 和 word tokenization，前者把文本进行“断句”，后者对文本进行“分词”。\n\n\n```python\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nimport matplotlib\nmatplotlib.use('Agg')\n```\n\n\n```python\n# 读入数据\ncorpus = open('./data/text.txt','r').read()\n# 查看类型\nprint(\"corpus的数据类型为:\",type(corpus))\n```\n\n    corpus的数据类型为: <class 'str'>\n    \n\n\n```python\n# 断句\nsentences = sent_tokenize(corpus)\nsentences\n```\n\n\n\n\n    [\"A ``knowledge engineer'' interviews experts in a certain domain and tries to embody their knowledge in a computer program for carrying out some task.\",\n     'How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI.',\n     'When this turned out not to be so, there were many disappointing results.',\n     'One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments.',\n     'It did better than medical students or practicing doctors, provided its limitations were observed.',\n     'Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doctors, hospitals, death, recovery, and events occurring in time.',\n     'Its interactions depended on a single patient being considered.',\n     'Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework.',\n     'In the present state of AI, this has to be true.',\n     'The usefulness of current expert systems depends on their users having common sense.']\n\n\n\n\n```python\n# 分词\nwords = word_tokenize(corpus)\nwords[:20]\n```\n\n\n\n\n    ['A',\n     '``',\n     'knowledge',\n     'engineer',\n     \"''\",\n     'interviews',\n     'experts',\n     'in',\n     'a',\n     'certain',\n     'domain',\n     'and',\n     'tries',\n     'to',\n     'embody',\n     'their',\n     'knowledge',\n     'in',\n     'a',\n     'computer']\n\n\n\n### 2.停用词\n\n> 在自然语言处理的很多任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对统计方法带来一些干扰，我们把这类词叫做**停用词**，通常我们会用一个停用词表把它们过滤出来。比如英语当中的**定冠词/不定冠词**(a,an,the等)。\n\n>关于机器学习中停用词的产出与收集方法，大家可以参见知乎讨论[机器学习中如何收集停用词](https://www.zhihu.com/question/34939177)\n\n\n```python\n# 导入内置停用词\nfrom nltk.corpus import stopwords\n```\n\n\n```python\nstop_words = stopwords.words('english')\n#看头10个\nstop_words[0:10]\n```\n\n\n\n\n    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n\n\n\n\n```python\n# 使用列表推导式去掉停用词\nfiltered_corpus = [w for w in words if not w in stop_words]\nfiltered_corpus[:20]\n```\n\n\n\n\n    ['A',\n     '``',\n     'knowledge',\n     'engineer',\n     \"''\",\n     'interviews',\n     'experts',\n     'certain',\n     'domain',\n     'tries',\n     'embody',\n     'knowledge',\n     'computer',\n     'program',\n     'carrying',\n     'task',\n     '.',\n     'How',\n     'well',\n     'works']\n\n\n\n\n```python\n# 查看停用词数量\nprint(\"我们总共剔除的停用词数量为：\", len(words)-len(filtered_corpus))\n```\n\n    我们总共剔除的停用词数量为： 72\n    \n\n### 词性标注\n\n\n>词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n\n>词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。\n\n>词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。\n>词性标注的最简单做法是选取最高频词性，主流的做法可以分为基于规则和基于统计的方法，包括：\n* 基于最大熵的词性标注\n* 基于统计最大概率输出词性\n* 基于HMM的词性标注\n\n\n```python\n# 词性标注\nfrom nltk import pos_tag\ntags = pos_tag(filtered_corpus)\ntags[:20]\n```\n\n\n\n\n    [('A', 'DT'),\n     ('``', '``'),\n     ('knowledge', 'NN'),\n     ('engineer', 'NN'),\n     (\"''\", \"''\"),\n     ('interviews', 'NNS'),\n     ('experts', 'NNS'),\n     ('certain', 'JJ'),\n     ('domain', 'NN'),\n     ('tries', 'NNS'),\n     ('embody', 'VBP'),\n     ('knowledge', 'JJ'),\n     ('computer', 'NN'),\n     ('program', 'NN'),\n     ('carrying', 'NN'),\n     ('task', 'NN'),\n     ('.', '.'),\n     ('How', 'WRB'),\n     ('well', 'RB'),\n     ('works', 'VBZ')]\n\n\n\n具体的词性标注编码和含义见如下对应表：\n\n| POS Tag | Description | Example |\n| --- | --- | --- |\n| CC | coordinating conjunction | and |\n| CD | cardinal number | 1, third |\n| DT | determiner | the |\n| EX | existential there | there, is |\n| FW | foreign word | d’hoevre |\n| IN | preposition or subordinating conjunction | in, of, like |\n| JJ | adjective | big |\n| JJR | adjective, comparative | bigger |\n| JJS | adjective, superlative | biggest |\n| LS | list marker | 1) |\n| MD | modal | could, will |\n| NN | noun, singular or mass | door |\n| NNS | noun plural | doors |\n| NNP | proper noun, singular | John |\n| NNPS | proper noun, plural | Vikings |\n| PDT | predeterminer | both the boys |\n| POS | possessive ending | friend‘s |\n| PRP | personal pronoun | I, he, it |\n| PRP$ | possessive pronoun | my, his |\n| RB | adverb | however, usually, naturally, here, good |\n| RBR | adverb, comparative | better |\n| RBS | adverb, superlative | best |\n| RP | particle | give up |\n| TO | to | to go, to him |\n| UH | interjection | uhhuhhuhh |\n| VB | verb, base form | take |\n| VBD | verb, past tense | took |\n| VBG | verb, gerund or present participle | taking |\n| VBN | verb, past participle | taken |\n| VBP | verb, sing. present, non-3d | take |\n| VBZ | verb, 3rd person sing. present | takes |\n| WDT | wh-determiner | which |\n| WP | wh-pronoun | who, what |\n| WP\\$ | possessive wh-pronoun | whose |\n| WRB | wh-abverb | where, when |\n\n### 3.chunking/组块分析\n\n\n分块是命名实体识别的基础，词性给出来的句子成分的属性，但有时候，更多的信息(比如句子句法结构)可以帮助我们对句子中的模式挖掘更充分。举个例子，”古天乐赞助了很多小学“中的头部古天乐是一个人名(命名实体)\n\n组块分析是一个非常有用的从文本抽取信息的方法，提取组块需要用到正则表达式：\n\n\n```python\nfrom nltk.chunk import RegexpParser\nfrom nltk import sent_tokenize,word_tokenize\n```\n\n\n```python\n# 写一个匹配名词的模式\npattern = \"\"\"\n    NP: {<JJ>*<NN>+}\n    {<JJ>*<NN><CC>*<NN>+}\n    \"\"\"\n```\n\n\n```python\n# 定义组块分析器\nchunker = RegexpParser(pattern)\n```\n\n\n```python\n# 一段文本\ntext = \"\"\"\nhe National Wrestling Association was an early professional wrestling sanctioning body created in 1930 by \nthe National Boxing Association (NBA) (now the World Boxing Association, WBA) as an attempt to create\na governing body for professional wrestling in the United States. The group created a number of \"World\" level \nchampionships as an attempt to clear up the professional wrestling rankings which at the time saw a number of \ndifferent championships promoted as the \"true world championship\". The National Wrestling Association's NWA \nWorld Heavyweight Championship was later considered part of the historical lineage of the National Wrestling \nAlliance's NWA World Heavyweight Championship when then National Wrestling Association champion Lou Thesz \nwon the National Wrestling Alliance championship, folding the original championship into one title in 1949.\"\"\"\n```\n\n\n```python\n# 分句\ntokenized_sentence = nltk.sent_tokenize(text)\n# 分词\ntokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]\n# 词性标注\ntagged_words = [nltk.pos_tag(word) for word in tokenized_words]\n# 识别NP组块\nword_tree = [chunker.parse(word) for word in tagged_words]\n```\n\n\n```python\n# 本功能底层非matplotlib实现，无法在不可跳出弹窗的环境中使用，云平台不可使用，请本地尝试\n# 示例如下方图片所示\n\n# word_tree[0].draw() # 会跳出弹窗，显示如下的解析图\n```\n\n![](../img/L2_NLTK_parse.png)\n\n### 4.命名实体识别\n\n\n命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。通常包括两部分：1) 实体边界识别；2) 确定实体类别（人名、地名、机构名或其他）。\n\n\n```python\nfrom nltk import ne_chunk, pos_tag,  word_tokenize\nsentence = \"John studies at Stanford University.\"\nprint(ne_chunk(pos_tag(word_tokenize(sentence))))\n```\n\n    (S\n      (PERSON John/NNP)\n      studies/NNS\n      at/IN\n      (ORGANIZATION Stanford/NNP University/NNP)\n      ./.)\n    \n\n命名实体识别也非常推荐大家使用 <a href=\"https://stanfordnlp.github.io/CoreNLP/\">stanford core nlp modules</a> 作为nltk的NER工具库，通常来说它速度更快，而且有更改的识别准确度。\n\n### 5.Stemming和Lemmatizing \n\n\n很多时候我们需要对英文当中的时态语态等做归一化，这个时候我们就需要stemming和lemmatizing这样的操作了。比如\"running\"是进行时，但是这个词表征的含义和\"run\"是一致的，我们在识别语义的时候，希望能消除这种差异化。\n\n\n```python\n# 可以用PorterStemmer\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nstemmer.stem(\"running\")\n```\n\n\n\n\n    'run'\n\n\n\n\n```python\nstemmer.stem(\"makes\")\n```\n\n\n\n\n    'make'\n\n\n\n\n```python\nstemmer.stem(\"swimming\")\n```\n\n\n\n\n    'swim'\n\n\n\n\n```python\n# 也可以用\nfrom nltk.stem import SnowballStemmer\nstemmer2 = SnowballStemmer(\"english\")\nstemmer2.stem(\"growing\")\n```\n\n\n\n\n    'grow'\n\n\n\n\n```python\n# Lemmatization和Stemmer很类似，不同的地方在于它还考虑了词义关联等信息\n# Stemmer的速度更快，但是它通常只是一系列的规则\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize(\"makes\")\n```\n\n\n\n\n    'make'\n\n\n\n### 6.WordNet与词义解析\n\n\n\n```python\nfrom nltk.corpus import wordnet as wn\nwn.synsets('man')\n```\n\n\n\n\n    [Synset('man.n.01'),\n     Synset('serviceman.n.01'),\n     Synset('man.n.03'),\n     Synset('homo.n.02'),\n     Synset('man.n.05'),\n     Synset('man.n.06'),\n     Synset('valet.n.01'),\n     Synset('man.n.08'),\n     Synset('man.n.09'),\n     Synset('man.n.10'),\n     Synset('world.n.08'),\n     Synset('man.v.01'),\n     Synset('man.v.02')]\n\n\n\n\n```python\n# 第一种词义\nwn.synsets('man')[0].definition()\n```\n\n\n\n\n    'an adult person who is male (as opposed to a woman)'\n\n\n\n\n```python\n# 第二种词义\nwn.synsets('man')[1].definition()\n```\n\n\n\n\n    'someone who serves in the armed forces; a member of a military force'\n\n\n\n\n```python\nwn.synsets('dog')\n```\n\n\n\n\n    [Synset('dog.n.01'),\n     Synset('frump.n.01'),\n     Synset('dog.n.03'),\n     Synset('cad.n.01'),\n     Synset('frank.n.02'),\n     Synset('pawl.n.01'),\n     Synset('andiron.n.01'),\n     Synset('chase.v.01')]\n\n\n\n\n```python\n# 查词义\nwn.synsets('dog')[0].definition()\n```\n\n\n\n\n    'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'\n\n\n\n\n```python\n# 造句\ndog = wn.synset('dog.n.01')\ndog.examples()[0]\n```\n\n\n\n\n    'the dog barked all night'\n\n\n\n\n```python\n# 上位词\ndog.hypernyms()\n```\n\n\n\n\n    [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n\n\n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson1/3.re_example/","content":"\n## 简单爬虫与正则表达式应用\n\n\n有个非常热门的自然语言处理垂直技术叫做**知识图谱**，知识图谱的构建需要依托于大量的实体和关系，很多这样的内容是可以从互联网上取到的。我们这里举一个最简单的应用，我们用正则表达式把搜狗百科的一些词条和解释抽取出来。\n\n\n```python\n# 引入爬虫工具库\nimport requests as rq\nimport re\n```\n\n\n```python\n# 发送请求\npage = rq.get(\"https://baike.sogou.com/v231013.htm\")\n```\n\n\n```python\n# 返回状态码正常\npage.status_code\n```\n\n\n\n\n    200\n\n\n\n\n```python\n# 词条正则表达式抽取\ntitle_pattern = re.compile(r'<h1 id=\"title\".*?>(.*?)</h1>') \ntitle = title_pattern.search(page.text) \nprint(title.group(1))\n```\n\n    自然语言处理\n    \n\n\n```python\n# 词条正则表达式抽取\ncontent_pattern = re.compile(r'<p>(.*?)<\\\\/p>') \ncontents = content_pattern.findall(page.text) \nprint(contents)\n```\n\n    ['<a class=\\\\\"ed_image_link\\\\\" title=\\\\\"点击查看大图\\\\\" href=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/4129/20180605223458-2107092668_png_520_397_107898.jpg/0\\\\\" target=\\\\\"_blank\\\\\"><img title=\\\\\"自然语言处理\\\\\" alt=\\\\\"自然语言处理\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/4169/cut-20180605223503-1613411374_jpg_473_355_11578.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"165\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" /><\\\\/a>语言是人类区别其他动物的本质特性。在所有生物中，只有人类才具有语言能力。人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，它也是人工智能的一个重要，甚至核心部分。', '用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=664318&amp;ss_c=ssc.citiao.link\\\\\">计算机语言<\\\\/a>；人们也可通过它进一步了解人类的语言能力和智能的机制。', '实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=51401303&amp;ss_c=ssc.citiao.link\\\\\">自然语言理解<\\\\/a>，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。历史上对自然语言理解研究得较多，而对自然语言生成研究得较少。但这种状况已有所改变。', '无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=143528697&amp;ss_c=ssc.citiao.link\\\\\">机器翻译系统<\\\\/a>、全文信息检索系统、自动文摘系统等。', '自然语言处理，即实现人机间自然语言通信，或实现自然语言理解和自然语言生成是十分困难的。造成困难的根本原因是自然语言文本和对话的各个层次上广泛存在的各种各样的歧义性或多义性（ambiguity）。', '一个中文文本从形式上看是由汉字（包括标点符号等）组成的一个字符串。由字可组成词，由词可组成词组，由词组可组成句子，进而由一些句子组成段、节、章、篇。无论在上述的各种层次：字（符）、词、词组、句子、段，\\\\u2026\\\\u2026还是在下一层次向上一层次转变中都存在着歧义和多义现象，即形式上一样的一段字符串，在不同的场景或不同的语境下，可以理解成不同的词串、词组串等，并有不同的意义。一般情况下，它们中的大多数都是可以根据相应的语境和场景的规定而得到解决的。也就是说，从总体上说，并不存在歧义。这也就是我们平时并不感到自然语言歧义，和能用自然语言进行正确交流的原因。但是一方面，我们也看到，为了消解歧义，是需要极其大量的知识和进行推理的。如何将这些知识较完整地加以收集和整理出来；又如何找到合适的形式，将它们存入计算机系统中去；以及如何有效地利用它们来消除歧义，都是工作量极大且十分困难的工作。这不是少数人短时期内可以完成的，还有待长期的、系统的工作。', '以上说的是，一个中文文本或一个汉字（含标点符号等）串可能有多个含义。它是自然语言理解中的主要困难和障碍。反过来，一个相同或相近的意义同样可以用多个中文文本或多个汉字串来表示。', '因此，自然语言的形式（字符串）与其意义之间是一种多对多的关系。其实这也正是自然<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=70315066&amp;ss_c=ssc.citiao.link\\\\\">语言的魅力<\\\\/a>所在。但从计算机处理的角度看，我们必须消除歧义，而且有人认为它正是自然语言理解中的中心问题，即要把带有潜在歧义的自然语言输入转换成某种无歧义的计算机内部表示。', '歧义现象的广泛存在使得消除它们需要大量的知识和推理，这就给基于语言学的方法、基于知识的方法带来了巨大的困难，因而以这些方法为主流的自然语言处理研究几十年来一方面在理论和方法方面取得了很多成就，但在能处理大规模真实文本的系统研制方面，成绩并不显著。研制的一些系统大多数是小规模的、研究性的演示系统。', '目前存在的问题有两个方面：一方面，迄今为止的语法都限于分析一个孤立的句子，上下文关系和谈话环境对本句的约束和影响还缺乏系统的研究，因此分析歧义、词语省略、代词所指、同一句话在不同场合或由不同的人说出来所具有的不同含义等问题，尚无明确规律可循，需要加强语用学的研究才能逐步解决。另一方面，人理解一个句子不是单凭语法，还运用了大量的有关知识，包括生活知识和专门知识，这些知识无法全部贮存在计算机里。因此一个书面理解系统只能建立在有限的词汇、句型和特定的主题范围内；计算机的贮存量和运转速度大大提高之后，才有可能适当扩大范围.', '以上存在的问题成为自然语言理解在机器翻译应用中的主要难题，这也就是当今机器翻译系统的译文质量离理想目标仍相差甚远的原因之一；而译文质量是机译系统成败的关键。中国数学家、语言学家<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=62243&amp;ss_c=ssc.citiao.link\\\\\">周海中<\\\\/a>教授曾在经典论文《机器翻译五十年》中指出：要提高机译的质量，首先要解决的是语言本身问题而不是程序设计问题；单靠若干程序来做机译系统，肯定是无法提高机译质量的；另外在人类尚未明了大脑是如何进行语言的模糊识别和逻辑判断的情况下，机译要想达到\\\\u201c信、达、雅\\\\u201d的程度是不可能的。', '最早的自然语言理解方面的研究工作是机器翻译。1949年，美国人威弗首先提出了机器翻译设计方案。<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=371024&amp;ss_c=ssc.citiao.link\\\\\">20世纪60年代<\\\\/a>，国外对机器翻译曾有大规模的研究工作，耗费了巨额费用，但人们当时显然是低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展不大。主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。', '大约90年代开始，自然语言处理领域发生了巨大的变化。这种变化的两个明显的特征是：', '（1）对系统输入，要求研制的自然语言处理系统能处理大规模的真实文本，而不是如以前的研究性系统那样，只能处理很少的词条和典型句子。只有这样，研制的系统才有真正的实用价值。', '（2）对系统的输出，鉴于真实地理解自然语言是十分困难的，对系统并不要求能对自然语言文本进行深层的理解，但要能从中抽取有用的信息。例如，对自然语言文本进行自动地提取索引词，过滤，检索，自动提取重要信息，进行自动摘要等等。', '同时，由于强调了\\\\u201c大规模\\\\u201d，强调了\\\\u201c真实文本\\\\u201d，下面两方面的基础性工作也得到了重视和加强。', '（1）大规模真实<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=7616056&amp;ss_c=ssc.citiao.link\\\\\">语料库<\\\\/a>的研制。大规模的经过不同深度加工的真实文本的语料库，是研究自然语言统计性质的基础。没有它们，统计方法只能是无源之水。', '（2）大规模、信息丰富的词典的编制工作。规模为几万，十几万，甚至几十万词，含有丰富的信息（如包含词的搭配信息）的计算机可用词典对自然语言处理的重要性是很明显的。', '自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。', '现代NLP算法是基于机器学习，特别是统计机器学习。机器学习范式是不同于一般之前的尝试语言处理。语言处理任务的实现，通常涉及直接用手的大套规则编码。', '许多不同类的<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=138836896&amp;ss_c=ssc.citiao.link\\\\\">机器学习算法<\\\\/a>已应用于自然语言处理任务。这些算法的输入是一大组从输入数据生成的\\\\u201c特征\\\\u201d。一些最早使用的算法，如<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=501657&amp;ss_c=ssc.citiao.link\\\\\">决策树<\\\\/a>，产生硬的if-then规则类似于手写的规则，是再普通的系统体系。然而，越来越多的研究集中于统计模型，这使得基于附加实数值的权重，每个输入要素柔软，概率的决策。此类模型具有能够表达许多不同的可能的答案，而不是只有一个相对的确定性，产生更可靠的结果时，这种模型被包括作为较大系统的一个组成部分的优点。', '自然语言处理研究逐渐从词汇语义成分的语义转移，进一步的，叙事的理解。然而人类水平的自然语言处理，是一个人工智能完全问题。它是相当于解决中央的人工智能问题使计算机和人一样聪明，或强大的AI。自然语言处理的未来一般也因此密切结合人工智能发展。<sup><a href=\\\\\"#quote1\\\\\">[1]<\\\\/a><a name=\\\\\"ref_1\\\\\"><\\\\/a><\\\\/sup>', '<b>数据稀疏与平滑技术<\\\\/b>', '大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题，符合经典的zip&amp;apos;f定律。如IBM, Brown：366M英语语料训练trigram，在测试语料中，有14.7%的trigram和2.2%的bigram在训练语料中未出现。', '数据稀疏问题定义：\\\\u201cThe problem of data sparseness, alsoknown as the zero-<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=154825467&amp;ss_c=ssc.citiao.link\\\\\">frequency<\\\\/a> problem ariseswhen analyses contain configurations thatnever occurred in the training corpus. Then it isnot possible to estimate probabilities from observedfrequencies, and some other estimation schemethat can generalize (that configurations) from thetraining data has to be used. \\\\u2014\\\\u2014 Dagan\\\\u201d。', '人们为理论模型实用化而进行了众多尝试与努力，诞生了一系列经典的平滑技术，它们的基本思想是\\\\u201c降低已出现n-gram条件概率分布，以使未出现的n-gram条件概率分布非零\\\\u201d，且经数据平滑后一定保证概率和为1，详细如下：', '加一<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=8721410&amp;ss_c=ssc.citiao.link\\\\\">平滑法<\\\\/a>，又称拉普拉斯定律，其保证每个n-gram在训练语料中至少出现1次，以bigram为例，公式如图：', '<a class=\\\\\"ed_image_link\\\\\" title=\\\\\"点击查看大图\\\\\" href=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/3266/20170622143410-104061414.jpg/0\\\\\" target=\\\\\"_blank\\\\\"><img title=\\\\\"公式\\\\\" alt=\\\\\"公式\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/3266/20170622143410-104061414.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"57\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" /><\\\\/a>', '其中，V是所有bigram的个数。', '其基本思想是利用频率的类别信息对频率进行平滑。调整出现频率为c的n-gram频率为c*：', '<a class=\\\\\"ed_image_link\\\\\" title=\\\\\"点击查看大图\\\\\" href=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/24199/20170622143410-1145620648.jpg/0\\\\\" target=\\\\\"_blank\\\\\"><img title=\\\\\"公式\\\\\" alt=\\\\\"公式\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/24199/20170622143410-1145620648.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"46\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" /><\\\\/a>', '直接的改进策略就是\\\\u201c对出现次数超过某个阈值的gram，不进行平滑，阈值一般取8~10\\\\u201d，其他方法请参见\\\\u201cSimple Good-Turing\\\\u201d。', '不管是Add-one，还是Good Turing平滑技术，对于未出现的n-gram都一视同仁，难免存在不合理（事件发生概率存在差别），所以这里再介绍一种<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=53516710&amp;ss_c=ssc.citiao.link\\\\\">线性插值<\\\\/a>平滑技术，其基本思想是将高阶模型和低阶模型作线性组合，利用低元n-gram模型对<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=64870702&amp;ss_c=ssc.citiao.link\\\\\">高元<\\\\/a>n-gram模型进行线性插值。因为在没有足够的数据对高元n-gram模型进行概率估计时，低元n-gram模型通常可以提供有用的信息。公式如下如右图1：', '<a class=\\\\\"ed_image_link\\\\\" title=\\\\\"点击查看大图\\\\\" href=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/32200/20170622143410-1381408667.jpg/0\\\\\" target=\\\\\"_blank\\\\\"><img title=\\\\\"Interpolation Smoothing\\\\\" alt=\\\\\"Interpolation Smoothing\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/32200/20170622143410-1381408667.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"55\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" /><\\\\/a>', '扩展方式（上下文相关）为如右图2：', '<a class=\\\\\"ed_image_link\\\\\" title=\\\\\"点击查看大图\\\\\" href=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/8518/20160729230636-538071122.jpg/0\\\\\" target=\\\\\"_blank\\\\\"><img title=\\\\\"扩展方式\\\\\" alt=\\\\\"扩展方式\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/8518/20160729230636-538071122.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"56\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" /><\\\\/a>λs可以通过<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=9130286&amp;ss_c=ssc.citiao.link\\\\\">EM算法<\\\\/a>来估计，具体步骤如下：', '<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=527990&amp;ss_c=ssc.citiao.link\\\\\">自动机<\\\\/a> 形式逻辑 统计机器学习汉语语言学 形式语法理论', '<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=7616056&amp;ss_c=ssc.citiao.link\\\\\">语料库<\\\\/a> 词典', '汉字编码词法分析 <a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=68199025&amp;ss_c=ssc.citiao.link\\\\\">句法分析<\\\\/a> 语义分析 文本生成语音识别', '<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=61261936&amp;ss_c=ssc.citiao.link\\\\\">文本分类<\\\\/a>和聚类 信息检索和过滤<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=55417743&amp;ss_c=ssc.citiao.link\\\\\">信息抽取<\\\\/a><a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=54780291&amp;ss_c=ssc.citiao.link\\\\\">问答系统<\\\\/a>拼音汉字转换系统 机器翻译 新信息检测', '虽然上述新趋势给自然语言处理领域带来了成果，但从理论方法的角度看，由于采集、整理、表示和有效应用大量知识的困难，这些系统更依赖于统计学的方法和其他\\\\u201c简单\\\\u201d的方法或技巧。而这些统计学的方法和其他\\\\u201c简单\\\\u201d的方法似乎也快达到它们的极限了，因此，<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=64870621&amp;ss_c=ssc.citiao.link\\\\\">就现在<\\\\/a>而言，在自然语言处理界广泛争论的一个问题便是：要取得新的更大的进展，主要有待于理论上的突破呢，还是可由已有的方法的完善和优化实现？答案还不清楚。大致上，更多的语言学家倾向于前一种意见，而更多的工程师则倾向于后一种意见。回答或许在\\\\u201c中间\\\\u201d，即应将基于知识和推理的深层方法与基于统计等\\\\u201c浅层\\\\u201d方法结合起来。', '自然语言处理的基础是各类自然语言处理<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=276796&amp;ss_c=ssc.citiao.link\\\\\">数据集<\\\\/a>，如tc-corpus-train（<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=7616056&amp;ss_c=ssc.citiao.link\\\\\">语料库<\\\\/a>训练集）、面向文本分类研究的中英文新闻分类语料、以IG卡方等特征词选择方法生成的多维度ARFF格式中文VSM模型、万篇随机抽取论文中文DBLP资源、用于非监督中文分词算法的中文分词词库、UCI评价排序数据、带有初始化说明的情感分析数据集等。', 'OpenNLP是一个基于Java机器学习<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=57704855&amp;ss_c=ssc.citiao.link\\\\\">工具包<\\\\/a>，用于处理自然语言文本。支持大多数常用的 NLP 任务，例如：标识化、句子切分、部分词性标注、名称抽取、组块、解析等。', 'FudanNLP主要是为中文自然语言处理而开发的工具包，也包含为实现这些任务的<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=138836896&amp;ss_c=ssc.citiao.link\\\\\">机器学习算法<\\\\/a>和<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=276796&amp;ss_c=ssc.citiao.link\\\\\">数据集<\\\\/a>。本工具包及其包含数据集使用LGPL3.0许可证。开发语言为Java。', '功能：', '1.<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=61261936&amp;ss_c=ssc.citiao.link\\\\\">文本分类<\\\\/a>新闻聚类', '2. 中文分词 词性标注 实体名识别 关键词抽取 <a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=69649288&amp;ss_c=ssc.citiao.link\\\\\">依存句法<\\\\/a>分析 时间短语识别', '3. 结构化学习 在线学习 层次分类 聚类 精确推理', '<b>语言技术平台（Language Technology Platform，LTP）<\\\\/b>是<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=72861&amp;ss_c=ssc.citiao.link\\\\\">哈工大<\\\\/a><a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=58337644&amp;ss_c=ssc.citiao.link\\\\\">社会计算<\\\\/a>与信息检索研究中心历时十年开发的一整套中文<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=7557009&amp;ss_c=ssc.citiao.link\\\\\">语言处理系统<\\\\/a>。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=154949709&amp;ss_c=ssc.citiao.link\\\\\">动态链接库<\\\\/a>（Dynamic Link Library, DLL）的<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=350170&amp;ss_c=ssc.citiao.link\\\\\">应用程序接口<\\\\/a>，<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=58488477&amp;ss_c=ssc.citiao.link\\\\\">可视化工具<\\\\/a>，并且能够以网络服务（Web Service）的形式进行使用。', '在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。', '许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。', '自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析(Parse)出多棵剖析树(Parse Tree)，而我们必须要仰赖语意及前后文的信息才能在其中选择一棵最为适合的剖析树。', '例如<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=101981285&amp;ss_c=ssc.citiao.link\\\\\">语音处理<\\\\/a>时遇到外国口音或地方口音,或者在文本的处理中处理拼写,语法或者<a class=\\\\\"ed_inner_link\\\\\" target=\\\\\"_blank\\\\\" href=\\\\\"/lemma/ShowInnerLink.htm?lemmaId=609092&amp;ss_c=ssc.citiao.link\\\\\">光学字符识别<\\\\/a>(OCR)的错误。', '句子常常并不只是字面上的意思；例如，\\\\u201c你能把盐递过来吗\\\\u201d，一个好的回答应当是把盐递过去；在大多数上下文环境中，\\\\u201c能\\\\u201d将是糟糕的回答，虽说回答\\\\u201c不\\\\u201d或者\\\\u201c太远了我拿不到\\\\u201d也是可以接受的。再者，如果一门课程上一年没开设，对于提问\\\\u201c这门课程去年有多少学生没通过？\\\\u201d回答\\\\u201c去年没开这门课\\\\u201d要比回答\\\\u201c没人没通过\\\\u201d好。']\n    \n\n\n```python\nlist(map(lambda x:re.sub(\"<a .*?>|<\\\\\\/[ab]>\", \"\",x), contents))\n```\n\n\n\n\n    ['<img title=\\\\\"自然语言处理\\\\\" alt=\\\\\"自然语言处理\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/4169/cut-20180605223503-1613411374_jpg_473_355_11578.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"165\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" />语言是人类区别其他动物的本质特性。在所有生物中，只有人类才具有语言能力。人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，它也是人工智能的一个重要，甚至核心部分。',\n     '用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言；人们也可通过它进一步了解人类的语言能力和智能的机制。',\n     '实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。历史上对自然语言理解研究得较多，而对自然语言生成研究得较少。但这种状况已有所改变。',\n     '无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。',\n     '自然语言处理，即实现人机间自然语言通信，或实现自然语言理解和自然语言生成是十分困难的。造成困难的根本原因是自然语言文本和对话的各个层次上广泛存在的各种各样的歧义性或多义性（ambiguity）。',\n     '一个中文文本从形式上看是由汉字（包括标点符号等）组成的一个字符串。由字可组成词，由词可组成词组，由词组可组成句子，进而由一些句子组成段、节、章、篇。无论在上述的各种层次：字（符）、词、词组、句子、段，\\\\u2026\\\\u2026还是在下一层次向上一层次转变中都存在着歧义和多义现象，即形式上一样的一段字符串，在不同的场景或不同的语境下，可以理解成不同的词串、词组串等，并有不同的意义。一般情况下，它们中的大多数都是可以根据相应的语境和场景的规定而得到解决的。也就是说，从总体上说，并不存在歧义。这也就是我们平时并不感到自然语言歧义，和能用自然语言进行正确交流的原因。但是一方面，我们也看到，为了消解歧义，是需要极其大量的知识和进行推理的。如何将这些知识较完整地加以收集和整理出来；又如何找到合适的形式，将它们存入计算机系统中去；以及如何有效地利用它们来消除歧义，都是工作量极大且十分困难的工作。这不是少数人短时期内可以完成的，还有待长期的、系统的工作。',\n     '以上说的是，一个中文文本或一个汉字（含标点符号等）串可能有多个含义。它是自然语言理解中的主要困难和障碍。反过来，一个相同或相近的意义同样可以用多个中文文本或多个汉字串来表示。',\n     '因此，自然语言的形式（字符串）与其意义之间是一种多对多的关系。其实这也正是自然语言的魅力所在。但从计算机处理的角度看，我们必须消除歧义，而且有人认为它正是自然语言理解中的中心问题，即要把带有潜在歧义的自然语言输入转换成某种无歧义的计算机内部表示。',\n     '歧义现象的广泛存在使得消除它们需要大量的知识和推理，这就给基于语言学的方法、基于知识的方法带来了巨大的困难，因而以这些方法为主流的自然语言处理研究几十年来一方面在理论和方法方面取得了很多成就，但在能处理大规模真实文本的系统研制方面，成绩并不显著。研制的一些系统大多数是小规模的、研究性的演示系统。',\n     '目前存在的问题有两个方面：一方面，迄今为止的语法都限于分析一个孤立的句子，上下文关系和谈话环境对本句的约束和影响还缺乏系统的研究，因此分析歧义、词语省略、代词所指、同一句话在不同场合或由不同的人说出来所具有的不同含义等问题，尚无明确规律可循，需要加强语用学的研究才能逐步解决。另一方面，人理解一个句子不是单凭语法，还运用了大量的有关知识，包括生活知识和专门知识，这些知识无法全部贮存在计算机里。因此一个书面理解系统只能建立在有限的词汇、句型和特定的主题范围内；计算机的贮存量和运转速度大大提高之后，才有可能适当扩大范围.',\n     '以上存在的问题成为自然语言理解在机器翻译应用中的主要难题，这也就是当今机器翻译系统的译文质量离理想目标仍相差甚远的原因之一；而译文质量是机译系统成败的关键。中国数学家、语言学家周海中教授曾在经典论文《机器翻译五十年》中指出：要提高机译的质量，首先要解决的是语言本身问题而不是程序设计问题；单靠若干程序来做机译系统，肯定是无法提高机译质量的；另外在人类尚未明了大脑是如何进行语言的模糊识别和逻辑判断的情况下，机译要想达到\\\\u201c信、达、雅\\\\u201d的程度是不可能的。',\n     '最早的自然语言理解方面的研究工作是机器翻译。1949年，美国人威弗首先提出了机器翻译设计方案。20世纪60年代，国外对机器翻译曾有大规模的研究工作，耗费了巨额费用，但人们当时显然是低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展不大。主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。',\n     '大约90年代开始，自然语言处理领域发生了巨大的变化。这种变化的两个明显的特征是：',\n     '（1）对系统输入，要求研制的自然语言处理系统能处理大规模的真实文本，而不是如以前的研究性系统那样，只能处理很少的词条和典型句子。只有这样，研制的系统才有真正的实用价值。',\n     '（2）对系统的输出，鉴于真实地理解自然语言是十分困难的，对系统并不要求能对自然语言文本进行深层的理解，但要能从中抽取有用的信息。例如，对自然语言文本进行自动地提取索引词，过滤，检索，自动提取重要信息，进行自动摘要等等。',\n     '同时，由于强调了\\\\u201c大规模\\\\u201d，强调了\\\\u201c真实文本\\\\u201d，下面两方面的基础性工作也得到了重视和加强。',\n     '（1）大规模真实语料库的研制。大规模的经过不同深度加工的真实文本的语料库，是研究自然语言统计性质的基础。没有它们，统计方法只能是无源之水。',\n     '（2）大规模、信息丰富的词典的编制工作。规模为几万，十几万，甚至几十万词，含有丰富的信息（如包含词的搭配信息）的计算机可用词典对自然语言处理的重要性是很明显的。',\n     '自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。',\n     '现代NLP算法是基于机器学习，特别是统计机器学习。机器学习范式是不同于一般之前的尝试语言处理。语言处理任务的实现，通常涉及直接用手的大套规则编码。',\n     '许多不同类的机器学习算法已应用于自然语言处理任务。这些算法的输入是一大组从输入数据生成的\\\\u201c特征\\\\u201d。一些最早使用的算法，如决策树，产生硬的if-then规则类似于手写的规则，是再普通的系统体系。然而，越来越多的研究集中于统计模型，这使得基于附加实数值的权重，每个输入要素柔软，概率的决策。此类模型具有能够表达许多不同的可能的答案，而不是只有一个相对的确定性，产生更可靠的结果时，这种模型被包括作为较大系统的一个组成部分的优点。',\n     '自然语言处理研究逐渐从词汇语义成分的语义转移，进一步的，叙事的理解。然而人类水平的自然语言处理，是一个人工智能完全问题。它是相当于解决中央的人工智能问题使计算机和人一样聪明，或强大的AI。自然语言处理的未来一般也因此密切结合人工智能发展。<sup>[1]<\\\\/sup>',\n     '<b>数据稀疏与平滑技术',\n     '大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题，符合经典的zip&amp;apos;f定律。如IBM, Brown：366M英语语料训练trigram，在测试语料中，有14.7%的trigram和2.2%的bigram在训练语料中未出现。',\n     '数据稀疏问题定义：\\\\u201cThe problem of data sparseness, alsoknown as the zero-frequency problem ariseswhen analyses contain configurations thatnever occurred in the training corpus. Then it isnot possible to estimate probabilities from observedfrequencies, and some other estimation schemethat can generalize (that configurations) from thetraining data has to be used. \\\\u2014\\\\u2014 Dagan\\\\u201d。',\n     '人们为理论模型实用化而进行了众多尝试与努力，诞生了一系列经典的平滑技术，它们的基本思想是\\\\u201c降低已出现n-gram条件概率分布，以使未出现的n-gram条件概率分布非零\\\\u201d，且经数据平滑后一定保证概率和为1，详细如下：',\n     '加一平滑法，又称拉普拉斯定律，其保证每个n-gram在训练语料中至少出现1次，以bigram为例，公式如图：',\n     '<img title=\\\\\"公式\\\\\" alt=\\\\\"公式\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/3266/20170622143410-104061414.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"57\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" />',\n     '其中，V是所有bigram的个数。',\n     '其基本思想是利用频率的类别信息对频率进行平滑。调整出现频率为c的n-gram频率为c*：',\n     '<img title=\\\\\"公式\\\\\" alt=\\\\\"公式\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/24199/20170622143410-1145620648.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"46\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" />',\n     '直接的改进策略就是\\\\u201c对出现次数超过某个阈值的gram，不进行平滑，阈值一般取8~10\\\\u201d，其他方法请参见\\\\u201cSimple Good-Turing\\\\u201d。',\n     '不管是Add-one，还是Good Turing平滑技术，对于未出现的n-gram都一视同仁，难免存在不合理（事件发生概率存在差别），所以这里再介绍一种线性插值平滑技术，其基本思想是将高阶模型和低阶模型作线性组合，利用低元n-gram模型对高元n-gram模型进行线性插值。因为在没有足够的数据对高元n-gram模型进行概率估计时，低元n-gram模型通常可以提供有用的信息。公式如下如右图1：',\n     '<img title=\\\\\"Interpolation Smoothing\\\\\" alt=\\\\\"Interpolation Smoothing\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/32200/20170622143410-1381408667.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"55\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" />',\n     '扩展方式（上下文相关）为如右图2：',\n     '<img title=\\\\\"扩展方式\\\\\" alt=\\\\\"扩展方式\\\\\" src=\\\\\"https://pic.baike.soso.com/ugc/baikepic2/8518/20160729230636-538071122.jpg/300\\\\\" width=\\\\\"220\\\\\" height=\\\\\"56\\\\\" class=\\\\\"ed_imgfloat_right\\\\\" mark=\\\\\"\\\\\" style=\\\\\"\\\\\" />λs可以通过EM算法来估计，具体步骤如下：',\n     '自动机 形式逻辑 统计机器学习汉语语言学 形式语法理论',\n     '语料库 词典',\n     '汉字编码词法分析 句法分析 语义分析 文本生成语音识别',\n     '文本分类和聚类 信息检索和过滤信息抽取问答系统拼音汉字转换系统 机器翻译 新信息检测',\n     '虽然上述新趋势给自然语言处理领域带来了成果，但从理论方法的角度看，由于采集、整理、表示和有效应用大量知识的困难，这些系统更依赖于统计学的方法和其他\\\\u201c简单\\\\u201d的方法或技巧。而这些统计学的方法和其他\\\\u201c简单\\\\u201d的方法似乎也快达到它们的极限了，因此，就现在而言，在自然语言处理界广泛争论的一个问题便是：要取得新的更大的进展，主要有待于理论上的突破呢，还是可由已有的方法的完善和优化实现？答案还不清楚。大致上，更多的语言学家倾向于前一种意见，而更多的工程师则倾向于后一种意见。回答或许在\\\\u201c中间\\\\u201d，即应将基于知识和推理的深层方法与基于统计等\\\\u201c浅层\\\\u201d方法结合起来。',\n     '自然语言处理的基础是各类自然语言处理数据集，如tc-corpus-train（语料库训练集）、面向文本分类研究的中英文新闻分类语料、以IG卡方等特征词选择方法生成的多维度ARFF格式中文VSM模型、万篇随机抽取论文中文DBLP资源、用于非监督中文分词算法的中文分词词库、UCI评价排序数据、带有初始化说明的情感分析数据集等。',\n     'OpenNLP是一个基于Java机器学习工具包，用于处理自然语言文本。支持大多数常用的 NLP 任务，例如：标识化、句子切分、部分词性标注、名称抽取、组块、解析等。',\n     'FudanNLP主要是为中文自然语言处理而开发的工具包，也包含为实现这些任务的机器学习算法和数据集。本工具包及其包含数据集使用LGPL3.0许可证。开发语言为Java。',\n     '功能：',\n     '1.文本分类新闻聚类',\n     '2. 中文分词 词性标注 实体名识别 关键词抽取 依存句法分析 时间短语识别',\n     '3. 结构化学习 在线学习 层次分类 聚类 精确推理',\n     '<b>语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年开发的一整套中文语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口，可视化工具，并且能够以网络服务（Web Service）的形式进行使用。',\n     '在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。',\n     '许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。',\n     '自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析(Parse)出多棵剖析树(Parse Tree)，而我们必须要仰赖语意及前后文的信息才能在其中选择一棵最为适合的剖析树。',\n     '例如语音处理时遇到外国口音或地方口音,或者在文本的处理中处理拼写,语法或者光学字符识别(OCR)的错误。',\n     '句子常常并不只是字面上的意思；例如，\\\\u201c你能把盐递过来吗\\\\u201d，一个好的回答应当是把盐递过去；在大多数上下文环境中，\\\\u201c能\\\\u201d将是糟糕的回答，虽说回答\\\\u201c不\\\\u201d或者\\\\u201c太远了我拿不到\\\\u201d也是可以接受的。再者，如果一门课程上一年没开设，对于提问\\\\u201c这门课程去年有多少学生没通过？\\\\u201d回答\\\\u201c去年没开这门课\\\\u201d要比回答\\\\u201c没人没通过\\\\u201d好。']\n\n\n\n![](../img/xiniu_neteasy.png)\n\n\n```python\n\n```\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson1/2.regular_expression/","content":"\n## Python正则表达式\n\n\n正则表达式是**处理字符串**的强大工具，拥有独特的语法和独立的处理引擎。<br>\n\n我们在大文本中匹配字符串时，有些情况用str自带的函数(比如index, find, in)可能可以完成，有些情况会稍稍复杂一些(比如说找出所有“像邮箱”的字符串，所有和xiniuedu/netease相关的句子)，这个时候我们需要一个某种模式的工具，这个时候**正则表达式**就派上用场了。<br>\n\n自然语言处理的各种模型和算法要发挥作用离不开数据，离不开“干净”的数据，而现实生活中的数据形态和干净程度不一，我们经常要做一些数据清洗和信息抽取的工作，这时候正则表达式就可以发挥及其强大的匹配功能了。说起来，正则表达式是一套引擎，并不是Python语言独有的功能或者工具库，下面我们就来了解一下正则表达式吧。\n\n## 1.学习与验证工具\n\n我们最喜爱的正则表达式在线验证工具之一是http://regexr.com/ ，大家可以在线的方式学习与验证正则表达式的对错，左边还有对应的工具和速查表。\n\n![](../img/L1_re.png)\n\n## 2.正则表达式语法\n\n\n下面是一张有些同学比较熟的图，我们俗称python正则表达式小抄，大家可以遵循一般字符、预定义字符集、数量词、边界匹配、逻辑分组 和 特殊构造的逻辑去逐步学习。\n\n当你要匹配 **一个/多个/任意个 数字/字母/非数字/非字母/某几个字符/任意字符**，想要 **贪婪/非贪婪** 匹配，想要捕获匹配出来的 **第一个/所有** 内容的时候，记得这里有个小手册供你参考。\n\n![](http://life.chinaunix.net/bbsfile/forum/month_1012/101218124873e7f28d80d99801.jpg)\n\n## 3.挑战与提升\n\n长期做自然语言处理的同学正则表达式都非常熟，在若干年前硕士刚毕业时曾有一段时间写了大量的正则表达式，以至于同事间开玩笑说，只要是符合某种规律或者模式的串，肯定分分钟能匹配出来。正则表达式属于“短时间内习得且受益终身的技能”\n\n对于想练习正则表达式，或者短期内快速get复杂技能，or想挑战更复杂的正则表达式的同学们。\n请戳[正则表达式进阶练习](https://alf.nu/RegexGolf)\n\n各位宝宝enjoy yourself\n\n\n![](http://ml.xiniuedu.com/regext_2.png)\n\n## 4.Python案例\n\n\n### re模块\nPython通过re模块提供对正则表达式的支持。\n\n使用re的一般步骤是\n* 1.将正则表达式的字符串形式编译为Pattern实例\n* 2.使用Pattern实例处理文本并获得匹配结果（一个Match实例）\n* 3.使用Match实例获得信息，进行其他的操作。\n\n\n```python\n# encoding: UTF-8\nimport re\n \n# 将正则表达式编译成Pattern对象\npattern = re.compile(r'hello.*\\!')\n \n# 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None\nmatch = pattern.match('hello, pastor! How are you?')\n \nif match:\n    # 使用Match获得分组信息\n    print match.group()\n```\n\n    hello, pastor!\n    \n\n#### re.compile(strPattern[, flag]):\n\n这个方法是Pattern类的工厂方法，用于将字符串形式的正则表达式编译为Pattern对象。 \n\n第二个参数flag是匹配模式，取值可以使用按位或运算符'|'表示同时生效，比如re.I | re.M。\n\n当然，你也可以在regex字符串中指定模式，比如**re.compile('pattern', re.I | re.M)**等价于**re.compile('(?im)pattern')** \n\nflag可选值有：\n\n* re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同）\n* re.M(MULTILINE): 多行模式，改变'^'和'$'的行为（参见上图）\n* re.S(DOTALL): 点任意匹配模式，改变'.'的行为\n* re.L(LOCALE): 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定\n* re.U(UNICODE): 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性\n* re.X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。以下两个正则表达式是等价的：\n\n\n```python\nregex_1 = re.compile(r\"\"\"\\d +  # 数字部分\n                         \\.    # 小数点部分\n                         \\d *  # 小数的数字部分\"\"\", re.X)\nregex_2 = re.compile(r\"\\d+\\.\\d*\")\n```\n\n### Match\n\nMatch对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息。\n\n#### match属性：\n\n* string: 匹配时使用的文本。\n* re: 匹配时使用的Pattern对象。\n* pos: 文本中正则表达式开始搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。\n* endpos: 文本中正则表达式结束搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。\n* lastindex: 最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。\n* lastgroup: 最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None。\n\n#### 方法：\n\n* group([group1, …]): <br>\n获得一个或多个分组截获的字符串；指定多个参数时将以元组形式返回。group1可以使用编号也可以使用别名；编号0代表整个匹配的子串；不填写参数时，返回group(0)；没有截获字符串的组返回None；截获了多次的组返回最后一次截获的子串。\n* groups([default]): <br>\n以元组形式返回全部分组截获的字符串。相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。\n* groupdict([default]): <br>\n返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。\n* start([group]): <br>\n返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0。\n* end([group]): <br>\n返回指定的组截获的子串在string中的结束索引（子串最后一个字符的索引+1）。group默认值为0。\n* span([group]): <br>\n返回(start(group), end(group))。\n* expand(template): <br>\n将匹配到的分组代入template中然后返回。template中可以使用\\id或\\g<id>、\\g<name>引用分组，但不能使用编号0。\\id与\\g<id>是等价的；但\\10将被认为是第10个分组，如果你想表达\\1之后是字符'0'，只能使用\\g<1>0。\n\n\n```python\nimport re\nm = re.match(r'(\\w+) (\\w+)(?P<sign>.*)', 'hello pastor!')\n \nprint \"m.string:\", m.string\nprint \"m.re:\", m.re\nprint \"m.pos:\", m.pos\nprint \"m.endpos:\", m.endpos\nprint \"m.lastindex:\", m.lastindex\nprint \"m.lastgroup:\", m.lastgroup\n \nprint \"m.group(1,2):\", m.group(1, 2)\nprint \"m.groups():\", m.groups()\nprint \"m.groupdict():\", m.groupdict()\nprint \"m.start(2):\", m.start(2)\nprint \"m.end(2):\", m.end(2)\nprint \"m.span(2):\", m.span(2)\nprint r\"m.expand(r'\\2 \\1\\3'):\", m.expand(r'\\2 \\1\\3')\n```\n\n    m.string: hello pastor!\n    m.re: <_sre.SRE_Pattern object at 0x10b111be0>\n    m.pos: 0\n    m.endpos: 18\n    m.lastindex: 3\n    m.lastgroup: sign\n    m.group(1,2): ('hello', 'pastor')\n    m.groups(): ('hello', 'pastor', '!')\n    m.groupdict(): {'sign': '!'}\n    m.start(2): 6\n    m.end(2): 17\n    m.span(2): (6, 17)\n    m.expand(r'\\2 \\1\\3'): pastor hello!\n    \n\n### Pattern\n\nPattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。\n\nPattern不能直接实例化，必须使用re.compile()进行构造。\n\nPattern提供了几个可读属性用于获取表达式的相关信息：\n* pattern: 编译时用的表达式字符串。\n* flags: 编译时用的匹配模式。数字形式。\n* groups: 表达式中分组的数量。\n* groupindex: 以表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。\n\n\n```python\nimport re\np = re.compile(r'(\\w+) (\\w+)(?P<sign>.*)', re.DOTALL)\n \nprint \"p.pattern:\", p.pattern\nprint \"p.flags:\", p.flags\nprint \"p.groups:\", p.groups\nprint \"p.groupindex:\", p.groupindex\n```\n\n    p.pattern: (\\w+) (\\w+)(?P<sign>.*)\n    p.flags: 16\n    p.groups: 3\n    p.groupindex: {'sign': 3}\n    \n\n### 使用pattern\n\n* **match(string[, pos[, endpos]]) | re.match(pattern, string[, flags])**: <br>\n**这个方法将从string的pos下标处起尝试匹配pattern**:\n    * 如果pattern结束时仍可匹配，则返回一个Match对象\n    * 如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。 \n    * pos和endpos的默认值分别为0和len(string)。 <br>\n    **注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符'$'。 **\n\n\n* **search(string[, pos[, endpos]]) | re.search(pattern, string[, flags])**: <br>\n**这个方法从string的pos下标处起尝试匹配pattern**\n    * 如果pattern结束时仍可匹配，则返回一个Match对象\n    * 若无法匹配，则将pos加1后重新尝试匹配，直到pos=endpos时仍无法匹配则返回None。 \n    * pos和endpos的默认值分别为0和len(string))\n\n\n```python\n# encoding: UTF-8 \nimport re \n \n# 将正则表达式编译成Pattern对象 \npattern = re.compile(r'H.*g') \n \n# 使用search()查找匹配的子串，不存在能匹配的子串时将返回None \n# 这个例子中使用match()无法成功匹配 \nmatch = pattern.search('hello pastor!') \n \nif match: \n    # 使用Match获得分组信息 \n    print match.group() \n```\n\n    pastor\n    \n\n* **split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]):** \n    * 按照能够匹配的子串将string分割后返回列表。\n    * maxsplit用于指定最大分割次数，不指定将全部分割。 \n\n\n```python\nimport re\n \np = re.compile(r'\\d+')\nprint p.split('one1two2three3four4')\n```\n\n    ['one', 'two', 'three', 'four', '']\n    \n\n* **findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags])**: \n    * 搜索string，以列表形式返回全部能匹配的子串。\n\n\n```python\nimport re\n \np = re.compile(r'\\d+')\nprint p.findall('one1two2three3four4')\n```\n\n    ['1', '2', '3', '4']\n    \n\n* **finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]): **\n    * 搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器。 \n\n\n```python\nimport re\n \np = re.compile(r'\\d+')\nfor m in p.finditer('one1two2three3four4'):\n    print m.group()\n```\n\n    1\n    2\n    3\n    4\n    \n\n* **sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]): **\n    * 使用repl替换string中每一个匹配的子串后返回替换后的字符串。 \n        * 当repl是一个字符串时，可以使用\\id或\\g<id>、\\g<name>引用分组，但不能使用编号0。 \n        * 当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。 \ncount用于指定最多替换次数，不指定时全部替换。\n\n\n```python\nimport re\n \np = re.compile(r'(\\w+) (\\w+)')\ns = 'i say, hello pastor!'\n \nprint p.sub(r'\\2 \\1', s)\n \ndef func(m):\n    return m.group(1).title() + ' ' + m.group(2).title()\n \nprint p.sub(func, s)\n```\n\n    say i, pastor hello!\n    I Say, Hello pastor!\n    \n\n* **subn(repl, string[, count]) |re.sub(pattern, repl, string[, count]): **\n    * 返回 (sub(repl, string[, count]), 替换次数)。\n\n\n```python\nimport re\n \np = re.compile(r'(\\w+) (\\w+)')\ns = 'i say, hello pastor!'\n \nprint p.subn(r'\\2 \\1', s)\n \ndef func(m):\n    return m.group(1).title() + ' ' + m.group(2).title()\n \nprint p.subn(func, s)\n```\n\n    ('say i, pastor hello!', 2)\n    ('I Say, Hello pastor!', 2)\n    \n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson1/1.basic_text_processing/","content":"\n# 基本文本处理操作\n\n\nNLP处理的对象是文本字符串内容，大家需要熟悉一些基本的文本字符串操作，这里以python为例，帮大家复习以下的中英文字符串操作：\n* 替换\n* 截取\n* 复制\n* 连接\n* 分割\n* 排序\n* 比较\n* 查找\n* 包含\n* 大小写转换\n\n### 1.清理与替换\n\n\n\n```python\nen_str = \" hello world, hello, my name is pastor! \"\n```\n\n\n```python\n# 去空格及特殊符号  \nen_str.strip().lstrip().rstrip(',') \n```\n\n\n```python\n# 字符串替换\nen_str.replace('hello', 'hi')\n```\n\n\n\n\n    'hi world, hi, my name is pastor!'\n\n\n\n\n```python\nzh_str = \" 大家好，我叫陆超 \"\n```\n\n\n```python\n# 去空格及特殊符号  \nzh_str.strip().lstrip().rstrip(',') \n```\n\n\n```python\n# 字符串替换\nzh_str.strip().replace('ttt', 'xxx')\n```\n\n\n\n\n    ' 大家好，我叫xxx '\n\n\n\n\n```python\n# 删除\nzh_str.strip().replace('大家好，', '')\n```\n\n### 2.截取\n\n\n\n```python\nmy_str = \"大家好，我是李雪琴，我在北京大学，你吃饭没呢？\"\n```\n\n\n```python\n# 从左往右index从0开始，可以用index进行切片(左闭右开)\nmy_str[0:3]\n```\n\n\n\n\n    '大家好'\n\n\n\n\n```python\n# 从左往右index从0开始，可以用index进行切片(左闭右开)\nmy_str[4:4+5]\n```\n\n\n\n\n    '我是李雪琴'\n\n\n\n\n```python\n# 从右往左index从-1开始，可以用index进行切片(左闭右开)\nmy_str[-1-5:-1]\n```\n\n\n\n\n    '你吃饭没呢'\n\n\n\n\n```python\n# 间隔截取\nmy_str[::2]\n```\n\n\n\n\n    '大好我李琴我北大，吃没？'\n\n\n\n\n```python\n# 翻转\nmy_str[::-1]\n```\n\n\n\n\n    '？呢没饭吃你，学大京北在我，琴雪李是我，好家大'\n\n\n\n### 3.连接与分割\n\n\n\n```python\nstr1 = \"大家好，我是陆超，真好！\"\nstr2 = \"大家好，我是李雪琴，你吃饭没呢？\"\nstr1+str2\n```\n\n\n\n\n    '大家好，我是陆超，真好！大家好，我是李雪琴，你吃饭没呢？'\n\n\n\n\n```python\n# 通过join的方式连接\nstrs = ['我是陆超', \"我是李雪琴\", \"我是xxx，好high哟,感觉人生已经达到了高潮,感觉人生已经达到了巅峰\"]\n\"；\".join(strs)\n```\n\n\n\n\n    '我是陆超；我是李雪琴；我是xxx，好high哟,感觉人生已经达到了高潮,感觉人生已经达到了巅峰'\n\n\n\n\n```python\n# 通过split的方式切分\ntmp_str = \"我是陆超；我是李雪琴；我是毛毛姐，好high哟,感觉人生已经达到了高潮,感觉人生已经达到了巅峰\"\ntmp_str.split(\"；\")\n```\n\n\n\n\n    ['我是陆超', '我是李雪琴', '我是毛毛姐，好high哟,感觉人生已经达到了高潮,感觉人生已经达到了巅峰']\n\n\n\n### 4.比较与排序\n\n\n\n```python\nen_strs = ['ABc', 'aCd', 'CdE', 'xYz']\n```\n\n\n```python\n# 以字母序排列，注意是以返回值形态返回排序结果，不改变原list\nsorted(en_strs)\n```\n\n\n\n\n    ['ABc', 'CdE', 'aCd', 'xYz']\n\n\n\n\n```python\n# 自定义排序方式\ndef sort_fun(x):\n    return x[1].lower()\n\nsorted(en_strs, key=sort_fun)\n```\n\n\n\n\n    ['ABc', 'aCd', 'CdE', 'xYz']\n\n\n\n\n```python\nsorted(en_strs, key=lambda x:x[2].lower())\n```\n\n\n\n\n    ['ABc', 'aCd', 'CdE', 'xYz']\n\n\n\n### 查找与包含\n\n\n\n```python\n# 查找可以用index和find\nzh_str = \"我是陆超；我是李雪琴；我是毛毛姐，好high哟,感觉人生已经达到了高潮,感觉人生已经达到了巅峰\"\n```\n\n\n```python\nzh_str.index(\"陆超\")\n```\n\n\n\n\n    2\n\n\n\n\n```python\nzh_str.index(\"毛毛姐\")\n```\n\n\n\n\n    13\n\n\n\n\n```python\n# zh_str.index(\"来了老弟\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    ValueError                                Traceback (most recent call last)\n\n    <ipython-input-29-fa2c269b272d> in <module>()\n    ----> 1 zh_str.index(\"来了老弟\")\n    \n\n    ValueError: substring not found\n\n\n\n```python\nzh_str.find(\"毛毛姐\")\n```\n\n\n\n\n    13\n\n\n\n\n```python\nzh_str.find(\"来了老弟\")\n```\n\n\n\n\n    -1\n\n\n\n### 大小写与其他变化\n\n\n\n```python\nen_str = 'hello, my name is Patrick'\n```\n\n\n```python\nen_str.lower()\n```\n\n\n\n\n    'hello, my name is patrick'\n\n\n\n\n```python\nen_str.upper()\n```\n\n\n\n\n    'HELLO, MY NAME IS PATRICK'\n\n\n\n\n```python\nen_str.capitalize()\n```\n\n\n\n\n    'Hello, my name is patrick'\n\n\n\n\n```python\nhelp(str)\n```\n\n    Help on class str in module builtins:\n    \n    class str(object)\n     |  str(object='') -> str\n     |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n     |  \n     |  Create a new string object from the given object. If encoding or\n     |  errors is specified, then the object must expose a data buffer\n     |  that will be decoded using the given encoding and error handler.\n     |  Otherwise, returns the result of object.__str__() (if defined)\n     |  or repr(object).\n     |  encoding defaults to sys.getdefaultencoding().\n     |  errors defaults to 'strict'.\n     |  \n     |  Methods defined here:\n     |  \n     |  __add__(self, value, /)\n     |      Return self+value.\n     |  \n     |  __contains__(self, key, /)\n     |      Return key in self.\n     |  \n     |  __eq__(self, value, /)\n     |      Return self==value.\n     |  \n     |  __format__(...)\n     |      S.__format__(format_spec) -> str\n     |      \n     |      Return a formatted version of S as described by format_spec.\n     |  \n     |  __ge__(self, value, /)\n     |      Return self>=value.\n     |  \n     |  __getattribute__(self, name, /)\n     |      Return getattr(self, name).\n     |  \n     |  __getitem__(self, key, /)\n     |      Return self[key].\n     |  \n     |  __getnewargs__(...)\n     |  \n     |  __gt__(self, value, /)\n     |      Return self>value.\n     |  \n     |  __hash__(self, /)\n     |      Return hash(self).\n     |  \n     |  __iter__(self, /)\n     |      Implement iter(self).\n     |  \n     |  __le__(self, value, /)\n     |      Return self<=value.\n     |  \n     |  __len__(self, /)\n     |      Return len(self).\n     |  \n     |  __lt__(self, value, /)\n     |      Return self<value.\n     |  \n     |  __mod__(self, value, /)\n     |      Return self%value.\n     |  \n     |  __mul__(self, value, /)\n     |      Return self*value.n\n     |  \n     |  __ne__(self, value, /)\n     |      Return self!=value.\n     |  \n     |  __new__(*args, **kwargs) from builtins.type\n     |      Create and return a new object.  See help(type) for accurate signature.\n     |  \n     |  __repr__(self, /)\n     |      Return repr(self).\n     |  \n     |  __rmod__(self, value, /)\n     |      Return value%self.\n     |  \n     |  __rmul__(self, value, /)\n     |      Return self*value.\n     |  \n     |  __sizeof__(...)\n     |      S.__sizeof__() -> size of S in memory, in bytes\n     |  \n     |  __str__(self, /)\n     |      Return str(self).\n     |  \n     |  capitalize(...)\n     |      S.capitalize() -> str\n     |      \n     |      Return a capitalized version of S, i.e. make the first character\n     |      have upper case and the rest lower case.\n     |  \n     |  casefold(...)\n     |      S.casefold() -> str\n     |      \n     |      Return a version of S suitable for caseless comparisons.\n     |  \n     |  center(...)\n     |      S.center(width[, fillchar]) -> str\n     |      \n     |      Return S centered in a string of length width. Padding is\n     |      done using the specified fill character (default is a space)\n     |  \n     |  count(...)\n     |      S.count(sub[, start[, end]]) -> int\n     |      \n     |      Return the number of non-overlapping occurrences of substring sub in\n     |      string S[start:end].  Optional arguments start and end are\n     |      interpreted as in slice notation.\n     |  \n     |  encode(...)\n     |      S.encode(encoding='utf-8', errors='strict') -> bytes\n     |      \n     |      Encode S using the codec registered for encoding. Default encoding\n     |      is 'utf-8'. errors may be given to set a different error\n     |      handling scheme. Default is 'strict' meaning that encoding errors raise\n     |      a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and\n     |      'xmlcharrefreplace' as well as any other name registered with\n     |      codecs.register_error that can handle UnicodeEncodeErrors.\n     |  \n     |  endswith(...)\n     |      S.endswith(suffix[, start[, end]]) -> bool\n     |      \n     |      Return True if S ends with the specified suffix, False otherwise.\n     |      With optional start, test S beginning at that position.\n     |      With optional end, stop comparing S at that position.\n     |      suffix can also be a tuple of strings to try.\n     |  \n     |  expandtabs(...)\n     |      S.expandtabs(tabsize=8) -> str\n     |      \n     |      Return a copy of S where all tab characters are expanded using spaces.\n     |      If tabsize is not given, a tab size of 8 characters is assumed.\n     |  \n     |  find(...)\n     |      S.find(sub[, start[, end]]) -> int\n     |      \n     |      Return the lowest index in S where substring sub is found,\n     |      such that sub is contained within S[start:end].  Optional\n     |      arguments start and end are interpreted as in slice notation.\n     |      \n     |      Return -1 on failure.\n     |  \n     |  format(...)\n     |      S.format(*args, **kwargs) -> str\n     |      \n     |      Return a formatted version of S, using substitutions from args and kwargs.\n     |      The substitutions are identified by braces ('{' and '}').\n     |  \n     |  format_map(...)\n     |      S.format_map(mapping) -> str\n     |      \n     |      Return a formatted version of S, using substitutions from mapping.\n     |      The substitutions are identified by braces ('{' and '}').\n     |  \n     |  index(...)\n     |      S.index(sub[, start[, end]]) -> int\n     |      \n     |      Like S.find() but raise ValueError when the substring is not found.\n     |  \n     |  isalnum(...)\n     |      S.isalnum() -> bool\n     |      \n     |      Return True if all characters in S are alphanumeric\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  isalpha(...)\n     |      S.isalpha() -> bool\n     |      \n     |      Return True if all characters in S are alphabetic\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  isdecimal(...)\n     |      S.isdecimal() -> bool\n     |      \n     |      Return True if there are only decimal characters in S,\n     |      False otherwise.\n     |  \n     |  isdigit(...)\n     |      S.isdigit() -> bool\n     |      \n     |      Return True if all characters in S are digits\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  isidentifier(...)\n     |      S.isidentifier() -> bool\n     |      \n     |      Return True if S is a valid identifier according\n     |      to the language definition.\n     |      \n     |      Use keyword.iskeyword() to test for reserved identifiers\n     |      such as \"def\" and \"class\".\n     |  \n     |  islower(...)\n     |      S.islower() -> bool\n     |      \n     |      Return True if all cased characters in S are lowercase and there is\n     |      at least one cased character in S, False otherwise.\n     |  \n     |  isnumeric(...)\n     |      S.isnumeric() -> bool\n     |      \n     |      Return True if there are only numeric characters in S,\n     |      False otherwise.\n     |  \n     |  isprintable(...)\n     |      S.isprintable() -> bool\n     |      \n     |      Return True if all characters in S are considered\n     |      printable in repr() or S is empty, False otherwise.\n     |  \n     |  isspace(...)\n     |      S.isspace() -> bool\n     |      \n     |      Return True if all characters in S are whitespace\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  istitle(...)\n     |      S.istitle() -> bool\n     |      \n     |      Return True if S is a titlecased string and there is at least one\n     |      character in S, i.e. upper- and titlecase characters may only\n     |      follow uncased characters and lowercase characters only cased ones.\n     |      Return False otherwise.\n     |  \n     |  isupper(...)\n     |      S.isupper() -> bool\n     |      \n     |      Return True if all cased characters in S are uppercase and there is\n     |      at least one cased character in S, False otherwise.\n     |  \n     |  join(...)\n     |      S.join(iterable) -> str\n     |      \n     |      Return a string which is the concatenation of the strings in the\n     |      iterable.  The separator between elements is S.\n     |  \n     |  ljust(...)\n     |      S.ljust(width[, fillchar]) -> str\n     |      \n     |      Return S left-justified in a Unicode string of length width. Padding is\n     |      done using the specified fill character (default is a space).\n     |  \n     |  lower(...)\n     |      S.lower() -> str\n     |      \n     |      Return a copy of the string S converted to lowercase.\n     |  \n     |  lstrip(...)\n     |      S.lstrip([chars]) -> str\n     |      \n     |      Return a copy of the string S with leading whitespace removed.\n     |      If chars is given and not None, remove characters in chars instead.\n     |  \n     |  partition(...)\n     |      S.partition(sep) -> (head, sep, tail)\n     |      \n     |      Search for the separator sep in S, and return the part before it,\n     |      the separator itself, and the part after it.  If the separator is not\n     |      found, return S and two empty strings.\n     |  \n     |  replace(...)\n     |      S.replace(old, new[, count]) -> str\n     |      \n     |      Return a copy of S with all occurrences of substring\n     |      old replaced by new.  If the optional argument count is\n     |      given, only the first count occurrences are replaced.\n     |  \n     |  rfind(...)\n     |      S.rfind(sub[, start[, end]]) -> int\n     |      \n     |      Return the highest index in S where substring sub is found,\n     |      such that sub is contained within S[start:end].  Optional\n     |      arguments start and end are interpreted as in slice notation.\n     |      \n     |      Return -1 on failure.\n     |  \n     |  rindex(...)\n     |      S.rindex(sub[, start[, end]]) -> int\n     |      \n     |      Like S.rfind() but raise ValueError when the substring is not found.\n     |  \n     |  rjust(...)\n     |      S.rjust(width[, fillchar]) -> str\n     |      \n     |      Return S right-justified in a string of length width. Padding is\n     |      done using the specified fill character (default is a space).\n     |  \n     |  rpartition(...)\n     |      S.rpartition(sep) -> (head, sep, tail)\n     |      \n     |      Search for the separator sep in S, starting at the end of S, and return\n     |      the part before it, the separator itself, and the part after it.  If the\n     |      separator is not found, return two empty strings and S.\n     |  \n     |  rsplit(...)\n     |      S.rsplit(sep=None, maxsplit=-1) -> list of strings\n     |      \n     |      Return a list of the words in S, using sep as the\n     |      delimiter string, starting at the end of the string and\n     |      working to the front.  If maxsplit is given, at most maxsplit\n     |      splits are done. If sep is not specified, any whitespace string\n     |      is a separator.\n     |  \n     |  rstrip(...)\n     |      S.rstrip([chars]) -> str\n     |      \n     |      Return a copy of the string S with trailing whitespace removed.\n     |      If chars is given and not None, remove characters in chars instead.\n     |  \n     |  split(...)\n     |      S.split(sep=None, maxsplit=-1) -> list of strings\n     |      \n     |      Return a list of the words in S, using sep as the\n     |      delimiter string.  If maxsplit is given, at most maxsplit\n     |      splits are done. If sep is not specified or is None, any\n     |      whitespace string is a separator and empty strings are\n     |      removed from the result.\n     |  \n     |  splitlines(...)\n     |      S.splitlines([keepends]) -> list of strings\n     |      \n     |      Return a list of the lines in S, breaking at line boundaries.\n     |      Line breaks are not included in the resulting list unless keepends\n     |      is given and true.\n     |  \n     |  startswith(...)\n     |      S.startswith(prefix[, start[, end]]) -> bool\n     |      \n     |      Return True if S starts with the specified prefix, False otherwise.\n     |      With optional start, test S beginning at that position.\n     |      With optional end, stop comparing S at that position.\n     |      prefix can also be a tuple of strings to try.\n     |  \n     |  strip(...)\n     |      S.strip([chars]) -> str\n     |      \n     |      Return a copy of the string S with leading and trailing\n     |      whitespace removed.\n     |      If chars is given and not None, remove characters in chars instead.\n     |  \n     |  swapcase(...)\n     |      S.swapcase() -> str\n     |      \n     |      Return a copy of S with uppercase characters converted to lowercase\n     |      and vice versa.\n     |  \n     |  title(...)\n     |      S.title() -> str\n     |      \n     |      Return a titlecased version of S, i.e. words start with title case\n     |      characters, all remaining cased characters have lower case.\n     |  \n     |  translate(...)\n     |      S.translate(table) -> str\n     |      \n     |      Return a copy of the string S in which each character has been mapped\n     |      through the given translation table. The table must implement\n     |      lookup/indexing via __getitem__, for instance a dictionary or list,\n     |      mapping Unicode ordinals to Unicode ordinals, strings, or None. If\n     |      this operation raises LookupError, the character is left untouched.\n     |      Characters mapped to None are deleted.\n     |  \n     |  upper(...)\n     |      S.upper() -> str\n     |      \n     |      Return a copy of S converted to uppercase.\n     |  \n     |  zfill(...)\n     |      S.zfill(width) -> str\n     |      \n     |      Pad a numeric string S with zeros on the left, to fill a field\n     |      of the specified width. The string S is never truncated.\n     |  \n     |  ----------------------------------------------------------------------\n     |  Static methods defined here:\n     |  \n     |  maketrans(x, y=None, z=None, /)\n     |      Return a translation table usable for str.translate().\n     |      \n     |      If there is only one argument, it must be a dictionary mapping Unicode\n     |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n     |      Character keys will be then converted to ordinals.\n     |      If there are two arguments, they must be strings of equal length, and\n     |      in the resulting dictionary, each character in x will be mapped to the\n     |      character at the same position in y. If there is a third argument, it\n     |      must be a string, whose characters will be mapped to None in the result.\n    \n    \n\n![](../img/xiniu_neteasy.png)\n\n\n```python\n\n```\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson1/0.natural language processing/","content":"\n# 自然语言处理概述\n\n\n![](../img/L1_nlp.png)\n\n![](../img/L1_nlp2.png)\n\n![](../img/L1_nlp3.png)\n\n![](../img/nlp_problems.png)\n\n![](../img/nlp_4_problems.png)\n\n![](../img/nlp_applications.png)\n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter2_Text_semantic_matching_based_on_deep_learning/04WordAVG/","content":"\n# Word Averaging\n\n\n\n```python\nimport gensim\nimport gensim.downloader as api\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = api.load(\"glove-twitter-25\")\n```\n\n\n```python\nprint(model.get_vector(\"dog\"))\nprint(model.get_vector(\"dog\").shape)\n```\n\n    [-1.2420e+00 -3.5980e-01  5.7285e-01  3.6675e-01  6.0021e-01 -1.8898e-01\n      1.2729e+00 -3.6921e-01  8.9080e-02  4.0339e-01  2.5130e-01 -2.5548e-01\n     -3.9209e+00 -1.1100e+00 -2.1308e-01 -2.3846e-01  9.5322e-01 -5.2750e-01\n     -7.8049e-04 -3.5771e-01  5.5582e-01  7.7869e-01  4.6874e-01 -7.7803e-01\n      7.8378e-01]\n    (25,)\n    \n\n对于单词来说，空间距离的远近代表了单词含义的相似度。\n\n\n```python\nmodel.most_similar(\"cat\")\n```\n\n### Create a GloVe Word Averaging Model\n\nword averaging模型的定义非常简单，就是把每个单词的vector取出来，计算一个平均向量表示整一段文本。\n\n\n```python\ndef wordavg(model, words):\n    return np.mean([model.get_vector(word) for word in words if word in model.vocab ], 0)\n```\n\n\n```python\ns1 = \"Natural language processing is a promising research area\"\ns2 = \"More and more researchers are working on natural language processing nowadays\"\n```\n\n\n```python\ns1_vec = wordavg(model, s1.lower().split())\ns2_vec = wordavg(model, s2.lower().split())\ncosine_similarity(s1_vec.reshape((1, -1)), s2_vec.reshape((1, -1)))\n```\n\n\n```python\nww2 = \"\"\"\nWorld War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 50 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\nJapan, which aimed to dominate Asia and the Pacific, was at war with China by 1937,[5][b] though neither side had declared war on the other. World War II is generally said to have begun on 1 September 1939,[6] with the invasion of Poland by Germany and subsequent declarations on Germany by France and the United Kingdom. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. Following the onset of campaigns in North Africa and East Africa, and the fall of France in mid 1940, the war continued primarily between the European Axis powers and the British Empire. War in the Balkans, the aerial Battle of Britain, the Blitz, and the long Battle of the Atlantic followed. On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history. This Eastern Front trapped the Axis, most crucially the German Wehrmacht, into a war of attrition. In December 1941, Japan launched a surprise attack on the United States and European colonies in the Pacific. Following an immediate U.S. declaration of war against Japan, supported by one from Great Britain, the European Axis powers quickly declared war on the U.S. in solidarity with their Japanese ally. Rapid Japanese conquests over much of the Western Pacific ensued, perceived by many in Asia as liberation from Western dominance and resulting in the support of several armies from defeated territories.\nThe Axis advance in the Pacific halted in 1942 when Japan lost the critical Battle of Midway; later, Germany and Italy were defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. Key setbacks in 1943, which included a series of German defeats on the Eastern Front, the Allied invasions of Sicily and Italy, and Allied victories in the Pacific, cost the Axis its initiative and forced it into strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained its territorial losses and turned toward Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in Central China, South China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.\nThe war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops, the suicide of Adolf Hitler and the German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, the Soviet entry into the war against Japan and its invasion of Manchuria, Japan announced its intention to surrender on 15 August 1945, cementing total victory in Asia for the Allies. Tribunals were set up by fiat by the Allies and war crimes trials were conducted in the wake of the war both against the Germans and the Japanese.\nWorld War II changed the political alignment and social structure of the globe. The United Nations (UN) was established to foster international co-operation and prevent future conflicts; the victorious great powers—China, France, the Soviet Union, the United Kingdom, and the United States—became the permanent members of its Security Council.[7] The Soviet Union and United States emerged as rival superpowers, setting the stage for the nearly half-century long Cold War. In the wake of European devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia. Most countries whose industries had been damaged moved towards economic recovery and expansion. Political integration, especially in Europe, emerged as an effort to end pre-war enmities and create a common identity.[8]\"\"\"\n\nww1 = \"\"\"World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously described as \"the war to end all wars\",[7] it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history.[8][9] It is also one of the deadliest conflicts in history,[10] with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide.[11]\nOn 28 June 1914, Gavrilo Princip, a Bosnian Serb Yugoslav nationalist, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand in Sarajevo, leading to the July Crisis.[12][13] In response, on 23 July Austria-Hungary issued an ultimatum to Serbia. Serbia's reply failed to satisfy the Austrians, and the two moved to a war footing.\nA network of interlocking alliances enlarged the crisis from a bilateral issue in the Balkans to one involving most of Europe. By July 1914, the great powers of Europe were divided into two coalitions: the Triple Entente—consisting of France, Russia and Britain—and the Triple Alliance of Germany, Austria-Hungary and Italy (the Triple Alliance was primarily defensive in nature, allowing Italy to stay out of the war in 1914).[14] Russia felt it necessary to back Serbia and, after Austria-Hungary shelled the Serbian capital of Belgrade on the 28th, partial mobilisation was approved.[15] General Russian mobilisation was announced on the evening of 30 July; on the 31st, Austria-Hungary and Germany did the same, while Germany demanded Russia demobilise within 12 hours.[16] When Russia failed to comply, Germany declared war on 1 August in support of Austria-Hungary, with Austria-Hungary following suit on 6th; France ordered full mobilisation in support of Russia on 2 August.[17]\nGerman strategy for a war on two fronts against France and Russia was to rapidly concentrate the bulk of its army in the West to defeat France within four weeks, then shift forces to the East before Russia could fully mobilise; this was later known as the Schlieffen Plan.[18] On 2 August, Germany demanded free passage through Belgium, an essential element in achieving a quick victory over France.[19] When this was refused, German forces invaded Belgium on 3 August and declared war on France the same day; the Belgian government invoked the 1839 Treaty of London and in compliance with its obligations under this, Britain declared war on Germany on 4 August.[20][21] On 12 August, Britain and France also declared war on Austria-Hungary; on the 23rd, Japan sided with the Entente, seizing German possessions in China and the Pacific. In November 1914, the Ottoman Empire entered the war on the side of the Alliance, opening fronts in the Caucasus, Mesopotamia and the Sinai Peninsula. The war was fought in and drew upon each powers' colonial empires as well, spreading the conflict to Africa and across the globe. The Entente and its allies would eventually become known as the Allied Powers, while the grouping of Austria-Hungary, Germany and their allies would become known as the Central Powers.\nThe German advance into France was halted at the Battle of the Marne and by the end of 1914, the Western Front settled into a battle of attrition, marked by a long series of trench lines that changed little until 1917 (the Eastern Front, by contrast, was marked by much greater exchanges of territory). In 1915, Italy joined the Allied Powers and opened a front in the Alps. The Kingdom of Bulgaria joined the Central Powers in 1915 and the Kingdom of Greece joined the Allies in 1917, expanding the war in the Balkans. The United States initially remained neutral, although by doing nothing to prevent the Allies from procuring American supplies whilst the Allied blockade effectively prevented the Germans from doing the same the U.S. became an important supplier of war material to the Allies. Eventually, after the sinking of American merchant ships by German submarines, and the revelation that the Germans were trying to incite Mexico to make war on the United States, the U.S. declared war on Germany on 6 April 1917. Trained American forces would not begin arriving at the front in large numbers until mid-1918, but ultimately the American Expeditionary Force would reach some two million troops.[22]\nThough Serbia was defeated in 1915, and Romania joined the Allied Powers in 1916 only to be defeated in 1917, none of the great powers were knocked out of the war until 1918. The 1917 February Revolution in Russia replaced the Tsarist autocracy with the Provisional Government, but continuing discontent at the cost of the war led to the October Revolution, the creation of the Soviet Socialist Republic, and the signing of the Treaty of Brest-Litovsk by the new government in March 1918, ending Russia's involvement in the war. This allowed the transfer of large numbers of German troops from the East to the Western Front, resulting in the German March 1918 Offensive. This offensive was initially successful, but the Allies rallied and drove the Germans back in their Hundred Days Offensive.[23] Bulgaria was the first Central Power to sign an armistice—the Armistice of Salonica on 29 September 1918. On 30 October, the Ottoman Empire capitulated, signing the Armistice of Mudros.[24] On 4 November, the Austro-Hungarian empire agreed to the Armistice of Villa Giusti. With its allies defeated, revolution at home, and the military no longer willing to fight, Kaiser Wilhelm abdicated on 9 November and Germany signed an armistice on 11 November 1918.\nWorld War I was a significant turning point in the political, cultural, economic, and social climate of the world. The war and its immediate aftermath sparked numerous revolutions and uprisings. The Big Four (Britain, France, the United States, and Italy) imposed their terms on the defeated powers in a series of treaties agreed at the 1919 Paris Peace Conference, the most well known being the German peace treaty—the Treaty of Versailles.[25] Ultimately, as a result of the war the Austro-Hungarian, German, Ottoman, and Russian Empires ceased to exist, with numerous new states created from their remains. However, despite the conclusive Allied victory (and the creation of the League of Nations during the Peace Conference, intended to prevent future wars), a Second World War would follow just over twenty years later.\"\"\"\n\nnetease = \"\"\"NetEase, Inc. (simplified Chinese: 网易; traditional Chinese: 網易; pinyin: WǎngYì) is a Chinese Internet technology company providing online services centered on content, community, communications and commerce. The company was founded in 1997 by Lebunto. NetEase develops and operates online PC and mobile games, advertising services, email services and e-commerce platforms in China. It is one of the largest Internet and video game companies in the world.[7]\nSome of NetEase's games include the Westward Journey series (Fantasy Westward Journey, Westward Journey Online II, Fantasy Westward Journey II, and New Westward Journey Online II), as well as other games, such as Tianxia III, Heroes of Tang Dynasty Zero and Ghost II. NetEase also partners with Blizzard Entertainment to operate local versions of Warcraft III, World of Warcraft, Hearthstone, StarCraft II, Diablo III: Reaper of Souls and Overwatch in China. They are also developing their very first self-developed VR multiplayer online game with an open world setting, which is called Nostos.[8]\"\"\"\n```\n\n\n```python\nww1_vec = wordavg(model, ww1.lower().split())\nww2_vec = wordavg(model, ww2.lower().split())\ncosine_similarity(ww1_vec.reshape((1, -1)), ww2_vec.reshape((1, -1)))\n```\n\n\n```python\nnetease_vec = wordavg(model, netease.lower().split())\ncosine_similarity(ww1_vec.reshape((1, -1)), netease_vec.reshape((1, -1)))\n```\n\n\n```python\ncosine_similarity(s1_vec.reshape((1, -1)), netease_vec.reshape((1, -1)))\n```\n\n\n```python\ns3 = \"She loves him too much\"\ns3_vec = wordavg(model, s3.lower().split())\ncosine_similarity(s1_vec.reshape((1, -1)), s3_vec.reshape((1, -1)))\n```\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"基于DRMM的问答匹配","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter2_Text_semantic_matching_based_on_deep_learning/03Question_and_answer_matching_based_on_DRMM/","content":"\n\n# 基于DRMM的问答匹配\n\n\nDRRM模型我们参考[MatchZoo](https://github.com/NTMC-Community/MatchZoo)的实现\n\n我们略过文本的预处理，训练和预测的代码，直接阅读模型的代码。模型使用Keras框架实现。\n\n```\n\n\"\"\"An implementation of DRMM Model.\"\"\"\nimport typing\n\nimport keras\nimport keras.backend as K\n\nfrom matchzoo import engine\n\n\nclass DRMM(engine.BaseModel):\n    \"\"\"\n    DRMM Model.\n\n    Examples:\n        >>> model = DRMM()\n        >>> model.params['mlp_num_layers'] = 1\n        >>> model.params['mlp_num_units'] = 5\n        >>> model.params['mlp_num_fan_out'] = 1\n        >>> model.params['mlp_activation_func'] = 'tanh'\n        >>> model.guess_and_fill_missing_params(verbose=0)\n        >>> model.build()\n        >>> model.compile()\n\n    \"\"\"\n\n    @classmethod\n    def get_default_params(cls) -> engine.ParamTable:\n        \"\"\":return: model default parameters.\"\"\"\n        params = super().get_default_params(with_embedding=True,\n                                            with_multi_layer_perceptron=True)\n        params.add(engine.Param(name='mask_value', value=-1,\n                                desc=\"The value to be masked from inputs.\"))\n        params['optimizer'] = 'adam'\n        params['input_shapes'] = [(5,), (5, 30,)]\n        return params\n\n    def build(self):\n        \"\"\"Build model structure.\"\"\"\n\n        # Scalar dimensions referenced here:\n        #   B = batch size (number of sequences)\n        #   D = embedding size\n        #   L = `input_left` sequence length\n        #   R = `input_right` sequence length\n        #   H = histogram size\n        #   K = size of top-k\n\n        # Left input and right input.\n        # query: shape = [B, L]\n        # doc: shape = [B, L, H]\n        # Note here, the doc is the matching histogram between original query\n        # and original document.\n        query = keras.layers.Input(\n            name='text_left',\n            shape=self._params['input_shapes'][0]\n        )\n        match_hist = keras.layers.Input(\n            name='match_histogram',\n            shape=self._params['input_shapes'][1]\n        )\n\n        embedding = self._make_embedding_layer()\n        # Process left input.\n        # shape = [B, L, D]\n        embed_query = embedding(query)\n        # shape = [B, L]\n        atten_mask = K.any(K.not_equal(query, self._params['mask_value']),\n                           axis=-1, keepdims=True)\n        atten_mask = K.cast(atten_mask, K.floatx())\n        atten_mask = K.expand_dims(atten_mask, axis=2)\n        # shape = [B, L, D]\n        attention_probs = self.attention_layer(embed_query, atten_mask)\n\n        # Process right input.\n        # shape = [B, L, 1]\n        dense_output = self._make_multi_layer_perceptron_layer()(match_hist)\n\n        # shape = [B, 1, 1]\n        dot_score = keras.layers.Dot(axes=[1, 1])(\n            [attention_probs, dense_output])\n\n        flatten_score = keras.layers.Flatten()(dot_score)\n\n        x_out = self._make_output_layer()(flatten_score)\n        self._backend = keras.Model(inputs=[query, match_hist], outputs=x_out)\n\n    @classmethod\n    def attention_layer(cls, attention_input: typing.Any,\n                        attention_mask: typing.Any = None\n                        ) -> keras.layers.Layer:\n        \"\"\"\n        Performs attention on the input.\n\n        :param attention_input: The input tensor for attention layer.\n        :param attention_mask: A tensor to mask the invalid values.\n        :return: The masked output tensor.\n        \"\"\"\n        # shape = [B, L, 1]\n        dense_input = keras.layers.Dense(1, use_bias=False)(attention_input)\n        if attention_mask is not None:\n            # Since attention_mask is 1.0 for positions we want to attend and\n            # 0.0 for masked positions, this operation will create a tensor\n            # which is 0.0 for positions we want to attend and -10000.0 for\n            # masked positions.\n\n            # shape = [B, L, 1]\n            dense_input = keras.layers.Lambda(\n                lambda x: x + (1.0 - attention_mask) * -10000.0,\n                name=\"attention_mask\"\n            )(dense_input)\n        # shape = [B, L, 1]\n        attention_probs = keras.layers.Lambda(\n            lambda x: keras.layers.activations.softmax(x, axis=1),\n            output_shape=lambda s: (s[0], s[1], s[2]),\n            name=\"attention_probs\"\n        )(dense_input)\n        return attention_probs\n\n```\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter2_Text_semantic_matching_based_on_deep_learning/02DSSM-based_question_semantic_similarity_matching/","content":"\n# 基于DSSM的问题语义相似度匹配\n## CNTK 303: Deep Structured Semantic Modeling with LSTM Networks\n\n\nDSSM的全称是Deep Structured Semantic Model或者Deep Semantic Similarity Model。\nDSSM由微软研究院深度学习研究中心开发，是一个利用深度神经网络把文本（句子，queries，实体等）表示成向量，并且计算文本相似度的模型和方法。\nDSSM在信息检索和网络文本排序中有广泛的应用([Huang et al. 2013](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/); [Shen et al. 2014a](https://www.microsoft.com/en-us/research/publication/learning-semantic-representations-using-convolutional-neural-networks-for-web-search/),[2014b](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/); [Palangi et al. 2016](https://www.microsoft.com/en-us/research/publication/deep-sentence-embedding-using-long-short-term-memory-networks-analysis-application-information-retrieval/)), 广告相关性, 实体搜索和有趣性任务([Gao et al. 2014a](https://www.microsoft.com/en-us/research/publication/modeling-interestingness-with-deep-neural-networks/), 问答([Yih et al., 2014](https://www.microsoft.com/en-us/research/publication/semantic-parsing-for-single-relation-question-answering/)), 图片描述([Fang et al., 2014](https://arxiv.org/abs/1411.4952)), 以及机器翻译 ([Gao et al., 2014b](https://www.microsoft.com/en-us/research/publication/learning-continuous-phrase-representations-for-translation-modeling/)) etc. \n\n\nDSSM可以被用作开发latent semantic models，把不同的实体投影到同一个低维度的语义空间，然后用于文本分类，排序等任务。举例来说，在网络搜索任务中，文本和搜索短语的相关性可以用vector之间的距离表示。[He et al., 2014](https://www.microsoft.com/en-us/research/publication/deep-learning-for-natural-language-processing-theory-and-practice-tutorial/).\n\n\n## Goal\n\n给定一对文本，例如搜索一个关键词和一组网络文本，模型会把他们分别转化成低维的连续向量，然后用cosine相似度来计算文本的相似性。\n\n![](http://kubicode.me/img/Study-With-Deep-Structured-Semantic-Model/dssm_arch.png)\n\n从上图中我们看到，给定一个query($Q$)和一组文档($D_1, D_2, \\ldots, D_n$)，模型可以生成一组隐向量表示(semantic features)，然后这些semantic features就可以用来计算文本相似度，最终用于文本排序。\n\n从上图中我们看到，query和document都被编码成了向量。\n虽然[bag of word](https://en.wikipedia.org/wiki/Bag-of-words_model)是人们常用的文本表示方式，但是它丢失了文本中单词之间的位置关系信息。\n卷积或者循环神经网络，由于它们编码单词位置信息的能力，在很多NLP问题上有更好的表现。在这份材料中，我们会使用LSTM模型来编码term vector [Palangi et. al.](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/LSTM_DSSM_IEEE_TASLP.pdf)。\n\n我们使用一个比较小的问答数据集来训练这个模型。这份notebook的作用是展示如何构建一个DSSM模型，而不是为了用它达到State-of-the-art的表现。\n\n\n\n```python\n# Import the relevant libraries\nimport math\nimport numpy as np\nimport os\nfrom __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n\n# import cntk as C\n# import cntk.tests.test_utils\n# cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n# C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n```\n\n## Data Preparation\n\n### Download\n\n我们使用一组问答数据集来展示如何使用DSSM模型。\n这组数据集包含很多对[问答](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf)句子。\n我们把这些数据预处理成两个部分：\n- 词汇文件：问题和回答各有一个单词文件。问题和答案分别有1204和1019个单词。\n- 问答句子：包含一个训练集和一个验证集。这些文本都被做成了[CTF格式](https://cntk.ai/pythondocs/CNTK_202_Language_Understanding.html)。训练集有3500对句子，验证集有409对。\n\n\n\n```python\nlocation = os.path.normpath('data/DSSM')\ndata = {\n  'train': { 'file': 'train.pair.tok.ctf' },\n  'val':{ 'file': 'valid.pair.tok.ctf' },\n  'query': { 'file': 'vocab_Q.wl' },\n  'answer': { 'file': 'vocab_A.wl' }\n}\n\nimport requests\n\ndef download(url, filename):\n    \"\"\" utility function to download a file \"\"\"\n    response = requests.get(url, stream=True)\n    with open(filename, \"wb\") as handle:\n        for data in response.iter_content():\n            handle.write(data)\n\nif not os.path.exists(location):\n    os.mkdir(location)\n     \nfor item in data.values():\n    path = os.path.normpath(os.path.join(location, item['file']))\n\n    if os.path.exists(path):\n        print(\"Reusing locally cached:\", path)\n        \n    else:\n        print(\"Starting download:\", item['file'])\n        url = \"http://www.cntk.ai/jup/dat/DSSM/%s.csv\"%(item['file'])\n        print(url)\n        download(url, path)\n        print(\"Download completed\")\n    item['file'] = path\n```\n\n    Starting download: train.pair.tok.ctf\n    http://www.cntk.ai/jup/dat/DSSM/train.pair.tok.ctf.csv\n    Download completed\n    Starting download: valid.pair.tok.ctf\n    http://www.cntk.ai/jup/dat/DSSM/valid.pair.tok.ctf.csv\n    Download completed\n    Starting download: vocab_Q.wl\n    http://www.cntk.ai/jup/dat/DSSM/vocab_Q.wl.csv\n    Download completed\n    Starting download: vocab_A.wl\n    http://www.cntk.ai/jup/dat/DSSM/vocab_A.wl.csv\n    Download completed\n    \n\n### 数据读取\n\n我们用CTF deserializer来读取数据。当然，你也可以选择用别的方法自己预处理数据。这里提供的CTF reader也提供打乱样本顺序的功能。\n\n\n```python\n# Define the vocabulary size (QRY-stands for question and ANS stands for answer)\nQRY_SIZE = 1204\nANS_SIZE = 1019\n\ndef create_reader(path, is_training):\n    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n         query = C.io.StreamDef(field='S0', shape=QRY_SIZE,  is_sparse=True),\n         answer  = C.io.StreamDef(field='S1', shape=ANS_SIZE, is_sparse=True)\n     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n```\n\n\n```python\ntrain_file = data['train']['file']\nprint(train_file)\n\nif os.path.exists(train_file):\n    train_source = create_reader(train_file, is_training=True)\nelse:\n    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(train_file, os.getcwd()))\n\nvalidation_file = data['val']['file']\nprint(validation_file)\nif os.path.exists(validation_file):\n    val_source = create_reader(validation_file, is_training=False)\nelse:\n    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(validation_file, os.getcwd()))\n```\n\n    data\\DSSM\\train.pair.tok.ctf\n    data\\DSSM\\valid.pair.tok.ctf\n    \n\n## Model creation\n\nLSTM-RNN模型可以按照顺序读入句子中的单词，抽取单词中的信息，然后embed成一个vector。\n在DSSM模型中，我们采用句子的最后一个hidden state来作为整个句子的vector表示。\n这个vector通过两次Feedforward神经网络就可以作为query vector。\n\n\n\n                                                                        \"query vector\"\n                                                                              ^\n                                                                              |\n                                                                          +-------+  \n                                                                          | Dense |  \n                                                                          +-------+  \n                                                                              ^         \n                                                                              |         \n                                                                         +---------+  \n                                                                         | Dropout |  \n                                                                         +---------+\n                                                                              ^\n                                                                              |         \n                                                                          +-------+  \n                                                                          | Dense |  \n                                                                          +-------+  \n                                                                              ^         \n                                                                              |         \n                                                                          +------+   \n                                                                          | last |  \n                                                                          +------+  \n                                                                              ^  \n                                                                              |         \n                              +------+   +------+   +------+   +------+   +------+   \n                         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |\n                              +------+   +------+   +------+   +------+   +------+   \n                                  ^          ^          ^          ^          ^\n                                  |          |          |          |          |\n                              +-------+  +-------+  +-------+  +-------+  +-------+\n                              | Embed |  | Embed |  | Embed |  | Embed |  | Embed | \n                              +-------+  +-------+  +-------+  +-------+  +-------+\n                                  ^          ^          ^          ^          ^\n                                  |          |          |          |          |\n                    query  ------>+--------->+--------->+--------->+--------->+\n    \n \n类似地，我们可以把答案句子编码成answer vector。我们首先定义模型的输入，分别是query和answer的sequence，\n\n\n```python\n# Create the containers for input feature (x) and the label (y)\nqry = C.sequence.input_variable(QRY_SIZE)\nans = C.sequence.input_variable(ANS_SIZE)\n```\n\n每个CNTK的sequence都包含一个dynamic axis，表示sequence的长度。\n直观来说，当你的sequence有不同的长度和不同的单词表大小，他们都应该有一个dynamic axis。\n这时候就需要声明named axis。\n\n\n```python\n# Create the containers for input feature (x) and the label (y)\naxis_qry = C.Axis.new_unique_dynamic_axis('axis_qry')\nqry = C.sequence.input_variable(QRY_SIZE, sequence_axis=axis_qry)\n\naxis_ans = C.Axis.new_unique_dynamic_axis('axis_ans')\nans = C.sequence.input_variable(ANS_SIZE, sequence_axis=axis_ans)\n```\n\n在创建模型之前我们先定义一些模型的参数。\n\n\n```python\nEMB_DIM   = 25 # Embedding dimension\nHIDDEN_DIM = 50 # LSTM dimension\nDSSM_DIM = 25 # Dense layer dimension  \nNEGATIVE_SAMPLES = 5\nDROPOUT_RATIO = 0.2\n```\n\n\n```python\ndef create_model(qry, ans):\n    with C.layers.default_options(initial_state=0.1):\n        qry_vector = C.layers.Sequential([\n            C.layers.Embedding(EMB_DIM, name='embed'),\n            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n            C.sequence.last,\n            C.layers.Dense(DSSM_DIM, activation=C.relu, name='q_proj'),\n            C.layers.Dropout(DROPOUT_RATIO, name='dropout qdo1'),\n            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='q_enc')\n        ])\n        \n        ans_vector = C.layers.Sequential([\n            C.layers.Embedding(EMB_DIM, name='embed'),\n            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n            C.sequence.last,\n            C.layers.Dense(DSSM_DIM, activation=C.relu, name='a_proj'),\n            C.layers.Dropout(DROPOUT_RATIO, name='dropout ado1'),\n            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='a_enc')\n        ])\n\n    return {\n        'query_vector': qry_vector(qry),\n        'answer_vector': ans_vector(ans)\n    }\n\n# Create the model and store reference in `network` dictionary\nnetwork = create_model(qry, ans)\n\nnetwork['query'], network['axis_qry'] = qry, axis_qry\nnetwork['answer'], network['axis_ans'] = ans, axis_ans\n```\n\n## Training\n\n现在我们已经创建了模型，下一步就是找到一个合适的损失函数。这个损失函数的功能是，如果我们的问题和一个正确的答案匹配在一起，这个损失就应该是一个接近0的很小的数字，如果问题和答案不匹配，那么损失函数应该给我们返回一个接近1的数字。换句话说，这个损失函数最大化问题和正确答案之间的相似度，最小化问题与错误答案之间的相似度。\n\nDSSM经常被用在信息检索类问题中。往往给定一个搜索的短语或问题，我们需要在海量的文本中寻找正确答案。输入的数据是一个问题和一个潜在的答案（文本或者广告），这些文本或者广告可能会被点击。我们的目标是要提高被点击的概率，也就是说被搜索到的文档或广告与搜索关键词比较相关。一种做法是训练一个分类器，这个分类器可以预测链接是否被点开。为了训练这样一个模型，我们需要被点开的搜索短语和链接，也需要没有被点开的链接。一种模拟没有被点开的链接的方法是从当前minibatch中随机采样其他query产生的链接。这就是 `cosine_distance_with_negative_samples` 这个function在做的事情。注意，这个function的返回值1表示正确的问题与答案，0表示错误的问题与答案，我们把它叫做*similarity*。所以，我们用1-`cosine_distance_with_negative_samples`作为损失函数。\n\n\n```python\ndef create_loss(vector_a, vector_b):\n    qry_ans_similarity = C.cosine_distance_with_negative_samples(vector_a, \\\n                                                                 vector_b, \\\n                                                                 shift=1, \\\n                                                                 num_negative_samples=5)\n    return 1 - qry_ans_similarity\n```\n\n\n```python\n# Model parameters\nMAX_EPOCHS = 5\nEPOCH_SIZE = 10000\nMINIBATCH_SIZE = 50\n```\n\n\n```python\n# Create trainer\ndef create_trainer(reader, network):\n    \n    # Setup the progress updater\n    progress_writer = C.logging.ProgressPrinter(tag='Training', num_epochs=MAX_EPOCHS)\n\n    # Set learning parameters\n    lr_per_sample     = [0.0015625]*20 + \\\n                        [0.00046875]*20 + \\\n                        [0.00015625]*20 + \\\n                        [0.000046875]*10 + \\\n                        [0.000015625]\n    lr_schedule       = C.learning_parameter_schedule_per_sample(lr_per_sample, \\\n                                                 epoch_size=EPOCH_SIZE)\n    mms               = [0]*20 + [0.9200444146293233]*20 + [0.9591894571091382]\n    mm_schedule       = C.learners.momentum_schedule(mms, \\\n                                                     epoch_size=EPOCH_SIZE, \\\n                                                     minibatch_size=MINIBATCH_SIZE)\n    l2_reg_weight     = 0.0002\n\n    model = C.combine(network['query_vector'], network['answer_vector'])\n\n    #Notify the network that the two dynamic axes are indeed same\n    query_reconciled = C.reconcile_dynamic_axes(network['query_vector'], network['answer_vector'])\n  \n    network['loss'] = create_loss(query_reconciled, network['answer_vector'])\n    network['error'] = None\n\n    print('Using momentum sgd with no l2')\n    dssm_learner = C.learners.momentum_sgd(model.parameters, lr_schedule, mm_schedule)\n\n    network['learner'] = dssm_learner\n \n    print('Using local learner')\n    # Create trainer\n    return C.Trainer(model, (network['loss'], network['error']), network['learner'], progress_writer)    \n```\n\n\n```python\n# Instantiate the trainer\ntrainer = create_trainer(train_source, network)\n```\n\n    Using momentum sgd with no l2\n    Using local learner\n    \n\n\n```python\n# Train \ndef do_train(network, trainer, train_source):\n    # define mapping from intput streams to network inputs\n    input_map = {\n        network['query']: train_source.streams.query,\n        network['answer']: train_source.streams.answer\n        } \n\n    t = 0\n    for epoch in range(MAX_EPOCHS):         # loop over epochs\n        epoch_end = (epoch+1) * EPOCH_SIZE\n        while t < epoch_end:                # loop over minibatches on the epoch\n            data = train_source.next_minibatch(MINIBATCH_SIZE, input_map= input_map)  # fetch minibatch\n            trainer.train_minibatch(data)               # update model with it\n            t += MINIBATCH_SIZE\n\n        trainer.summarize_training_progress()\n```\n\n\n```python\ndo_train(network, trainer, train_source)\n```\n\n    Learning rate per 1 samples: 0.0015625\n    Momentum per 1 samples: 0.0\n    Finished Epoch[1 of 5]: [Training] loss = 0.343046 * 1522, metric = 0.00% * 1522 5.720s (266.1 samples/s);\n    Finished Epoch[2 of 5]: [Training] loss = 0.102804 * 1530, metric = 0.00% * 1530 3.464s (441.7 samples/s);\n    Finished Epoch[3 of 5]: [Training] loss = 0.066461 * 1525, metric = 0.00% * 1525 3.402s (448.3 samples/s);\n    Finished Epoch[4 of 5]: [Training] loss = 0.048511 * 1534, metric = 0.00% * 1534 3.390s (452.5 samples/s);\n    Finished Epoch[5 of 5]: [Training] loss = 0.035384 * 1510, metric = 0.00% * 1510 3.383s (446.3 samples/s);\n    \n\n## Validate\n\n当我们训练完模型后，我们需要选择一个训练与验证错误率相近的模型。\n可以通过选择不同的epoch数量来选择更好的模型。\n通过这种方式选择的模型最终被用于预测。\n\n\n```python\n# Validate\ndef do_validate(network, val_source):\n    # process minibatches and perform evaluation\n    progress_printer = C.logging.ProgressPrinter(tag='Evaluation', num_epochs=0)\n\n    val_map = {\n        network['query']: val_source.streams.query,\n        network['answer']: val_source.streams.answer\n        } \n\n    evaluator = C.eval.Evaluator(network['loss'], progress_printer)\n\n    while True:\n        minibatch_size = 100\n        data = val_source.next_minibatch(minibatch_size, input_map=val_map)\n        if not data:                                 # until we hit the end\n            break\n\n        evaluator.test_minibatch(data)\n\n    evaluator.summarize_test_progress()\n```\n\n\n```python\ndo_validate(network, val_source)\n```\n\n    Finished Evaluation [1]: Minibatch[1-35]: metric = 0.02% * 410;\n    \n\n## 预测\n\n我们会把query和answer都转化成vector。然后计算它们之间的cosine similarity。这些cosine similarity的分数可以用来对搜索的网页排序。\n\n\n```python\n# load dictionaries\nquery_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\nanswers_wl = [line.rstrip('\\n') for line in open(data['answer']['file'])]\nquery_dict = {query_wl[i]:i for i in range(len(query_wl))}\nanswers_dict = {answers_wl[i]:i for i in range(len(answers_wl))}\n\n# let's run a sequence through\nqry = 'BOS what contribution did  e1  made to science in 1665 EOS'\nans = 'BOS book author book_editions_published EOS'\nans_poor = 'BOS language human_language main_country EOS'\n\nqry_idx = [query_dict[w+' '] for w in qry.split()] # convert to query word indices\nprint('Query Indices:', qry_idx)\n\nans_idx = [answers_dict[w+' '] for w in ans.split()] # convert to answer word indices\nprint('Answer Indices:', ans_idx)\n\nans_poor_idx = [answers_dict[w+' '] for w in ans_poor.split()] # convert to fake answer word indices\nprint('Poor Answer Indices:', ans_poor_idx)\n```\n\n    Query Indices: [1202, 1154, 267, 321, 357, 648, 1070, 905, 549, 6, 1203]\n    Answer Indices: [1017, 135, 91, 137, 1018]\n    Poor Answer Indices: [1017, 501, 452, 533, 1018]\n    \n\n\n```python\n# Create the one hot representations\nqry_onehot = np.zeros([len(qry_idx),len(query_dict)], np.float32)\nfor t in range(len(qry_idx)):\n    qry_onehot[t,qry_idx[t]] = 1\n    \nans_onehot = np.zeros([len(ans_idx),len(answers_dict)], np.float32)\nfor t in range(len(ans_idx)):\n    ans_onehot[t,ans_idx[t]] = 1\n    \nans_poor_onehot = np.zeros([len(ans_poor_idx),len(answers_dict)], np.float32)\nfor t in range(len(ans_poor_idx)):\n    ans_poor_onehot[t, ans_poor_idx[t]] = 1\n```\n\n\n```python\nqry_embedding = network['query_vector'].eval([qry_onehot])\nans_embedding = network['answer_vector'].eval([ans_onehot])\nans_poor_embedding = network['answer_vector'].eval([ans_poor_onehot])\n\nfrom scipy.spatial.distance import cosine\n\nprint('Query to Answer similarity:', 1-cosine(qry_embedding, ans_embedding))\nprint('Query to poor-answer similarity:', 1-cosine(qry_embedding, ans_poor_embedding))\n```\n\n    Query to Answer similarity: 0.99995367043\n    Query to poor-answer similarity: 0.999941420215\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter2_Text_semantic_matching_based_on_deep_learning/01LSTM-based_supervised_learning_semantic_expression_extraction/","content":"\n# 基于LSTM的监督学习语义表达抽取\n## InferSent\n\n\n\n[InferSent](https://github.com/facebookresearch/InferSent)的官方代码可以从GitHub上找到。\n\n我们这里省略数据预处理和训练的环节，只看模型的定义部分。模型利用PyTorch实现。\n\n<img src=\"./img/snli.png\" alt=\"drawing\" width=\"300\"/>\n\n```\n\n\"\"\"\nMain module for Natural Language Inference\n\"\"\"\n\n\nclass NLINet(nn.Module):\n    def __init__(self, config):\n        super(NLINet, self).__init__()\n\n        # classifier\n        self.nonlinear_fc = config['nonlinear_fc']\n        self.fc_dim = config['fc_dim']\n        self.n_classes = config['n_classes']\n        self.enc_lstm_dim = config['enc_lstm_dim']\n        self.encoder_type = config['encoder_type']\n        self.dpout_fc = config['dpout_fc']\n\n        self.encoder = eval(self.encoder_type)(config)\n        self.inputdim = 4*2*self.enc_lstm_dim\n        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n                                        else self.inputdim\n        if self.nonlinear_fc: # 非线性的神经网络分类器\n            self.classifier = nn.Sequential(\n                nn.Dropout(p=self.dpout_fc),\n                nn.Linear(self.inputdim, self.fc_dim),\n                nn.Tanh(),\n                nn.Dropout(p=self.dpout_fc),\n                nn.Linear(self.fc_dim, self.fc_dim),\n                nn.Tanh(),\n                nn.Dropout(p=self.dpout_fc),\n                nn.Linear(self.fc_dim, self.n_classes),\n                )\n        else: # 线性神经网络分类器\n            self.classifier = nn.Sequential(\n                nn.Linear(self.inputdim, self.fc_dim),\n                nn.Linear(self.fc_dim, self.fc_dim),\n                nn.Linear(self.fc_dim, self.n_classes)\n                )\n\n    def forward(self, s1, s2):\n        # s1 : (s1, s1_len)\n        u = self.encoder(s1) # 编码句子1\n        v = self.encoder(s2) # 编码句子2\n\n        features = torch.cat((u, v, torch.abs(u-v), u*v), 1) # feature engineering\n        output = self.classifier(features) # 分类\n        return output\n\n    def encode(self, s1):\n        emb = self.encoder(s1)\n        return emb\n    \n    \n```\n\n```\n\n\"\"\"\nLSTM encoder\n\"\"\"\n\n\nclass LSTMEncoder(nn.Module):\n    def __init__(self, config):\n        super(LSTMEncoder, self).__init__()\n        self.bsize = config['bsize']\n        self.word_emb_dim = config['word_emb_dim']\n        self.enc_lstm_dim = config['enc_lstm_dim']\n        self.pool_type = config['pool_type']\n        self.dpout_model = config['dpout_model']\n\n        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n                                bidirectional=False, dropout=self.dpout_model)\n\n    def forward(self, sent_tuple):\n        # sent_len [max_len, ..., min_len] (batch)\n        # sent (seqlen x batch x worddim)\n\n        sent, sent_len = sent_tuple\n\n        # 按照句子的长短排序，并保留原始的idx顺序\n        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n        sent = sent.index_select(1, torch.cuda.LongTensor(idx_sort))\n\n        # 用pytorch自带的函数处理RNN的padding问题\n        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n        # LSTM编码序列\n        sent_output = self.enc_lstm(sent_packed)[1][0].squeeze(0)  # batch x 2*nhid\n\n        # 把句子返回原来的顺序\n        idx_unsort = np.argsort(idx_sort)\n        emb = sent_output.index_select(0, torch.cuda.LongTensor(idx_unsort))\n\n        return emb\n    \n```\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter1_Text_similarity_and_its_application/01Edit_Distance_Calculation_based_on_Python/","content":"\n# 编辑距离计算python实现\n\n\n<img src=\"./img/editdistance.png\" alt=\"drawing\" width=\"600\"/>\n\n<img src=\"./img/edittable.png\" alt=\"drawing\" width=\"300\"/>\n\n\n```python\ndef editDistDP(s1, s2): \n    m = len(s1)\n    n = len(s2)\n    # 创建一张表格记录所有子问题的答案\n    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n  \n    # 从下往上填充DP表格\n    for i in range(m+1): \n        for j in range(n+1): \n  \n            # 如果第一个字符串为空，唯一的编辑方法就是添加第二个字符串\n            if i == 0: \n                dp[i][j] = j    # Min. operations = j \n  \n            # 如果第二个字符串为空，唯一的方法就是删除第一个字符串中的所有字母\n            elif j == 0: \n                dp[i][j] = i    # Min. operations = i \n  \n            # 如果两个字符串结尾字母相同，我们就可以忽略最后的字母\n            elif s1[i-1] == s2[j-1]: \n                dp[i][j] = dp[i-1][j-1] \n  \n            # 如果结尾字母不同，那我们就需要考虑三种情况，取最小的编辑距离\n            else: \n                dp[i][j] = 1 + min(dp[i][j-1],        # 添加 \n                                   dp[i-1][j],        # 删除\n                                   dp[i-1][j-1])    # 替换\n  \n    return dp[m][n] \n\n\n```\n\n\n```python\ns1 = \"natural language processing is a promising research area\"\ns2 = \"more researchers are working on natural language processing nowadays\"\n```\n\n\n```python\nww2 = \"\"\"\nWorld War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 50 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\nJapan, which aimed to dominate Asia and the Pacific, was at war with China by 1937,[5][b] though neither side had declared war on the other. World War II is generally said to have begun on 1 September 1939,[6] with the invasion of Poland by Germany and subsequent declarations on Germany by France and the United Kingdom. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. Following the onset of campaigns in North Africa and East Africa, and the fall of France in mid 1940, the war continued primarily between the European Axis powers and the British Empire. War in the Balkans, the aerial Battle of Britain, the Blitz, and the long Battle of the Atlantic followed. On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history. This Eastern Front trapped the Axis, most crucially the German Wehrmacht, into a war of attrition. In December 1941, Japan launched a surprise attack on the United States and European colonies in the Pacific. Following an immediate U.S. declaration of war against Japan, supported by one from Great Britain, the European Axis powers quickly declared war on the U.S. in solidarity with their Japanese ally. Rapid Japanese conquests over much of the Western Pacific ensued, perceived by many in Asia as liberation from Western dominance and resulting in the support of several armies from defeated territories.\nThe Axis advance in the Pacific halted in 1942 when Japan lost the critical Battle of Midway; later, Germany and Italy were defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. Key setbacks in 1943, which included a series of German defeats on the Eastern Front, the Allied invasions of Sicily and Italy, and Allied victories in the Pacific, cost the Axis its initiative and forced it into strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained its territorial losses and turned toward Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in Central China, South China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.\nThe war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops, the suicide of Adolf Hitler and the German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, the Soviet entry into the war against Japan and its invasion of Manchuria, Japan announced its intention to surrender on 15 August 1945, cementing total victory in Asia for the Allies. Tribunals were set up by fiat by the Allies and war crimes trials were conducted in the wake of the war both against the Germans and the Japanese.\nWorld War II changed the political alignment and social structure of the globe. The United Nations (UN) was established to foster international co-operation and prevent future conflicts; the victorious great powers—China, France, the Soviet Union, the United Kingdom, and the United States—became the permanent members of its Security Council.[7] The Soviet Union and United States emerged as rival superpowers, setting the stage for the nearly half-century long Cold War. In the wake of European devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia. Most countries whose industries had been damaged moved towards economic recovery and expansion. Political integration, especially in Europe, emerged as an effort to end pre-war enmities and create a common identity.[8]\"\"\"\n\nww1 = \"\"\"World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously described as \"the war to end all wars\",[7] it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history.[8][9] It is also one of the deadliest conflicts in history,[10] with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide.[11]\nOn 28 June 1914, Gavrilo Princip, a Bosnian Serb Yugoslav nationalist, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand in Sarajevo, leading to the July Crisis.[12][13] In response, on 23 July Austria-Hungary issued an ultimatum to Serbia. Serbia's reply failed to satisfy the Austrians, and the two moved to a war footing.\nA network of interlocking alliances enlarged the crisis from a bilateral issue in the Balkans to one involving most of Europe. By July 1914, the great powers of Europe were divided into two coalitions: the Triple Entente—consisting of France, Russia and Britain—and the Triple Alliance of Germany, Austria-Hungary and Italy (the Triple Alliance was primarily defensive in nature, allowing Italy to stay out of the war in 1914).[14] Russia felt it necessary to back Serbia and, after Austria-Hungary shelled the Serbian capital of Belgrade on the 28th, partial mobilisation was approved.[15] General Russian mobilisation was announced on the evening of 30 July; on the 31st, Austria-Hungary and Germany did the same, while Germany demanded Russia demobilise within 12 hours.[16] When Russia failed to comply, Germany declared war on 1 August in support of Austria-Hungary, with Austria-Hungary following suit on 6th; France ordered full mobilisation in support of Russia on 2 August.[17]\nGerman strategy for a war on two fronts against France and Russia was to rapidly concentrate the bulk of its army in the West to defeat France within four weeks, then shift forces to the East before Russia could fully mobilise; this was later known as the Schlieffen Plan.[18] On 2 August, Germany demanded free passage through Belgium, an essential element in achieving a quick victory over France.[19] When this was refused, German forces invaded Belgium on 3 August and declared war on France the same day; the Belgian government invoked the 1839 Treaty of London and in compliance with its obligations under this, Britain declared war on Germany on 4 August.[20][21] On 12 August, Britain and France also declared war on Austria-Hungary; on the 23rd, Japan sided with the Entente, seizing German possessions in China and the Pacific. In November 1914, the Ottoman Empire entered the war on the side of the Alliance, opening fronts in the Caucasus, Mesopotamia and the Sinai Peninsula. The war was fought in and drew upon each powers' colonial empires as well, spreading the conflict to Africa and across the globe. The Entente and its allies would eventually become known as the Allied Powers, while the grouping of Austria-Hungary, Germany and their allies would become known as the Central Powers.\nThe German advance into France was halted at the Battle of the Marne and by the end of 1914, the Western Front settled into a battle of attrition, marked by a long series of trench lines that changed little until 1917 (the Eastern Front, by contrast, was marked by much greater exchanges of territory). In 1915, Italy joined the Allied Powers and opened a front in the Alps. The Kingdom of Bulgaria joined the Central Powers in 1915 and the Kingdom of Greece joined the Allies in 1917, expanding the war in the Balkans. The United States initially remained neutral, although by doing nothing to prevent the Allies from procuring American supplies whilst the Allied blockade effectively prevented the Germans from doing the same the U.S. became an important supplier of war material to the Allies. Eventually, after the sinking of American merchant ships by German submarines, and the revelation that the Germans were trying to incite Mexico to make war on the United States, the U.S. declared war on Germany on 6 April 1917. Trained American forces would not begin arriving at the front in large numbers until mid-1918, but ultimately the American Expeditionary Force would reach some two million troops.[22]\nThough Serbia was defeated in 1915, and Romania joined the Allied Powers in 1916 only to be defeated in 1917, none of the great powers were knocked out of the war until 1918. The 1917 February Revolution in Russia replaced the Tsarist autocracy with the Provisional Government, but continuing discontent at the cost of the war led to the October Revolution, the creation of the Soviet Socialist Republic, and the signing of the Treaty of Brest-Litovsk by the new government in March 1918, ending Russia's involvement in the war. This allowed the transfer of large numbers of German troops from the East to the Western Front, resulting in the German March 1918 Offensive. This offensive was initially successful, but the Allies rallied and drove the Germans back in their Hundred Days Offensive.[23] Bulgaria was the first Central Power to sign an armistice—the Armistice of Salonica on 29 September 1918. On 30 October, the Ottoman Empire capitulated, signing the Armistice of Mudros.[24] On 4 November, the Austro-Hungarian empire agreed to the Armistice of Villa Giusti. With its allies defeated, revolution at home, and the military no longer willing to fight, Kaiser Wilhelm abdicated on 9 November and Germany signed an armistice on 11 November 1918.\nWorld War I was a significant turning point in the political, cultural, economic, and social climate of the world. The war and its immediate aftermath sparked numerous revolutions and uprisings. The Big Four (Britain, France, the United States, and Italy) imposed their terms on the defeated powers in a series of treaties agreed at the 1919 Paris Peace Conference, the most well known being the German peace treaty—the Treaty of Versailles.[25] Ultimately, as a result of the war the Austro-Hungarian, German, Ottoman, and Russian Empires ceased to exist, with numerous new states created from their remains. However, despite the conclusive Allied victory (and the creation of the League of Nations during the Peace Conference, intended to prevent future wars), a Second World War would follow just over twenty years later.\"\"\"\n\nnetease = \"\"\"NetEase, Inc. (simplified Chinese: 网易; traditional Chinese: 網易; pinyin: WǎngYì) is a Chinese Internet technology company providing online services centered on content, community, communications and commerce. The company was founded in 1997 by Lebunto. NetEase develops and operates online PC and mobile games, advertising services, email services and e-commerce platforms in China. It is one of the largest Internet and video game companies in the world.[7]\nSome of NetEase's games include the Westward Journey series (Fantasy Westward Journey, Westward Journey Online II, Fantasy Westward Journey II, and New Westward Journey Online II), as well as other games, such as Tianxia III, Heroes of Tang Dynasty Zero and Ghost II. NetEase also partners with Blizzard Entertainment to operate local versions of Warcraft III, World of Warcraft, Hearthstone, StarCraft II, Diablo III: Reaper of Souls and Overwatch in China. They are also developing their very first self-developed VR multiplayer online game with an open world setting, which is called Nostos.[8]\"\"\"\n\n```\n\n\n```python\neditDistDP(s1.split(), s2.split())\n```\n\n\n\n\n    9\n\n\n\n\n```python\neditDistDP(ww1.split(), ww2.split())\n```\n\n\n\n\n    946\n\n\n\n\n```python\neditDistDP(ww1.split(), netease.split())\n```\n\n\n\n\n    1039\n\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/10Visual_Text_Task_Image_Caption_and_VQA/Chapter2_Visual_Q_A/VQA/","content":"\n# 视觉问答机器人（VQA) 原理与实现\n\n\n## 本章概述\n    2.1 视觉问答机器人问题介绍\n    2.2 基于图像信息和文本信息抽取匹配的VQA实现方案\n    2.3 基于注意力（attention）的深度学习VQA实现方案\n    2.4 【实战】使用keras完成CNN+RNN基础VQA模型\n    2.5 【实战】基于attention 的深度学习VQA模型实现\n\n\n## 2.1 视觉问答机器人问题介绍\n- 视觉问答任务的定义是对于一张图片和一个跟这幅图片相关的问题，机器需要根据图片信息对问题进行回答。\n- 输入：一张图片和一个关于图片信息的问题，常见的问题形式有选择题，判断题\n- 输出：挑选出正确答案\n![](./img/vqa-example.jpg)\n- 问题: how many players are in the image?\n- 答案: eleven \n- 人可以清楚地指出图片中的运动员，而且不会把观众也计算在内，我们希望AI机器人也能够对图片信息进行理解，根据问题进行筛选，之后返回正确的答案。\n\n## 2.1 视觉问答机器人问题介绍\n- 视觉问答任务本质上是一个多模态的研究问题。这个任务需要我们结合自然语言处理（NLP）和计算机视觉（CV)的技术来进行回答。\n- 自然语言处理（NLP）\n    - 先理解问题\n    - 再产生答案\n    - 举一个在NLP领域常见的基于文本的Q&A问题：how many bridges are there in Paris?\n    - 一个NLP Q&A 系统需要首先识别出这是一个什么类型的问题，比如这里是一个“how many” 关于计数的问题，所以答案应该是一个数字。之后系统需要提取出哪个物体（object）需要机器去计数，比如这里是 “bridges“。最后需要我们提取出问题中的背景（context），比如这个问题计数的限定范围是在巴黎这个城市。\n    - 当一个Q&A系统分析完问题，系统需要根据知识库（knowledge base）去得到答案。\n- 机器视觉（CV)\n    - VQA区别于传统的text QA在于搜索答案和推理部分都是基于图片的内容。所以系统需要进行目标检测（object detection），再进行分类（classification），之后系统需要对图片中物体之间的关系进行推理。\n- 总结来说，一个好的VQA系统需要具备能够解决传统的NLP及CV的基础任务，所以这是一个交叉学科，多模态的研究问题。\n\n## 2.1 视觉问答机器人问题介绍¶\n- 图片数据集\n    - Microsoft Common Objects in Context (MSCOCO) 包含了328000张图片，91类物体，2500000个标注数据，这些物体能够被一个4岁小孩轻易地识别出来。\n![](./img/dataset-coco.jpg)\n- 常见的VQA数据集：一个好的数据集需要尽量避免数据采集过程中的偏差（bias），比如说一个数据集中，90%的判断题的答案都是yes，那么一个只输出yes的系统的准确率有90%。\n    - DAtaset for QUestion Answering on Real-world images (DAQUAR)，第一个重要的VQA数据集，包含了6794个训练样本，5674个测试样本，图片都来自NYU-Depth V2数据集，平均一张图片包含了9个问题答案对（QA pair），这个数据集的缺点是数据太小，不足以训练一个复杂的VQA系统。\n![](./img/dataset-daquar.jpg)\n\n    - COCO-QA数据集使用了MSCOCO中123287张图片，其中78736个QA对作为训练，38948个QA对作为测试。这个数据集是通过对MSCOCO中的图片标题（caption）使用NLP工具自动生成出问题和答案对（QA pair），比如一个标题“two chairs in a room”，可以生成一个问题”how many chairs are there？“，所有的答案都是一个单词。虽然这个数据集足够大，但是这种产生QA pair的方法会使得语法错误，或者信息不完整地错误。而且这个数据集只包含了4类问题，且这四类问题的数量不均等，object（69.84%），color（16.59%），counting（7.47%）， location（6.10%）\n![](./img/dataset-coco-qa.jpg)\n    - the VQA dataset 相对来说更大一些，出来204721张来自MSCOCO的图片，还包含了50000张抽象的卡通图片。一张图片平均有3个问题，一个问题平均有10个答案，总共有超过760000个问题和10000000个答案。全部问题和答案对都是Amazon Mechanical Turk上让人标注的。同时问题包括开放性问题和多选项问题。对于开放性问题，至少3个人提供了一模一样的答案才能作为正确的答案。对于多选题，他们创建了18个候选答案，其中正确（correct）答案是10个人都认为正确的一个答案，有可能（plausible）答案是由三个人没有看过图片只根据问题提供的三个答案，常见（popular）答案是由10个最常见的回答组成（yes，no，1，2，3，4，white，red，blue，green），随机（random）答案是从其他问题的正确答案中随机挑选出来的一个答案。这个数据集的缺点是有些问题太主观了。另一个缺点是有些问题根本不需要图片信息，比如“how many legs does the dog have?” 或者 “what color are the trees?”\n![](./img/dataset-vqa.jpg)\n![](./img/question-type.png)\n\n## 2.2 基于图像信息和文本信息抽取匹配的VQA实现方案\n- 通常，一个VQA系统包含了以下三个步骤：\n    1. 抽取问题特征\n    2. 抽取图片特征\n    3. 结合图片和问题特征去生成答案\n![](./img/visual-question-answering-approach.jpg)\n\n## 2.2 基于图像信息和文本信息抽取匹配的VQA实现方案\n- 抽取问题特征\n    - 我们通常可以用Bag-of-Words (BOW) 或者LSTM去编码一个问题信息\n- 抽取图片信息\n    - 我们通常使用在ImageNet上预训练好的CNN模型\n- 生成答案经常被简化为一个分类问题\n- 各种方法之间比较不一样的是如何把文字特征与图片特征结合。比如我们可以通过把两个特征拼接（concatenation）在一起之后接上一个线性分类器。或者通过Bayesian的方法去预测问题，图片及答案三者之间的特征分布的关系。\n\n## 2.2 基于图像信息和文本信息抽取匹配的VQA实现方案\n- 基本方法（baselines), Antol et al. (2016) \"VQA: Visual Question Answering\"，该文章提出通过简单的特征拼接（concatenation）或者element-wise sum/product的方式去融合文本和图片的特征。其中图片特征使用了VGGNet最后一层的1024维特征，文本特征有以下两种方法\n    1. 使用BOW的方法去编码一个问题的文本特征，之后再用一个多层的感知器（multi-layer perceptron，MLP）去预测答案。其中MLP包含了两个隐含层，1000个隐含元，使用了tanh 非线性函数，0.5的dropout。\n    2. 一个LSTM模型，通过softmax 去预测答案\n![](./img/baseline.png)\n- 这些基本方法的结果很有意思，如果一个模型只使用了文本特征，其正确率为48.09%，如果一个模型只使用了图片特征，其正确率为28.13%，而他们最好的模型是使用了LSTM去编码文本特征的，能达到53.74%的正确率。而且多选题的结果会显著好于开放式问题的效果。所有的模型预测的结果都远不如人类的表现。\n![](./img/baseline-results.png)  \n\n## 2.3 基于注意力（attention）的深度学习VQA实现方案\n- 基于注意力的深度学习VQA方法是通过关注图片中相关的部位来获得答案，比如一个问题“what color is the ball?\"，则图片中包含了球ball这个object的小区域是比其他区域更具有信息量，比其他区域更相关。相似的，”color“ 和”ball“也比其他单词更加相关。\n- 另一个常见的VQA方案是使用位置注意力（spatial attention）去生成关于区域（region）的位置特征，并训练一个CNN网络。一般有两种方法去获得一张图片关于方位的区域。\n    1. 通过将一张图片划分成网格状（grid），并根据问题与图片特征去预测每一个网格的attention weight，将图片的CNN的feature通过加权求和的方式得到attention weighted feature，再通过attention weighted feature发现相对比较重要的区域\n![](./img/approach-attention.jpg)\n    2. 通过目标识别的方式生成很多bounding box\n![](./img/approach-attention-bounding-box.jpg)\n- 根据生成的区域（region），使用问题去找到最相关的区域，并利用这些区域去生成答案。\n\n## 2.3 基于注意力（attention）的深度学习VQA实现方案\n- Yang et al. 2016  Stacked Attention Networks for Image Question Answering，提出了一个基于堆叠注意力的VQA系统\n- 图片使用CNN 编码\n    $$f_I = CNN_{vgg}(I)$$\n![](./img/san-cnn.png)    \n- 问题使用LSTM编码\n    $$h_t= LSTM(q), ~~ h_t=CNN(q)$$\n![](./img/san-q-rnn.png)\n![](./img/san-q-cnn.png)\n- Stacked Attention，多次重复question-image attention\n![](./img/san.png)\n\n## 2.3 基于注意力（attention）的深度学习VQA实现方案\n- Kazemi (2017 et al.) Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering，提出了一个基于注意力的VQA系统\n![](./img/vqa-attend.png)\n- 图片使用CNN 编码\n    $$\\phi = CNN(I)$$\n- 问题使用LSTM编码\n    $$s= LSTM(E_q)$$\n- Stacked Attention\n    $$\\alpha_{c,l} \\propto \\exp F_c(s, \\phi_l) ,~~  \\sum_{l=1}^L \\alpha_{c,l}=1, ~~ x_c = \\sum_l \\alpha_{c,l}\\phi_l$$\n- classifier, 其中G=[G_1, G_2, ..., G_M]是两层的全连接层\n    $$P(a_i|I,q) \\propto \\exp G_i(x,s),~~ x=[x_1, x2,...,x_C]$$\n![](./img/vqa-model.png)\n\n## 2.4 【实战】使用keras完成CNN+RNN基础VQA模型\n- Keras VQA Demo https://github.com/iamaaditya/VQA_Demo\n    1. Keras version 2.0+\n    2. Tensorflow 1.2+ \n    3. scikit-learn\n    4. Spacy version 2.0+，用于下载Glove Word embeddings\n    ```bash\n    python -m spacy download en_vectors_web_lg\n    ```\n    5. OpenCV，用于resize图片成224x224大小\n    6. VGG 16，预训练好的权重\n\n```bash\npython demo.py -image_file_name test.jpg -question \"Is there a man in the picture?\"\n```\n![](./test.jpg)\n\n\n\n```bash\n%%bash\n! git clone https://github.com/iamaaditya/VQA_Demo\n! cd VQA_Demo\n```\n\n    Cloning into 'VQA_Demo'...\n    \n\n\n```python\ndef VQA_MODEL():\n    image_feature_size          = 4096\n    word_feature_size           = 300\n    number_of_LSTM              = 3\n    number_of_hidden_units_LSTM = 512\n    max_length_questions        = 30\n    number_of_dense_layers      = 3\n    number_of_hidden_units      = 1024\n    activation_function         = 'tanh'\n    dropout_pct                 = 0.5\n\n\n    # Image model\n    model_image = Sequential()\n    model_image.add(Reshape((image_feature_size,), input_shape=(image_feature_size,)))\n\n    # Language Model\n    model_language = Sequential()\n    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True, input_shape=(max_length_questions, word_feature_size)))\n    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True))\n    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=False))\n\n    # combined model\n    model = Sequential()\n    model.add(Merge([model_language, model_image], mode='concat', concat_axis=1))\n\n    for _ in xrange(number_of_dense_layers):\n        model.add(Dense(number_of_hidden_units, kernel_initializer='uniform'))\n        model.add(Activation(activation_function))\n        model.add(Dropout(dropout_pct))\n\n    model.add(Dense(1000))\n    model.add(Activation('softmax'))\n\n    return model\n```\n\n![](./img/model_vqa.png)\n\n\n```python\n# 载入库\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, argparse\nimport cv2, spacy, numpy as np\nfrom keras.models import model_from_json\nfrom keras.optimizers import SGD\nfrom sklearn.externals import joblib\nfrom keras import backend as K\nfrom keras.utils.vis_utils import plot_model\nK.set_image_data_format('channels_first')\n#K.set_image_dim_ordering('th')\n```\n\n    Using TensorFlow backend.\n    \n\n\n```python\n# 载入模型的权重\n# 需要下载 VGG weights\nVQA_model_file_name      = 'models/VQA/VQA_MODEL.json'\nVQA_weights_file_name   = 'models/VQA/VQA_MODEL_WEIGHTS.hdf5'\nlabel_encoder_file_name  = 'models/VQA/FULL_labelencoder_trainval.pkl'\nCNN_weights_file_name   = 'models/CNN/vgg16_weights.h5'\n\n```\n\n\n```python\n# 编译图像模型\ndef get_image_model(CNN_weights_file_name):\n    ''' Takes the CNN weights file, and returns the VGG model update \n    with the weights. Requires the file VGG.py inside models/CNN '''\n    from models.CNN.VGG import VGG_16\n    image_model = VGG_16(CNN_weights_file_name)\n    image_model.layers.pop()\n    image_model.layers.pop()\n    # this is standard VGG 16 without the last two layers\n    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n    # one may experiment with \"adam\" optimizer, but the loss function for\n    # this kind of task is pretty standard\n    image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n    return image_model\n```\n\n\n```python\n# 获得图像特征\ndef get_image_features(image_file_name):\n    ''' Runs the given image_file to VGG 16 model and returns the \n    weights (filters) as a 1, 4096 dimension vector '''\n    image_features = np.zeros((1, 4096))\n    # Magic_Number = 4096  > Comes from last layer of VGG Model\n\n    # Since VGG was trained as a image of 224x224, every new image\n    # is required to go through the same transformation\n    im = cv2.resize(cv2.imread(image_file_name), (224, 224))\n    im = im.transpose((2,0,1)) # convert the image to RGBA\n\n    \n    # this axis dimension is required because VGG was trained on a dimension\n    # of 1, 3, 224, 224 (first axis is for the batch size\n    # even though we are using only one image, we have to keep the dimensions consistent\n    im = np.expand_dims(im, axis=0) \n\n    image_features[0,:] = image_model.predict(im)[0]\n    return image_features\n```\n\n\n```python\n# 获得问题特征\ndef get_question_features(question):\n    ''' For a given question, a unicode string, returns the time series vector\n    with each word (token) transformed into a 300 dimension representation\n    calculated using Glove Vector '''\n    word_embeddings = spacy.load('en_vectors_web_lg')\n    tokens = word_embeddings(question)\n    question_tensor = np.zeros((1, 30, 300))\n    for j in xrange(len(tokens)):\n        question_tensor[0,j,:] = tokens[j].vector\n    return question_tensor\n```\n\n\n```python\n# 构建VQA系统\ndef get_VQA_model(VQA_model_file_name, VQA_weights_file_name):\n    ''' Given the VQA model and its weights, compiles and returns the model '''\n\n    # thanks the keras function for loading a model from JSON, this becomes\n    # very easy to understand and work. Alternative would be to load model\n    # from binary like cPickle but then model would be obfuscated to users\n    vqa_model = model_from_json(open(VQA_model_file_name).read())\n    vqa_model.load_weights(VQA_weights_file_name)\n    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n    return vqa_model\n```\n\n\n```python\nimage_model = get_image_model(CNN_weights_file_name)\nplot_model(image_model, to_file='model_vgg.png') \n```\n\n\n```python\n# 测试一张图片和问题\nimage_file_name = 'test.jpg'\nquestion = u\"What vehicle is in the picture?\"\n# 获取图片特征\nimage_features = get_image_features(image_file_name)\n# 获取问题特征\nquestion_features = get_question_features(question)\n\ny_output = model_vqa.predict([question_features, image_features])\n\n# This task here is represented as a classification into a 1000 top answers\n# this means some of the answers were not part of training and thus would \n# not show up in the result.\n# These 1000 answers are stored in the sklearn Encoder class\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nlabelencoder = joblib.load(label_encoder_file_name)\nfor label in reversed(np.argsort(y_output)[0,-5:]):\n    print(str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label))\n```\n\n## 【2.5 实战】基于attention 的深度学习VQA模型实现\n- pytorch Attention VQA https://github.com/Cyanogenoid/pytorch-vqa\n    - python 3.6\n    - torch\n    - torchvision\n    - h5py\n    - tqdm\n\n\n```bash\n%%bash\n# 下载github repo\ngit clone https://github.com/Cyanogenoid/pytorch-vqa --recursive\n```\n\n    Submodule path 'resnet': checked out '9332392b01317d57e92f81e00933c48f423ff503'\n    \n\n    Cloning into 'pytorch-vqa'...\n    Submodule 'resnet' (https://github.com/Cyanogenoid/pytorch-resnet) registered for path 'resnet'\n    Cloning into '/Users/jjhu/MT/slides/MT-course/vqa/pytorch-vqa/resnet'...\n    \n\n\n```bash\n%%bash\n# 预处理图片与vocab\npython preprocess-images.py\npython preprocess-vocab.py\n```\n\n\n```bash\n%%bash\n# 开始训练模型\npython train.py\n```\n\n![](./img/train_log.png)\n\n### 训练代码\n```python \n# 训练的main 函数\ndef main():\n    if len(sys.argv) > 1:\n        name = ' '.join(sys.argv[1:])\n    else:\n        from datetime import datetime\n        name = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n    target_name = os.path.join('logs', '{}.pth'.format(name))\n    print('will save to {}'.format(target_name))\n\n    cudnn.benchmark = True\n\n    # 加载训练数据及validation数据\n    train_loader = data.get_loader(train=True)\n    val_loader = data.get_loader(val=True)\n    \n    # 加载vqa模型及优化器\n    net = nn.DataParallel(model.Net(train_loader.dataset.num_tokens)).cuda()\n    optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad])\n\n    tracker = utils.Tracker()\n    config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n\n    for i in range(config.epochs):\n        _ = run(net, train_loader, optimizer, tracker, train=True, prefix='train', epoch=i)\n        r = run(net, val_loader, optimizer, tracker, train=False, prefix='val', epoch=i)\n\n        results = {\n            'name': name,\n            'tracker': tracker.to_dict(),\n            'config': config_as_dict,\n            'weights': net.state_dict(),\n            'eval': {\n                'answers': r[0],\n                'accuracies': r[1],\n                'idx': r[2],\n            },\n            'vocab': train_loader.dataset.vocab,\n        }\n        torch.save(results, target_name)\n        \ndef run(net, loader, optimizer, tracker, train=False, prefix='', epoch=0):\n    \"\"\" Run an epoch over the given loader \"\"\"\n    if train:\n        net.train()\n        tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n    else:\n        net.eval()\n        tracker_class, tracker_params = tracker.MeanMonitor, {}\n        answ = []\n        idxs = []\n        accs = []\n\n    tq = tqdm(loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n    loss_tracker = tracker.track('{}_loss'.format(prefix), tracker_class(**tracker_params))\n    acc_tracker = tracker.track('{}_acc'.format(prefix), tracker_class(**tracker_params))\n\n    log_softmax = nn.LogSoftmax().cuda()\n    for v, q, a, idx, q_len in tq:\n        var_params = {\n            'volatile': not train,\n            'requires_grad': False,\n        }\n        v = Variable(v.cuda(async=True), **var_params)\n        q = Variable(q.cuda(async=True), **var_params)\n        a = Variable(a.cuda(async=True), **var_params)\n        q_len = Variable(q_len.cuda(async=True), **var_params)\n\n        out = net(v, q, q_len)\n        nll = -log_softmax(out)\n        loss = (nll * a / 10).sum(dim=1).mean()\n        acc = utils.batch_accuracy(out.data, a.data).cpu()\n\n        if train:\n            global total_iterations\n            update_learning_rate(optimizer, total_iterations)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_iterations += 1\n        else:\n            # store information about evaluation of this minibatch\n            _, answer = out.data.cpu().max(dim=1)\n            answ.append(answer.view(-1))\n            accs.append(acc.view(-1))\n            idxs.append(idx.view(-1).clone())\n\n        loss_tracker.append(loss.data[0])\n        # acc_tracker.append(acc.mean())\n        for a in acc:\n            acc_tracker.append(a.item())\n        fmt = '{:.4f}'.format\n        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))\n\n    if not train:\n        answ = list(torch.cat(answ, dim=0))\n        accs = list(torch.cat(accs, dim=0))\n        idxs = list(torch.cat(idxs, dim=0))\n        return answ, accs, idxs\n\n```\n\n### attention VQA 模型代码讲解\n```python\n\nclass Net(nn.Module):\n    \"\"\" Re-implementation of ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' [0]\n    [0]: https://arxiv.org/abs/1704.03162\n    \"\"\"\n\n    def __init__(self, embedding_tokens):\n        super(Net, self).__init__()\n        question_features = 1024\n        vision_features = config.output_features\n        glimpses = 2\n\n        self.text = TextProcessor(\n            embedding_tokens=embedding_tokens,\n            embedding_features=300,\n            lstm_features=question_features,\n            drop=0.5,\n        )\n        self.attention = Attention(\n            v_features=vision_features,\n            q_features=question_features,\n            mid_features=512,\n            glimpses=2,\n            drop=0.5,\n        )\n        self.classifier = Classifier(\n            in_features=glimpses * vision_features + question_features,\n            mid_features=1024,\n            out_features=config.max_answers,\n            drop=0.5,\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n                init.xavier_uniform(m.weight)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def forward(self, v, q, q_len):\n        q = self.text(q, list(q_len.data))\n\n        v = v / (v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n        a = self.attention(v, q)\n        v = apply_attention(v, a)\n\n        combined = torch.cat([v, q], dim=1)\n        answer = self.classifier(combined)\n        return answer\n```\n\n### 分类器\n```python \nclass Classifier(nn.Sequential):\n    def __init__(self, in_features, mid_features, out_features, drop=0.0):\n        super(Classifier, self).__init__()\n        self.add_module('drop1', nn.Dropout(drop))\n        self.add_module('lin1', nn.Linear(in_features, mid_features))\n        self.add_module('relu', nn.ReLU())\n        self.add_module('drop2', nn.Dropout(drop))\n        self.add_module('lin2', nn.Linear(mid_features, out_features))\n```\n\n### attention 层\n```python \nclass Attention(nn.Module):\n    def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n        super(Attention, self).__init__()\n        self.v_conv = nn.Conv2d(v_features, mid_features, 1, bias=False)  # let self.lin take care of bias\n        self.q_lin = nn.Linear(q_features, mid_features)\n        self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n\n        self.drop = nn.Dropout(drop)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, v, q):\n        v = self.v_conv(self.drop(v))\n        q = self.q_lin(self.drop(q))\n        q = tile_2d_over_nd(q, v)\n        x = self.relu(v + q)\n        x = self.x_conv(self.drop(x))\n        return x\n\n\ndef apply_attention(input, attention):\n    \"\"\" Apply any number of attention maps over the input.\n        The attention map has to have the same size in all dimensions except dim=1.\n    \"\"\"\n    n, c = input.size()[:2]\n    glimpses = attention.size(1)\n\n    # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n    input = input.view(n, c, -1)\n    attention = attention.view(n, glimpses, -1)\n    s = input.size(2)\n\n    # apply a softmax to each attention map separately\n    # since softmax only takes 2d inputs, we have to collapse the first two dimensions together\n    # so that each glimpse is normalized separately\n    attention = attention.view(n * glimpses, -1)\n    attention = F.softmax(attention)\n\n    # apply the weighting by creating a new dim to tile both tensors over\n    target_size = [n, glimpses, c, s]\n    input = input.view(n, 1, c, s).expand(*target_size)\n    attention = attention.view(n, glimpses, 1, s).expand(*target_size)\n    weighted = input * attention\n    # sum over only the spatial dimension\n    weighted_mean = weighted.sum(dim=3)\n    # the shape at this point is (n, glimpses, c, 1)\n    return weighted_mean.view(n, -1)\n\n```\n\n## 本章小结\n    2.1 视觉问答机器人问题介绍\n    2.2 基于图像信息和文本信息抽取匹配的VQA实现方案\n    2.3 基于注意力（attention）的深度学习VQA实现方案\n    2.4 【实战】使用keras完成CNN+RNN基础VQA模型\n    2.5 【实战】基于attention 的深度学习VQA模型实现\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter1_Text_similarity_and_its_application/03Cosine_similarity/","content":"\n## Cosine similarity\n\n\nbag of words计算文本相似度\n\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef bow_cosine(s1, s2):\n    vectorizer = CountVectorizer()\n    vectorizer.fit([s1, s2])\n    X = vectorizer.transform([s1, s2])\n    print(X.toarray())\n\n    print(cosine_similarity(X[0], X[1]))\n    \ns1 = \"Natural language processing is a promising research area\"\ns2 = \"More and more researchers are working on natural language processing nowadays\"\nbow_cosine(s1, s2)\n```\n\n    [[0 0 1 1 1 0 1 0 0 1 1 1 0 0]\n     [1 1 0 0 1 2 1 1 1 1 0 0 1 1]]\n    [[0.31448545]]\n    \n\nTFIDF计算文本相似度\n\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tfidf_cosine(s1, s2):\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit([s1, s2])\n    X = vectorizer.transform([s1, s2])\n    print(X.toarray())\n    print(cosine_similarity(X[0], X[1]))\n    \ntfidf_cosine(s1, s2)\n```\n\n    [[0.         0.         0.42567716 0.42567716 0.30287281 0.\n      0.30287281 0.         0.         0.30287281 0.42567716 0.42567716\n      0.         0.        ]\n     [0.29464404 0.29464404 0.         0.         0.20964166 0.58928809\n      0.20964166 0.29464404 0.29464404 0.20964166 0.         0.\n      0.29464404 0.29464404]]\n    [[0.19048428]]\n    \n\n\n```python\nww2 = \"\"\"\nWorld War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945. The vast majority of the world's countries—including all the great powers—eventually formed two opposing military alliances: the Allies and the Axis. A state of total war emerged, directly involving more than 100 million people from over 30 countries. The major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 50 to 85 million fatalities, most of whom were civilians in the Soviet Union and China. It included massacres, the genocide of the Holocaust, strategic bombing, premeditated death from starvation and disease, and the only use of nuclear weapons in war.[1][2][3][4]\nJapan, which aimed to dominate Asia and the Pacific, was at war with China by 1937,[5][b] though neither side had declared war on the other. World War II is generally said to have begun on 1 September 1939,[6] with the invasion of Poland by Germany and subsequent declarations on Germany by France and the United Kingdom. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. Following the onset of campaigns in North Africa and East Africa, and the fall of France in mid 1940, the war continued primarily between the European Axis powers and the British Empire. War in the Balkans, the aerial Battle of Britain, the Blitz, and the long Battle of the Atlantic followed. On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history. This Eastern Front trapped the Axis, most crucially the German Wehrmacht, into a war of attrition. In December 1941, Japan launched a surprise attack on the United States and European colonies in the Pacific. Following an immediate U.S. declaration of war against Japan, supported by one from Great Britain, the European Axis powers quickly declared war on the U.S. in solidarity with their Japanese ally. Rapid Japanese conquests over much of the Western Pacific ensued, perceived by many in Asia as liberation from Western dominance and resulting in the support of several armies from defeated territories.\nThe Axis advance in the Pacific halted in 1942 when Japan lost the critical Battle of Midway; later, Germany and Italy were defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. Key setbacks in 1943, which included a series of German defeats on the Eastern Front, the Allied invasions of Sicily and Italy, and Allied victories in the Pacific, cost the Axis its initiative and forced it into strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained its territorial losses and turned toward Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in Central China, South China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.\nThe war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops, the suicide of Adolf Hitler and the German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, the Soviet entry into the war against Japan and its invasion of Manchuria, Japan announced its intention to surrender on 15 August 1945, cementing total victory in Asia for the Allies. Tribunals were set up by fiat by the Allies and war crimes trials were conducted in the wake of the war both against the Germans and the Japanese.\nWorld War II changed the political alignment and social structure of the globe. The United Nations (UN) was established to foster international co-operation and prevent future conflicts; the victorious great powers—China, France, the Soviet Union, the United Kingdom, and the United States—became the permanent members of its Security Council.[7] The Soviet Union and United States emerged as rival superpowers, setting the stage for the nearly half-century long Cold War. In the wake of European devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia. Most countries whose industries had been damaged moved towards economic recovery and expansion. Political integration, especially in Europe, emerged as an effort to end pre-war enmities and create a common identity.[8]\"\"\"\n\nww1 = \"\"\"World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously described as \"the war to end all wars\",[7] it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history.[8][9] It is also one of the deadliest conflicts in history,[10] with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide.[11]\nOn 28 June 1914, Gavrilo Princip, a Bosnian Serb Yugoslav nationalist, assassinated the Austro-Hungarian heir Archduke Franz Ferdinand in Sarajevo, leading to the July Crisis.[12][13] In response, on 23 July Austria-Hungary issued an ultimatum to Serbia. Serbia's reply failed to satisfy the Austrians, and the two moved to a war footing.\nA network of interlocking alliances enlarged the crisis from a bilateral issue in the Balkans to one involving most of Europe. By July 1914, the great powers of Europe were divided into two coalitions: the Triple Entente—consisting of France, Russia and Britain—and the Triple Alliance of Germany, Austria-Hungary and Italy (the Triple Alliance was primarily defensive in nature, allowing Italy to stay out of the war in 1914).[14] Russia felt it necessary to back Serbia and, after Austria-Hungary shelled the Serbian capital of Belgrade on the 28th, partial mobilisation was approved.[15] General Russian mobilisation was announced on the evening of 30 July; on the 31st, Austria-Hungary and Germany did the same, while Germany demanded Russia demobilise within 12 hours.[16] When Russia failed to comply, Germany declared war on 1 August in support of Austria-Hungary, with Austria-Hungary following suit on 6th; France ordered full mobilisation in support of Russia on 2 August.[17]\nGerman strategy for a war on two fronts against France and Russia was to rapidly concentrate the bulk of its army in the West to defeat France within four weeks, then shift forces to the East before Russia could fully mobilise; this was later known as the Schlieffen Plan.[18] On 2 August, Germany demanded free passage through Belgium, an essential element in achieving a quick victory over France.[19] When this was refused, German forces invaded Belgium on 3 August and declared war on France the same day; the Belgian government invoked the 1839 Treaty of London and in compliance with its obligations under this, Britain declared war on Germany on 4 August.[20][21] On 12 August, Britain and France also declared war on Austria-Hungary; on the 23rd, Japan sided with the Entente, seizing German possessions in China and the Pacific. In November 1914, the Ottoman Empire entered the war on the side of the Alliance, opening fronts in the Caucasus, Mesopotamia and the Sinai Peninsula. The war was fought in and drew upon each powers' colonial empires as well, spreading the conflict to Africa and across the globe. The Entente and its allies would eventually become known as the Allied Powers, while the grouping of Austria-Hungary, Germany and their allies would become known as the Central Powers.\nThe German advance into France was halted at the Battle of the Marne and by the end of 1914, the Western Front settled into a battle of attrition, marked by a long series of trench lines that changed little until 1917 (the Eastern Front, by contrast, was marked by much greater exchanges of territory). In 1915, Italy joined the Allied Powers and opened a front in the Alps. The Kingdom of Bulgaria joined the Central Powers in 1915 and the Kingdom of Greece joined the Allies in 1917, expanding the war in the Balkans. The United States initially remained neutral, although by doing nothing to prevent the Allies from procuring American supplies whilst the Allied blockade effectively prevented the Germans from doing the same the U.S. became an important supplier of war material to the Allies. Eventually, after the sinking of American merchant ships by German submarines, and the revelation that the Germans were trying to incite Mexico to make war on the United States, the U.S. declared war on Germany on 6 April 1917. Trained American forces would not begin arriving at the front in large numbers until mid-1918, but ultimately the American Expeditionary Force would reach some two million troops.[22]\nThough Serbia was defeated in 1915, and Romania joined the Allied Powers in 1916 only to be defeated in 1917, none of the great powers were knocked out of the war until 1918. The 1917 February Revolution in Russia replaced the Tsarist autocracy with the Provisional Government, but continuing discontent at the cost of the war led to the October Revolution, the creation of the Soviet Socialist Republic, and the signing of the Treaty of Brest-Litovsk by the new government in March 1918, ending Russia's involvement in the war. This allowed the transfer of large numbers of German troops from the East to the Western Front, resulting in the German March 1918 Offensive. This offensive was initially successful, but the Allies rallied and drove the Germans back in their Hundred Days Offensive.[23] Bulgaria was the first Central Power to sign an armistice—the Armistice of Salonica on 29 September 1918. On 30 October, the Ottoman Empire capitulated, signing the Armistice of Mudros.[24] On 4 November, the Austro-Hungarian empire agreed to the Armistice of Villa Giusti. With its allies defeated, revolution at home, and the military no longer willing to fight, Kaiser Wilhelm abdicated on 9 November and Germany signed an armistice on 11 November 1918.\nWorld War I was a significant turning point in the political, cultural, economic, and social climate of the world. The war and its immediate aftermath sparked numerous revolutions and uprisings. The Big Four (Britain, France, the United States, and Italy) imposed their terms on the defeated powers in a series of treaties agreed at the 1919 Paris Peace Conference, the most well known being the German peace treaty—the Treaty of Versailles.[25] Ultimately, as a result of the war the Austro-Hungarian, German, Ottoman, and Russian Empires ceased to exist, with numerous new states created from their remains. However, despite the conclusive Allied victory (and the creation of the League of Nations during the Peace Conference, intended to prevent future wars), a Second World War would follow just over twenty years later.\"\"\"\n\nnetease = \"\"\"NetEase, Inc. (simplified Chinese: 网易; traditional Chinese: 網易; pinyin: WǎngYì) is a Chinese Internet technology company providing online services centered on content, community, communications and commerce. The company was founded in 1997 by Lebunto. NetEase develops and operates online PC and mobile games, advertising services, email services and e-commerce platforms in China. It is one of the largest Internet and video game companies in the world.[7]\nSome of NetEase's games include the Westward Journey series (Fantasy Westward Journey, Westward Journey Online II, Fantasy Westward Journey II, and New Westward Journey Online II), as well as other games, such as Tianxia III, Heroes of Tang Dynasty Zero and Ghost II. NetEase also partners with Blizzard Entertainment to operate local versions of Warcraft III, World of Warcraft, Hearthstone, StarCraft II, Diablo III: Reaper of Souls and Overwatch in China. They are also developing their very first self-developed VR multiplayer online game with an open world setting, which is called Nostos.[8]\"\"\"\n\nbow_cosine(ww1, ww2)\nbow_cosine(ww1, netease)\n```\n\n    [[1 1 3 ... 0 1 1]\n     [0 1 0 ... 1 0 0]]\n    [[0.9197938]]\n    [[1 1 3 ... 0 0 0]\n     [0 0 0 ... 1 1 1]]\n    [[0.42054778]]\n    \n\n\n```python\ntfidf_cosine(ww1, ww2)\ntfidf_cosine(ww1, netease)\n```\n\n    [[0.00907415 0.00645633 0.02722245 ... 0.         0.00907415 0.00907415]\n     [0.         0.00796931 0.         ... 0.01120059 0.         0.        ]]\n    [[0.88292567]]\n    [[0.00878095 0.00878095 0.02634285 ... 0.         0.         0.        ]\n     [0.         0.         0.         ... 0.0551607  0.0551607  0.0551607 ]]\n    [[0.32048435]]\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter2_Chapter1_Text_Classification_based_on_Deep_Learning/1.text-classification/","content":"\n# 词嵌入与fine-tuning\n\n\n很多高阶的深度学习自然语言处理任务，都可以用词向量作为基础。我们课程的很多任务，可以用预训练好的word2vec初始化，接下来进行fine-tuning。如本章的文本分类。\n其基本思路是将离散的词嵌入到连续的空间中，并以此作为词的表示输入到下层的任务中去。\n![](http://i.stack.imgur.com/fYxO9.png)\n\n## 如何使用?\n\n* 从头训练 \n * 就像word2vec一样, 这一层是可学习的, 用随机数initialize , 通过BP去调整.\n* pre-trained + fine tuning \n * 用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 然后继续学习.\n* pre-trained + static \n * 用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 并且这些参数保持固定, 不参与网络的学习.\n\n# 基于卷积神经网络的文本分类\n\n\n* 参考文章：[《paddlepaddle情感分析》](http://paddlepaddle.org/documentation/docs/zh/1.2/beginners_guide/basics/understand_sentiment/index.html)\n\n## CNN\n\n\n\n传统CNN包含卷积层、全连接层等组件，并采用softmax多类别分类器和多类交叉熵损失函数，一个典型的卷积神经网络如图所示，我们先介绍用来构造CNN的常见组件。\n\n<p align=\"center\">\n<img src=\"https://github.com/PaddlePaddle/book/blob/develop/03.image_classification/image/lenet.png?raw=true\"><br/>\n</p>\n\n\n- 卷积层(convolution layer): 执行卷积操作提取底层到高层的特征，发掘出图片局部关联性质和空间不变性质。\n- 池化层(pooling layer): 执行降采样操作。通过取卷积输出特征图中局部区块的最大值(max-pooling)或者均值(avg-pooling)。降采样也是图像处理中常见的一种操作，可以过滤掉一些不重要的高频信息。\n- 全连接层(fully-connected layer，或者fc layer): 输入层到隐藏层的神经元是全部连接的。\n- 非线性变化: 卷积层、全连接层后面一般都会接非线性变化层，例如Sigmoid、Tanh、ReLu等来增强网络的表达能力，在CNN里最常使用的为ReLu激活函数。\n- Dropout: 在模型训练阶段随机让一些隐层节点权重不工作，提高网络的泛化能力，一定程度上防止过拟合。\n\n另外，在训练过程中由于每层参数不断更新，会导致下一次输入分布发生变化，这样导致训练过程需要精心设计超参数。如2015年Sergey Ioffe和Christian Szegedy提出了Batch Normalization (BN)算法中，每个batch对网络中的每一层特征都做归一化，使得每层分布相对稳定。BN算法不仅起到一定的正则作用，而且弱化了一些超参数的设计。经过实验证明，BN算法加速了模型收敛过程，在后来较深的模型中被广泛使用。\n\n## 文本卷积神经网络（CNN）\n\n\n\n卷积神经网络经常用来处理具有类似网格拓扑结构（grid-like topology）的数据。例如，图像可以视为二维网格的像素点，自然语言可以视为一维的词序列。卷积神经网络可以提取多种局部特征，并对其进行组合抽象得到更高级的特征表示。实验表明，卷积神经网络能高效地对图像及文本问题进行建模处理。\n\n卷积神经网络主要由卷积（convolution）和池化（pooling）操作构成，其应用及组合方式灵活多变，种类繁多。本小结我们以如图所示的网络进行讲解：\n\n<p align=\"center\">\n<img src=\"https://github.com/PaddlePaddle/book/blob/develop/05.recommender_system/image/text_cnn.png?raw=true\" width = \"80%\" align=\"center\"/><br/>\n\n</p>\n\n假设待处理句子的长度为$n$，其中第$i$个词的词向量（word embedding）为$x_i\\in\\mathbb{R}^k$，$k$为维度大小。\n\n首先，进行词向量的拼接操作：将每$h$个词拼接起来形成一个大小为$h$的词窗口，记为$x_{i:i+h-1}$，它表示词序列$x_{i},x_{i+1},\\ldots,x_{i+h-1}$的拼接，其中，$i$表示词窗口中第一个词在整个句子中的位置，取值范围从$1$到$n-h+1$，$x_{i:i+h-1}\\in\\mathbb{R}^{hk}$。\n\n其次，进行卷积操作：把卷积核(kernel)$w\\in\\mathbb{R}^{hk}$应用于包含$h$个词的窗口$x_{i:i+h-1}$，得到特征$c_i=f(w\\cdot x_{i:i+h-1}+b)$，其中$b\\in\\mathbb{R}$为偏置项（bias），$f$为非线性激活函数，如$sigmoid$。将卷积核应用于句子中所有的词窗口${x_{1:h},x_{2:h+1},\\ldots,x_{n-h+1:n}}$，产生一个特征图（feature map）：\n\n$$c=[c_1,c_2,\\ldots,c_{n-h+1}], c \\in \\mathbb{R}^{n-h+1}$$\n\n接下来，对特征图采用时间维度上的最大池化（max pooling over time）操作得到此卷积核对应的整句话的特征$\\hat c$，它是特征图中所有元素的最大值：\n\n$$\\hat c=max(c)$$\n\n对于一般的短文本分类问题，上文所述的简单的文本卷积网络即可达到很高的正确率。若想得到更抽象更高级的文本特征表示，可以构建深层文本卷积神经网络。\n\n\n\n# 基于LSTM的文本分类\n\n\n## 循环神经网络（RNN）\n\n\n循环神经网络是一种能对序列数据进行精确建模的有力工具。实际上，循环神经网络的理论计算能力是图灵完备的。自然语言是一种典型的序列数据（词序列），近年来，循环神经网络及其变体（如long short term memory等）在自然语言处理的多个领域，如语言模型、句法解析、语义角色标注（或一般的序列标注）、语义表示、图文生成、对话、机器翻译等任务上均表现优异甚至成为目前效果最好的方法。\n\n<p align=\"center\">\n<img src=\"https://github.com/PaddlePaddle/book/blob/develop/06.understand_sentiment/image/rnn.png?raw=true\" width = \"60%\" align=\"center\"/><br/>\n</p>\n<p align=\"center\">\n<img src=\"img/rnn1.png\" width = \"60%\" align=\"center\"/><br/>\n</p>\n\n循环神经网络按时间展开后如图所示：在第$t$时刻，网络读入第$t$个输入$x_t$（向量表示）及前一时刻隐层的状态值$h_{t-1}$（向量表示，$h_0$一般初始化为$0$向量），计算得出本时刻隐层的状态值$h_t$，重复这一步骤直至读完所有输入。如果将循环神经网络所表示的函数记为$f$，则其公式可表示为：\n\n$$h_t=f(x_t,h_{t-1})=\\sigma(W_{xh}x_t+W_{hh}h_{t-1}+b_h)$$\n\n其中$W_{xh}$是输入到隐层的矩阵参数，$W_{hh}$是隐层到隐层的矩阵参数，$b_h$为隐层的偏置向量（bias）参数，$\\sigma$为$sigmoid$函数。  \n\n在处理自然语言时，一般会先将词（one-hot表示）映射为其词向量（word embedding）表示，然后再作为循环神经网络每一时刻的输入$x_t$。此外，可以根据实际需要的不同在循环神经网络的隐层上连接其它层。如，可以把一个循环神经网络的隐层输出连接至下一个循环神经网络的输入构建深层（deep or stacked）循环神经网络，或者提取最后一个时刻的隐层状态作为句子表示进而使用分类模型等等。\n\n## 长短期记忆网络（LSTM）\n\n\n\n对于较长的序列数据，循环神经网络的训练过程中容易出现梯度消失或爆炸现象。为了解决这一问题，Hochreiter S, Schmidhuber J. (1997)提出了LSTM(long short term memory)。  \n\n相比于简单的循环神经网络，LSTM增加了记忆单元$c$、输入门$i$、遗忘门$f$及输出门$o$。这些门及记忆单元组合起来大大提升了循环神经网络处理长序列数据的能力。若将基于LSTM的循环神经网络表示的函数记为$F$，则其公式为：\n\n$$ h_t=F(x_t,h_{t-1})$$\n\n$F$由下列公式组合而成：\n$$ i_t = \\sigma{(W_{xi}x_t+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_i)} $$\n$$ f_t = \\sigma(W_{xf}x_t+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_f) $$\n$$ c_t = f_t\\odot c_{t-1}+i_t\\odot tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c) $$\n$$ o_t = \\sigma(W_{xo}x_t+W_{ho}h_{t-1}+W_{co}c_{t}+b_o) $$\n$$ h_t = o_t\\odot tanh(c_t) $$\n其中，$i_t, f_t, c_t, o_t$分别表示输入门，遗忘门，记忆单元及输出门的向量值，带角标的$W$及$b$为模型参数，$tanh$为双曲正切函数，$\\odot$表示逐元素（elementwise）的乘法操作。输入门控制着新输入进入记忆单元$c$的强度，遗忘门控制着记忆单元维持上一时刻值的强度，输出门控制着输出记忆单元的强度。三种门的计算方式类似，但有着完全不同的参数，它们各自以不同的方式控制着记忆单元$c$，如图所示：\n\n<p align=\"center\">\n<img src=\"https://github.com/PaddlePaddle/book/blob/develop/06.understand_sentiment/image/lstm.png?raw=true\" width = \"65%\" align=\"center\"/><br/>\n</p>\n<p align=\"center\">\n<img src=\"img/LSTM1.png\" width = \"60%\" align=\"center\"/><br/>\n</p>\n\n<p align=\"center\">\n<img src=\"img/LSTM3.jpg\" width=1050><br/>\n</p>\n\nLSTM通过给简单的循环神经网络增加记忆及控制门的方式，增强了其处理远距离依赖问题的能力。类似原理的改进还有Gated Recurrent Unit (GRU)，其设计更为简洁一些。**这些改进虽然各有不同，但是它们的宏观描述却与简单的循环神经网络一样（如图所示），即隐状态依据当前输入及前一时刻的隐状态来改变，不断地循环这一过程直至输入处理完毕：**\n\n$$ h_t=Recrurent(x_t,h_{t-1})$$\n\n其中，$Recrurent$可以表示简单的循环神经网络、GRU或LSTM。\n\n## RCNN（循环卷积神经网络）\n\n\n* 参考文章[《Recurrent Convolutional Neural Networks for Text Classification》](http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/)\n\n双向循环神经网络\n<p align=\"center\">\n<img src=\"img/bilstm.png\" width=450><br/>\n</p>\nRCNN模型架构图如下：\n\n<p align=\"center\">\n<img src=\"http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig1.png\" width=950><br/>\n</p>\n\n首先，构造CNN的卷积层，卷积层的本质是一个BiRNN模型，通过正向和反向循环来构造一个单词的下文和上文，如下式：\n\n![](http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig2.png)\n\n得到单词的上下文表示之后，用拼接的方式来表示这个单词，如下式：\n\n![](http://rsarxiv.github.io/2016/05/27/Recurrent-Convolutional-Neural-Networks-for-Text-Classification-PaperWeekly/fig3.png)\n\n将该词向量放入一个单层神经网络中，得到所谓的潜语义向量（latent semantic vector），这里卷积层的计算结束了，时间复杂度仍是O(n)。接下来进行池化层（max-pooling），即将刚刚得到的所有单词的潜语义向量中每个维度上最大的值选出组成一个新的向量，这里采用max-pooling可以将向量中最大的特征提取出来，从而获取到整个文本的信息。池化过程时间复杂度也是O(n)，所以整个模型的时间复杂度是O(n)。得到文本特征向量之后，进行分类。\n\n# Transformer/selt-attention介绍\n\n\nTransformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络。其关键是自注意力机制（Self Attention）。所以我们主要介绍selt-attention。\n\n当我们想对句子“he animal didn't cross the street because it was too tired”中“it”这个词编码时，注意力机制的基本思想是认为这个句话中每个词对it的语义均会有贡献。那怎么综合这些贡献呢，就是直接将每个词的embedding向量**加权求和**。\n所以关键的问题是如何得到每个词各自的权重，关系更近的词的权重更大。比如这句话中\"The Animal\"的权重就应该更大，它们的信息应该更多地编码到“it”中。\n自注意力机制得到权重的方法非常简单，就是两个词向量的内积。最终通过一个softmax将各个权重归一化。\n![](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n在上图中，颜色的粗细代表该词的权重大小，权重由该词与“it”的内积得到，最终通过一个softmax将各个权重归一化。\n\n自注意力机制其实是最原始意义的卷积的思想的推广，因为卷积本身就是一种“加权求和”。\n\n* 参考文章：[《The Illustrated Transformer》](https://jalammar.github.io/illustrated-transformer/)\n* 参考文章：[《基于Transformer的神经机器翻译》](https://colab.research.google.com/drive/1Wt9Jwynnki6lipwUcy0Sz5WKG7MYSGs0#scrollTo=3twSbimFUgQq)\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/11Text_similarity_calculation_and_text_matching/Chapter1_Text_similarity_and_its_application/02Text_Similarity_Judgment_Based_on_Simhash/","content":"\n## Jaccard Similarity\n\n\n\n```python\ndef jaccard_sim(s1, s2):\n    a = set(s1.split()) \n    print(len(a))\n    b = set(s2.split()) \n    print(len(b))\n    c = a.intersection(b)\n    print(len(c))\n    print(c)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ns1 = \"Natural language processing is a promising research area\"\ns2 = \"More and more researchers are working on natural language processing nowadays\"\nprint(jaccard_sim(s1, s2))\n```\n\n    8\n    11\n    2\n    {'processing', 'language'}\n    0.11764705882352941\n    \n\n# 基于simhash的相似文本判断\n\n\n## SimHash\n\nReference: [A Python Implementation of simhash algorithm](https://leons.im/posts/a-python-implementation-of-simhash-algorithm/)\n\n- 选择一个hashsize，例如32\n    - V = [0] * 32\n- 把一段text变成features (shingles)\n    - howareyouiamfinethanks\n    - ['how', 'owa', 'war', 'are', 'rey', 'eyo', 'you', 'oui', 'uia', 'iam', 'amf', 'mfi', 'fin', 'ine', 'net', 'eth', 'tha', 'han', 'ank', 'nks’]\n- 把每个feature都hash成32位\n- 对于每个hash的每个位置，如果位置上是1就把V[i]加1，如果不是就把V[i]减1\n- 最后，如果V[i]>0就设为1，否则设为0，得到的V就是我们想要的simhash\n\n\n\n```python\n# Created by 1e0n in 2013\nfrom __future__ import division, unicode_literals\n\nimport re\nimport sys\nimport hashlib\nimport logging\nimport numbers\nimport collections\nfrom itertools import groupby\n\nif sys.version_info[0] >= 3:\n    basestring = str\n    unicode = str\n    long = int\nelse:\n    range = xrange\n\n\ndef _hashfunc(x): # 使用的hash函数\n    return int(hashlib.md5(x).hexdigest(), 16)\n\n\nclass Simhash(object):\n\n    def __init__(\n        self, value, f=64, reg=r'[\\w\\u4e00-\\u9fcc]+', hashfunc=None, log=None\n    ):\n        \"\"\"\n        `f` is the dimensions of fingerprints\n\n        `reg` is meaningful only when `value` is basestring and describes\n        what is considered to be a letter inside parsed string. Regexp\n        object can also be specified (some attempt to handle any letters\n        is to specify reg=re.compile(r'\\w', re.UNICODE))\n\n        `hashfunc` accepts a utf-8 encoded string and returns a unsigned\n        integer in at least `f` bits.\n        \"\"\"\n\n        self.f = f\n        self.reg = reg\n        self.value = None\n\n        if hashfunc is None:\n            self.hashfunc = _hashfunc\n        else:\n            self.hashfunc = hashfunc\n\n        if log is None:\n            self.log = logging.getLogger(\"simhash\")\n        else:\n            self.log = log\n\n\n        if isinstance(value, Simhash):\n            self.value = value.value\n        elif isinstance(value, basestring):\n#             print(\"build by text\")\n            self.build_by_text(unicode(value))\n        elif isinstance(value, collections.Iterable):\n            self.build_by_features(value)\n        elif isinstance(value, numbers.Integral):\n            self.value = value\n        else:\n            raise Exception('Bad parameter with type {}'.format(type(value)))\n\n    def __eq__(self, other):\n        \"\"\"\n        Compare two simhashes by their value.\n\n        :param Simhash other: The Simhash object to compare to\n        \"\"\"\n        return self.value == other.value\n\n    def _slide(self, content, width=4):\n        return [content[i:i + width] for i in range(max(len(content) - width + 1, 1))]\n\n    def _tokenize(self, content):\n        content = content.lower()\n        content = ''.join(re.findall(self.reg, content))\n        ans = self._slide(content)\n        return ans\n\n    def build_by_text(self, content):\n        features = self._tokenize(content)\n        features = {k:sum(1 for _ in g) for k, g in groupby(sorted(features))}\n        return self.build_by_features(features)\n\n    def build_by_features(self, features):\n        \"\"\"\n        `features` might be a list of unweighted tokens (a weight of 1\n                   will be assumed), a list of (token, weight) tuples or\n                   a token -> weight dict.\n        \"\"\"\n        v = [0] * self.f # 初始化 [0,0,0,...]\n        masks = [1 << i for i in range(self.f)] # [1, 10, 100, 1000...]\n        if isinstance(features, dict):\n            features = features.items()\n        for f in features: \n            if isinstance(f, basestring):\n                h = self.hashfunc(f.encode('utf-8')) # hash成32位\n                w = 1\n            else:\n                assert isinstance(f, collections.Iterable)\n                h = self.hashfunc(f[0].encode('utf-8'))\n                w = f[1]\n            for i in range(self.f):\n                v[i] += w if h & masks[i] else -w\n        ans = 0\n        for i in range(self.f): # 计算结果\n            if v[i] > 0: # 如果大于0，就把那一位变成1\n                ans |= masks[i] \n        self.value = ans\n\n    def distance(self, another):\n        assert self.f == another.f\n        x = (self.value ^ another.value) & ((1 << self.f) - 1) # XOR\n        ans = 0\n        while x:\n            ans += 1\n            x &= x - 1\n        return ans\n\n```\n\n\n```python\nx = 4\nans = 0\nwhile x:\n    ans += 1\n    x &= x - 1\nans\n```\n\n\n\n\n    1\n\n\n\n\n```python\nx = 7\nx &= x-1\nx\n```\n\n\n\n\n    6\n\n\n\n\n```python\n3 & 4\n```\n\n\n\n\n    0\n\n\n\n\n```python\ndef get_features(s):\n    width = 3\n    s = s.lower()\n    s = re.sub(r'[^\\w]+', '', s)\n    return [s[i:i+width] for i in range(max(len(s) - width + 1, 1))]\n\nprint(hex(Simhash(get_features(\"How are you? I am fine. Thanks. \")).value))\nprint(hex(Simhash(get_features(\"How are u? I am fine.       Thanks. \")).value))\nprint(hex(Simhash(get_features(\"How r you? I      am fine. Thanks. \")).value))\n```\n\n    0x4d4da690b5a57e47\n    0x69deac90b5a15eeb\n    0x4f08a4f4b5a13a4b\n    \n\n\n```python\nprint(Simhash('aa').distance(Simhash('bb')))\nprint(Simhash('aa').distance(Simhash('aa')))\n```\n\n    31\n    0\n    \n\n## Simhash Index\n\n\n```python\nclass SimhashIndex(object):\n\n    def __init__(self, objs, f=64, k=2, log=None):\n        \"\"\"\n        `objs` is a list of (obj_id, simhash)\n        obj_id is a string, simhash is an instance of Simhash\n        `f` is the same with the one for Simhash\n        `k` is the tolerance\n        \"\"\"\n        self.k = k\n        self.f = f\n        count = len(objs)\n\n        if log is None:\n            self.log = logging.getLogger(\"simhash\")\n        else:\n            self.log = log\n\n        self.log.info('Initializing %s data.', count)\n\n        self.bucket = collections.defaultdict(set)\n\n        for i, q in enumerate(objs):\n            if i % 10000 == 0 or i == count - 1:\n                self.log.info('%s/%s', i + 1, count)\n\n            self.add(*q)\n\n    def get_near_dups(self, simhash):\n        \"\"\"\n        `simhash` is an instance of Simhash\n        return a list of obj_id, which is in type of str\n        \"\"\"\n        assert simhash.f == self.f\n\n        ans = set()\n\n        for key in self.get_keys(simhash):\n            dups = self.bucket[key]\n            self.log.debug('key:%s', key)\n            if len(dups) > 200:\n                self.log.warning('Big bucket found. key:%s, len:%s', key, len(dups))\n\n            for dup in dups:\n                sim2, obj_id = dup.split(',', 1)\n                sim2 = Simhash(long(sim2, 16), self.f)\n\n                d = simhash.distance(sim2)\n                if d <= self.k:\n                    ans.add(obj_id)\n        return list(ans)\n\n    def add(self, obj_id, simhash):\n        \"\"\"\n        `obj_id` is a string\n        `simhash` is an instance of Simhash\n        \"\"\"\n        assert simhash.f == self.f\n\n        for key in self.get_keys(simhash):\n            v = '%x,%s' % (simhash.value, obj_id)\n            self.bucket[key].add(v)\n\n    def delete(self, obj_id, simhash):\n        \"\"\"\n        `obj_id` is a string\n        `simhash` is an instance of Simhash\n        \"\"\"\n        assert simhash.f == self.f\n\n        for key in self.get_keys(simhash):\n            v = '%x,%s' % (simhash.value, obj_id)\n            if v in self.bucket[key]:\n                self.bucket[key].remove(v)\n\n    @property\n    def offsets(self):\n        \"\"\"\n        You may optimize this method according to <http://www.wwwconference.org/www2007/papers/paper215.pdf>\n        \"\"\"\n        return [self.f // (self.k + 1) * i for i in range(self.k + 1)]\n\n    def get_keys(self, simhash):\n        for i, offset in enumerate(self.offsets):\n            if i == (len(self.offsets) - 1):\n                m = 2 ** (self.f - offset) - 1\n            else:\n                m = 2 ** (self.offsets[i + 1] - offset) - 1\n            c = simhash.value >> offset & m\n            yield '%x:%x' % (c, i)\n\n    def bucket_size(self):\n        return len(self.bucket)\n\n```\n\n\n```python\ndata = {\n    1: u'How are you? I am fine. blar blar blar blar blar Thanks.', \n    2: u'How are you i am fine. blar blar blar blar blar Thanks.', \n    3: u'This is a simhash test', \n}\n\nobjs = [(str(k), Simhash(get_features(v))) for k, v in data.items()]\nindex = SimhashIndex(objs, k=3)\n\nprint(index.bucket_size())\ns1 = Simhash(get_features(u'How are you i am fine. blar blar blar blar blar thanks'))\nprint(index.get_near_dups(s1))\n\nindex.add('4', s1)\nprint(index.get_near_dups(s1))\n```\n\n    8\n    ['1', '2']\n    ['1', '4', '2']\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/8Machine_Translation/2.Machine_Translation_Model_based_on_Seq2Seq/Machine_Translation_Model_based_on_Seq2Seq/","content":"\n# 基于seq2seq的机器翻译模型\n\n\n## 本章概述\n\n\n- 基础seq2seq编解码模型及应用\n    - 简介\n    - 应用：神经机器翻译\n- 基于注意力机制的seq2seq机器翻译模型\n    - 词向量\n    - RNN的解码器，编码器\n    - 上下文内容向量\n    - 注意力机制\n    - 可视化\n- 【实战】基于keras完成的基础seq2seq机器翻译模型\n- 【实战】基于tensorflow的google版本seq2seq机器翻译模型\n\n## 1.seq2seq（序列到序列模型）简介\n\n\n- 对于很多自然语言处理任务，比如**聊天机器人，机器翻译，自动文摘，智能问答**等，传统的解决方案都是**检索式(从候选集中选出答案)**，这对素材的完善程度要求很高。\n- 随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，它是目前自然语言处理技术中非常重要和流行的一个模型，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型运用于翻译与对话问答这一类序列型任务的先河，并且被证实在各主流语言之间的相互翻译以及语音助手中人机短问快答的应用中有着非常好的表现。\n\n参考资料:[Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n\n## 1.seq2seq（序列到序列模型）\n\n\n- 序列到序列的模型是非常有意思的NLP模型，我们的很多NLP任务，是文本到文本的映射(对应)，这个过程就像是下面图里展示的过程。\n- seq2seq模型不仅仅是用在NLP中的模型，它的输入也可以是语音信号或者图像表示。\n\n![](./img/[1]_seq2seq_1.gif)\n\n## 1.seq2seq 应用：神经机器翻译\n\n\n- 在NLP的任务中，大部分输入的是文本序列，输出的很多时候也是文本序列。\n- 下图所示的是一个典型的机器翻译任务中，输入的文本序列(源语言句子)到输出的文本序列(目标语言句子)之间的变换。\n![](./img/[2]_seq2seq_2.gif)\n\n## 2.编码解码模型\n- seq2seq 是由一个“编码解码器”（encoder-decoder）结构组成\n    - Encoder: 编码器处理输入序列中的每个元素(在这里可能是1个词)，将捕获的信息编译成向量（称为上下文内容向量）。\n    - Decoder: 在处理整个输入序列之后，编码器将上下文发送到解码器，解码器逐项开始产生输出序列。\n![](./img/[3]_seq2seq_3.gif)\n\n## 2. 编码解码模型\n\n\n- 应用：神经机器翻译（Neural Machine Translation)\n![](./img/[4]_seq2seq_4.gif)\n\n## 2. 编码解码模型\n\n\n- 输入： $x = (x_1,...,x_{T_x})$\n- 输出： $y = (y_1,...,y_{T_y})$\n\n    1. $h_t = RNN_{enc}(x_t, h_{t-1})$ , Encoder接受每一个word embedding $x_t$和上一个时刻的hidden state $h_{t-1}$。输出这个时刻的hidden state $h_t$。\n\n    2. $s_t = RNN_{dec}(\\hat{y}_{t-1},s_{t-1})$ ， Decoder接受上一个生成的单词的word embedding $\\hat{y}_{t-1}$，和上一个时间点的hidden state $s_{t-1}$。\n\n    3. $c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$ , attentional context vector是一个对于encoder输出的hidden states的一个加权平均。\n\n    4. $\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$ , 每一个encoder的hidden states对应的权重。\n\n    5. $e_{ij} = score(s_i, h_j)$ , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重 4.\n\n    6. $\\hat{s}_t = tanh(W_c[c_t;s_t])$, 将context vector 和 decoder的hidden states 串起来。\n\n    7. $p(y_t|y_{<t},x) = softmax(W_s\\hat{s}_t)$ ，计算最后的输出概率。\n\n### 2.1 词向量（word embedding）\n\n- 输入的数据(文本序列)中的每个元素(词)通常会被编码成一个稠密的向量 $x = (x_1,...,x_{T_x})$，这些向量叫做word embedding，如下图所示\n![](./img/embedding_seq2seq.png)\n\n### 2.2 循环神经网络(RNN)\n\n- 我们的encoder和decoder都会借助于循环神经网络(RNN)这类特殊的神经网络完成，循环神经网络会接受每个位置(时间点)上的输入，同时经过处理进行信息融合，并可能会在某些位置(时间点)上输出。如下图所示。\n    1. Encoder: $h_t = RNN_{enc}(x_t, h_{t-1})$ \n    2. Decoder: $s_t = RNN_{dec}(\\hat{y}_{t-1},s_{t-1})$\n![](./img/[5]_RNN_1.gif)\n\n### 2.3 上下文向量（context vector）\n\n- 编码器会将一整句话的信息编译到一个向量中，这个向量总结了这一句话的主要信息，称之为上下文向量\n- 一般我们会采取RNN 编译完最后一个单词时的输出向量$h_{T_x}$ 作为上下文向量\n![](./img/context.png)\n\n### 2.4 举例\n\n- 动态地展示整个编码器和解码器，分拆的步骤过程大概是下面这个样子。\n![](./img/[6]_seq2seq_6.gif)\n\n### 2.4 举例\n\n- 更详细的演示\n![](./img/[7]_seq2seq_7.gif)\n\n### 2.5 注意力机制 （Attention）\n\n- 如果把所有句子信息都压缩到一个定长的上下文向量中，当遇到长句子的时候，编码器很难保存句子中的所有信息。\n- 我们考虑到提升效果，不会寄希望于把所有的内容都放到一个上下文向量(context vector)中，而是会采用一个叫做**注意力模型**的模型来动态处理和解码，动态的图如下所示。\n![](./img/[8]_seq2seq_8.gif)\n\n### 2.5 注意力机制\n\n- 在解码阶段，解码器根据已生成的序列 $y_{<i}$，将当前时刻hidden state $s_i$, 对编码器中的hidden states $h_j, j\\in[1,T_x]$ 计算权重。\n    \n    <h4><center> $e_{ij} = score(s_i, h_j), ~~\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}$ </center></h4>\n    \n- 根据权重，对编码器中的hidden states求加权和，得到attentional context vector\n    <h4><center> $c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j$</center></h4>\n\n\n### 2.6 解码\n\n- 带注意力的解码器RNN接收的上一个单词的词向量(embedding)和一个初始的解码器隐藏状态(hidden state)\n- RNN处理输入，产生输出和新的隐藏状态向量\n- attention的步骤：使用编码器隐藏状态(hidden state)和$h_4$来计算该时刻的attentional context vector $C_4$\n- 把h4和C4拼接成一个向量$\\hat{s}_t=[h_t,C_t]$，再通过一个全连接层（fully-connected layer）和softmax完成解码，$p(y_t|y_{<t},x) = softmax(W_s\\hat{s}_t)$\n- 每个时间点上重复这个操作\n![](./img/attention_tensor_dance.gif)\n\n### 2.6 解码\n\n- 这个动态解码的过程展示成下述图所示的过程\n![](./img/[11]_seq2seq_9.gif)\n\n### 2.7 可视化（Visualization）\n\n- 注意力机制是一个很神奇地可以学习源语言和目标语言之间词和词对齐关系的方式\n![](./img/attention_sentence.png)\n\n## 3 [实战] 基于OpenNMT完成的基础seq2seq机器翻译模型\n\n1. 处理数据\n2. 训练模型\n3. 翻译\n\n### 3.1 处理数据\n\n- 下载代码及数据\n- 预处理\n```bash\ncd $HOME/MT/\ngit clone https://github.com/OpenNMT/OpenNMT-py.git \nopennmt=$HOME/MT/OpenNMT-py\npython $opennmt/preprocess.py \\\n    -train_src $opennmt/data/src-train.txt \\\n      -train_tgt $opennmt/data/tgt-train.txt \\\n    -valid_src $opennmt/data/src-val.txt \\\n      -valid_tgt $opennmt/data/tgt-val.txt \\\n    -save_data $opennmt/data/demo\n```\n\n### 3.2 编码器（Encoder）\n\n- 将词转换成词向量，再通过RNN encoder 生成下一个hidden state\n```python \nclass RNNEncoder(EncoderBase):\n    \"\"\"rnn_type (:obj:`str`): one of [RNN, LSTM, GRU, SRU]\n       bidirectional (bool) : use a bidirectional RNN\n       num_layers (int) : number of stacked layers\n       hidden_size (int) : hidden size of each layer\n       dropout (float) : dropout value for :obj:`nn.Dropout`\n       embeddings (:obj:`onmt.modules.Embeddings`): embedding module to use\n    \"\"\"\n    def __init__(self, rnn_type, bidirectional, num_layers, \n                hidden_size, dropout=0.0, embeddings=None, use_bridge=False):\n        super(RNNEncoder, self).__init__()\n        num_directions = 2 if bidirectional else 1\n        hidden_size = hidden_size // num_directions\n        self.embeddings = embeddings\n\n        self.rnn, self.no_pack_padded_seq = \\\n            rnn_factory(rnn_type,\n                        input_size=embeddings.embedding_size,\n                        hidden_size=hidden_size,\n                        num_layers=num_layers,\n                        dropout=dropout,\n                        bidirectional=bidirectional)\n    def forward(self, src, lengths=None):\n        emb = self.embeddings(src)\n        packed_emb = emb\n        memory_bank, encoder_final = self.rnn(packed_emb)\n```\n\n### 3.3 解码器\n\n- `init_state` 初始化RNN的hidden state \n- `_run_forward_pass` 通过对memory_bank 计算attention，计算当前单词预测的概率\n```python\nclass RNNDecoderBase(nn.Module):\n    \"\"\"rnn_type (:obj:`str`): one of [RNN, LSTM, GRU, SRU]\n       num_layers (int) : number of stacked layers\n       hidden_size (int) : hidden size of each layer\n       attn_type (str) : see :obj:`onmt.modules.GlobalAttention`\n       dropout (float) : dropout value for :obj:`nn.Dropout`\n       embeddings (:obj:`onmt.modules.Embeddings`): embedding module to use\n    \"\"\"\n    def __init__(self, rnn_type, num_layers, hidden_size, attn_type=\"general\", \n                 attn_func=\"softmax\", dropout=0.0, embeddings=None):\n        super(RNNDecoderBase, self).__init__()\n        # Basic attributes.\n        self.decoder_type = 'rnn'\n        self.bidirectional_encoder = bidirectional_encoder\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.embeddings = embeddings\n        self.dropout = nn.Dropout(dropout)\n        # Decoder state\n        self.state = {}\n        # Build the RNN.\n        self.rnn = self._build_rnn(rnn_type,\n                                   input_size=self._input_size,\n                                   hidden_size=hidden_size,\n                                   num_layers=num_layers,\n                                   dropout=dropout)\n    def init_state(self, src, memory_bank, encoder_final):\n        pass\n    def _run_forward_pass(self, tgt, memory_bank, memory_lengths=None):\n        pass\n```\n\n### 3.4 损失函数\n\n- 对计算预测的单词和参考单词的negative log-likelihood (NLL)\n```python\ncriterion = nn.NLLLoss(ignore_index=padding_idx, reduction='sum')\n```\n\n### 3.5 训练\n\n```bash\nopennmt=$HOME/MT/OpenNMT-py\npython $opennmt/train.py -data $opennmt/data/demo -save_model $opennmt/demo-model\n```\n```\n[2019-01-21 23:15:10,522 INFO] encoder: 16506500\n[2019-01-21 23:15:10,522 INFO] decoder: 41613820\n[2019-01-21 23:15:10,522 INFO] * number of parameters: 58120320\n[2019-01-21 23:15:10,523 INFO] Starting training on CPU, could be very slow\n[2019-01-21 23:15:10,523 INFO] Start training...\n[2019-01-21 23:15:10,707 INFO] Loading dataset from data/demo.train.0.pt, number of examples: 10000\n[2019-01-21 23:17:32,401 INFO] Step 50/100000; acc:4.21; ppl:9741.36; xent:9.18; lr:1.0; 0/500 tok/s; 142 sec\n[2019-01-21 23:19:49,994 INFO] Step 100/100000; acc:5.13; ppl:3308.13; xent:8.10; lr:1.0; 0/525 tok/s; 279 sec\n```\n\n### 3.6 翻译\n\n```bash\nopennmt=$HOME/MT/OpenNMT-py\npython $opennmt/translate.py \\\n    -model $opennmt/demo-model_XYZ.pt \\\n    -src $opennmt/data/src-test.txt \\\n    -output $opennmt/pred.txt -replace_unk -verbose\n```\n\n## 4 基于TensorFlow的google版seq2seq机器翻译模型\n\ngoogle的这个教程使用高版本tensorflow（TensorFlow 1.2+）的 seq2seq API完成，该API使seq2seq模型的构建过程干净、简单、易读，主要包括以下内容：\n\n- 使用 tf.data 中最新输入的管道对动态调整的输入序列进行预处理。\n- 使用批量填充和序列长度 bucketing，提高训练速度和推理速度。\n- 使用通用结构和训练时间表训练 seq2seq 模型，包括多种注意力机制和固定抽样。\n- 使用 in-graph 集束搜索在 seq2seq 模型中进行推理。\n- 优化 seq2seq 模型，以实现在多 GPU 设置中的模型训练。\n\n### 4.1 安装TensorFlow 及nmt\n\n- 安装 TensorFlow，请按照以下安装指导：https://www.tensorflow.org/install/。\n```bash\ngit clone https://github.com/tensorflow/nmt/\n```\n- 主要代码在 model.py 文件中。在网络的底层，编码器和解码器 RNN 接收到以下输入：首先是原句子，然后是从编码到解码模式的过渡边界符号`「<s>」`，最后是目标语句。对于训练来说，我们将为系统提供以下张量，它们是以时间为主（time-major）的格式，并包括了单词索引：\n\n    - encoder_inputs [max_encoder_time, batch_size]：源输入词。\n    - decoder_inputs [max_decoder_time, batch_size]：目标输入词。\n    - decoder_outputs [max_decoder_time, batch_size]：目标输出词，这些是 decoder_inputs 按一个时间步向左移动，并且在右边有句子结束符。\n\n### 4.2 词向量\n\n给定单词的分类属性，模型首先必须查找词来源和目标嵌入以检索相应的词表征。为了令该嵌入层能够运行，我们首先需要为每一种语言选定一个词汇表。通常，选定词汇表大小 V，那么频率最高的 V 个词将视为唯一的。而所有其他的词将转换并打上「unknown」标志，因此所有的词将有相同的嵌入。我们通常在训练期间嵌入权重，并且每种语言都有一套。\n\n```python\n# Embedding\nembedding_encoder = variable_scope.get_variable(\n    \"embedding_encoder\", [src_vocab_size, embedding_size], ...)# Look up embedding:#   encoder_inputs: [max_time, batch_size]#   encoder_emp_inp: [max_time, batch_size, embedding_size]\nencoder_emb_inp = embedding_ops.embedding_lookup(\n    embedding_encoder, encoder_inputs)\n```\n\n\n### 4.3 编码器(encoder)\n\n\n- 词向量就能作为输入馈送到主神经网络中。该网络有两个多层循环神经网络组成，一个是原语言的编码器，另一个是目标语言的解码器。\n- 这两个 RNN 原则上可以共享相同的权重，然而在实践中，我们通常使用两组不同的循环神经网络参数（这些模型在拟合大型训练数据集上做得更好）。\n- 解码器 RNN 使用零向量作为它的初始状态\n\n```python\n# Build RNN cell\nencoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n# Run Dynamic RNN#   encoder_outpus: [max_time, batch_size, num_units]#   encoder_state: [batch_size, num_units]\nencoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n    encoder_cell, encoder_emb_inp,\n    sequence_length=source_seqence_length, time_major=True)\n```\n\n- 注意语句有不同的长度以避免浪费计算力，因此我们会通过 source_seqence_length 告诉 dynamic_rnn 精确的句子长度。因为我们的输入是以时间为主（time major）的，我们需要设定 time_major=True。\n\n### 4.4 解码器(decoder)\n\n\n- decoder 也需要访问源信息，一种简单的方式是用编码器最后的隐藏态 encoder_state 对其进行初始化。\n\n```python\n# Build RNN cell\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n```\n\n```python\n# Helper\nhelper = tf.contrib.seq2seq.TrainingHelper(\n    decoder_emb_inp, decoder_lengths, time_major=True)# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\nlogits = outputs.rnn_output\n```\n\n- 此处代码的核心是 BasicDecoder、获取 decoder_cell(类似于 encoder_cell) 的 decoder、helper 以及之前作为输入的 encoder_state。\n- 通过分离 decoders 和 helpers，我们能重复使用不同的代码库，例如 TrainingHelper 可由 GreedyEmbeddingHelper 进行替换\n\n### 4.5.梯度计算和优化器优化\n\n\n- 定义我们的 NMT 模型的前向传播，及计算反向传播\n\n```python\n# Calculate and clip gradients\nparameters = tf.trainable_variables()\ngradients = tf.gradients(train_loss, params)\nclipped_gradients, _ = tf.clip_by_global_norm(\n    gradients, max_gradient_norm)\n```\n\n- 训练 RNN 的一个重要步骤是梯度截断（gradient clipping）。这里，我们使用全局范数进行截断操作。最大值 max_gradient_norm 通常设置为 5 或 1。\n- 选择优化器。Adam 优化器是最常见的选择。选择一个学习率，learning_rate 的值通常在 0.0001 和 0.001 之间，且可设置为随着训练进程逐渐减小。\n\n```python\n# Optimization\noptimizer = tf.train.AdamOptimizer(learning_rate)\nupdate_step = optimizer.apply_gradients(\n    zip(clipped_gradients, params))\n```\n\n### 4.6 训练 NMT 模型\n\n\n- 开始训练第一个 NMT 模型，将越南语翻译为英语。代码的入口是 nmt.py。\n\n- 我们将使用小规模的 Ted 演讲双语语料库（133k 的训练样本）进行训练。数据可从以下链接找到：https://nlp.stanford.edu/projects/nmt/。\n\n- 我们将使用 tst2012 作为dev数据集，tst 2013 作为test数据集。\n\n```shell\nnmt/scripts/download_iwslt15.sh /tmp/nmt_data\n```\n\n- 运行以下命令行开始训练一个2层LSTM Seq2seq模型，128维隐单元，0.2的dropout：\n\n```python\nmkdir /tmp/nmt_model\npython -m nmt.nmt \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n``` \n```python\n# First evaluation, global step 0\n  eval dev: perplexity 17193.66\n  eval test: perplexity 17193.27\n# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017\n  sample train data:\n    src_reverse: </s> </s> Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .\n    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>\n  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00\n  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00\n  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00\n  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00\n  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00\n```\n\n### 4.7 翻译\n\n- 创建一个推理文件，用已经训练好的模型去翻译一些语句，详见 inference.py\n\n```shell\ncat > /tmp/my_infer_file.vi# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)\n\npython -m nmt.nmt \\\n    --model_dir=/tmp/nmt_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_model/output_infer\n\ncat /tmp/nmt_model/output_infer # To view the inference as output\n```\n\n## 本章小结\n\n\n- 基础seq2seq编解码模型及应用\n    - 简介\n    - 应用：神经机器翻译\n- 基于注意力机制的seq2seq机器翻译模型\n    - 词向量\n    - RNN的解码器，编码器\n    - 上下文内容向量\n    - 注意力机制\n    - 可视化\n- 【实战】基于keras完成的基础seq2seq机器翻译模型\n- 【实战】基于tensorflow的google版本seq2seq机器翻译模型\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter1_Text_Classification_based_on_Machine_Learning/4.Chinese_news_classification_based_on_Python/","content":"\n# python中文新闻分类\n\n* 参考代码：[【NLP文本分类】各种文本分类算法集锦，从入门到精通](https://www.kesci.com/home/project/5be7e948954d6e0010632ef2)\n* 参考代码：[NLP系列(4)_朴素贝叶斯实战与进阶](https://blog.csdn.net/longxinchen_ml/article/details/50629613)\n\n\n```python\nimport re  \nimport pickle\nimport random\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\n\n\n```\n\n\n```python\ndata = [[i.split(\"\\t\")[0],i.split(\"\\t\")[1]] for i in open(u\"data/cnews.train.txt\",\"r\", encoding=\"utf-8\")]\ndata[0]\n```\n\n\n\n\n    ['体育',\n     '鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末段，各项奖项的评选结果也即将出炉，其中评选最佳控卫的鲍勃-库西奖就将在下周最终四强战时公布，鲍勃-库西奖是由奈史密斯篮球名人堂提供，旨在奖励年度最佳大学控卫。最终获奖的球员也即将在以下几名热门人选中产生。〈〈〈\\xa0NCAA疯狂三月专题主页上线，点击链接查看精彩内容吉梅尔-弗雷戴特，杨百翰大学“弗神”吉梅尔-弗雷戴特一直都备受关注，他不仅仅是一名射手，他会用“终结对手脚踝”一样的变向过掉面前的防守者，并且他可以用任意一支手完成得分，如果他被犯规了，可以提前把这两份划入他的帐下了，因为他是一名命中率高达90%的罚球手。弗雷戴特具有所有伟大控卫都具备的一点特质，他是一位赢家也是一位领导者。“他整个赛季至始至终的稳定领导着球队前进，这是无可比拟的。”杨百翰大学主教练戴夫-罗斯称赞道，“他的得分能力毋庸置疑，但是我认为他带领球队获胜的能力才是他最重要的控卫职责。我们在主场之外的比赛(客场或中立场)共取胜19场，他都表现的很棒。”弗雷戴特能否在NBA取得成功？当然，但是有很多专业人士比我们更有资格去做出这样的判断。“我喜爱他。”凯尔特人主教练多克-里弗斯说道，“他很棒，我看过ESPN的片段剪辑，从剪辑来看，他是个超级巨星，我认为他很成为一名优秀的NBA球员。”诺兰-史密斯，杜克大学当赛季初，球队宣布大一天才控卫凯瑞-厄尔文因脚趾的伤病缺席赛季大部分比赛后，诺兰-史密斯便开始接管球权，他在进攻端上足发条，在ACC联盟(杜克大学所在分区)的得分榜上名列前茅，但同时他在分区助攻榜上也占据头名，这在众强林立的ACC联盟前无古人。“我不认为全美有其他的球员能在凯瑞-厄尔文受伤后，如此好的接管球队，并且之前毫无准备。”杜克主教练迈克-沙舍夫斯基赞扬道，“他会将比赛带入自己的节奏，得分，组织，领导球队，无所不能。而且他现在是攻防俱佳，对持球人的防守很有提高。总之他拥有了辉煌的赛季。”坎巴-沃克，康涅狄格大学坎巴-沃克带领康涅狄格在赛季初的毛伊岛邀请赛一路力克密歇根州大和肯塔基等队夺冠，他场均30分4助攻得到最佳球员。在大东赛区锦标赛和全国锦标赛中，他场均27.1分，6.1个篮板，5.1次助攻，依旧如此给力。他以疯狂的表现开始这个赛季，也将以疯狂的表现结束这个赛季。“我们在全国锦标赛中前进着，并且之前曾经5天连赢5场，赢得了大东赛区锦标赛的冠军，这些都归功于坎巴-沃克。”康涅狄格大学主教练吉姆-卡洪称赞道，“他是一名纯正的控卫而且能为我们得分，他有过单场42分，有过单场17助攻，也有过单场15篮板。这些都是一名6英尺175镑的球员所完成的啊！我们有很多好球员，但他才是最好的领导者，为球队所做的贡献也是最大。”乔丹-泰勒，威斯康辛大学全美没有一个持球者能像乔丹-泰勒一样很少失误，他4.26的助攻失误在全美遥遥领先，在大十赛区的比赛中，他平均35.8分钟才会有一次失误。他还是名很出色的得分手，全场砍下39分击败印第安纳大学的比赛就是最好的证明，其中下半场他曾经连拿18分。“那个夜晚他证明自己值得首轮顺位。”当时的见证者印第安纳大学主教练汤姆-克雷恩说道。“对一名控卫的所有要求不过是领导球队、使球队变的更好、带领球队成功，乔丹-泰勒全做到了。”威斯康辛教练博-莱恩说道。诺里斯-科尔，克利夫兰州大诺里斯-科尔的草根传奇正在上演，默默无闻的他被克利夫兰州大招募后便开始刻苦地训练，去年夏天他曾加练上千次跳投，来提高这个可能的弱点。他在本赛季与杨斯顿州大的比赛中得到40分20篮板和9次助攻，在他之前，过去15年只有一位球员曾经在NCAA一级联盟做到过40+20，他的名字是布雷克-格里芬。“他可以很轻松地防下对方王牌。”克利夫兰州大主教练加里-沃特斯如此称赞自己的弟子，“同时他还能得分，并为球队助攻，他几乎能做到一个成功的团队所有需要的事。”这其中四名球员都带领自己的球队进入到了甜蜜16强，虽然有3个球员和他们各自的球队被挡在8强的大门之外，但是他们已经表现的足够出色，不远的将来他们很可能出现在一所你熟悉的NBA球馆里。(clay)\\n']\n\n\n\n\n```python\nrandom.shuffle(data)\n```\n\n\n```python\nimport jieba\njieba.enable_parallel() #并行分词开启\nterms = [\" \".join([j for j in jieba.cut(i[1])]) \n         for i in data[:1000]]\n\n```\n\n\n```python\n#接下来用scikit-learn中的LabelEncoder将文本标签（Text Label）转化为数字(Integer)\n\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform([i[0] for i in data[:1000]])\n\n```\n\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer as TFIV\n# 初始化TFIV对象，去停用词，加2元语言模型\ntfv = TFIV(min_df=3,  max_features=None, strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = 'english')\n\n\n# 这一步有点慢，去喝杯茶刷会儿微博知乎歇会儿...\ntfv.fit(terms)\nX_all = tfv.transform(terms)\n\n\n```\n\n\n```python\nlen_train = 600\nX = X_all[:len_train] \ny_train = y[:len_train]\nX_test = X_all[len_train:]\ny_test = y[len_train:]\n\n```\n\n\n```python\n# 多项式朴素贝叶斯\nfrom sklearn.naive_bayes import MultinomialNB as MNB\n\n# 因为sklearn的NB只支持二分类，所以就简单粗暴地把10个类变成一个类。\ny_train_nb = [1 if i > 5 else 0 for i in y_train]\nmodel_NB = MNB()\nmodel_NB.fit(X, y_train_nb ) #特征数据直接灌进来\nMNB(alpha=1.0, class_prior=None, fit_prior=True) # ”alpha“是平滑参数，不需要掌握哈。\n\nfrom sklearn.cross_validation import cross_val_score\nimport numpy as np\n\nprint(\"多项式贝叶斯分类器20折交叉验证得分: \", np.mean(cross_val_score(model_NB, X, y_train_nb, cv=20, scoring='roc_auc')))\n\n```\n\n    多项式贝叶斯分类器20折交叉验证得分:  0.9858054226475279\n    \n\n\n```python\n# 折腾一下逻辑回归，恩\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.grid_search import GridSearchCV\n\nmodel_LR = LogisticRegression(C=.01) # C是正则化系数。\nmodel_LR.fit(X, y_train_nb)\nprint(\"20折交叉验证得分: \", np.mean(cross_val_score(model_LR, X, y_train_nb, cv=20, scoring='roc_auc')))\n\n```\n\n    20折交叉验证得分:  0.9685871876661348\n    \n\n\n```python\nfrom sklearn.svm import LinearSVC\nmodel_SVM = LinearSVC(C=.01) # C是正则化系数。\nmodel_SVM.fit(X, y_train_nb)\nprint(\"20折交叉验证得分: \", np.mean(cross_val_score(model_SVM, X, y_train_nb, cv=20, scoring='roc_auc')))\n```\n\n    20折交叉验证得分:  0.9811038011695905\n    \n\n\n```python\nmodel_LR = LogisticRegression(C=.01) # C是正则化系数。\nmodel_LR.fit(X, y_train)\nprint(\"20折交叉验证得分: \", np.mean(cross_val_score(model_LR, X, y_train, cv=20)))\n\n```\n\n    20折交叉验证得分:  0.27262206942163847\n    \n\n\n```python\nmodel_SVM = LinearSVC(C=.01) # C是正则化系数。\nmodel_SVM.fit(X, y_train)\nprint(\"20折交叉验证得分: \", np.mean(cross_val_score(model_SVM, X, y_train, cv=20)))\n```\n\n    20折交叉验证得分:  0.8104821053375002\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter1_Text_Classification_based_on_Machine_Learning/5.Text_sentiment_analyse_base_on_fasttext/","content":"\n# 基于fasttext的文本情感分析\n\n\n\n```python\nimport fastText\n```\n\n\n```python\nimport re  #正则表达式\nfrom bs4 import BeautifulSoup  #html标签处理\nimport pandas as pd\n\ndef review_to_wordlist(review):\n    '''\n    把IMDB的评论转成词序列\n    '''\n    # 去掉HTML标签，拿到内容\n    review_text = BeautifulSoup(review).get_text()\n    # 用正则表达式取出符合规范的部分\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    # 小写化所有的词，并转成词list\n    words = review_text.lower().split()\n    # 返回words\n    return words\n    \n# 使用pandas读入训练和测试csv文件\ntrain = pd.read_csv('data/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n\n```\n\n\n```python\nnum = len(train['review'])\n```\n\n\n```python\ntrain_num = int(num/5*4)\n```\n\n\n```python\ntrain_data = []\nfor i in range(0,train_num):\n    train_data.append(\" \".join(review_to_wordlist(train['review'][i])))\n\n```\n\n\n```python\ntest_data = []\nfor i in range(train_num,num):\n    test_data.append(\" \".join(review_to_wordlist(train['review'][i])))\n```\n\n\n```python\ny_train = train['sentiment'][:train_num]\ny_test = list(train['sentiment'][train_num:])\n```\n\n\n```python\ntrain_data[:10]\n```\n\n\n\n\n    ['with all this stuff going down at the moment with mj i ve started listening to his music watching the odd documentary here and there watched the wiz and watched moonwalker again maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent moonwalker is part biography part feature film which i remember going to see at the cinema when it was originally released some of it has subtle messages about mj s feeling towards the press and also the obvious message of drugs are bad m kay visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring some may call mj an egotist for consenting to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him the actual feature film bit when it finally starts is only on for minutes or so excluding the smooth criminal sequence and joe pesci is convincing as a psychopathic all powerful drug lord why he wants mj dead so bad is beyond me because mj overheard his plans nah joe pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno maybe he just hates mj s music lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence also the director must have had the patience of a saint when it came to filming the kiddy bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene bottom line this movie is for people who like mj on one level or another which i think is most people if not then stay away it does try and give off a wholesome message and ironically mj s bestest buddy in this movie is a girl michael jackson is truly one of the most talented people ever to grace this planet but is he guilty well with all the attention i ve gave this subject hmmm well i don t know because people can be different behind closed doors i know this for a fact he is either an extremely nice but stupid guy or one of the most sickest liars i hope he is not the latter',\n     'the classic war of the worlds by timothy hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate h g wells classic book mr hines succeeds in doing so i and those who watched his film with me appreciated the fact that it was not the standard predictable hollywood fare that comes out every year e g the spielberg version with tom cruise that had only the slightest resemblance to the book obviously everyone looks for different things in a movie those who envision themselves as amateur critics look only to criticize everything they can others rate a movie on more important bases like being entertained which is why most people never agree with the critics we enjoyed the effort mr hines put into being faithful to h g wells classic novel and we found it to be very entertaining this made it easy to overlook what the critics perceive to be its shortcomings',\n     'the film starts with a manager nicholas bell giving welcome investors robert carradine to primal park a secret project mutating a primal animal using fossilized dna like jurassik park and some scientists resurrect one of nature s most fearsome predators the sabretooth tiger or smilodon scientific ambition turns deadly however and when the high voltage fence is opened the creature escape and begins savagely stalking its prey the human visitors tourists and scientific meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre historical animals which are deadlier and bigger in addition a security agent stacy haiduk and her mate brian wimmer fight hardly against the carnivorous smilodons the sabretooths themselves of course are the real star stars and they are astounding terrifyingly though not convincing the giant animals savagely are stalking its prey and the group run afoul and fight against one nature s most fearsome predators furthermore a third sabretooth more dangerous and slow stalks its victims the movie delivers the goods with lots of blood and gore as beheading hair raising chills full of scares when the sabretooths appear with mediocre special effects the story provides exciting and stirring entertainment but it results to be quite boring the giant animals are majority made by computer generator and seem totally lousy middling performances though the players reacting appropriately to becoming food actors give vigorously physical performances dodging the beasts running bound and leaps or dangling over walls and it packs a ridiculous final deadly scene no for small kids by realistic gory and violent attack scenes other films about sabretooths or smilodon are the following sabretooth by james r hickox with vanessa angel david keith and john rhys davies and the much better bc by roland emmerich with with steven strait cliff curtis and camilla belle this motion picture filled with bloody moments is badly directed by george miller and with no originality because takes too many elements from previous films miller is an australian director usually working for television tidal wave journey to the center of the earth and many others and occasionally for cinema the man from snowy river zeus and roxanne robinson crusoe rating below average bottom of barrel',\n     'it must be assumed that those who praised this film the greatest filmed opera ever didn t i read somewhere either don t care for opera don t care for wagner or don t care about anything except their desire to appear cultured either as a representation of wagner s swan song or as a movie this strikes me as an unmitigated disaster with a leaden reading of the score matched to a tricksy lugubrious realisation of the text it s questionable that people with ideas as to what an opera or for that matter a play especially one by shakespeare is about should be allowed anywhere near a theatre or film studio syberberg very fashionably but without the smallest justification from wagner s text decided that parsifal is about bisexual integration so that the title character in the latter stages transmutes into a kind of beatnik babe though one who continues to sing high tenor few if any of the actors in the film are the singers and we get a double dose of armin jordan the conductor who is seen as the face but not heard as the voice of amfortas and also appears monstrously in double exposure as a kind of batonzilla or conductor who ate monsalvat during the playing of the good friday music in which by the way the transcendant loveliness of nature is represented by a scattering of shopworn and flaccid crocuses stuck in ill laid turf an expedient which baffles me in the theatre we sometimes have to piece out such imperfections with our thoughts but i can t think why syberberg couldn t splice in for parsifal and gurnemanz mountain pasture as lush as was provided for julie andrews in sound of music the sound is hard to endure the high voices and the trumpets in particular possessing an aural glare that adds another sort of fatigue to our impatience with the uninspired conducting and paralytic unfolding of the ritual someone in another review mentioned the bayreuth recording and knappertsbusch though his tempi are often very slow had what jordan altogether lacks a sense of pulse a feeling for the ebb and flow of the music and after half a century the orchestral sound in that set in modern pressings is still superior to this film',\n     'superbly trashy and wondrously unpretentious s exploitation hooray the pre credits opening sequences somewhat give the false impression that we re dealing with a serious and harrowing drama but you need not fear because barely ten minutes later we re up until our necks in nonsensical chainsaw battles rough fist fights lurid dialogs and gratuitous nudity bo and ingrid are two orphaned siblings with an unusually close and even slightly perverted relationship can you imagine playfully ripping off the towel that covers your sister s naked body and then stare at her unshaven genitals for several whole minutes well bo does that to his sister and judging by her dubbed laughter she doesn t mind at all sick dude anyway as kids they fled from russia with their parents but nasty soldiers brutally slaughtered mommy and daddy a friendly smuggler took custody over them however and even raised and trained bo and ingrid into expert smugglers when the actual plot lifts off years later they re facing their ultimate quest as the mythical and incredibly valuable white fire diamond is coincidentally found in a mine very few things in life ever made as little sense as the plot and narrative structure of white fire but it sure is a lot of fun to watch most of the time you have no clue who s beating up who or for what cause and i bet the actors understood even less but whatever the violence is magnificently grotesque and every single plot twist is pleasingly retarded the script goes totally bonkers beyond repair when suddenly and i won t reveal for what reason bo needs a replacement for ingrid and fred williamson enters the scene with a big cigar in his mouth and his sleazy black fingers all over the local prostitutes bo s principal opponent is an italian chick with big breasts but a hideous accent the preposterous but catchy theme song plays at least a dozen times throughout the film there s the obligatory we re falling in love montage and loads of other attractions my god what a brilliant experience the original french title translates itself as life to survive which is uniquely appropriate because it makes just as much sense as the rest of the movie none',\n     'i dont know why people think this is such a bad movie its got a pretty good plot some good action and the change of location for harry does not hurt either sure some of its offensive and gratuitous but this is not the only movie like that eastwood is in good form as dirty harry and i liked pat hingle in this movie as the small town cop if you liked dirty harry then you should see this one its a lot better than the dead pool',\n     'this movie could have been very good but comes up way short cheesy special effects and so so acting i could have looked past that if the story wasn t so lousy if there was more of a background story it would have been better the plot centers around an evil druid witch who is linked to this woman who gets migraines the movie drags on and on and never clearly explains anything it just keeps plodding on christopher walken has a part but it is completely senseless as is most of the movie this movie had potential but it looks like some really bad made for tv movie i would avoid this movie',\n     'i watched this video at a friend s house i m glad i did not waste money buying this one the video cover has a scene from the movie capricorn one the movie starts out with several clips of rocket blow ups most not related to manned flight sibrel s smoking gun is a short video clip of the astronauts preparing a video broadcast he edits in his own voice over instead of letting us listen to what the crew had to say the video curiously ends with a showing of the zapruder film his claims about radiation shielding star photography and others lead me to believe is he extremely ignorant or has some sort of ax to grind against nasa the astronauts or american in general his science is bad and so is this video',\n     'a friend of mine bought this film for and even then it was grossly overpriced despite featuring big names such as adam sandler billy bob thornton and the incredibly talented burt young this film was about as funny as taking a chisel and hammering it straight through your earhole it uses tired bottom of the barrel comedic techniques consistently breaking the fourth wall as sandler talks to the audience and seemingly pointless montages of hot girls adam sandler plays a waiter on a cruise ship who wants to make it as a successful comedian in order to become successful with women when the ship s resident comedian the shamelessly named dickie due to his unfathomable success with the opposite gender is presumed lost at sea sandler s character shecker gets his big break dickie is not dead he s rather locked in the bathroom presumably sea sick perhaps from his mouth he just vomited the worst film of all time',\n     'this movie is full of references like mad max ii the wild one and many others the ladybug s face it s a clear reference or tribute to peter lorre this movie is a masterpiece we ll talk much more about in the future']\n\n\n\n\n```python\nftrain = open(\"data/sentiment_train.txt\",\"w\")\nftest = open(\"data/sentiment_test.txt\",\"w\")\n\n```\n\n\n```python\ndef to_file(file, x, y):\n    for i, line in enumerate(x):\n        outline = line + \"\\t__label__\" + str(y[i]) + \"\\n\"\n        file.write(outline)\n    file.close()\n```\n\n\n```python\nto_file(ftrain, train_data, y_train)\n```\n\n\n```python\nto_file(ftest, test_data, y_test)\n```\n\n\n```python\ndef print_results(N, p, r):\n    print(\"N\\t\" + str(N))\n    print(\"P@{}\\t{:.3f}\".format(1, p))\n    print(\"R@{}\\t{:.3f}\".format(1, r))\n\n```\n\n\n```python\nfrom fastText import train_supervised\n\nmodel = train_supervised(\n    input=\"data/sentiment_train.txt\", epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1)\n```\n\n\n```python\nprint_results(*model.test(\"data/sentiment_test.txt\"))\n\n```\n\n    N\t5000\n    P@1\t0.891\n    R@1\t0.891\n    \n\n\n```python\nmodel.predict('the creators of south park in their own film here this is a brilliant film with a huge entertainment factor if you like naked gun films and are not young and not too mature or serious on your humor you ll love this')\n```\n\n\n\n\n    (('__label__1',), array([1.00001001]))\n\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter1_Text_Classification_based_on_Machine_Learning/3.Principle_of_facebook_fasttext/","content":"\n# facebook fasttext原理与操作\n\n\n* 参考文章：[《FastText原理总结》](https://blog.csdn.net/qq_16633405/article/details/80578431)\n\n## 1、应用场景\n\nfastText是一种Facebook AI Research在16年开源的一个文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression和neural network等模型，fastText在保持分类效果的同时，大大缩短了训练时间。\n\n## 2、优缺点\n\n* 适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”\n* 支持多语言表达：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。FastText的性能要比时下流行的word2vec工具明显好上不少，也比其他目前最先进的词态词汇表征要好。\n* fastText专注于文本分类，在许多标准问题上实现当下最好的表现（例如文本倾向性分析或标签预测）。\n\n## 3、FastText的原理\n\n\nfastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。\n\nfastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。\n序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。\nfastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。\nfastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。\n\n第一部分：fastText的模型架构类似于CBOW，两种模型都是基于Hierarchical Softmax，都是三层架构：输入层、 隐藏层、输出层。\n\n![](img/fasttext1.png)\n\n第二部分：层次之间的映射\n\n将输入层中的词和词组构成特征向量，再将特征向量通过线性变换映射到隐藏层，隐藏层通过求解最大似然函数，然后根据每个类别的权重和模型参数构建Huffman树，将Huffman树作为输出。\n\n具体的数学求解过程可参考博客：\nhttps://blog.csdn.net/yick_liao/article/details/62222153\n\n第三部分：fastText的N-gram特征\n\n常用的特征是词袋模型（将输入数据转化为对应的Bow形式）。但词袋模型不能考虑词之间的顺序，因此 fastText 还加入了 N-gram 特征。\n\n“我 爱 她” 这句话中的词袋模型特征是 “我”，“爱”, “她”。这些特征和句子 “她 爱 我” 的特征是一样的。\n\n如果加入 2-Ngram，第一句话的特征还有 “我-爱” 和 “爱-她”，这两句话 “我 爱 她” 和 “她 爱 我” 就能区别开来了。当然，为了提高效率，我们需要过滤掉低频的 N-gram。\n\n在fastText 中一个低维度向量与每个单词都相关。隐藏表征在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。这类表征被称为词袋（bag of words）（此处忽视词序）。在 fastText中也使用向量表征单词 n-gram来将局部词序考虑在内，这对很多文本分类问题来说十分重要。\n\n举例来说：fastText能够学会“男孩”、“女孩”、“男人”、“女人”指代的是特定的性别，并且能够将这些数值存在相关文档中。然后，当某个程序在提出一个用户请求（假设是“我女友现在在儿？”），它能够马上在fastText生成的文档中进行查找并且理解用户想要问的是有关女性的问题。\n\n\n\n## 4、FastText词向量与word2vec对比\n\nFastText= word2vec中 cbow + h-softmax的灵活使用\n灵活体现在两个方面：\n\n* 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是\n分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；\n* 模型的输入层：word2vec的输入层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容；\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/5Text_topic_extraction_and_representation_v2/1_tfidf_wordcloud/","content":"\n# 基于python的中文关键词抽取与可视化\n\n\n\n```python\nimport re\nimport jieba.analyse\n```\n\n### 对中文数据集进行预处理\n\n\n```python\ndef preprocess(input_file, output_file):\n    reader = open(input_file, 'r',encoding='utf-8')\n    writer = open(output_file, 'w',encoding='utf-8')\n    line=reader.readline()\n    while line:\n        content = line.split()[1:]\n        content = re.sub(r'\\W+', ' ',str(content), flags=re.U)\n        writer.write(str(content))\n        writer.write('\\n')\n        line = reader.readline()\n    reader.close()\n    writer.close()\n```\n\n### 实现分词，并且提取关键词\n\n\n```python\ndef segment(filename):\n    comment = open(filename, 'r', encoding='utf-8')\n    comment = comment.read()\n    # cutword = jieba.cut(comment)\n    jieba.analyse.set_stop_words('./data/stopwords.txt')\n    global key_words\n    key_words = jieba.analyse.extract_tags(comment, topK=2000, withWeight=True,\n                                           allowPOS=('nb', 'n', 'nr', 'ns', 'a', 'ad', 'an', 'nt', 'nz', 'v', 'd'))\n    for word, fre in key_words:\n        print(word, fre)\n```\n\n### 将segment函数中得到的关键词以词云的形式展示\n\n\n```python\ndef word_cloud():\n    from wordcloud import WordCloud\n    import matplotlib.pyplot as plt\n    def show_img(wc):\n        plt.figure()\n        plt.imshow(wc)\n        plt.axis(\"off\")\n    # 实例化，通过font_path传入一个支持中文的字体\n    wc = WordCloud(font_path=\"./data/simhei.ttf\",\n                   max_words=2000,\n                   width=1920,\n                   height=1080,\n                   background_color=\"black\",\n                   margin=5)\n    # 传入[（key，weight）,...]列表生成词云\n    wc.generate_from_frequencies(dict(key_words))\n    # 保存图片到本地\n    wc.to_file('santi2_textrank_filtered(nb_n_nr_ns_a_ad_an_nt_nz_v_d).png')\n    show_img(wc)\nif __name__ == '__main__':\n    preprocess('./data/cnews.train.txt', './data/cnews.train_preprocess.txt')\n    segment('./data/cnews.train_preprocess.txt')\n    word_cloud()\n```\n\n    Building prefix dict from the default dictionary ...\n    Dumping model to file cache /tmp/jieba.cache\n    Loading model cost 1.864 seconds.\n    Prefix dict has been built succesfully.\n    \n\n    基金 0.038938494929967225\n    市场 0.018431172585075574\n    中国 0.017764151409009915\n    电影 0.017709696718952603\n    玩家 0.017613022501007897\n    游戏 0.017054505297828995\n    学生 0.017042169359852505\n    留学 0.01662623586742577\n    新浪 0.014549000988154944\n    公司 0.013849230866601751\n    企业 0.013222656618665356\n    北京 0.013217184423032524\n    家具 0.01309905537294301\n    产品 0.012842064691443422\n    选择 0.012200456680474802\n    拍摄 0.01214002057200065\n    移民 0.01197479573736939\n    家居 0.01083607453354995\n    品牌 0.010779721209617201\n    时间 0.01060775417286808\n    像素 0.010221095601356282\n    篮板 0.010100998244521356\n    搭配 0.010028112516991625\n    导演 0.009895466211079649\n    地板 0.009702918915640976\n    行业 0.009586488887854898\n    消费者 0.009565690598300366\n    球队 0.009525308317062078\n    项目 0.00937847261451631\n    美国 0.009372989210349923\n    大学 0.008714985364634132\n    季后赛 0.0085301125972001\n    时尚 0.008493370344354198\n    房价 0.008188923748298877\n    申请 0.008187150870394908\n    组图 0.00795449970722255\n    希望 0.00794978771773597\n    情况 0.007919269085577104\n    价格 0.00787301683250087\n    国际 0.007797228790632386\n    手机 0.007747618164525401\n    专业 0.007615418721610992\n    影片 0.007499905597371454\n    政策 0.00746424154597777\n    功能 0.007453277678546458\n    装修 0.007437541048997151\n    提供 0.007362421188919015\n    经济 0.007269423045122842\n    热火 0.006925267918104445\n    支持 0.006921257566968478\n    签证 0.006908252441644847\n    表现 0.006896205463657039\n    能力 0.006880662203366403\n    佳能 0.0067944999609796854\n    留学生 0.0066361549744305545\n    国家 0.006610556939695148\n    开发商 0.006579902334682425\n    城市 0.0065798778043735994\n    机会 0.006527217491757947\n    湖人 0.006407443464951707\n    学校 0.006388024256782364\n    拥有 0.00634967470380498\n    显示 0.006319496746471788\n    球员 0.0061629188980870655\n    体育讯 0.006059730752940915\n    采用 0.006024143976112596\n    相关 0.005994362424993946\n    特别 0.005989805995517811\n    学习 0.005950629394760794\n    镜头 0.005948170725316123\n    楼市 0.0058924793856863006\n    票房 0.005810921428108636\n    家装 0.005800603354982697\n    上涨 0.005744604520658164\n    数据 0.005716560409753352\n    政府 0.005714356443643753\n    角色 0.005710081096609617\n    包括 0.0056870785956336725\n    土地 0.005661268007816952\n    型基金 0.005647436847449209\n    楼盘 0.005644688104024331\n    全国 0.00560635005000021\n    超过 0.0055621291110796965\n    关注 0.005533904125064906\n    信息 0.005510749135879885\n    上海 0.005480168778434192\n    观众 0.005395082551139403\n    方式 0.005365807766406648\n    开发 0.0053483025931554354\n    世界 0.005331627183767849\n    报道 0.005314525722844905\n    介绍 0.005260013068207379\n    风格 0.005256007243060709\n    媒体 0.005203638011011859\n    经理 0.0051870098400828004\n    主持人 0.005180969824412433\n    增长 0.005149139964768439\n    现场 0.00514599016833135\n    香港 0.005137092188760737\n    需求 0.00510936647802528\n    增加 0.005103271860083775\n    经验 0.00506696756542612\n    优势 0.0050195198173740106\n    英寸 0.004993204043498532\n    模式 0.004985734853390326\n    住房 0.004964084540097591\n    感觉 0.004950804162360932\n    股票 0.00493124480412198\n    推出 0.00492468918566284\n    喜欢 0.004905837902083298\n    朋友 0.0048861844107193735\n    中心 0.004853754849700091\n    课程 0.0048488653465725865\n    指数 0.0048216673609675085\n    资金 0.004787470524735243\n    带来 0.004784861804147731\n    地产 0.004776566114456782\n    机身 0.004757107635498883\n    系统 0.004750524843641248\n    索尼 0.004742501421278113\n    效果 0.004737347304128345\n    网游 0.004732751797328053\n    月份 0.004687958857022468\n    孩子 0.004658317541971536\n    评论 0.004654880155257165\n    高清 0.004652397071676922\n    橱柜 0.004616845278736297\n    债券 0.004612862794967085\n    火箭 0.004612638097105626\n    收益 0.0045956707148713215\n    购买 0.004569570511477794\n    分红 0.004518773033555886\n    台湾 0.004490279394290804\n    马刺 0.0044626922548225865\n    A股 0.0044595305445216185\n    机构 0.004455631017726601\n    发现 0.004410450776528219\n    环境 0.00440369724848728\n    命中 0.004383838582234115\n    专家 0.004378963821622204\n    相机 0.004376993313743524\n    接受 0.00435748539713462\n    成功 0.004354619997860852\n    封神 0.0043386755762860624\n    成绩 0.004337838687604857\n    过程 0.004331099090267877\n    防守 0.004330075826198143\n    最终 0.0043059500795880916\n    结束 0.004286991041141688\n    参加 0.004264201402531338\n    商家 0.004208243461787889\n    人士 0.004203242918372536\n    提高 0.004199234894818416\n    领先 0.004191690665264165\n    体验 0.00418638707234613\n    住宅 0.004161138482048123\n    文化 0.004143369341324123\n    上映 0.004093900446999977\n    整体 0.0040849379393590735\n    卖场 0.004076059164455798\n    出国 0.0040548587367974015\n    尼康 0.00403466914120361\n    造型 0.004034534408460005\n    投资者 0.00403405142278161\n    计划 0.004020645073110907\n    净值 0.00401121132572867\n    客户 0.00400460998167906\n    设计师 0.003980250006879028\n    光学 0.003955405282836669\n    全球 0.00395517913666947\n    提升 0.003940535801985147\n    演员 0.003935959257394581\n    家庭 0.003933993488187879\n    透露 0.003915269718307496\n    英国 0.0039106494693356125\n    社会 0.003890483171049435\n    用户 0.003888929045957515\n    奇才 0.003872477954491467\n    得分 0.0038708275973292467\n    客服 0.0038654057128044953\n    参与 0.0038646744419479273\n    技术 0.003844947889334931\n    标准 0.0038395886720573274\n    旗下 0.0038282095588434754\n    正式 0.003824684703405132\n    性感 0.0037844622617536354\n    配置 0.003768312598986931\n    银行 0.0037634125867054475\n    房子 0.0037557738703019633\n    地方 0.0037526438647141392\n    涨幅 0.003749275693683372\n    原因 0.0037281268277312273\n    越来越 0.0037181648873946533\n    不错 0.0037090334852443708\n    适合 0.003698808859597139\n    成交 0.0036971526076749577\n    发行 0.0036916072227670396\n    比例 0.003689972515069501\n    告诉 0.0036848332338123933\n    詹姆斯 0.0036743870416511876\n    精彩 0.0036665883627458834\n    导语 0.0036490604539368933\n    人员 0.003646925716115918\n    英语 0.0036467988727608945\n    视频 0.003643001549112982\n    促销 0.003639086839618374\n    加拿大 0.00363478630837733\n    空间 0.0036251743623590712\n    采访 0.0036163210452747376\n    材料 0.003612501613647583\n    仓位 0.00361177920986554\n    规模 0.003574363809864503\n    进攻 0.003547850772580225\n    装饰 0.003542107652880658\n    家长 0.00353821023371365\n    资产 0.0035342101567906637\n    建议 0.0035331111380414427\n    对手 0.0035242095517872447\n    排名 0.003511406487871429\n    更是 0.0035111172526837726\n    交流 0.00351053436864583\n    规划 0.0035077664235607735\n    马英九 0.0035012145479668354\n    手机游戏 0.0035000727279311562\n    风险 0.0034874174161145825\n    简单 0.0034645055344625203\n    创新 0.003458860542449605\n    成长 0.0034521321232714843\n    主场 0.0034443655686128023\n    业主 0.0034234471550530707\n    指出 0.003407737523062858\n    代表 0.003406698475053684\n    交易 0.003404658734312861\n    实木 0.0033947936239778907\n    很大 0.003393427639352011\n    考生 0.003382564583175187\n    明星 0.0033809040657658326\n    面对 0.0033751644620674923\n    二手房 0.003372438716364808\n    内线 0.0033536082146936905\n    全新 0.0033531818199284214\n    道具 0.0033494011342235213\n    经历 0.003347937883333726\n    稳定 0.0033475085261869307\n    湖人队 0.0033382562581118834\n    公布 0.0033283748955352014\n    享受 0.003321316253728749\n    平均 0.0033153637178941725\n    相比 0.00330829052816217\n    论坛 0.003300403856126512\n    下降 0.003298752621673538\n    有限公司 0.0032952491474421944\n    场景 0.003291024129446024\n    本赛季 0.00328648538793037\n    防抖 0.0032820140524288314\n    质量 0.0032760310903662067\n    价值 0.0032627692957411815\n    陈水扁 0.003258621030598347\n    装备 0.0032461560551061653\n    打造 0.0032424516669567107\n    区域 0.0032416603214178107\n    集团 0.0032277590927737828\n    饰演 0.00321731716707035\n    主演 0.0032089564412479867\n    拿到 0.003200981811594504\n    地块 0.003179640120661501\n    黑色 0.0031759965580811663\n    关系 0.0031707991509762118\n    姚明 0.003156727124280718\n    吸引 0.0031552770395190956\n    部门 0.003135995561163692\n    历史 0.003132860283978074\n    内容 0.003128438086653616\n    进一步 0.003114789064158707\n    领域 0.003105014535706157\n    韦德 0.0030990227278557114\n    老师 0.0030987650426881476\n    故事 0.003094377751229282\n    灵兽 0.003090384874113403\n    传统 0.0030797197259044206\n    优秀 0.003074228671052248\n    感受 0.0030701324538588287\n    拿下 0.0030677279608135283\n    公牛 0.0030519365423499548\n    照片 0.0030340439747329325\n    下跌 0.003029775719390404\n    日本 0.003029377403794727\n    水平 0.0030216938554298244\n    地区 0.003019592419506967\n    网站 0.0030135903289224097\n    语言 0.003012845684400236\n    具备 0.002982590788565429\n    技能 0.0029825053785640615\n    网友 0.002977777937325642\n    成本 0.002962159512877551\n    业绩 0.0029562949589379962\n    编辑 0.0029545692658789423\n    统计 0.0029346504095009995\n    依然 0.002923442289162953\n    经典 0.0029189294969802354\n    更好 0.0029155404315929744\n    最新 0.002910833184391266\n    对话 0.0029026492051779586\n    肯定 0.0029003622435866136\n    团队 0.002898934317788141\n    成立 0.002897620323635506\n    努力 0.0028873195838005424\n    业务 0.002885863481279254\n    建筑 0.00288493464583838\n    金融危机 0.002884265017112342\n    东西 0.002878462868295451\n    建材 0.002874305086433264\n    广州 0.002861577844595167\n    重点 0.0028510492313343237\n    完美 0.0028411352427324768\n    负责人 0.0028406836610190733\n    目标 0.002839793712288556\n    操作 0.002828477626014172\n    卫浴 0.0028249297205513247\n    实力 0.002822175043184031\n    连续 0.002813824033745556\n    发布 0.0028039656535988\n    职业 0.002801347677971787\n    更多 0.0028004848110913592\n    名校 0.0027981560283975534\n    组织 0.0027710414559393927\n    提出 0.0027671203863625757\n    对此 0.0027648056289682737\n    状态 0.0027589963842132264\n    电影节 0.002754398464835255\n    发生 0.002752448626267332\n    电话 0.0027476855170875296\n    贷款 0.002737688414023985\n    合同 0.00273127289432237\n    面积 0.0027302365589759177\n    加索尔 0.00272987313338473\n    木地板 0.002715842851955016\n    基础 0.00271268375611366\n    只能 0.0027039418643054736\n    费用 0.0027023313036091006\n    购房 0.0026909856653619868\n    高端 0.002688628566186931\n    网络 0.002683966092928995\n    大幅 0.0026734689036371096\n    找到 0.0026725936581708304\n    房屋 0.002668977007267188\n    魔术 0.0026600251899594014\n    压力 0.0026550497129138274\n    录取 0.0026548910338279694\n    曝光 0.002651413155212558\n    策略 0.0026353923640150888\n    大陆 0.0026243033619682507\n    报告 0.0026207282344855986\n    顾问 0.0026184225392017818\n    推动 0.0026180937352592716\n    条件 0.0026136104760778344\n    投篮 0.0026103394248921372\n    小牛 0.002610221824816189\n    解决 0.002604634494775559\n    人物 0.0025846691307157056\n    剧组 0.002579827785657485\n    阶段 0.0025788395266750494\n    申请人 0.0025763021203236325\n    尼克斯 0.0025733496147423737\n    单位 0.00257269697345459\n    色彩 0.002571613281903069\n    成交量 0.0025709620763837483\n    作品 0.0025697450279496092\n    产业 0.0025624590838204775\n    出色 0.0025549563663343003\n    主席 0.002549325577181039\n    轻松 0.002548705075796681\n    性能 0.0025473978187291286\n    剧情 0.0025471512842887138\n    小时 0.002546804986187622\n    同学 0.0025446563244058314\n    速度 0.0025370429466674525\n    数码相机 0.0025258869627231458\n    松下 0.0025154999213883975\n    点评 0.002513967288539998\n    客场 0.0025125425522385843\n    保证 0.0025107394416468702\n    收入 0.0025089565519081773\n    平台 0.0025060132353236144\n    学费 0.0025057265988110548\n    好莱坞 0.0024971285011983918\n    时代 0.0024954356825202823\n    导致 0.002495049603817633\n    内地 0.0024940476642017107\n    上市 0.002492166682012213\n    木门 0.002491331637487917\n    快速 0.0024883322105858775\n    专户 0.0024866312778947857\n    款式 0.002485764424874848\n    事情 0.002485111956570752\n    背景 0.0024816014081997404\n    商业 0.002476640685589296\n    意见 0.002476062922609643\n    外套 0.002475217714844984\n    上篮 0.0024740373105141002\n    趋势 0.0024711613729891157\n    出台 0.002470630032827967\n    程度 0.0024649151247923507\n    售价 0.002463402413905772\n    做好 0.0024612030092501865\n    很好 0.0024492248401321334\n    期待 0.002446533240111721\n    私募 0.002443643282781406\n    作用 0.002443455719409868\n    T恤 0.0024427203825329088\n    健康 0.002441407787497421\n    因素 0.0024362247727161518\n    华人 0.002436008400002075\n    保障 0.0024345642581809998\n    学院 0.0024238460430872568\n    院校 0.002421208012041212\n    球迷 0.0024207291738820545\n    官方 0.0024181719765512723\n    颜色 0.002415889416598111\n    画面 0.002409135295896735\n    行情 0.002405994310640837\n    真的 0.002402643842328935\n    发挥 0.002401018196761775\n    报价 0.002395839348574823\n    均价 0.0023892193606154534\n    减少 0.002387071889708358\n    开启 0.0023833060331024405\n    确实 0.002362375434111947\n    理念 0.002362212492121514\n    出席 0.0023536156320570238\n    流行 0.0023523360641219695\n    实施 0.002350624538511847\n    乐居 0.0023425062129930974\n    主题 0.0023324424525212935\n    博客 0.0023304046235326182\n    业内人士 0.0023288235965967176\n    访问 0.002327942407512594\n    篮下 0.0023262598390879534\n    用地 0.0023207759114475326\n    休闲 0.0023189045172819886\n    自然 0.0023158741609505544\n    女儿国 0.0023144732873263915\n    细节 0.0023068026917091437\n    举办 0.0023065314714682175\n    罗斯 0.0022985573483661915\n    总经理 0.0022957467120088017\n    体现 0.002294604717045825\n    反弹 0.002293608703388565\n    核心 0.002293576096313872\n    股市 0.002284752619747783\n    现象 0.002281838727400998\n    理财 0.0022794835612946207\n    成龙 0.0022792181447789696\n    气质 0.0022742872444022\n    单品 0.0022736089714344766\n    配合 0.0022624466272176983\n    老鹰 0.0022615232684114637\n    女士 0.002259775003683288\n    组合 0.0022575081413034694\n    品质 0.002249622993268178\n    开放 0.0022495519788405987\n    观点 0.002243187311524995\n    渠道 0.002241929818376565\n    改变 0.002238666915747783\n    出手 0.0022334415126054564\n    高考 0.0022282498757830015\n    可爱 0.002227237892797171\n    深圳 0.0022258139501048213\n    人数 0.0022235486091633934\n    黄蜂 0.002221620879012142\n    面临 0.0022207808842534533\n    加大 0.002203879681820404\n    奖学金 0.0022018196333549924\n    新闻 0.0021993496787776233\n    高校 0.002185582723782064\n    应对 0.0021790022327764025\n    数量 0.002177019859979279\n    毕业生 0.002173810701186373\n    会议 0.002170814701256725\n    快乐 0.0021668756749815394\n    三国 0.0021639098803961335\n    战略 0.00216385847027605\n    充满 0.0021588947671836852\n    霸王 0.0021581009338379345\n    罚球 0.0021572899067324194\n    事件 0.002151763567706095\n    波什 0.002148341259509712\n    打出 0.002147409985835577\n    不到 0.0021443567808165508\n    雷霆 0.002138661479149766\n    显得 0.002134182502053184\n    网络游戏 0.002129103905135332\n    提前 0.0021285214590044143\n    金融 0.0021209961268070414\n    中新网 0.0021180742665696728\n    关键 0.002117810017640929\n    海归 0.0021169337418758564\n    动作 0.0021150781107539002\n    保护 0.0021130135781881883\n    投入 0.0021126863888925617\n    工作人员 0.002102390227098652\n    全场 0.002099441754244358\n    沟通 0.00209704091102261\n    比分 0.002091929267884213\n    礼包 0.0020874623237816807\n    豪宅 0.0020868445874668027\n    宠物 0.0020850975537249662\n    风云 0.002073234731461584\n    方向 0.0020728108325049573\n    经销商 0.002072079458764525\n    邀请 0.0020708110124957263\n    知名 0.002063244385120107\n    人气 0.002057060112444712\n    北京市 0.0020549750065060163\n    元素 0.0020545174553396675\n    易建联 0.0020543904755661387\n    就业 0.002053733387282654\n    证明 0.0020512401447248746\n    利用 0.0020475393063925915\n    资源 0.002045049220674992\n    形象 0.002039455088769843\n    商品房 0.0020386808927264724\n    工程 0.0020378224360640233\n    展现 0.002035410611758\n    感谢 0.002033312695957437\n    报名 0.0020262849700407874\n    数码 0.0020235071127963533\n    感光度 0.0020220568081014316\n    特色 0.0020209249946375784\n    信心 0.0020205928722570785\n    时空 0.0020181596362336598\n    协会 0.0020172094453652506\n    副本 0.002012542498613593\n    完善 0.002010736275274251\n    安排 0.002008436916333016\n    追求 0.0020076486293962147\n    装扮 0.0020043483252379585\n    圣象 0.002004283390796233\n    措施 0.0020037065932041022\n    材质 0.002002754637179593\n    值得 0.0020020072935022725\n    华夏 0.0020006894055735313\n    建立 0.0020000297135759626\n    三分球 0.00199629812264791\n    购房者 0.0019962391535431646\n    爱情 0.0019953248415418862\n    预告片 0.0019942829593915146\n    成熟 0.0019926083107156536\n    个性 0.0019920025702088884\n    阿联 0.001991670527320052\n    沃尔 0.0019900946309174693\n    控制 0.0019891251588483004\n    降低 0.0019878774403535855\n    九州 0.001985498789486691\n    官方网站 0.0019822222301749117\n    胡锦涛 0.0019820603917695156\n    得手 0.0019818793628447328\n    毕业 0.0019817995015273576\n    发布会 0.001981569421996139\n    让玩家 0.0019809471603829884\n    国务院 0.00197611750448684\n    位置 0.001974123642701539\n    即可 0.0019647213089690704\n    至少 0.001964274446555207\n    创造 0.0019566788288059517\n    社区 0.0019450492514166372\n    证券 0.0019402788334617943\n    登陆 0.0019399322581201148\n    队友 0.0019391407249642187\n    别墅 0.0019385320140529426\n    公测 0.0019383410803774871\n    步行者 0.0019349653795033206\n    办法 0.0019295654392614248\n    感应器 0.0019293750090637746\n    三星 0.0019267960286615583\n    展示 0.0019245548618170459\n    范冰冰 0.0019242542430838148\n    长期 0.0019237655339942787\n    符合 0.0019196180932637824\n    出演 0.0019192133114285042\n    命中率 0.0019160828263145586\n    广角 0.0019139176238511008\n    本报记者 0.0019132019485290583\n    嘉宾 0.001910392079655934\n    制度 0.0019084864510878227\n    人才 0.0019080037051921672\n    人民币 0.0019067788482185234\n    强化 0.0019045738020986548\n    身份 0.001902291329897966\n    霍华德 0.0019001954491005542\n    嘉实 0.001897805835660183\n    出场 0.0018911711366401613\n    父母 0.001887626704660144\n    推荐 0.0018857878895825323\n    壁纸 0.001881975181859667\n    白色 0.0018801763503490066\n    收益率 0.0018781739888722071\n    做出 0.001877538830638689\n    年度 0.0018767696000647834\n    武林 0.0018747717784139471\n    参考价格 0.0018715238916507525\n    来到 0.0018707796791831662\n    优雅 0.0018702158731158265\n    处于 0.0018670819310036542\n    创意 0.0018660379132822505\n    首映 0.0018625550150541085\n    新增 0.0018564486646199588\n    想要 0.0018503390882397474\n    首发 0.0018493371722763385\n    失落 0.0018491083165433403\n    总监 0.0018489281568962973\n    纷纷 0.0018487603503332206\n    安东尼 0.0018434126166554285\n    内测 0.0018421457129566845\n    听力 0.0018389697224363182\n    配备 0.001837937213796482\n    注册 0.0018374678584031385\n    富士 0.0018369499405248143\n    年轻 0.0018322870026800738\n    阵容 0.0018322854873413495\n    潮流 0.0018304062354226884\n    变焦镜头 0.0018280063869710664\n    表演 0.0018231376695153757\n    角度 0.0018124429926491938\n    搭载 0.0018104426102891903\n    困难 0.0018089065594611969\n    负责 0.0018014830385067913\n    澳大利亚 0.0017982683448151203\n    分享 0.0017925056965614786\n    美凯龙 0.0017913282805241332\n    类型 0.0017904737904321164\n    葛优 0.0017902286390197389\n    分辨率 0.0017882945986995062\n    尺寸 0.0017875066693929034\n    屏幕 0.0017860895917887329\n    魅力 0.0017837412108390952\n    刚刚 0.001781756546176337\n    阅读 0.00178115151918029\n    英雄 0.0017806477240625042\n    开盘 0.00177876068571935\n    绿色 0.0017764685072066955\n    拆迁 0.0017723117594278962\n    本场 0.0017714988486054462\n    惊喜 0.0017712382455374802\n    理解 0.0017700044989179039\n    认可 0.0017632059020955585\n    低于 0.0017631834871844395\n    小区 0.0017612632336822562\n    官网 0.001760011352542942\n    开心 0.0017596831828374869\n    买房 0.0017586536950643561\n    音乐 0.0017575609108555097\n    纽约 0.0017525704375754962\n    涂料 0.0017522071526745256\n    扮演 0.0017516591986836312\n    方法 0.001751651163879089\n    澳洲 0.0017510179354513526\n    江湖 0.0017495719682939055\n    大片 0.0017483213293352352\n    科技 0.0017478413688189554\n    新片 0.0017477081692629703\n    力量 0.0017467556539180321\n    份额 0.001746573314429329\n    梦想 0.0017459924816524027\n    数字 0.0017437826550797282\n    力度 0.0017427169905553956\n    赎回 0.0017371741623896475\n    互动 0.0017371244383253724\n    执导 0.0017363767570289965\n    下载 0.0017302915171170893\n    概念 0.001729262760293769\n    较大 0.001728882027771601\n    跌幅 0.0017264097892976016\n    目的 0.0017260165187991826\n    播放 0.0017247023081280104\n    开拓者 0.0017244486066367085\n    火爆 0.001723912104434291\n    艺术 0.001723660241989443\n    万科 0.0017219493222161342\n    真实 0.0017217664711012615\n    终于 0.0017180135308621793\n    担心 0.0017170740653343287\n    中央 0.0017152160337933278\n    液晶屏 0.0017140076825651305\n    内置 0.0017131169331153812\n    电梯 0.001712353431274125\n    舒适 0.0017123307893453573\n    亮相 0.0017033845055610046\n    涉及 0.0017024602442921922\n    落后 0.0016954485505358428\n    女人 0.0016949808624045477\n    态度 0.001692220671399751\n    韩国 0.0016915288515391568\n    强大 0.0016880838176558713\n    剧本 0.0016850925140798197\n    回暖 0.0016833727254031827\n    评价 0.0016831652751365842\n    实际上 0.001677261594258174\n    金额 0.0016769081245748701\n    连衣裙 0.0016727968333498894\n    讨论 0.0016718266188643954\n    显示屏 0.0016699306562301753\n    资格 0.0016692882968268564\n    采购 0.001662455262144666\n    疯狂 0.001662099172087097\n    宝石 0.001661514157780806\n    战绩 0.0016603227766976618\n    居住 0.0016601926923482731\n    形式 0.0016580027531222445\n    公开 0.0016560765013131989\n    贡献 0.0016555911032999594\n    瓷砖 0.0016516438909454987\n    高达 0.001647306958891566\n    差距 0.001645182000578616\n    形势 0.0016436826506600966\n    居民 0.001642979010190347\n    办理 0.0016428074080639397\n    责任 0.0016414463318842444\n    董事长 0.0016389358951370977\n    利益 0.001637824878503171\n    方案 0.0016373031328212188\n    红星 0.0016369938544301154\n    焦点 0.0016363363493527604\n    中邮 0.0016347436406181776\n    物业 0.0016336110688513865\n    专区 0.001633411826379127\n    海报 0.0016315888357897861\n    传感器 0.0016280261034330831\n    图像 0.0016255238881108622\n    火箭队 0.001625402597088661\n    粉丝 0.0016252800962511848\n    意义 0.0016224580309321178\n    体系 0.0016220365849639044\n    魔幻 0.001620814817848298\n    推进 0.001614173969991874\n    内衣 0.001611743726074371\n    中锋 0.00161143512748163\n    推广 0.0016113324709573866\n    欧洲 0.0016101627299203113\n    指导 0.0016096238299756847\n    房源 0.0016090927532038247\n    呈现 0.0016090796920648354\n    接近 0.0016083040986137292\n    遭遇 0.001606614535760085\n    鼓励 0.0016064570649642186\n    女性 0.0016058110520760004\n    厨房 0.0016057535899916225\n    浪漫 0.0016016121236913563\n    掌上明珠 0.0016001645887162561\n    题材 0.0015995436872327765\n    规范 0.0015962311338009667\n    位于 0.0015923337175438876\n    属性 0.0015918600602843334\n    维护 0.0015896948524567042\n    联合 0.001586593279579855\n    通知 0.0015861515744758222\n    本报讯 0.0015826306837427011\n    外观 0.0015824999780748319\n    对阵 0.0015804268233932123\n    十足 0.0015793435037292363\n    邓超 0.0015783731702520335\n    再度 0.0015753544082168312\n    户型 0.0015742553376280402\n    给予 0.0015723057495864933\n    不用 0.0015709733314117648\n    格式 0.0015707763608262802\n    厂家 0.0015671154702161026\n    安装 0.0015661714840881867\n    大S 0.001565846399059557\n    套装 0.001564729262043145\n    法国 0.0015618212503567973\n    图案 0.0015541943636646156\n    张柏芝 0.0015533196278670804\n    骑士 0.0015519967653227712\n    精神 0.0015516465000580718\n    替补 0.0015502163050842598\n    高速 0.0015501157667672284\n    独特 0.0015476581379111163\n    募集 0.0015451426889814767\n    确保 0.0015444659126475852\n    累计 0.0015432321007466227\n    刺激 0.0015370523740324885\n    寻找 0.0015348091373457505\n    华丽 0.0015336974904026615\n    身材 0.0015294875096251326\n    棒棒 0.0015292620452300155\n    温家宝 0.0015281906641636213\n    改善 0.001527964961321299\n    机型 0.00152735806053556\n    委员会 0.0015257027191333255\n    回报 0.001522813463654577\n    话题 0.0015194230304159011\n    判断 0.0015164314481695416\n    焦距 0.0015163355735263466\n    群体 0.001515262722441636\n    回到 0.0015145147278976146\n    广告 0.0015103322546430639\n    结构 0.0015085144655013556\n    执行 0.0015084199002597194\n    无疑 0.0015071136022097842\n    激烈 0.0015055290027578712\n    酒店 0.0015046031021275336\n    认证 0.0015044483532158591\n    华语 0.0015039409707119336\n    陶瓷 0.0015033820811020003\n    封基 0.0015032125430971747\n    灵活 0.0015026849316378012\n    投中 0.0015017528462372587\n    主帅 0.0015007056386867434\n    增强 0.0014973798070283678\n    复合地板 0.0014953410088042753\n    宽松 0.0014930585724437318\n    文章 0.0014921738210518817\n    持有 0.0014913749607925603\n    系列赛 0.0014857103702872413\n    短片 0.001485189658347943\n    意味着 0.0014836756168852213\n    招生 0.001483523802467408\n    注重 0.0014824037651274312\n    定位 0.0014805424571536275\n    状况 0.0014774230645623796\n    工艺 0.0014763170437339201\n    展开 0.0014742122634358915\n    影院 0.0014726758230255805\n    变得 0.0014723725688496052\n    抢断 0.0014716118448159063\n    类似 0.0014712587253266271\n    把握 0.0014693075169769448\n    新浪网 0.0014692278204955795\n    篮网 0.001468674267311931\n    培养 0.0014665620069374303\n    曲美 0.0014656322295197454\n    频道 0.0014650639590957704\n    喜剧 0.001465044132679487\n    单反相机 0.0014618053994873408\n    市民 0.0014610606842978978\n    客户端 0.0014601565144794688\n    解释 0.001457631296806612\n    中关村 0.0014555533309467247\n    物品 0.001450956333184518\n    封闭式 0.0014496889409029827\n    攻击 0.0014494551939779796\n    移民局 0.0014492615294956972\n    改造 0.0014484972879856026\n    洛杉矶 0.0014483058088856824\n    冠军 0.0014481282937141631\n    资料 0.0014479958554990452\n    下滑 0.001447898286776397\n    新品 0.0014415603546612387\n    样板间 0.0014405786871347925\n    心理 0.0014396613687047194\n    环节 0.0014389169149752682\n    甜美 0.0014349809108866358\n    精选 0.0014345564581237141\n    光圈 0.0014325941256337512\n    意大利 0.0014319674011229048\n    喜爱 0.0014296657989352824\n    精品 0.0014291254096268355\n    成像 0.0014289421583355572\n    表达 0.0014285984376360722\n    城外诚 0.001428051915942316\n    顺利 0.0014280337628321113\n    地震 0.0014278023306002017\n    选购 0.0014276095128022371\n    担任 0.0014271615739290344\n    儿童 0.0014259014810756154\n    时刻 0.0014253277494734097\n    大赛 0.0014243622155203793\n    兴业 0.0014239879914126603\n    足够 0.0014238161210505193\n    女孩 0.0014235466652175147\n    成都 0.001421852027155278\n    可谓 0.001421263668167268\n    拍戏 0.0014200827027707704\n    地价 0.0014196987386068608\n    帅气 0.0014178457890609455\n    实用 0.0014166230182034657\n    微博 0.0014155251447498394\n    充值卡 0.0014147451791122639\n    美女 0.0014140209975916278\n    男人 0.001412814600958707\n    不好 0.0014113477167038652\n    新人 0.0014108625870446775\n    节能 0.0014091578712050925\n    红色 0.0014091403830466267\n    清晰 0.0014075957002156786\n    法律 0.0014074320930522377\n    迎来 0.0014065369836833617\n    海耶斯 0.0014055985365952822\n    回国 0.001405238790426825\n    热情 0.0014044670546704075\n    牛仔 0.00140445909094005\n    公募 0.0014029983735573631\n    性价比 0.0014020596262212792\n    衣服 0.0013997662167069356\n    穿着 0.001398372546117935\n    大奖 0.0013976032159943416\n    蕾丝 0.0013967349879611248\n    加快 0.0013964396950179416\n    也许 0.001396109851610641\n    高度 0.0013957152455545759\n    跳投 0.0013957127327564934\n    顶级 0.0013952801849324637\n    融入 0.0013950075119825585\n    入学 0.0013932206171684126\n    谢谢 0.001390587242126606\n    现金 0.001389217243275508\n    难以 0.0013888214398821172\n    连败 0.001384551693183491\n    公映 0.00138426829368541\n    简约 0.0013831722677229072\n    印象 0.0013824383109078864\n    乔丹 0.0013821345787391251\n    很快 0.001379833567141939\n    章子怡 0.0013791397090000654\n    画质 0.00137794483117241\n    领导 0.0013770022263638387\n    宝贝 0.001375919518880548\n    谈到 0.0013739678428991789\n    放弃 0.001371441358978003\n    提到 0.0013695768041107838\n    研究生 0.0013692700075693765\n    现实 0.0013690201898925382\n    看好 0.0013673305548655113\n    亮点 0.001366514390959283\n    利润 0.0013660088571237116\n    引发 0.0013644575818353413\n    坦言 0.0013638163421810208\n    扣篮 0.0013615829141943494\n    感情 0.0013561681395602185\n    武侠 0.001355611555195882\n    抢下 0.001355109240434647\n    流感 0.001353248103187122\n    老板 0.0013528951775935662\n    衬衫 0.0013489242714227515\n    摄影 0.0013484745168932583\n    德尔曼 0.0013467921687876144\n    阳光 0.0013461289445926992\n    处理器 0.0013411083859751502\n    加仓 0.0013403645175949807\n    著名 0.0013397164499303863\n    关心 0.0013392797808705202\n    认购 0.0013384584856348932\n    女星 0.0013379888331101941\n    赢得 0.0013377713422571156\n    合适 0.001336476102461299\n    麦基 0.0013355093879158055\n    短裤 0.0013347842022745622\n    全队 0.001334593504179795\n    表面 0.0013338937301942395\n    电脑 0.0013338067650863086\n    击败 0.0013335897759520926\n    勇士 0.0013335151165312586\n    名称 0.0013320333430135566\n    提醒 0.0013312042490628631\n    放在 0.0013297995141909177\n    面试 0.001329298169457283\n    熟悉 0.0013292846305752524\n    点击 0.0013279992560708972\n    评选 0.0013278970274102086\n    明珠 0.0013274166429517859\n    案例 0.001327387085840135\n    名字 0.001327278456948197\n    取消 0.001325898835737357\n    雅思 0.0013258475766822534\n    就读 0.0013252160504783476\n    文件 0.0013235983584221516\n    恢复 0.001321355841449676\n    基金净值 0.0013198542071848796\n    爵士 0.001319084902990064\n    掘金 0.0013186176365809264\n    申购 0.0013176628966137924\n    纪录 0.001316619924902207\n    公寓 0.0013164909445743881\n    台北 0.0013164436785634275\n    幸福 0.0013161959748956738\n    快门 0.0013145132911947816\n    想法 0.001307388352346041\n    难度 0.0013064360226533065\n    紧张 0.0013049095122503987\n    独立 0.0013037936215925395\n    上半场 0.0013033225611533655\n    级别 0.0013015797418773024\n    容量 0.0013007007275940533\n    更新 0.0013006874231619946\n    新加坡 0.001299866767225494\n    古天乐 0.001298203146998939\n    配套 0.0012981906267533867\n    秋瑾 0.0012911298328139153\n    地图 0.0012892706627050964\n    杜兰特 0.0012890722330433416\n    制造 0.0012884117722651578\n    支付 0.0012861351537452317\n    乐趣 0.001285990033434084\n    易方达 0.001285504188917207\n    置业 0.0012836566939993452\n    幻想 0.0012834755100866676\n    李连杰 0.0012814699265437542\n    作者 0.0012795800011612662\n    巨头 0.0012788410913831586\n    竞争力 0.0012774796209229127\n    战场 0.0012774233237022332\n    延续 0.0012756541018666824\n    影帝 0.0012753969321737855\n    姜文 0.0012750549299493106\n    地王 0.0012743815164078954\n    总理 0.0012709578604880977\n    受伤 0.001268977429919342\n    制定 0.00126885276513518\n    怪物 0.0012679253035816206\n    特效 0.0012678274957988343\n    演绎 0.0012676356413519325\n    奖项 0.0012673396528638454\n    战胜 0.0012638089155825062\n    最低 0.0012618669024687073\n    德国 0.0012594061311402413\n    人生 0.0012593200872656372\n    结婚 0.001258761952167708\n    名单 0.0012580311647526164\n    指标 0.001257889375814924\n    便宜 0.0012577781583908208\n    入市 0.0012556620027666153\n    银幕 0.0012555195995688315\n    主角 0.0012521791873120569\n    加盟 0.0012521067524800815\n    知识 0.001250121945180033\n    热门 0.0012486408939210707\n    低调 0.0012466449634578962\n    红利 0.0012466074367940693\n    律师 0.0012465477921001062\n    部长 0.0012459932885313216\n    吴伯雄 0.001245374869211364\n    幅度 0.0012440950583033675\n    摄像机 0.0012434155736717827\n    美丽 0.0012426781230923285\n    征收 0.0012403597403106291\n    基指 0.001240150348055169\n    英文 0.0012376582508998126\n    有望 0.001236992057445584\n    最为 0.001236316504730973\n    无限 0.0012355293302270423\n    民进党 0.0012341441192697553\n    重视 0.0012340443464873747\n    智能 0.001230451449250061\n    业界 0.0012286345521310754\n    单词 0.0012267729141482442\n    广东 0.0012255373102106172\n    总体 0.0012253877300200872\n    重庆 0.0012235951377956016\n    机制 0.0012234271106416755\n    人群 0.001222522764570207\n    协议 0.0012218908060492226\n    交银 0.0012213601912664544\n    申请者 0.0012197291365932913\n    回答 0.0012188735200230841\n    平衡 0.0012183168125198475\n    托福 0.0012182690202239773\n    实在 0.0012179039987325765\n    建材超市 0.001216243293323379\n    大盘 0.001214597376738748\n    增长率 0.0012143619586001022\n    教育部 0.0012131508247204214\n    航海 0.001212416749012912\n    危机 0.0012118734398200795\n    对象 0.0012115227412529423\n    女主角 0.0012096377396691106\n    门槛 0.0012086231475680615\n    上线 0.0012082474434886871\n    短裙 0.0012082319549285238\n    配饰 0.0012081605000560075\n    服务器 0.0012081418335488233\n    戏份 0.0012079480559530563\n    顾客 0.0012062850819284686\n    租赁 0.0012062343900408347\n    参考 0.0012061843531514585\n    基民 0.0012047619328661716\n    修改 0.0012031005072402584\n    讲述 0.0012012252239371082\n    时期 0.0012011467627665684\n    等待 0.0012008600781890633\n    条例 0.0012008371291821552\n    会见 0.0011992875803719333\n    办公室 0.0011972891005268891\n    视觉 0.0011967147038061273\n    礼服 0.0011957766473920128\n    实习 0.0011950668447237116\n    外线 0.0011927194517394528\n    公告 0.0011926858896004714\n    味道 0.0011913908504074294\n    南京 0.0011911723458034186\n    手段 0.0011910677913703172\n    摄像 0.0011903417034857164\n    较好 0.0011900432632852634\n    农村 0.00118987605272587\n    信托 0.0011898103804578988\n    套房 0.0011887098724482376\n    定制 0.0011880119938475697\n    自由 0.0011870674746374316\n    始终 0.0011860422511408334\n    损失 0.0011820228579792788\n    发放 0.0011818315508468232\n    硕士 0.0011804214397775\n    神秘 0.0011795531615897048\n    泡沫 0.001178487195481724\n    画壁 0.0011775164920927869\n    小奥 0.0011775164920927869\n    领导人 0.0011761630044682305\n    上市公司 0.001176030181363857\n    精英 0.0011757041106997526\n    优选 0.0011751883678215516\n    震荡 0.0011740620384167657\n    印花 0.0011702019826782977\n    笔者 0.001170196771032617\n    中学 0.0011697891766906834\n    统计数据 0.0011675330101512933\n    洛瑞 0.0011649897209003103\n    出让 0.0011634706896835399\n    年轻人 0.0011605991076987648\n    购物 0.001159453367212967\n    打折 0.0011589628462162181\n    理由 0.0011567329897534048\n    技巧 0.001156571394993354\n    激情 0.0011563098518236944\n    格里芬 0.001155596206124039\n    习惯 0.0011546892065389602\n    亚洲 0.0011538664371419545\n    打开 0.0011537121729537035\n    院线 0.0011537119618688356\n    到来 0.0011532836614337536\n    看法 0.0011522764458354513\n    提名 0.0011518555021791034\n    新房 0.0011510673555720696\n    女儿 0.0011503998940470745\n    获批 0.0011461995641115956\n    离开 0.0011444164278695953\n    租金 0.0011427619333752473\n    原则 0.0011417567404101694\n    史密斯 0.0011411413657222418\n    片场 0.0011404991797323274\n    流动性 0.0011399949055479008\n    皮草 0.0011399361785153575\n    刘德华 0.0011375593240651237\n    场面 0.0011374584281364927\n    演技 0.0011373934749838502\n    精致 0.0011366401988988173\n    打击 0.0011358285010660747\n    接触 0.0011341627797809475\n    连拍 0.0011336727929191193\n    控卫 0.0011336727929191193\n    资本 0.001133060825939424\n    谨慎 0.0011313576474229242\n    感兴趣 0.0011308004726363315\n    时髦 0.0011292478888632166\n    物业税 0.0011288927762751124\n    前场 0.0011284206981047062\n    马丁 0.0011281559516886637\n    取景器 0.001127732026685947\n    复苏 0.001127440973644628\n    走势 0.0011261729600743867\n    失误 0.0011261208798645343\n    子女 0.0011257859379691147\n    登场 0.0011255815817007818\n    西班牙 0.0011252489040605655\n    满意 0.001125013221309369\n    儿子 0.0011246405456808676\n    板块 0.0011240463130764535\n    动画 0.0011239963442815043\n    新政 0.0011226893274064993\n    复出 0.0011226034100533459\n    乐观 0.0011209545830046918\n    俄罗斯 0.0011207893092808415\n    影响力 0.0011199487129200728\n    周迅 0.0011199246044306652\n    传球 0.001118192370812789\n    质疑 0.0011181846546739808\n    程序 0.0011181362309842535\n    取景 0.0011180787880891194\n    时装 0.0011176851112357996\n    外国 0.0011174975418096586\n    起到 0.0011171026207820241\n    原本 0.0011163139548540848\n    版本 0.0011161986091097907\n    作出 0.0011161000436326772\n    宝箱 0.0011148826361304046\n    回升 0.001114202780614028\n    感到 0.0011137767914582838\n    成员 0.0011130524501603207\n    小巧 0.0011123711795368307\n    距离 0.0011123558219173144\n    聊天 0.0011122879467607373\n    租房 0.0011115916427990725\n    答案 0.0011109692176866465\n    回归 0.0011108772920928602\n    回应 0.0011085485235944935\n    身体 0.001107857015360724\n    意识 0.0011071475654780725\n    线条 0.0011066594021061312\n    建材市场 0.0011064836262889175\n    西装 0.001106027548582472\n    理想 0.0011059799300112538\n    手感 0.0011043375481286808\n    条纹 0.001103656517056665\n    登记 0.0011024600634982383\n    用于 0.0011013005750008253\n    卡片机 0.0011002383040342645\n    计算 0.0010999276704585642\n    北美 0.001099579395105851\n    写字楼 0.0010984927894516824\n    创业 0.0010978748404008017\n    高跟鞋 0.0010968703963654786\n    套机 0.00109609247934169\n    上看 0.00109609247934169\n    口碑 0.0010945422696331487\n    主教练 0.001094382871404456\n    利率 0.0010940499738375574\n    最多 0.0010929219687503113\n    展会 0.0010922467353974249\n    邮箱 0.0010915897531131545\n    魔法 0.0010914413571569807\n    不想 0.0010905821371426409\n    欧美 0.001090323495010853\n    优质 0.0010902501517820267\n    拓展 0.0010893991990667453\n    事业 0.0010891698208565052\n    心态 0.0010853450166514835\n    传奇 0.0010833245810966876\n    好友 0.0010816752294147511\n    资深 0.0010811446684831623\n    卡片 0.001080238707429709\n    带动 0.0010788262511312193\n    收紧 0.0010781235015767483\n    下半场 0.0010775061443456166\n    青睐 0.001075476414180937\n    敬请 0.0010726612691696284\n    获奖 0.0010724847015656043\n    签约 0.0010719669987431335\n    总裁 0.001071795341098228\n    货币 0.0010713931628387493\n    承担 0.0010710786762786436\n    落实 0.0010702238155104091\n    员工 0.0010672132888067901\n    信贷 0.001067142995003078\n    合成 0.0010663588347012717\n    连胜 0.00106598539269598\n    兴趣 0.0010657932655356833\n    引导 0.001062646415328183\n    厂商 0.0010621435748555935\n    动态 0.0010616896478066091\n    上场 0.0010613855371841092\n    承诺 0.0010609010474515882\n    杰克逊 0.0010602522482390925\n    漂亮 0.0010602255476064599\n    套餐 0.00105911645911864\n    徐若 0.0010585121657642606\n    绿卡 0.0010585121657642606\n    错过 0.001057935391391819\n    世纪 0.001057107008343238\n    年龄 0.0010557154534564554\n    简洁 0.0010530838700750836\n    本土 0.0010529841640199613\n    迪拜 0.0010529631328948512\n    详细 0.0010520829156404731\n    美国大学 0.0010512371214098785\n    山猫 0.0010509360306076205\n    新手 0.0010509360306076205\n    算是 0.0010501844606993437\n    档期 0.001049022597135814\n    伤病 0.001049022597135814\n    总统 0.0010487985130813156\n    陈嘉 0.0010487367319674645\n    释放 0.0010483555259206728\n    荣誉 0.0010474863702249991\n    固定 0.0010469293985439112\n    股份 0.0010466536980649245\n    腰带 0.0010459999843802639\n    分析师 0.0010453378230576918\n    技术移民 0.0010448804019444903\n    协调 0.0010445192843163975\n    新西兰 0.0010441156416365594\n    打工 0.0010437668410799954\n    补贴 0.001043556531313309\n    秘书长 0.001043266363274942\n    暂停 0.0010427191421180139\n    占据 0.0010420347369940928\n    放映 0.0010402124728597483\n    增仓 0.0010397220089755459\n    集体 0.0010384147837622958\n    吴彦祖 0.0010379606641832684\n    赵薇 0.0010375133443545916\n    气息 0.0010373878481391436\n    科学 0.001037385590191337\n    地址 0.0010370644532369524\n    提示 0.0010367135794316633\n    上演 0.0010360780030078981\n    分差 0.0010334586233793075\n    增持 0.0010334586233793075\n    考察 0.0010324808774000236\n    剩下 0.0010321757289816025\n    总冠军 0.0010320785001943233\n    局面 0.0010315807552806005\n    记录 0.0010300447939330113\n    功夫 0.001028910105179352\n    截止 0.0010282107718406538\n    感光 0.0010277641036026876\n    华裔 0.0010272200278764586\n    资料片 0.0010269310607992454\n    上衣 0.0010263208523052377\n    承认 0.0010259922440583494\n    木材 0.0010255696833362201\n    开户数 0.0010254657792063081\n    氏族 0.0010237701859798028\n    超越 0.0010233553332678429\n    电视 0.0010231112290684586\n    低迷 0.0010224866388290517\n    学历 0.0010218612075256815\n    更换 0.0010215950408430246\n    听歌 0.0010209318521868312\n    情感 0.0010187717065428268\n    生存 0.0010185615905743408\n    共有 0.0010177782295156586\n    已有 0.0010177398343034932\n    广场 0.0010176795797623012\n    官员 0.0010171003080892609\n    动力 0.0010162260497598283\n    新华网 0.0010161744350793824\n    规则 0.0010161073392960039\n    友友 0.001014668466590593\n    财富 0.0010141725169209425\n    面料 0.0010122473178700302\n    做法 0.0010119419377523674\n    并未 0.001011364822214416\n    首付 0.0010107247325548665\n    理性 0.0010099942176604974\n    地位 0.0010092912039678312\n    证书 0.0010092280996045842\n    类别 0.0010086583237288694\n    降价 0.0010086353113619396\n    兄弟 0.0010084361800692103\n    民族 0.0010083644325722502\n    硬盘 0.0010083318580116573\n    有限 0.0010073571831173353\n    林志玲 0.0010067400017976344\n    带给 0.0010066978963058667\n    小牛队 0.0010050393681354868\n    赤壁 0.0010041719557745555\n    财政 0.0010036532330589126\n    走向 0.0010032714094433152\n    宜家 0.0010010522051773088\n    收到 0.0010008816327472933\n    商品住宅 0.0010005901434843806\n    法院 0.0010004659343850473\n    获悉 0.0010001500146575818\n    杭州 0.0010000730861267633\n    奢华 0.0009999615569063565\n    收取 0.0009998398238869896\n    接到 0.0009976007545457188\n    预科 0.0009945942621368226\n    永远 0.0009944592728176875\n    地点 0.0009943181689150223\n    积累 0.000994244818415812\n    登录 0.0009938908409954498\n    教学 0.0009930444320894025\n    意外 0.000992537648095092\n    房间 0.0009912167979854295\n    门派 0.0009910299234820491\n    高层 0.00098896399498998\n    孙中山 0.0009836389884048096\n    丰厚 0.000983285527859903\n    节奏 0.0009830769458198752\n    伤害 0.0009819677736430036\n    相当于 0.0009812679704410598\n    武器 0.0009781041015935873\n    奥斯卡 0.0009776682176684663\n    陈幸妤 0.0009770881530131636\n    赢球 0.0009770881530131636\n    词汇 0.0009737315337892621\n    债市 0.0009729128451218778\n    输给 0.0009729122143459206\n    神话 0.0009728373014331304\n    销量 0.0009711801758835319\n    锁定 0.0009711216193639514\n    随意 0.0009709052105721876\n    帮派 0.0009688111311704716\n    青春 0.0009684349033325946\n    涨价 0.0009674204567435526\n    总决赛 0.0009662678545667544\n    观望 0.0009654454650481197\n    电池 0.0009651308370831336\n    甲醛 0.0009650989927146214\n    公立 0.0009646478177108098\n    缺乏 0.0009641653116366065\n    转换 0.0009618116833550395\n    效率 0.0009611265226673425\n    大厦 0.000960930528399607\n    氛围 0.0009607884169540964\n    灰色 0.0009606614401863283\n    银瑞信 0.0009605110538026328\n    土地储备 0.0009586649877542276\n    存储卡 0.0009582979962244488\n    未能 0.0009580066854061796\n    入围 0.0009562246411026289\n    教授 0.0009558187464053073\n    开机 0.0009553705579884999\n    批准 0.0009542131773232007\n    实行 0.0009524005744230265\n    杜琪峰 0.0009520346106282107\n    针织 0.0009513435884699071\n    尚未 0.0009508184689571824\n    留下 0.0009502343862025226\n    欣赏 0.0009473638670897876\n    群众 0.0009468410060906126\n    出品 0.0009466566178436558\n    国产 0.0009458332194248889\n    有趣 0.0009456172924802494\n    营业税 0.0009453645701354396\n    潜力 0.0009448183808056112\n    客厅 0.000944270561988526\n    澳门 0.0009435857198092185\n    金属 0.0009427802751793941\n    风情 0.0009422767136355954\n    商品 0.000941454132923625\n    大牌 0.0009409883878266336\n    缴纳 0.0009403084435722111\n    输出 0.0009396881872777085\n    木家具 0.0009390158342019463\n    抑制 0.0009388285061207382\n    半场 0.0009381786727479534\n    领取 0.0009380841574516353\n    想象 0.0009375959332994302\n    带领 0.0009373864958886767\n    有利于 0.0009363425685985147\n    和平 0.0009355479876319436\n    高于 0.000935398037665962\n    黄金 0.0009352278392518161\n    确认 0.0009351423580053597\n    温暖 0.0009339636166287714\n    收费 0.0009336920054340943\n    日常 0.0009328753827585\n    担保 0.0009311412278739439\n    和谐 0.0009310747091338232\n    持有人 0.0009289534118245381\n    赚钱 0.0009284386968216904\n    准确 0.0009274685357355653\n    前景 0.0009265900064505272\n    沙发 0.0009265621335145951\n    先进 0.0009252592468432709\n    约定 0.0009245253896402059\n    案件 0.0009244162188645682\n    获取 0.0009242237676032064\n    大会 0.000923421143634689\n    花园 0.0009223572427261897\n    权威部门 0.000921869315320995\n    按键 0.0009216802124539169\n    回来 0.0009212612981866897\n    排行榜 0.0009211356240385085\n    偏股 0.0009207176826470195\n    收获 0.0009204790922644557\n    太阳 0.0009201531800872647\n    短信 0.0009194147438225995\n    折价率 0.000919263825048057\n    长裙 0.0009190226701964766\n    质感 0.0009184948797005305\n    再起 0.0009181566718569374\n    原创 0.0009171583160326142\n    挑选 0.0009165013114153798\n    窃听 0.0009163737746714658\n    个案 0.0009158205213603677\n    效应 0.0009157363433199261\n    为准 0.0009150426931601243\n    天津 0.000913819725746552\n    震撼 0.0009136519126454319\n    优惠政策 0.0009133815300039137\n    打算 0.0009126363085919446\n    政治 0.0009121777805859877\n    账户 0.0009110954903474924\n    辛亥革命 0.000909311700272456\n    软件 0.0009089249613992115\n    孙俪 0.000908190911454543\n    性格 0.0009061346985122716\n    爆发 0.0009059850343740448\n    识别 0.0009054385046853969\n    减仓 0.0009047811841797476\n    蝙蝠侠 0.0009030788525275388\n    视野 0.0009030684307061496\n    公民 0.0009027683360565837\n    汤唯 0.0009019275258583048\n    武生 0.0009010282495255084\n    偶像 0.0009001670644619749\n    医院 0.0008999980328029183\n    妈妈 0.0008998346793606664\n    陆续 0.0008997005062142062\n    绿衫 0.0008995659089588326\n    围巾 0.0008987339004182273\n    皮尔斯 0.0008971488861802035\n    星空 0.0008969063761948419\n    强势 0.0008966589532026564\n    所有人 0.0008961471982297667\n    人性化 0.000895675495211474\n    招商 0.0008948918822723145\n    伴随 0.0008947531793749486\n    基德 0.0008945249787212072\n    争议 0.0008938677354515776\n    刘青云 0.0008926461711976109\n    中信 0.0008923013628992574\n    慢慢 0.0008922583915234077\n    老百姓 0.0008921646747920416\n    情绪 0.000891701810357143\n    差异 0.0008911185081975717\n    开放式 0.0008911014523977627\n    发送 0.0008904097363165967\n    商场 0.0008903728419034148\n    国度 0.0008903575810424039\n    女生 0.0008898201521371686\n    自主 0.0008887940117593461\n    感动 0.0008886919995569858\n    打球 0.0008885115196413741\n    新高 0.0008884103824039452\n    国土 0.0008873906523755066\n    率先 0.0008872162240072931\n    熊市 0.0008871426691857125\n    油漆 0.0008864959307427194\n    男主角 0.0008864861901654288\n    蓝色 0.000885808860610261\n    超出 0.0008851905820392158\n    自信 0.0008850634389030768\n    是从 0.0008849777376397919\n    休息 0.0008846569503811493\n    首款 0.000884469894314852\n    中文 0.0008836799024045739\n    演出 0.000883478457499941\n    重仓股 0.0008830731660176091\n    星河湾 0.0008829671068895438\n    说法 0.0008819155596651184\n    牛仔裤 0.0008815959362827772\n    给出 0.0008807555352666221\n    缩小 0.0008804890854195135\n    国际化 0.000879859045020551\n    长城 0.0008777030277915308\n    总价 0.0008776294343064168\n    却是 0.0008770442161747285\n    无奈 0.0008763079101369066\n    读书 0.0008760297297332443\n    投资人 0.0008756315700701639\n    携手 0.0008752121119080514\n    项链 0.0008748532805354395\n    罗伊 0.0008742208246977615\n    债券市场 0.0008736742383910617\n    开工 0.0008733793669443986\n    封盖 0.0008730412769546937\n    美好 0.0008722946058835789\n    中海 0.0008718188631965527\n    关键时刻 0.0008714724752253525\n    两国 0.0008713972129329718\n    后卫 0.0008712192049213197\n    副总经理 0.0008706162428440672\n    观察 0.0008701657458780568\n    心情 0.0008681834146503832\n    选举 0.0008678701134109945\n    豹纹 0.0008670795475615282\n    小姐 0.000867061064186936\n    拒绝 0.000866875154663104\n    低价 0.0008668144685494754\n    蜘蛛侠 0.0008666643826675574\n    券商 0.0008661863275757999\n    图片 0.0008649954409761588\n    可惜 0.0008648185963601976\n    声音 0.0008643513300340289\n    商城 0.0008642057450269113\n    参观 0.0008641595208743082\n    天域 0.0008639774617874179\n    除尘 0.000863803841080121\n    凯文 0.0008628042347949232\n    内销 0.0008616330857966391\n    界面 0.0008610292492534768\n    格局 0.000860191543633704\n    联邦 0.0008598642653003627\n    水准 0.0008598094175831664\n    新盘 0.0008580838266846372\n    业之峰 0.0008580838266846372\n    学位 0.0008576179324126317\n    危险 0.0008565755237455092\n    建筑面积 0.0008560144190772919\n    买家 0.0008557024968333057\n    搞笑 0.0008553966402923494\n    成果 0.0008546578011185664\n    大学生 0.000852356451217038\n    片子 0.0008522738583301242\n    美的 0.000851820441088399\n    变形金刚 0.000851820441088399\n    好看 0.0008516866275315075\n    个股 0.0008516484825561606\n    医疗 0.000851104158741111\n    范儿 0.0008501159372244097\n    等效 0.0008486252442467681\n    鹏华 0.0008455570554921608\n    新疆 0.0008445505504563009\n    电视剧 0.0008437072686151596\n    活力 0.000843502679965028\n    一大 0.000842721209469061\n    盈利 0.0008424957904643431\n    转移 0.0008421989440240114\n    基地 0.0008419081796600207\n    远远 0.0008416014942307674\n    首选 0.000841466860614484\n    前提 0.0008412738178105125\n    合格 0.0008410619695712\n    见面 0.0008409179577503767\n    品种 0.0008402165879156012\n    易居 0.0008392936698959225\n    荷银 0.0008392936698959225\n    同意 0.0008392796312597273\n    态势 0.0008383033313062831\n    华盛顿 0.0008377946491566297\n    雄鹿 0.0008376946052674237\n    遗憾 0.0008373481319402307\n    播放器 0.0008373389604348566\n    事实 0.000836502579519321\n    夸张 0.0008365018248462257\n    创下 0.0008360092966106016\n    增添 0.0008355410635677943\n    楼价 0.0008334865136343275\n    超市 0.0008329891531575151\n    动漫 0.000832951237621659\n    液晶 0.0008328203180366223\n    墙面 0.0008319449433167301\n    发表 0.000831915112186475\n    走出 0.0008315600709895085\n    停止 0.0008313802862464903\n    博士 0.00083109080796737\n    全明星 0.0008310834908686393\n    视角 0.0008307827934033086\n    完整 0.000830493818754031\n    独家 0.000830467264770587\n    思路 0.0008297987548643954\n    巴黎 0.0008281520711733323\n    五星 0.0008279231766635492\n    年报 0.0008269737146073218\n    中国政府 0.0008262872027019268\n    记得 0.0008261733765467248\n    同步 0.0008261361829470706\n    平稳 0.000825384207681905\n    菜单 0.000825319263480425\n    背心 0.0008252559861703747\n    深刻 0.0008250557957766042\n    队伍 0.0008248666149599644\n    带有 0.0008245296416249387\n    饰品 0.0008241538083810288\n    代理 0.0008231905854410938\n    外交部 0.0008227775946622657\n    气氛 0.0008222647066984793\n    尊重 0.000822095214042658\n    双重 0.0008220263594840765\n    院长 0.0008219738144984557\n    主打 0.0008218863242157411\n    看点 0.0008214198723789854\n    互联网 0.0008208921927220716\n    层面 0.0008202292854859942\n    邮件 0.0008201233075378207\n    永久 0.0008190910885791242\n    前锋 0.000819009994162191\n    民众 0.0008188425966981964\n    巴特勒 0.0008183951932981257\n    祝福 0.0008176782554393597\n    德克 0.0008175321448437266\n    诱惑 0.0008165013290229086\n    审批 0.0008164298856834977\n    打破 0.0008159853969790404\n    设立 0.0008145857523275308\n    选股 0.0008142401275109696\n    眼球 0.000814188125083579\n    李冰冰 0.000812644701405467\n    全世界 0.0008124675532082518\n    查询 0.0008122588122981649\n    地铁 0.000812000073796487\n    对抗 0.0008108666090878751\n    试点 0.000809848133242925\n    居室 0.0008097446858167786\n    时机 0.0008092923407241464\n    公示 0.0008085421855461826\n    设施 0.0008084488744327766\n    建仓 0.0008082507431078786\n    张艺谋 0.0008079015363027231\n    几率 0.0008075576633049629\n    埃文斯 0.0008074339975621831\n    迹象 0.000806938832758851\n    随机 0.0008065243154836261\n    估计 0.000805749836039179\n    王者 0.0008050315538107745\n    庞大 0.0008048932854094908\n    可能性 0.0008048655670901804\n    投票 0.0008040386741501041\n    阿伦 0.0008036325797697356\n    赴美 0.0008028716773093796\n    精力 0.0008028094955739557\n    成就 0.0008028028793498735\n    高价 0.0008025988945575019\n    跟踪 0.0008021199097540807\n    精美 0.0008018008575334564\n    毕比 0.0008017133563184932\n    韩庚 0.0008017133563184932\n    中欧 0.0008009191393716897\n    保利 0.0008007545582894101\n    频繁 0.0008006440108236238\n    演讲 0.0008006334430914064\n    收集 0.0007995837090793352\n    披露 0.0007993404687361232\n    大胆 0.0007988875392943063\n    贺岁 0.0007985625051369143\n    秘密 0.0007981279497152528\n    极品 0.0007978328675332049\n    额度 0.0007973101630286874\n    接口 0.0007963665011145167\n    手游 0.0007954499707222549\n    达成 0.0007948669261431935\n    金牌 0.0007943273638667454\n    幸运 0.0007936156832777553\n    想到 0.0007933225233525875\n    分数 0.0007932109507710951\n    尴尬 0.0007931257747210239\n    空中网 0.0007927584434848258\n    权益 0.0007919388799041848\n    教师 0.0007914921169018822\n    呼吁 0.0007903739909067809\n    看过 0.0007900391287286417\n    小盘 0.0007898400248602302\n    宏观经济 0.0007898174384081103\n    小斯 0.0007891865851260167\n    诛神 0.0007891865851260167\n    魏楠 0.0007891865851260167\n    理光 0.000788850104779284\n    早已 0.0007878065459839837\n    决赛 0.000787535354705285\n    称号 0.0007870659777560455\n    记者会 0.0007864798014807654\n    京城 0.0007859665295619317\n    发出 0.0007843566176098079\n    肯塔基 0.0007839554323674801\n    麻烦 0.0007839442578673059\n    股东 0.0007838758486895884\n    信息技术 0.0007838556783290031\n    反超 0.0007835028256743552\n    大众 0.000783362062862918\n    博洛尼 0.0007829231995297785\n    地图搜索 0.0007819303670180622\n    花费 0.0007818323085945224\n    整合 0.0007812775290279592\n    予以 0.000781007081246603\n    追捧 0.0007806877864307438\n    审核 0.0007805835642616528\n    提交 0.000780511212965979\n    缩水 0.0007804374795810287\n    专卖店 0.0007803970960233564\n    更衣室 0.0007802332045094634\n    解读 0.000779102543907344\n    稳健 0.0007784455003824118\n    备受 0.0007784154470292114\n    元件 0.0007770601597345697\n    大礼包 0.0007770168160961925\n    外形 0.0007769294737491558\n    舒淇 0.0007766598139335402\n    汽车 0.0007760194461191035\n    制片人 0.0007757793960184421\n    分为 0.0007754089809265643\n    最快 0.0007747446360909428\n    预算 0.0007741011783884371\n    父亲 0.0007734348324737161\n    温州 0.0007732903074230237\n    我会 0.0007730774203625446\n    思考 0.0007729309187766906\n    陷入 0.0007719949301961569\n    查看 0.0007719925589828022\n    较低 0.0007719867610316058\n    楼面地价 0.0007718197346802362\n    精心 0.0007715246355791583\n    堪称 0.0007714597076854887\n    国王 0.0007711105625336582\n    队员 0.0007710034585046903\n    新一轮 0.0007708634217206649\n    放大 0.0007707312560481853\n    暂时 0.0007694286383263783\n    模特 0.0007693664810247947\n    电器 0.0007690362357156195\n    流程 0.000768913676664812\n    国民党 0.0007686422848423959\n    店面 0.0007683526033176942\n    曲线 0.0007680777890389721\n    较高 0.0007679130971531298\n    人口 0.0007675532271491374\n    新城 0.0007675318458117962\n    开业 0.0007673620980702425\n    超值 0.0007667482189753626\n    开拍 0.0007667134794099047\n    音效 0.0007663208922422623\n    舞台 0.0007662749635580022\n    到场 0.0007662348524223034\n    形容 0.0007655424039093718\n    利好 0.0007650272045726748\n    周琦 0.0007647038670596095\n    号线 0.0007641330427410638\n    板材 0.0007629825191163137\n    缺席 0.0007619877172794374\n    组队 0.0007617913167861998\n    灰尘 0.0007617101057566112\n    企业家 0.0007616981928745963\n    开设 0.000761664851201273\n    单价 0.0007608072888180282\n    强劲 0.0007601553396796416\n    规格 0.0007600831836638087\n    认同 0.0007599324176918281\n    开局 0.0007592792208887946\n    诞生 0.0007592117242139441\n    回家 0.0007591603989283036\n    古典 0.0007587212601510367\n    基金业 0.0007580582616552216\n    红毯 0.0007578696571448255\n    温馨 0.0007578065655280444\n    徐克 0.0007576877376082884\n    杰森 0.0007574209730876132\n    体会 0.000756929826938321\n    典型 0.0007568111190862771\n    文凭 0.0007567701636706301\n    好像 0.0007563106566343615\n    学术 0.0007562411120536183\n    家人 0.000755818763290293\n    扩张 0.0007556555054365147\n    海南 0.0007556529782680655\n    协商 0.0007556060980538698\n    留学人员 0.0007555978914464746\n    冯小刚 0.0007554105096286273\n    伦敦 0.0007547645755610777\n    阵营 0.0007547025834126292\n    民生 0.0007544444750866439\n    休斯敦 0.0007543785850937955\n    母亲 0.0007538205659837845\n    诚信 0.0007528919701368541\n    确诊 0.0007522099418989509\n    折扣 0.000752041334332917\n    缺少 0.0007508826254856379\n    拉开 0.0007498899426846477\n    看重 0.0007497509137140163\n    粉色 0.0007493308961876695\n    工业 0.0007487493104165979\n    势头 0.0007483576841942813\n    杂志 0.0007483434140187093\n    礼物 0.0007470761907000196\n    电子 0.0007470398061067756\n    道路 0.000746738539963975\n    抢眼 0.000745850155252141\n    税收 0.0007456058553120254\n    到位 0.0007453920602272467\n    抵达 0.0007453714859777908\n    会晤 0.0007444890671086488\n    细腻 0.0007444842147702463\n    智慧 0.000743576792738418\n    流畅 0.0007430932620253212\n    市话费 0.0007428551851436207\n    相互 0.0007425469454923023\n    总部 0.0007423726080241424\n    现状 0.0007422020845751896\n    颁奖 0.0007416542820194638\n    热血 0.0007410610951993817\n    山东 0.0007402714194727075\n    签署 0.0007396618125944962\n    必备 0.0007394485257141107\n    四国 0.0007393001563484617\n    无需 0.0007392645478364572\n    热点 0.0007390703449579158\n    失败 0.0007386461409680957\n    次数 0.0007384916057255845\n    战争 0.000738404826938206\n    拉动 0.0007383173882874141\n    录音 0.0007382652322617523\n    手续 0.0007381002027347768\n    关注度 0.0007378433484351921\n    现身 0.0007377706010963049\n    转型 0.0007376679138817662\n    走进 0.0007374815385033598\n    听到 0.0007367043336613619\n    帕特森 0.0007359212590897874\n    专访 0.0007357887382324177\n    赠送 0.0007350658288298165\n    正面 0.0007349858159044992\n    溢价 0.0007348567976419349\n    全高清 0.0007334922026895097\n    一同 0.0007333225618570684\n    标志 0.0007320446172851586\n    布局 0.0007319462270111359\n    高品质 0.000731578655837877\n    金色 0.0007314135594403868\n    马甲 0.0007312824042538409\n    出战 0.0007311946194817086\n    保留 0.0007310259954805559\n    盛宴 0.0007304159667740187\n    争夺 0.0007300873218964412\n    区别 0.0007296066990984636\n    周刊 0.0007293419916556878\n    交通 0.0007288768909091595\n    委托 0.0007286907314948014\n    草案 0.000728305995747495\n    缺阵 0.000728289397199628\n    钱德勒 0.0007276828640619802\n    能源 0.0007273838038458092\n    引进 0.0007272026768844041\n    单元 0.000726749712212189\n    信号 0.0007264160922544488\n    豪华 0.0007264025149357382\n    攻略 0.0007261222553235097\n    银华 0.0007256781258713663\n    账号 0.0007256503136639343\n    穿衣 0.0007256503136639343\n    依法 0.0007253104642815041\n    季度 0.0007246063859658402\n    内心 0.0007240156393453103\n    机遇 0.0007237490619798184\n    高手 0.0007234936832553997\n    拨打 0.0007233303453632756\n    素质 0.0007228589723325344\n    裙子 0.0007226848980557297\n    不算 0.0007209672317359739\n    主力 0.0007205972735582538\n    篮球 0.0007200079658132945\n    外援 0.0007196590030745098\n    总额 0.0007193896000206425\n    盖帽 0.0007177191089441628\n    到达 0.0007172655947548431\n    老人 0.000717260168217114\n    估值 0.0007172317724342907\n    资讯 0.0007168669739926389\n    集合 0.0007167565533641138\n    白蛇传 0.0007167403886958231\n    开通 0.0007163111356570709\n    传媒 0.0007162042823294642\n    保障性 0.0007159143737833523\n    编剧 0.0007153695605568627\n    实践 0.0007150852525872136\n    生意 0.0007148123327145481\n    必美 0.000714025957971158\n    拍卖 0.0007139479573180295\n    相册 0.0007135913840877049\n    回忆 0.0007133403097783279\n    回落 0.0007129450930263403\n    途径 0.0007120079391537271\n    工作室 0.000711976681341758\n    猛犸 0.0007117381163696281\n    牛市 0.0007114681992063395\n    改进 0.0007098929237770128\n    富国 0.0007087711369119414\n    城乡 0.0007087650671320889\n    女将 0.0007084837217719885\n    年薪 0.0007078171625428111\n    吴尊 0.0007077625723749197\n    开场 0.0007075903581227396\n    观看 0.0007071394379995128\n    高位 0.0007065957335499076\n    彰显 0.0007064972326295572\n    移民部 0.0007064407152836392\n    主创 0.0007060486236548391\n    认定 0.0007060099891155645\n    家具市场 0.0007058132516446226\n    扩展 0.0007054783009675823\n    协助 0.0007047393265730415\n    都市 0.0007044718730532331\n    小户型 0.0007043460256301151\n    神奇 0.0007042220860699281\n    包含 0.0007041716690335652\n    峰会 0.0007037566661637209\n    火热 0.0007028465489734029\n    防水 0.0007028238254059176\n    探索 0.0007027584954791046\n    台词 0.000702463231699425\n    经贸 0.0007022227608982462\n    增值 0.0007016401944730874\n    实时 0.0007014883508915872\n    运行 0.0007013039770572256\n    不小 0.0007005605091422688\n    人文 0.0007003094641936316\n    相继 0.0006995526943472619\n    开衫 0.000699157821311643\n    联合国 0.0006990054893550238\n    居留 0.0006985076804978637\n    纳入 0.0006984718838155842\n    名额 0.0006980034006152749\n    小幅 0.0006974633285649155\n    品位 0.0006967987122528665\n    签订 0.0006966150452826071\n    充值 0.0006964358451975847\n    共识 0.0006963210684525443\n    大幅度 0.000696007492759527\n    携带 0.0006960050438481984\n    合法 0.0006959010442645448\n    房祖名 0.0006954845072450784\n    暴扣 0.0006952358011824433\n    路线 0.0006951605090915478\n    廉租房 0.0006944725453063774\n    明白 0.0006944574487531507\n    吸引力 0.0006942355346362791\n    公平 0.0006941192657496432\n    思想 0.0006938069314843648\n    斯科拉 0.0006938020263451347\n    走访 0.0006932871733496273\n    \n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter1_Text_Classification_based_on_Machine_Learning/2.LR_SVM_Text_Classification/","content":"\n# 逻辑回归/SVM与文本分类\n\n\n* 参考文章：[《从原理到应用：简述Logistics回归算法》](https://www.jiqizhixin.com/articles/2018-05-13-3)\n\n## Part 1 Logistic Regression\n\n\n### 1.1 什么是 Logistic 回归？\n\n\n和很多其他机器学习算法一样，逻辑回归也是从统计学中借鉴来的，尽管名字里有回归俩字儿，但它不是一个需要预测连续结果的回归算法。\n\n与之相反，Logistic 回归是二分类任务的首选方法。它输出一个 0 到 1 之间的离散二值结果。简单来说，它的结果不是 1 就是 0。\n\n癌症检测算法可看做是 Logistic 回归问题的一个简单例子，这种算法输入病理图片并且应该辨别患者是患有癌症（1）或没有癌症（0）。\n\n它是如何工作的?\nLogistic 回归通过使用其固有的 logistic 函数估计概率，来衡量因变量（我们想要预测的标签）与一个或多个自变量（特征）之间的关系。\n\n然后这些概率必须二值化才能真地进行预测。这就是 logistic 函数的任务，也称为 Sigmoid 函数。Sigmoid 函数是一个 S 形曲线，它可以将任意实数值映射到介于 0 和 1 之间的值，但并不能取到 0或1。然后使用阈值分类器将 0 和 1 之间的值转换为 0 或 1。\n\n下面的图片说明了 logistic 回归得出预测所需的所有步骤。\n\n\n![](https://image.jiqizhixin.com/uploads/editor/164bdc1d-46ae-4c67-868f-a36d8c129af0/1526191840515.png)\n\n\n下面是 logistic 函数（sigmoid 函数）的图形表示：\n\n![](https://image.jiqizhixin.com/uploads/editor/1fe28e0e-94a3-42aa-a1fa-1c46e21cc29d/1526191840278.png)\n\n我们希望随机数据点被正确分类的概率最大化，这就是最大似然估计。最大似然估计是统计模型中估计参数的通用方法。\n\n![](https://www.zhihu.com/equation?tex=C%28%5Ctheta%29+%3D+%5C%7B_%7B-log%281-h_%5Ctheta%28x%29%29%2C+y%3D0%7D%5E%7B-log%28h_%5Ctheta%28x%29%29%2Cy%3D1%7D%2C+where%3A+h_%5Ctheta%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D)\n\n你可以使用不同的方法（如优化算法）来最大化概率。牛顿法也是其中一种，可用于查找许多不同函数的最大值（或最小值），包括似然函数。也可以用梯度下降法代替牛顿法。\n\n### 1.2 Logistic 回归 vs 线性回归\n\n\n你可能会好奇：logistic 回归和线性回归之间的区别是什么。逻辑回归得到一个离散的结果，但线性回归得到一个连续的结果。预测房价的模型算是返回连续结果的一个好例子。该值根据房子大小或位置等参数的变化而变化。离散的结果总是一件事（你有癌症）或另一个（你没有癌症）。\n\n### 1.3 优缺点\n\n\nLogistic 回归是一种被人们广泛使用的算法，因为它非常高效，不需要太大的计算量，又通俗易懂，且很容易调整，并且输出校准好的预测概率。\n\n与线性回归一样，当你去掉与输出变量无关的属性以及相似度高的属性时，logistic 回归效果确实会更好。因此特征处理在 Logistic 和线性回归的性能方面起着重要的作用。\n\nLogistic 回归的另一个优点是它非常容易实现，且训练起来很高效。在研究中，我通常以 Logistic 回归模型作为基准，再尝试使用更复杂的算法。\n\n由于其简单且可快速实现的原因，Logistic 回归也是一个很好的基准，你可以用它来衡量其他更复杂的算法的性能。\n\n它的一个缺点就是我们不能用 logistic 回归来解决非线性问题，因为它的决策边界是线性的。我们来看看下面的例子，两个类各有俩实例。\n\n![](https://image.jiqizhixin.com/uploads/editor/439f4df7-544b-436d-9fc0-582aaa7c097f/1526191840966.png)\n\n显然，我们不可能在不出错的情况下划出一条直线来区分这两个类。使用简单的决策树是个更好的选择。\n\n![](https://image.jiqizhixin.com/uploads/editor/c473eda3-cb03-4433-ad89-bd81e88a5f33/1526191840677.png)\n\nLogistic 回归并非最强大的算法之一，它可以很容易地被更为复杂的算法所超越，另一个缺点是它高度依赖正确的数据表示。\n\n这意味着逻辑回归在你已经确定了所有重要的自变量之前还不会成为一个有用的工具。由于其结果是离散的，Logistic 回归只能预测分类结果。\n\n### 1.4 何时适用\n\n\n就像我已经提到的那样，Logistic 回归通过线性边界将你的输入分成两个「区域」，每个类别划分一个区域。因此，你的数据应当是线性可分的，如下图所示的数据点：\n\n![](https://image.jiqizhixin.com/uploads/editor/cc1f0a41-fa18-4499-b12b-374da215ebe9/1526191840804.png)\n\n换句话说：当 Y 变量只有两个值时（例如，当你面临分类问题时），您应该考虑使用逻辑回归。注意，你也可以将 Logistic 回归用于多类别分类，下一节中将会讨论。\n\n### 1.5 多分类任务\n\n\n优先推荐softmax\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/001ce4c2c74e78a66a4d7d04ab92cbd0d0fdec02)\n\n补充：\n1）一对多（OVA）\n按照这个策略，你可以训练 10 个二分类器，每个数字一个。这意味着训练一个分类器来检测 0，一个检测 1，一个检测 2，以此类推。当你想要对图像进行分类时，只需看看哪个分类器的预测分数最高\n\n2）一对一（OVO）\n按照这个策略，要为每一对数字训练一个二分类器。这意味着要训练一个可以区分 0s 和 1s 的分类器，一个可以区分 0s 和 2s 的分类器，一个可以区分 1s 和 2s 的分类器，等等。如果有 N 个类别，则需要训练 N×N（N-1）/ 2 个分类器，对于 MNIST 数据集，需要 45 个分类器。\n\n3) 其它分类算法\n其他常见的分类算法有朴素贝叶斯、决策树、随机森林、支持向量机、k-近邻等等。我们将在其他文章中讨论它们，但别被这些机器学习算法的数量吓到。请注意，最好能够真正了解 4 或 5 种算法，并将精力集中在特征处理上，这也是未来工作的主题。\n\n\n\n### 1.6 小结\n\n在这篇文章中，你已了解什么是 Logistic 回归，以及它是如何工作的。你现在对其优缺点也了有深刻的了解，并且知道何时用它。\n\n此外，你还探索了使用 Logistic 回归与 sklearn 进行多分类的方法，以及为什么前者是比其他机器学习算法更好的基准算法。\n\n\n原文链接：https://towardsdatascience.com/the-logistic-regression-algorithm-75fe48e21cfa\n\n\n\n## Part 2 SVM\n\n\n###  2.1 svm简介\n\n* 参考文章：[《支持向量机(img/svm)是什么意思？》](https://www.zhihu.com/question/21094489/answer/86273196)\n\n在很久以前的情人节，大侠要去救他的爱人，但魔鬼和他玩了一个游戏。魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。”\n\n![](img/svm1.png)\n于是大侠这样放，干的不错？\n![](img/svm2.png)\n然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。\n![](img/svm3.png)\nSVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。\n![](img/svm4.png)\n现在即使魔鬼放了更多的球，棍仍然是一个好的分界线。\n![](img/svm5.png)\n然后，在SVM 工具箱中有另一个更加重要的 trick。 魔鬼看到大侠已经学会了一个trick，于是魔鬼给了大侠一个新的挑战。\n![](img/svm6.png) 现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。\n![](img/svm7.png)\n\n现在，从魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。\n![](img/svm8.png)\n\n再之后，无聊的大人们，把这些球叫做 「data」，把棍子 叫做 「classifier」, 最大间隙trick 叫做「optimization」， 拍桌子叫做「kernelling」, 那张纸叫做「hyperplane」。\n\n图片来源：Support Vector Machines explained well\n\n\n\n### 2.2 如何计算svm最优超平面\n\n\n* 参考文章：[《支持向量机 (SVM)分类器原理分析与基本应用》](https://www.cnblogs.com/muchen/p/6297027.html)\n\n1. 首先根据算法思想 - \"找到具有最小间隔的样本点，然后拟合出一个到这些样本点距离和最大的线段/平面。\" 写出目标函数：\n\n![](img/svm21.png)\n\n该式子的解就是待求的回归系数。\n\n然而，这是一个嵌套优化问题，非常难进行直接优化求解。为了解这个式子，还需要以下步骤。\n\n2. 不去计算内层的min优化，而是将距离值界定到一个范围 - 大于1，即最近的样本点，也即支持向量到超平面的距离为1。下图可以清楚表示这个意思：\n\n![](img/svm22.png)\n\n\n去掉min操作，代之以界定：label * (wTx + b) >= 1。\n\n3. 这样得到的式子就是一个带不等式的优化问题，可以采用拉格朗日乘子法(KKT条件)去求解。\n\n具体步骤推论本文不给出。推导结果为：\n\n![](img/svm23.png)\n\n\n另外，可加入松弛系数 C，用于控制 \"最大化间隔\" 和\"保证大部分点的函数间隔小于1.0\" 这两个目标的权重。\n\n将 α >= 0 条件改为 C >= α >= 0 即可。\n\nα 是用于求解过程中的一个向量，它和要求的结果回归系数是一一对应的关系。\n\n将其中的 α 解出后，便可依据如下两式子(均为推导过程中出现的式子)进行转换得到回归系数：\n\n![](img/svm24.png)\n![](img/svm25.png)\n\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/4Text_Classification/Chapter1_Text_Classification_based_on_Machine_Learning/1.Naive_bayes_Chinese_text_classification/","content":"\n# 朴素贝叶斯模型与中文文本分类\n\n\n* 参考文章：[《朴素贝叶斯模型与中文文本分类》](http://blog.csdn.net/han_xiaoyang/article/details/50616559)\n\n## 1. 引言\n\n\n贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级自然语言处理模型也可以从它演化而来。因此，学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口。\n\n## 2. 贝叶斯公式\n\n\n贝叶斯公式就一行：\n\n> <center> $P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)} $ </center>\n\n而它其实是由以下的联合概率公式推导出来：\n\n> <center>$P(Y,X) = P(Y|X)P(X)=P(X|Y)P(Y) $</center>\n\n其中 $P(Y)$ 叫做先验概率， $P(Y|X)$ 叫做后验概率，$P(Y,X)$叫做联合概率。\n\n额，恩，没了，贝叶斯最核心的公式就这么些。\n\n## 3. 用机器学习的视角理解贝叶斯公式\n\n\n在机器学习的视角下，我们把$X$理解成**“具有某特征”**，把$Y$理解成**“类别标签”**(一般机器学习问题中都是`X=>特征`, `Y=>结果`对吧)。在最简单的二分类问题(`是`与`否`判定)下，我们将$Y$理解成**“属于某类**”的标签。于是贝叶斯公式就变形成了下面的样子:\n\n> $P(“属于某类”|“具有某特征”)=\\frac{P(“具有某特征”|“属于某类”)P(“属于某类”)}{P(“具有某特征”)} $\n\n 我们尝试更口(shuo)语(ren)化(hua)的方式解释一下上述公式：\n\n> $P(“属于某类”|“具有某特征”)=$在已知某样本“具有某特征”的条件下，该样本“属于某类”的概率。所以叫做**『后验概率』**。\n\n> $P(“具有某特征”|“属于某类”)=$在已知某样本“属于某类”的条件下，该样本“具有某特征”的概率。\n\n> $P(“属于某类”) =$（在未知某样本具有该“具有某特征”的条件下，）该样本“属于某类”的概率。所以叫做**『先验概率』**。\n\n> $P(“具有某特征”) =$(在未知某样本“属于某类”的条件下，)该样本“具有某特征”的概率。\n\n而我们二分类问题的最终目的就是要**判断$P(“属于某类”|“具有某特征”)$是否大于1/2**就够了。贝叶斯方法把计算**“具有某特征的条件下属于某类”**的概率转换成需要计算**“属于某类的条件下具有某特征”**的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于有监督学习方法。\n\n这里再补充一下，一般**『先验概率』、『后验概率』是相对**出现的，比如$P(Y)$与$P(Y|X)$是关于$Y$的先验概率与后验概率，$P(X)$与$P(X|Y)$是关于$X$的先验概率与后验概率。\n\n## 4. 垃圾邮件识别\n\n\n举个例子好啦，我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是**判断$P(“垃圾邮件”|“具有某特征”)$是否大于1/2**。现在假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：\n\n> “我司可办理正规发票（保真）17%增值税发票点数优惠！”\n\n也就是**判断概率$P(“垃圾邮件”|“我司可办理正规发票（保真）17\\%增值税发票点数优惠！”)$是否大于1/2**。\n\n咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！好，具体点说：\n\n> $P(“垃圾邮件”|“我司可办理正规发票（保真）17\\%增值税发票点数优惠！”)$\n>$ =\\frac{垃圾邮件中出现这句话的次数}{垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数}$\n\n## 5. 分词\n\n\n然后同学们开始朝我扔烂白菜和臭鸡蛋，“骗纸！！误人子弟！！你以为发垃圾邮件的人智商都停留在20世纪吗！！你以为它们发邮件像抄作业一样不改内容吗！！哪来那么多相同的句子！！\"。\n\n咳咳，表闹，确实，在我们这样的样本容量下，『完全击中』的句子很少甚至没有（无法满足大数定律，），算出来的概率会很失真。一方面找到庞大的训练集是一件非常困难的事情，另一方面其实对于任何的训练集，我们都可以构造出一个从未在训练集中出现的句子作为垃圾邮件（真心的，之前看过朴素贝叶斯分类分错的邮件，我觉得大中华同胞创(zao)新(jia)的能力简直令人惊(fa)呀(zhi)）。\n\n一个很悲哀但是很现实的结论：\n**训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的。**\n\n所以解决方法是？\n对啦！**句子的可能性无限，但是词语就那么些！！**汉语常用字2500个，常用词语也就56000个(你终于明白小学语文老师的用心良苦了)。按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如**“我司可办理正规发票，17%增值税发票点数优惠！”**，这句话就比之前那句话少了**“（保真）”**这个词，但是意思基本一样。如果把这些情况也考虑进来，那样本数量就会增加，这就方便我们计算了。\n\n于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如**“正规发票”**可以作为一个单独的词语，**“增值税”**也可以作为一个单独的词语等等。\n\n> 句子**“我司可办理正规发票，17%增值税发票点数优惠！”就可以变成（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）**。\n\n于是你接触到了中文NLP中，最最最重要的技术之一：**分词**！！！也就是**把一整句话拆分成更细粒度的词语来进行表示**。咳咳，另外，分词之后**去除标点符号、数字甚至无关成分(停用词)是特征预处理中的一项技术**。\n\n**中文分词是一个专门的技术领域(我不会告诉你某搜索引擎厂码砖工有专门做分词的！！！)，我们将在下一篇文章探讨，这里先将其作为一个已知情况进行处理。具体细节请见下回分晓**\n\n我们观察（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，**这可以理解成一个向量：向量的每一维度都表示着该特征词在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。**\n\n因此贝叶斯公式就变成了：\n\n> $P(“垃圾邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）$\n> $=\\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|\"垃圾邮件\"）P(“垃圾邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }$\n\n>$P(“正常邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）$\n>$=\\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|\"正常邮件\"）P(“正常邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }$\n\n\n## 6. 条件独立假设\n\n\n有些同学说...好像...似乎...经过上面折腾，概率看起来更复杂了(-｡-;)\n那...那我们简化一下...\n\n概率$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|\"垃圾邮件\"）$依旧不够好求，我们引进一个**很朴素的近似**。为了让公式显得更加紧凑，我们令字母S表示“垃圾邮件”,令字母H表示“正常邮件”。近似公式如下：\n\n>$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）$\n>$=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S）$\n>$×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)$\n\n这就是传说中的**条件独立假设**。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。\n\n于是我们就只需要比较以下两个式子的大小：\n\n>$C = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)$\n>$×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)$\n>$\\overline{C}=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)$\n>$×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”) $\n\n厉害！酱紫处理后**式子中的每一项都特别好求**！只需要**分别统计各类邮件中该关键词出现的概率**就可以了！！！比如：\n\n>$P(“发票”|S）=\\frac{垃圾邮件中所有“发票”的次数}{垃圾邮件中所有词语的次数}$\n\n统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。\n\n## 7. 朴素贝叶斯(Naive Bayes)，“Naive”在何处？\n\n\n**加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法（Naive Bayes）。** Naive的发音是“乃一污”，意思是“朴素的”、“幼稚的”、**“蠢蠢的”**。咳咳，也就是说，大神们取名说该方法是一种比较萌蠢的方法，为啥？\n\n将句子（“我”,“司”,“可”,“办理”,“正规发票”) 中的 （“我”,“司”）与（“正规发票”）调换一下顺序，就变成了一个新的句子（“正规发票”,“可”,“办理”, “我”, “司”)。新句子与旧句子的意思完全不同。**但由于乘法交换律，朴素贝叶斯方法中算出来二者的条件概率完全一样！**计算过程如下：\n\n>$P(（“我”,“司”,“可”,“办理”,“正规发票”)|S)$\n>$=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S) $\n>$=P(“正规发票”|S)P(“可”|S)P(“办理”|S)P(“我”|S)P(“司”|S）$\n>$=P(（“正规发票”,“可”,“办理”, “我”, “司”)|S)$\n\n**也就是说，在朴素贝叶斯眼里，“我司可办理正规发票”与“正规发票可办理我司”完全相同。朴素贝叶斯失去了词语之间的顺序信息。**这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作**词袋子模型(bag of words)**。\n\n![词袋子配图](http://ww1.sinaimg.cn/large/b57cc2efly1fxcjker3m9j205k07kq3u.jpg)\n\n词袋子模型与人们的日常经验完全不同。比如，在条件独立假设的情况下， **“武松打死了老虎”与“老虎打死了武松”被它认作一个意思了**。恩，朴素贝叶斯就是这么单纯和直接，对比于其他分类器，好像是显得有那么点萌蠢。\n\n## 8. 简单高效，吊丝逆袭\n\n\n虽然说朴素贝叶斯方法萌蠢萌蠢的，但实践证明在垃圾邮件识别的应用还**令人诧异地好**。Paul Graham先生自己简单做了一个朴素贝叶斯分类器，**“1000封垃圾邮件能够被过滤掉995封，并且没有一个误判”。**（Paul Graham《黑客与画家》）\n\n那个...效果为啥好呢？\n\n“有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性**各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大**。具体的数学公式请参考[这篇 paper](http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf)。”（刘未鹏《：平凡而又神奇的贝叶斯方法》）\n\n恩，这个分类器中最简单直接看似萌蠢的小盆友『朴素贝叶斯』，实际上却是**简单、实用、且强大**的。\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/3Text_Representation/Chapter2_Advanced_Text_Representation/Advanced_Text_Representation/","content":"\n# 文本表示进阶\n\n\n## 1. 预训练在图像领域的应用\n\n* 参考文章：[《从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史》](https://zhuanlan.zhihu.com/p/49271699)\n\n自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。\n\n![图1](./img/Image_pre-training.png)\n\n\n那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。\n\n这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。\n\n那么新的问题来了，为什么这种预训练的思路是可行的？\n\n![](./img/Image_pre-training_application.png)\n\n\n\n目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。\n\n![](./img/Image_pre-training_application3.png)\n\n一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。\n\n听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”\n\n\n### 1.2 图像与NLP的粗略对应关系\n\n* 参考文章： [《 为什么相比于计算机视觉(cv)，自然语言处理(nlp)领域的发展要缓慢？- 刘知远的回答》](https://www.zhihu.com/question/295962495/answer/525588484)\n\n\n将图像和语言中的处理对象做一个不太严谨的对应。如下图所示，大体上像素类似于语言中的字母；图像中的对象类似于语言中的单词/概念；图像中对象组成的场景类似于语言中的句子表达的语义；视频则类似于语言中的篇章（文章）。\n![](./img/Correspondence_between_image&NLP.png)\n\n在这种类比下看，NLP/IR在单词层面的处理要比CV中的图像识别简单得多，只需要做一下tokenization、lemmatization、stemming等（中文复杂一些需要额外做自动分词），就可以利用关键词匹配完成很多任务，例如信息检索、文本分类、拼写纠错、情感分析、关键词提取等等，实际上已经得到非常广泛的应用，如搜索引擎、拼音输入法、新闻分类、阅读推荐等。\n\n而由于图像中对象的复杂性和多样性，仅在对象识别层面，甚至特定的人脸识别，还有很多技术挑战。只不过是近年来，由于深度学习对非结构数据的强大表示和学习能力，开始让对象识别走向了实用化。而进入到更高层面，例如面向图像的场景图构建，面向文本的句法语义分析，都需要对复杂语境（上下文）的精准而强大的建模能力。\n\n### 1.3 NLP相对图像的特点\n\n词作为NLP的基本要素，比像素的抽象程度更高，已经加入了人类数万年进化而来的抽象经验。\n光是对词的表示已经耗费了科学家非常多的精力。所以之前的NLP预训练工作主要集中于对词的表示。\n\n\n## 2. ELMO：基于上下文的word-embedding\n\n\n* 参考文章：[《从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史》](https://zhuanlan.zhihu.com/p/49271699)\n\nELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。\n\n\n![](./img/ELMO1.png)\n\n\n\nELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 W_i 的上下文去正确预测单词 W_i ， W_i 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 W_i 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。\n\n\n\n![](./img/ELMO2.png)\n\n\n上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。\n\n\n![](./img/ELMO3.png)\n\n\n\n前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。\n\n\n![](./img/ELMO4.png)\n\n\n\n\n\nELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。\n\n\n## 3. GPT: Transformer建模句子信息\n\n\n### 3.1. 从词向量到句子向量\n* 无监督句子表示：将句子表示成定长向量\n* 基线模型：word2vec\n* 现有模型：AE，LM，Skip-Thoughts\n  * 本身的信息\n  * 上下文的信息\n  * 任务的信息\n\n#### 3.1.1. Skip-Thoughts\n* 类似skip-gram,关注句子与上下文句子的共现关系\n![](./img/skip-thoughts.png)\n\n### 3.2 Transformer/self-attention介绍\nTransformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络。其关键是自注意力机制（Self Attention）。所以我们主要介绍self-attention。\n\n当我们想对句子“The animal didn't cross the street because it was too tired”中“it”这个词编码时，注意力机制的基本思想是认为这个句话中每个词对it的语义均会有贡献。那怎么综合这些贡献呢，就是直接将每个词的embedding向量**加权求和**。\n所以关键的问题是如何得到每个词各自的权重，关系更近的词的权重更大。比如这句话中\"The Animal\"的权重就应该更大，它们的信息应该更多地编码到“it”中。\n自注意力机制得到权重的方法非常简单，就是两个词向量的内积。最终通过一个softmax将各个权重归一化。\n![](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n在上图中，颜色的粗细代表该词的权重大小，权重由该词与“it”的内积得到，最终通过一个softmax将各个权重归一化。\n\n自注意力机制其实是最原始意义的卷积的思想的推广，因为卷积本身就是一种“加权求和”。\n\n* 参考文章：[《The Illustrated Transformer》](https://jalammar.github.io/illustrated-transformer/)\n* 参考文章：[《基于Transformer的神经机器翻译》](https://colab.research.google.com/drive/1Wt9Jwynnki6lipwUcy0Sz5WKG7MYSGs0#scrollTo=3twSbimFUgQq)\n\n### 3.3 GPT\n\n* 参考文章：[《从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史》](https://zhuanlan.zhihu.com/p/49271699)\n\n\n![](./img/gpt1.png)\n\nGPT是“Generative Pre-Training”的简称，从名字看其含义是指的**生成式的预训练**。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 W_i 单词的上下文去正确预测单词 W_i ， W_i 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 W_i 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。\n\n\n上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。\n\n![](./img/gpt2.png)\n\n\n上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程对，这跟那个模式是一模一样的。\n\n这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？\n\n\n![](./img/gpt3.png)\n\n\n\nGPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。\n\n\n![](./img/gpt4.png)\n\n\n\nGPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。\n\n## 4. BERT：预训练双向Transformer\n\n* 参考文章 [《NLP的游戏规则从此改写？从word2vec, ELMo到BERT》](https://zhuanlan.zhihu.com/p/47488095)\n\nBERT的全称是Bidirectional Encoder Representation from Transformers。\n\n### 深层双向的encoding\n\n首先，它指出，对上下文相关的词向量的学习上，先前的预训练模型还不够！虽然在下游有监督任务中，encoding的方式已经是花里胡哨非常充分了，深度双向encoding基本成了许多复杂下游任务的标配（比如MRC, dialogue）。但是在预训练模型上，先前的最先进模型也只是基于传统的语言模型来做，而传统的语言模型是单向的（数学上已经定义了），即\n\n$$p(s)=p(w0)\\cdot p(w1|w0)\\cdot p(w2|w1,w0)\\cdot p(w3|w2, w1,w0) …p(wn|context)$$\n\n而且往往都很浅（想象一下LSTM堆三层就train不动了，就要上各种trick了），比如ELMo。\n\n另外，虽然ELMo有用双向RNN来做encoding，但是这两个方向的RNN其实是分开训练的，只是在最后在loss层做了个简单相加。这样就导致对于每个方向上的单词来说，在被encoding的时候始终是看不到它另一侧的单词的。而显然句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的。\n\n### Masked LM\n\n顾名思义，Masked LM就是说，我们不是像传统LM那样给定已经出现过的词，去预测下一个词，而是直接把整个句子的一部分词（随机选择）盖住（make it masked），这样模型不就可以放心的去做双向encoding了嘛，然后就可以放心的让模型去预测这些盖住的词是啥。这个任务其实最开始叫做cloze test（大概翻译成“完形填空测试”）。\n\n这样显然会导致一些小问题。这样虽然可以放心的双向encoding了，但是这样在encoding时把这些盖住的标记也给encoding进去了╮(￣▽￣””)╭而这些mask标记在下游任务中是不存在的呀。。。那怎么办呢？对此，为了尽可能的把模型调教的忽略这些标记的影响，作者通过如下方式来告诉模型“这些是噪声是噪声！靠不住的！忽略它们吧！”，对于一个被盖住的单词：\n\n* 有80%的概率用“[mask]”标记来替换\n* 有10%的概率用随机采样的一个单词来替换\n* 有10%的概率不做替换（虽然不做替换，但是还是要预测哈）\n\n### Encoder\n\n在encoder的选择上，作者并没有用烂大街的bi-lstm，而是使用了可以做的更深、具有更好并行性的Transformer encoder来做。这样每个词位的词都可以无视方向和距离的直接把句子中的每个词都有机会encoding进来。另一方面我主观的感觉Transformer相比lstm更容易免受mask标记的影响，毕竟self-attention的过程完全可以把mask标记针对性的削弱匹配权重，但是lstm中的输入门是如何看待mask标记的那就不得而知了。\n\n等下，小夕在之前的文章中也说过了，直接用Transformer encoder显然不就丢失位置信息了嘛？难道作者这里也像Transformer原论文中那样搞了个让人怕怕的sin、cos函数编码位置？并木有，作者这里很简单粗暴的直接去训练了一个position embedding ╮(￣▽￣””)╭ 这里就是说，比如我把句子截断到50的长度，那么我们就有50个位置嘛，所以就有50个表征位置的单词，即从位置0一直到位置49。。。然后给每个位置词一个随机初始化的词向量，再随他们训练去吧（很想说这特喵的也能work？太简单粗暴了吧。。。）。另外，position embedding和word embedding的结合方式上，BERT里选择了直接相加。\n\n最后，在深度方面，最终BERT完全版的encoder丧心病狂的叠加了24层的multi-head attention block（要知道对话里的SOTA模型DAM也才用了5层…）。。。而且每个block包含16抽头、1024隐单元╮(￣▽￣””)╭此处打出标语：money is all you need （划掉）\n\n### 学习句子与句对关系表示\n\n像之前说的，在很多任务中，仅仅靠encoding是不足以完成任务的（这个只是学到了一堆token级的特征），还需要捕捉一些句子级的模式，来完成SLI、QA、dialogue等需要句子表示、句间交互与匹配的任务。对此，BERT又引入了另一个极其重要却又极其轻量级的任务，来试图把这种模式也学习到。\n\n### 句子级负采样\n\n还记得小夕在前面word2vec章节说过的，word2vec的一个精髓是引入了一个优雅的负采样任务来学习词向量（word-level representation）嘛。那么如果我们把这个负采样的过程给generalize到sentence-level呢？这便是BERT学习sentence-level representation的关键啦。\n\nBERT这里跟word2vec做法类似，不过构造的是一个句子级的分类任务。即首先给定的一个句子（相当于word2vec中给定context），它下一个句子即为正例（相当于word2vec中的正确词），随机采样一个句子作为负例（相当于word2vec中随机采样的词），然后在该sentence-level上来做二分类（即判断句子是当前句子的下一句还是噪声）。通过这个简单的句子级负采样任务，BERT就可以像word2vec学习词表示那样轻松学到句子表示啦。\n\n### 句子级表示\n\n等等，前面说了这么半天，还没有说句子该怎么表示呢。。。\n\nBERT这里并没有像下游监督任务中的普遍做法一样，在encoding的基础上再搞个全局池化之类的，它首先在每个sequence（对于句子对任务来说是两个拼起来的句子，对于其他任务来说是一个句子）前面加了一个特殊的token，记为[CLS]，如图\n\n![](./img/bert1.png)\n\nps：这里的[sep]是句子之间的分隔符，BERT同时支持学习句对的表示，这里是[SEP]便是为了区分句对的切割点。\n\n然后让encoder对[CLS]进行深度encoding，深度encoding的最高隐层即为整个句子/句对的表示啦。这个做法乍一看有点费解，不过别忘了，Transformer是可以无视空间和距离的把全局信息encoding进每个位置的，而[CLS]作为句子/句对的表示是直接跟分类器的输出层连接的，因此其作为梯度反传路径上的“关卡”，当然会想办法学习到分类相关的上层特征啦。\n\n另外，为了让模型能够区分里面的每个词是属于“左句子”还是“右句子”，作者这里引入了“segment embedding”的概念来区分句子。对于句对来说，就用embedding A和embedding B来分别代表左句子和右句子；而对于句子来说，就只有embedding A啦。这个embedding A和B也是随模型训练出来的。\n\nps: 这做法跟position embedding一样感觉简单粗暴，实在很费解为什么BERT用在“quora question pairs”这种理论上需要网络保持对称的任务上依然能work，心情复杂\n\n\n\n所以最终BERT每个token的表示由token原始的词向量token embedding、前文提到的position embedding和这里的segment embedding三部分相加而成，如图。\n\n\n\n### 简洁到过分的下游任务接口\n\n真正体现出BERT这个模型是龙骨级模型而不再是词向量的，就是其到各个下游任务的接口设计了，或者换个更洋气的词叫迁移策略。\n\n![](./img/bert2.png)\n\n\n首先，既然句子和句子对的上层表示都得到了，那么当然对于文本分类任务和文本匹配任务（文本匹配其实也是一种文本分类任务，只不过输入是文本对）来说，只需要用得到的表示（即encoder在[CLS]词位的顶层输出）加上一层MLP就好了呀～\n\n\n\n既然文本都被深度双向encoding了，那么做序列标注任务就只需要加softmax输出层就好了呀，连CRF都不用了呀～\n\n\n\n让小夕更木有想到的是，在span抽取式任务如SQuAD上，把深度encoding和深度attention这俩大礼包省掉就算了，甚至都敢直接把输出层的pointer net给丢掉了？直接像DrQA那样傲娇的用两个线性分类器分别输出span的起点和终点？不多说了，已跪。\n\n![](./img/bert3.png)\n\n\n### 最后来看一下实验效果\n\n![](./img/bert4.png)\n\n\n\n\n\n\n嗯，这很Google。\n\n此论文一出，小夕非常开心，因为很多之前的想法都不用去做实验验证了，因为已经被BERT摁死了(｡ ́︿ ̀｡)分类、标注和迁移任务都可以从头开始了，SQuAD的造楼计划也可以停了，感谢BERT没有跑生成任务，这给人带来了一点想象空间。嗯，手动微笑流泪。\n\n最后，喜欢小夕的小哥哥小姐姐们欢迎通过下方打赏按钮或者点击下方小广告鼓励小夕哦，爱你们🌹🌹～\n\n参考文献\n\n[1] 2018 | BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding<br>\n[2] 2018NAACL | Deep contextualized word representations<br>\n[3] 2018 ACL | Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network<br>\n[4] 2018ICLR | Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution<br>\n[5] 2017TACL | Enriching Word Vectors with Subword Information<br>\n[6] 2017ACL | Deep Pyramid Convolutional Neural Networks for Text Categorization<br>\n[7] 2017 | Convolutional Sequence to Sequence Learning<br>\n[8] 2017 | Do Convolutional Networks need to be Deep for Text Classification ?<br>\n[9] 2016 | Convolutional Neural Networks for Text Categorization/ Shallow Word-level vs. Deep Character-level<br>\n[10] 2013NIPS | Distributed-representations-of-words-and-phrases-and-their-compositionality<br>\n\n## 5. 基于BERT进行fine-tuning\n\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n\n```python\n\n```\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/7text_generation_v2/couplet_with_seq2seq/couplet_with_seq2seq/","content":"\n# seq2seq构建写对联AI\n\n\n本案例代码参考[基于google seq2seq的对联生成](https://github.com/wb14123/seq2seq-couplet)\n\n### 问题背景介绍\n对联又称对子，对仗工整，平仄协调，是一字一音的汉文语言独特的艺术形式，是中国传统文化瑰宝。对联的上下联有着非常工整的对应关系，我们可以尝试使用神经网络学习对应关系，进而完成对对联任务，而之前提到的seq2seq模型，是非常典型的序列映射学习模型，可以在本场景下使用。\n\n![](../img/couplet.jpeg)\n\n### seq2seq对对联\n\n\n这里构建的对对联AI应用也是seq2seq模型，使用的就是我们在上一门中讲解到的模型。\n\n![](../img/[1]_seq2seq_1.gif)\n\n![](../img/[8]_seq2seq_8.gif)\n\n![](../img/attention_tensor_dance.gif)\n\n## 数据读取\n\n\n```python\nfrom queue import Queue\nfrom threading import Thread\nimport random\n\n'''\n序列截断与补齐，保持一样的长度\n'''\ndef padding_seq(seq):\n    results = []\n    max_len = 0\n    for s in seq:\n        if max_len < len(s):\n            max_len = len(s)\n    for i in range(0, len(seq)):\n        l = max_len - len(seq[i])\n        results.append(seq[i] + [0 for j in range(l)])\n    return results\n\n'''\n把文本序列映射为下标id序列\n'''\ndef encode_text(words, vocab_indices):\n    return [vocab_indices[word] for word in words if word in vocab_indices]\n\n'''\n把输出的下标id序列映射回文本序列\n'''\ndef decode_text(labels, vocabs, end_token = '</s>'):\n    results = []\n    for idx in labels:\n        word = vocabs[idx]\n        if word == end_token:\n            return ' '.join(results)\n        results.append(word)\n    return ' '.join(results)\n\n'''\n加载词表\n'''\ndef read_vocab(vocab_file):\n     f = open(vocab_file, 'rb')\n     vocabs = [line.decode('utf8')[:-1] for line in f]\n     f.close()\n     return vocabs\n\n'''\n数据读取器\n'''\nclass SeqReader():\n    def __init__(self, input_file, target_file, vocab_file, batch_size,\n            queue_size = 2048, worker_size = 2, end_token = '</s>',\n            padding = True, max_len = 50):\n        self.input_file = input_file\n        self.target_file = target_file\n        self.end_token = end_token\n        self.batch_size = batch_size\n        self.padding = padding\n        self.max_len = max_len\n        # 读取词汇表\n        self.vocabs = read_vocab(vocab_file)\n        # 构建词汇与下标对应的字典\n        self.vocab_indices = dict((c, i) for i, c in enumerate(self.vocabs))\n        self.data_queue = Queue(queue_size)\n        self.worker_size = worker_size\n        # 计算全量数据有多少个batch\n        with open(self.input_file, 'rb') as f:\n            for i, l in enumerate(f):\n                pass\n            f.close()\n            self.single_lines = i+1\n        self.data_size = int(self.single_lines / batch_size)\n        self.data_pos = 0\n        self._init_reader()\n\n\n    def start(self):\n        return\n    '''\n        for i in range(self.worker_size):\n            t = Thread(target=self._init_reader())\n            t.daemon = True\n            t.start()\n    '''\n\n    # 读取一个batch的数据\n    def read_single_data(self):\n        if self.data_pos >= len(self.data):\n            random.shuffle(self.data)\n            self.data_pos = 0\n        result = self.data[self.data_pos]\n        self.data_pos += 1\n        return result\n\n    # 读取数据到batch字典中\n    def read(self):\n        while True:\n            batch = {'in_seq': [],\n                    'in_seq_len': [],\n                    'target_seq': [],\n                    'target_seq_len': []}\n            for i in range(0, self.batch_size):\n                item = self.read_single_data()\n                batch['in_seq'].append(item['in_seq'])\n                batch['in_seq_len'].append(item['in_seq_len'])\n                batch['target_seq'].append(item['target_seq'])\n                batch['target_seq_len'].append(item['target_seq_len'])\n            if self.padding:\n                batch['in_seq'] = padding_seq(batch['in_seq'])\n                batch['target_seq'] = padding_seq(batch['target_seq'])\n            yield batch\n\n\n    # 读取文件，准备成序列对\n    def _init_reader(self):\n        self.data = []\n        input_f = open(self.input_file, 'rb')\n        target_f = open(self.target_file, 'rb')\n        for input_line in input_f:\n            input_line = input_line.decode('utf-8')[:-1]\n            target_line = target_f.readline().decode('utf-8')[:-1]\n            input_words = [x for x in input_line.split(' ') if x != '']\n            if len(input_words) >= self.max_len:\n                input_words = input_words[:self.max_len-1]\n            input_words.append(self.end_token)\n            target_words = [x for x in target_line.split(' ') if x != '']\n            if len(target_words) >= self.max_len:\n                target_words = target_words[:self.max_len-1]\n            target_words = ['<s>',] + target_words\n            target_words.append(self.end_token)\n            in_seq = encode_text(input_words, self.vocab_indices)\n            target_seq = encode_text(target_words, self.vocab_indices)\n            self.data.append({\n                'in_seq': in_seq,\n                'in_seq_len': len(in_seq),\n                'target_seq': target_seq,\n                'target_seq_len': len(target_seq) - 1\n            })\n        input_f.close()\n        target_f.close()\n        self.data_pos = len(self.data)\n```\n\n## 评估函数\n\n\n```python\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Python implementation of BLEU and smooth-BLEU.\n\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n\"\"\"\n\nimport collections\nimport math\n\n\ndef _get_ngrams(segment, max_order):\n  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"\n  ngram_counts = collections.Counter()\n  for order in range(1, max_order + 1):\n    for i in range(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i+order])\n      ngram_counts[ngram] += 1\n  return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n  \"\"\"Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  \"\"\"\n  matches_by_order = [0] * max_order\n  possible_matches_by_order = [0] * max_order\n  reference_length = 0\n  translation_length = 0\n  for (references, translation) in zip(reference_corpus,\n                                       translation_corpus):\n    reference_length += min(len(r) for r in references)\n    translation_length += len(translation)\n\n    merged_ref_ngram_counts = collections.Counter()\n    for reference in references:\n      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n    translation_ngram_counts = _get_ngrams(translation, max_order)\n    overlap = translation_ngram_counts & merged_ref_ngram_counts\n    for ngram in overlap:\n      matches_by_order[len(ngram)-1] += overlap[ngram]\n    for order in range(1, max_order+1):\n      possible_matches = len(translation) - order + 1\n      if possible_matches > 0:\n        possible_matches_by_order[order-1] += possible_matches\n\n  precisions = [0] * max_order\n  for i in range(0, max_order):\n    if smooth:\n      precisions[i] = ((matches_by_order[i] + 1.) /\n                       (possible_matches_by_order[i] + 1.))\n    else:\n      if possible_matches_by_order[i] > 0:\n        precisions[i] = (float(matches_by_order[i]) /\n                         possible_matches_by_order[i])\n      else:\n        precisions[i] = 0.0\n\n  if min(precisions) > 0:\n    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n    geo_mean = math.exp(p_log_sum)\n  else:\n    geo_mean = 0\n\n  ratio = float(translation_length) / reference_length\n\n  if ratio > 1.0:\n    bp = 1.\n  else:\n    bp = math.exp(1 - 1. / ratio)\n\n  bleu = geo_mean * bp\n\n  return (bleu, precisions, bp, ratio, translation_length, reference_length)\n\n```\n\n## 定义seq2seq\n\n\n```python\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nfrom tensorflow.python.layers import core as layers_core\n\n# 设定LSTM的cell类型\ndef getLayeredCell(layer_size, num_units, input_keep_prob,\n        output_keep_prob=1.0):\n    return rnn.MultiRNNCell([rnn.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell',num_units=num_units),\n        input_keep_prob, output_keep_prob) for i in range(layer_size)])\n\n# 双向RNN\ndef bi_encoder(embed_input, in_seq_len, num_units, layer_size, input_keep_prob):\n    # 对输入编码\n    bi_layer_size = int(layer_size / 2)\n    encode_cell_fw = getLayeredCell(bi_layer_size, num_units, input_keep_prob)\n    encode_cell_bw = getLayeredCell(bi_layer_size, num_units, input_keep_prob)\n    bi_encoder_output, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw = encode_cell_fw,\n            cell_bw = encode_cell_bw,\n            inputs = embed_input,\n            sequence_length = in_seq_len,\n            dtype = embed_input.dtype,\n            time_major = False)\n\n    # 拼接 编码的output和state\n    encoder_output = tf.concat(bi_encoder_output, -1)\n    encoder_state = []\n    for layer_id in range(bi_layer_size):\n        encoder_state.append(bi_encoder_state[0][layer_id])\n        encoder_state.append(bi_encoder_state[1][layer_id])\n    encoder_state = tuple(encoder_state)\n    return encoder_output, encoder_state\n\n# 加“注意力”的解码器\ndef attention_decoder_cell(encoder_output, in_seq_len, num_units, layer_size,\n        input_keep_prob):\n    # 可以选择不同的注意力机制\n    attention_mechanim = tf.contrib.seq2seq.BahdanauAttention(num_units,\n            encoder_output, in_seq_len, normalize = True)\n    # attention_mechanim = tf.contrib.seq2seq.LuongAttention(num_units,\n    #         encoder_output, in_seq_len, scale = True)\n    cell = getLayeredCell(layer_size, num_units, input_keep_prob)\n    cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanim,\n            attention_layer_size=num_units)\n    return cell\n\n# 输出端的全连接层\ndef decoder_projection(output, output_size):\n    return tf.layers.dense(output, output_size, activation=None,\n            use_bias=False, name='output_mlp')\n\n# 训练阶段解码器部分\ndef train_decoder(encoder_output, in_seq_len, target_seq, target_seq_len,\n        encoder_state, num_units, layers, embedding, output_size,\n        input_keep_prob, projection_layer):\n    # 解码结构的cell\n    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len, num_units,\n            layers, input_keep_prob)\n    # batch size\n    batch_size = tf.shape(in_seq_len)[0]\n    # 初始状态\n    init_state = decoder_cell.zero_state(batch_size, tf.float32).clone(\n            cell_state=encoder_state)\n    # 训练器\n    helper = tf.contrib.seq2seq.TrainingHelper(\n                target_seq, target_seq_len, time_major=False)\n    # 解码器\n    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,\n            init_state, output_layer=projection_layer)\n    # 解码输出\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n            maximum_iterations=100)\n    return outputs.rnn_output\n\n\n# 预测阶段的解码过程\ndef infer_decoder(encoder_output, in_seq_len, encoder_state, num_units, layers,\n        embedding, output_size, input_keep_prob, projection_layer):\n    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len, num_units,\n            layers, input_keep_prob)\n\n    batch_size = tf.shape(in_seq_len)[0]\n    init_state = decoder_cell.zero_state(batch_size, tf.float32).clone(\n            cell_state=encoder_state)\n\n    # TODO: start tokens and end tokens are hard code\n    \"\"\"\n    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n            embedding, tf.fill([batch_size], 0), 1)\n    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,\n            init_state, output_layer=projection_layer)\n    \"\"\"\n    # 使用beam search解码\n    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n        cell=decoder_cell,\n        embedding=embedding,\n        start_tokens=tf.fill([batch_size], 0),\n        end_token=1,\n        initial_state=init_state,\n        beam_width=10,\n        output_layer=projection_layer,\n        length_penalty_weight=1.0)\n\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n            maximum_iterations=100)\n    return outputs.sample_id\n\n\n# 序列到序列模型\ndef seq2seq(in_seq, in_seq_len, target_seq, target_seq_len, vocab_size,\n        num_units, layers, dropout):\n    in_shape = tf.shape(in_seq)\n    batch_size = in_shape[0]\n\n    if target_seq != None:\n        input_keep_prob = 1 - dropout\n    else:\n        input_keep_prob = 1\n\n\n    projection_layer=layers_core.Dense(vocab_size, use_bias=False)\n\n    # 对输入和输出序列做embedding\n    with tf.device('/gpu:0'):\n        embedding = tf.get_variable(\n                name = 'embedding',\n                shape = [vocab_size, num_units])\n    embed_input = tf.nn.embedding_lookup(embedding, in_seq, name='embed_input')\n\n    # 编码\n    encoder_output, encoder_state = bi_encoder(embed_input, in_seq_len,\n            num_units, layers, input_keep_prob)\n\n\n    # 解码\n    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len, num_units,\n            layers, input_keep_prob)\n    batch_size = tf.shape(in_seq_len)[0]\n    init_state = decoder_cell.zero_state(batch_size, tf.float32).clone(\n            cell_state=encoder_state)\n\n    if target_seq != None:\n        embed_target = tf.nn.embedding_lookup(embedding, target_seq,\n                name='embed_target')\n        helper = tf.contrib.seq2seq.TrainingHelper(\n                    embed_target, target_seq_len, time_major=False)\n    else:\n        # TODO: start tokens and end tokens are hard code\n        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                embedding, tf.fill([batch_size], 0), 1)\n    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,\n            init_state, output_layer=projection_layer)\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n            maximum_iterations=100)\n    if target_seq != None:\n        return outputs.rnn_output\n    else:\n        return outputs.sample_id\n\n# 损失函数\ndef seq_loss(output, target, seq_len):\n    target = target[:, 1:]\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output,\n            labels=target)\n    batch_size = tf.shape(target)[0]\n    loss_mask = tf.sequence_mask(seq_len, tf.shape(output)[1])\n    cost = cost * tf.to_float(loss_mask)\n    return tf.reduce_sum(cost) / tf.to_float(batch_size)\n```\n\n## 模型定义\n\n\n```python\nimport tensorflow as tf\nfrom os import path\nimport random\n\n\nclass Model():\n\n    def __init__(self, train_input_file, train_target_file,\n            test_input_file, test_target_file, vocab_file,\n            num_units, layers, dropout,\n            batch_size, learning_rate, output_dir,\n            save_step = 100, eval_step = 1000,\n            param_histogram=False, restore_model=False,\n            init_train=True, init_infer=False):\n        self.num_units = num_units\n        self.layers = layers\n        self.dropout = dropout\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.save_step = save_step\n        self.eval_step = eval_step\n        self.param_histogram = param_histogram\n        self.restore_model = restore_model\n        self.init_train = init_train\n        self.init_infer = init_infer\n\n        if init_train:\n            self.train_reader = SeqReader(train_input_file,\n                    train_target_file, vocab_file, batch_size)\n            self.train_reader.start()\n            self.train_data = self.train_reader.read()\n            self.eval_reader = SeqReader(test_input_file, test_target_file,\n                    vocab_file, batch_size)\n            self.eval_reader.start()\n            self.eval_data = self.eval_reader.read()\n\n        self.model_file = path.join(output_dir, 'model.ckpl')\n        self.log_writter = tf.summary.FileWriter(output_dir)\n\n        if init_train:\n            self._init_train()\n            self._init_eval()\n\n        if init_infer:\n            self.infer_vocabs =read_vocab(vocab_file)\n            self.infer_vocab_indices = dict((c, i) for i, c in\n                    enumerate(self.infer_vocabs))\n            self._init_infer()\n            self.reload_infer_model()\n\n\n    def gpu_session_config(self):\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        return config\n\n\n    def _init_train(self):\n        self.train_graph = tf.Graph()\n        with self.train_graph.as_default():\n            self.train_in_seq = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n            self.train_in_seq_len = tf.placeholder(tf.int32, shape=[self.batch_size])\n            self.train_target_seq = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n            self.train_target_seq_len = tf.placeholder(tf.int32, shape=[self.batch_size])\n            output = seq2seq(self.train_in_seq, self.train_in_seq_len,\n                    self.train_target_seq, self.train_target_seq_len,\n                    len(self.train_reader.vocabs),\n                    self.num_units, self.layers, self.dropout)\n            self.train_output = tf.argmax(tf.nn.softmax(output), 2)\n            self.loss = seq_loss(output, self.train_target_seq,\n                    self.train_target_seq_len)\n            params = tf.trainable_variables()\n            gradients = tf.gradients(self.loss, params)\n            clipped_gradients, _ = tf.clip_by_global_norm(\n                        gradients, 0.5)\n            self.train_op = tf.train.AdamOptimizer(\n                    learning_rate=self.learning_rate\n                ).apply_gradients(zip(clipped_gradients,params))\n            if self.param_histogram:\n                for v in tf.trainable_variables():\n                    tf.summary.histogram('train_' + v.name, v)\n            tf.summary.scalar('loss', self.loss)\n            self.train_summary = tf.summary.merge_all()\n            self.train_init = tf.global_variables_initializer()\n            self.train_saver = tf.train.Saver()\n        self.train_session = tf.Session(graph=self.train_graph,\n                config=self.gpu_session_config())\n\n\n    def _init_eval(self):\n        self.eval_graph = tf.Graph()\n        with self.eval_graph.as_default():\n            self.eval_in_seq = tf.placeholder(tf.int32, shape=[self.batch_size, None])\n            self.eval_in_seq_len = tf.placeholder(tf.int32, shape=[self.batch_size])\n            self.eval_output = seq2seq(self.eval_in_seq,\n                    self.eval_in_seq_len, None, None,\n                    len(self.eval_reader.vocabs),\n                    self.num_units, self.layers, self.dropout)\n            if self.param_histogram:\n                for v in tf.trainable_variables():\n                    tf.summary.histogram('eval_' + v.name, v)\n            self.eval_summary = tf.summary.merge_all()\n            self.eval_saver = tf.train.Saver()\n        self.eval_session = tf.Session(graph=self.eval_graph,\n                config=self.gpu_session_config())\n\n\n    def _init_infer(self):\n        self.infer_graph = tf.Graph()\n        with self.infer_graph.as_default():\n            self.infer_in_seq = tf.placeholder(tf.int32, shape=[1, None])\n            self.infer_in_seq_len = tf.placeholder(tf.int32, shape=[1])\n            self.infer_output = seq2seq(self.infer_in_seq,\n                    self.infer_in_seq_len, None, None,\n                    len(self.infer_vocabs),\n                    self.num_units, self.layers, self.dropout)\n            self.infer_saver = tf.train.Saver()\n        self.infer_session = tf.Session(graph=self.infer_graph,\n                config=self.gpu_session_config())\n\n\n    # 训练\n    def train(self, epochs, start=0):\n        if not self.init_train:\n            raise Exception('Train graph is not inited!')\n        with self.train_graph.as_default():\n            if path.isfile(self.model_file + '.meta') and self.restore_model:\n                print(\"Reloading model file before training.\")\n                self.train_saver.restore(self.train_session, self.model_file)\n            else:\n                self.train_session.run(self.train_init)\n            total_loss = 0\n            for step in range(start, epochs):\n                data = next(self.train_data)\n                in_seq = data['in_seq']\n                in_seq_len = data['in_seq_len']\n                target_seq = data['target_seq']\n                target_seq_len = data['target_seq_len']\n                output, loss, train, summary = self.train_session.run(\n                        [self.train_output, self.loss, self.train_op, self.train_summary],\n                        feed_dict={\n                            self.train_in_seq: in_seq,\n                            self.train_in_seq_len: in_seq_len,\n                            self.train_target_seq: target_seq,\n                            self.train_target_seq_len: target_seq_len})\n                total_loss += loss\n                self.log_writter.add_summary(summary, step)\n                if step % self.save_step == 0:\n                    self.train_saver.save(self.train_session, self.model_file)\n                    print(\"Saving model. Step: %d, loss: %f\" % (step,\n                        total_loss / self.save_step))\n                    # print sample output\n                    sid = random.randint(0, self.batch_size-1)\n                    input_text =decode_text(in_seq[sid],\n                        self.eval_reader.vocabs)\n                    output_text =decode_text(output[sid],\n                            self.train_reader.vocabs)\n                    target_text =decode_text(target_seq[sid],\n                            self.train_reader.vocabs).split(' ')[1:]\n                    target_text = ' '.join(target_text)\n                    print('******************************')\n                    print('src: ' + input_text)\n                    print('output: ' + output_text)\n                    print('target: ' + target_text)\n                if step % self.eval_step == 0:\n                    bleu_score = self.eval(step)\n                    print(\"Evaluate model. Step: %d, score: %f, loss: %f\" % (\n                        step, bleu_score, total_loss / self.save_step))\n                    eval_summary = tf.Summary(value=[tf.Summary.Value(\n                        tag='bleu', simple_value=bleu_score)])\n                    self.log_writter.add_summary(eval_summary, step)\n                if step % self.save_step == 0:\n                    total_loss = 0\n\n    # 评估\n    def eval(self, train_step):\n        with self.eval_graph.as_default():\n            self.eval_saver.restore(self.eval_session, self.model_file)\n            bleu_score = 0\n            target_results = []\n            output_results = []\n            for step in range(0, self.eval_reader.data_size):\n                data = next(self.eval_data)\n                in_seq = data['in_seq']\n                in_seq_len = data['in_seq_len']\n                target_seq = data['target_seq']\n                target_seq_len = data['target_seq_len']\n                outputs = self.eval_session.run(\n                        self.eval_output,\n                        feed_dict={\n                            self.eval_in_seq: in_seq,\n                            self.eval_in_seq_len: in_seq_len})\n                for i in range(len(outputs)):\n                    output = outputs[i]\n                    target = target_seq[i]\n                    output_text =decode_text(output,\n                            self.eval_reader.vocabs).split(' ')\n                    target_text =decode_text(target[1:],\n                            self.eval_reader.vocabs).split(' ')\n                    prob = int(self.eval_reader.data_size * self.batch_size / 10)\n                    target_results.append([target_text])\n                    output_results.append(output_text)\n                    if random.randint(1, prob) == 1:\n                        print('====================')\n                        input_text =decode_text(in_seq[i],\n                                self.eval_reader.vocabs)\n                        print('src:' + input_text)\n                        print('output: ' + ' '.join(output_text))\n                        print('target: ' + ' '.join(target_text))\n            return compute_bleu(target_results, output_results)[0] * 100\n\n\n    def reload_infer_model(self):\n        with self.infer_graph.as_default():\n            self.infer_saver.restore(self.infer_session, self.model_file)\n\n\n    def infer(self, text):\n        if not self.init_infer:\n            raise Exception('Infer graph is not inited!')\n        with self.infer_graph.as_default():\n            in_seq =encode_text(text.split(' ') + ['</s>',],\n                    self.infer_vocab_indices)\n            in_seq_len = len(in_seq)\n            outputs = self.infer_session.run(self.infer_output,\n                    feed_dict={\n                        self.infer_in_seq: [in_seq],\n                        self.infer_in_seq_len: [in_seq_len]})\n            output = outputs[0]\n            output_text =decode_text(output, self.infer_vocabs)\n            return output_text\n```\n\n## 模型训练\n\n\n```python\nm = Model(\n        './couplet/train/in.txt',\n        './couplet/train/out.txt',\n        './couplet/test/in.txt',\n        './couplet/test/out.txt',\n        './couplet/vocabs',\n        num_units=1024, layers=4, dropout=0.2,\n        batch_size=32, learning_rate=0.001,\n        output_dir='./models/output_couplet',\n        restore_model=False)\n\nm.train(5000000)\n```\n\n    WARNING:tensorflow:From /home/xiniu/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n    Instructions for updating:\n    seq_dim is deprecated, use seq_axis instead\n    WARNING:tensorflow:From /home/xiniu/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n    Instructions for updating:\n    batch_dim is deprecated, use batch_axis instead\n    Saving model. Step: 0, loss: 0.983172\n    ******************************\n    src: 拾 云 补 我 丹 青 色\n    output: 邳 邳 汵 汵 汵 汵 码 汵 汵 釆 釆 芎 枞 芎 邳 邳 邳 邳 邳 邳 幮 幮 幮 幮 幮 幮 幮 幮\n    target: 留 白 遗 谁 锦 绣 文\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:当 阳 桥 下 英 雄 胆\n    output: \n    target: 马 嵬 坡 前 美 女 魂\n    ====================\n    src:身 逢 盛 世 ， 谁 舞 龙 头 兴 伟 业\n    output: \n    target: 誉 满 神 州 ， 喜 看 科 大 展 雄 风\n    ====================\n    src:四 面 涛 声 ， 万 里 黄 河 天 上 去\n    output: \n    target: 重 霄 鹤 影 ， 一 川 碧 雪 画 中 飞\n    ====================\n    src:三 秩 繁 荣 ， 万 众 同 圆 中 国 梦\n    output: \n    target: 八 方 勤 奋 ， 九 州 齐 步 小 康 程\n    ====================\n    src:家 山 依 旧 绿 ， 夕 照 微 红 ， 不 见 炊 烟 四 起\n    output: \n    target: 鬓 发 已 然 衰 ， 乡 音 犹 在 ， 笑 邀 老 酒 同 沽\n    ====================\n    src:古 道 寒 鸦 噪\n    output: \n    target: 闲 池 老 鹤 栖\n    ====================\n    src:醉 翁 亭 里 醉 醉 翁 ， 亭 醒 翁 醉\n    output: \n    target: 上 阶 楼 内 上 上 阶 ， 楼 下 阶 上\n    ====================\n    src:头 曲 三 曲 交 响 曲\n    output: \n    target: 八 章 九 章 合 奏 章\n    ====================\n    src:春 露 含 嫣 泛 紫 气\n    output: \n    target: 野 花 吐 馨 迎 朝 阳\n    ====================\n    src:裕 民 壮 志 兴 宏 业\n    output: \n    target: 通 政 雄 才 展 大 猷\n    ====================\n    src:想 当 年 抗 战 保 疆 ， 革 命 先 驱 忘 死 舍 生 ， 赢 胜 利 曙 光 初 吐\n    output: \n    target: 看 今 日 开 来 继 往 ， 弄 潮 后 辈 扬 鞭 策 马 ， 创 辉 煌 夙 愿 终 圆\n    ====================\n    src:尚 有 闲 情 摩 古 画\n    output: \n    target: 坤 崇 厚 德 播 春 风\n    ====================\n    src:一 剪 梅 花 三 弄 影\n    output: \n    target: 万 年 春 梦 九 回 肠\n    ====================\n    src:梦 窄 何 须 天 测\n    output: \n    target: 心 宽 不 必 地 量\n    ====================\n    src:黄 岳 霞 飞 ， 十 分 春 色 和 茶 煮\n    output: \n    target: 皖 江 潮 涌 ， 一 派 涛 声 谐 韵 流\n    ====================\n    src:长 天 易 启 逍 遥 意\n    output: \n    target: 斗 室 难 囚 自 在 心\n    Evaluate model. Step: 0, score: 0.000000, loss: 0.983172\n    Saving model. Step: 100, loss: 69.179844\n    ******************************\n    src: 酒 肉 僧 人 中 岳 客\n    output: 一 一 ， ， ， ，\n    target: 丹 青 和 尚 全 州 来\n    Saving model. Step: 200, loss: 64.788209\n    ******************************\n    src: 树 倚 深 堂 秋 影 静\n    output: 月 月 月 月 月 月\n    target: 风 吹 小 月 雁 声 稀\n    Saving model. Step: 300, loss: 63.650225\n    ******************************\n    src: 龙 虎 榜 中 人 第 一\n    output: 一 山 一 月 ， 春\n    target: 烟 花 队 里 醉 千 场\n    Saving model. Step: 400, loss: 62.874522\n    ******************************\n    src: 三 勋 恩 泽 永\n    output: 一 年 一 春 春\n    target: 一 代 德 操 馨\n    Saving model. Step: 500, loss: 61.886383\n    ******************************\n    src: 弃 汝 子 并 弃 汝 妻 ， 此 际 殊 多 不 了 事\n    output: 一 人 有 ， ， ， ， ， ， 有 有 有 有 人 人\n    target: 哭 吾 婿 亦 哭 吾 女 ， 从 今 永 作 未 亡 人\n    Saving model. Step: 600, loss: 62.753094\n    ******************************\n    src: 山 吹 鸣 凤 曲\n    output: 月 月 月 花 香\n    target: 水 忆 钓 鱼 人\n    Saving model. Step: 700, loss: 64.927043\n    ******************************\n    src: 秋 风 瑟 瑟 莲 花 落\n    output: 柳 月 柳 月 柳 柳 寒\n    target: 春 雨 潇 潇 竹 叶 青\n    Saving model. Step: 800, loss: 60.404067\n    ******************************\n    src: 沿 途 都 是 灵 官 殿\n    output: 不 子 无 心 不 有 情\n    target: 大 劫 难 逃 白 虎 堂\n    Saving model. Step: 1000, loss: 60.141472\n    ******************************\n    src: 句 里 寒 梅 香 淡 淡\n    output: 花 花 柳 水 醉 花 花\n    target: 画 中 少 女 乐 陶 陶\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:信 笔 书 绝 妙\n    output: 风 心 有 上 心\n    target: 随 机 应 变 通\n    ====================\n    src:天 昊 昊 ， 地 茫 茫 ， 雄 姿 粗 犷 须 眉 气\n    output: 风 风 风 ， ， 月 ， ， ， 风 风 风 上 上 心\n    target: 水 潺 潺 ， 风 习 习 ， 俊 美 羞 赧 倩 女 情\n    ====================\n    src:联 兴 五 载 ， 历 雪 经 霜 ， 卫 滨 大 地 联 花 艳\n    output: 春 风 春 ， ， ， 风 风 ， ， ， 风 风 风 海 海 春\n    target: 韵 振 千 秋 ， 陶 情 寄 志 ， 古 郡 楹 坛 韵 律 长\n    ====================\n    src:月 出 深 峰 里\n    output: 花 花 月 水 花\n    target: 星 摇 积 浪 中\n    ====================\n    src:南 北 东 西 ， 门 庭 若 市 初 一 好\n    output: 春 风 一 海 ， ， 风 一 万 万 万 春\n    target: 赵 钱 孙 李 ， 儿 女 满 堂 十 五 圆\n    ====================\n    src:非 常 老 练 不 轻 举\n    output: 不 有 无 心 有 有 心\n    target: 特 别 从 容 勿 易 行\n    ====================\n    src:穿 云 过 雨 逍 遥 燕\n    output: 竹 月 花 花 柳 水 花\n    target: 泼 墨 挥 毫 快 意 人\n    ====================\n    src:时 过 境 迁 ， 古 道 斜 阳 悲 瘦 马\n    output: 风 风 水 月 ， ， 风 风 月 水 花 花\n    target: 珠 黄 人 老 ， 天 涯 芳 草 忆 华 年\n    ====================\n    src:多 把 芳 菲 泛 春 酒\n    output: 风 风 无 月 月 月 花\n    target: 不 知 风 月 属 何 人\n    ====================\n    src:再 见\n    output: 无 心\n    target: 频 过\n    ====================\n    src:小 康 路 上 ， 尤 须 修 身 养 性\n    output: 心 心 有 ， ， ， 心 有 有 有 心\n    target: 奔 梦 途 中 ， 更 应 立 德 正 行\n    ====================\n    src:吟 诗 最 喜 遇 佳 句\n    output: 月 月 春 风 水 水 花\n    target: 修 道 何 须 拜 圣 人\n    ====================\n    src:春 雪 无 尘 空 四 大\n    output: 花 花 一 里 万 一 花\n    target: 慧 灯 有 耀 悟 群 生\n    ====================\n    src:几 番 风 雨 ， 送 走 一 穷 二 白\n    output: 两 杯 一 月 ， ， 一 一 一 一 花\n    target: 六 十 春 秋 ， 迎 来 万 紫 千 红\n    ====================\n    src:窗 含 一 卷 春 秋 画\n    output: 竹 月 花 花 月 水 花\n    target: 岭 蕴 千 行 雪 月 诗\n    ====================\n    src:三 壶 浊 酒 愁 方 尽\n    output: 几 里 花 来 月 月 花\n    target: 一 夜 秋 风 恨 正 长\n    ====================\n    src:九 重 天 子 垂 青 问\n    output: 几 里 无 心 一 里 心\n    target: 一 榻 先 生 卧 白 云\n    Evaluate model. Step: 1000, score: 0.000000, loss: 60.141472\n    Saving model. Step: 1100, loss: 59.905542\n    ******************************\n    src: 天 鹅 展 翅 ， 文 化 飘 香 ， 三 门 胜 迹 邀 驴 友\n    output: 春 水 春 春 ， ， 千 千 ， ， ， 风 风 千 万 千 来\n    target: 佳 节 举 杯 ， 黄 河 助 兴 ， 廿 届 春 风 奋 马 蹄\n    Saving model. Step: 1200, loss: 59.397225\n    ******************************\n    src: 道 德 开 基 ， 泽 流 山 海\n    output: 和 风 国 国 ， 国 国 国 春\n    target: 文 章 佐 世 ， 炳 焕 乾 坤\n    Saving model. Step: 1300, loss: 57.859651\n    ******************************\n    src: 春 雨 兴 龙 角\n    output: 春 风 水 水 春\n    target: 秋 风 振 凤 毛\n    Saving model. Step: 1400, loss: 58.316416\n    ******************************\n    src: 豪 气 冲 天 云 雾 散\n    output: 风 风 月 月 月 新 香\n    target: 雄 风 盖 世 口 碑 多\n    Saving model. Step: 1500, loss: 58.508467\n    ******************************\n    src: 从 晋 陶 潜 入 此 村 来 ， 鸡 犬 相 闻 ， 一 桃 叶 青 了 一 竹 露\n    output: 人 风 一 ， ， ， ， ， ， 风 月 月 月 ， ， 一 一 一 一 一 千 心\n    target: 自 宋 苏 轼 吟 诗 句 道 ， 风 情 复 习 ， 几 野 桑 睡 着 几 家 蚕\n    Saving model. Step: 1600, loss: 58.547522\n    ******************************\n    src: 南 国 飞 花 ， 嫣 红 姹 紫 ， 奋 起 龚 州 花 扮 美\n    output: 春 中 岁 彩 ， 春 春 春 春 ， 春 春 春 福 福 新 春\n    target: 西 江 流 韵 ， 播 瑞 腾 辉 ， 争 先 禹 甸 韵 生 香\n    Saving model. Step: 1700, loss: 57.584847\n    ******************************\n    src: 锦 衾 重 自 暖\n    output: 人 月 有 无 人\n    target: 红 烛 剪 还 明\n    Saving model. Step: 1800, loss: 57.852394\n    ******************************\n    src: 落 黄 心 绪 乱\n    output: 老 月 月 云 香\n    target: 飞 白 笔 锋 寒\n    Saving model. Step: 1900, loss: 57.353564\n    ******************************\n    src: 大 刀 阔 斧 真 来 劲\n    output: 大 世 风 山 有 有 来\n    target: 小 米 步 枪 也 立 功\n    Saving model. Step: 2000, loss: 57.791187\n    ******************************\n    src: 一 缕 乡 思 弹 不 断\n    output: 一 分 月 月 不 无 来\n    target: 几 丛 欲 草 剪 还 生\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:燕 舞 晴 空 ， 帘 前 细 赏 诗 中 画\n    output: 风 中 绿 月 ， 落 月 风 风 月 月 情\n    target: 牛 耕 沃 土 ， 岭 表 频 添 锦 上 花\n    ====================\n    src:山 林 草 木 森 森 景\n    output: 日 下 风 风 入 白 风\n    target: 岭 岫 石 岩 磊 磊 峡\n    ====================\n    src:信 笔 书 绝 妙\n    output: 清 风 月 有 来\n    target: 行 文 点 又 无\n    ====================\n    src:崤 山 聚 首 ， 搭 优 势 平 台 ， 异 彩 三 门 春 正 好\n    output: 大 日 同 中 ， ， ， 风 风 国 ， 千 风 风 月 梦 新 来\n    target: 中 部 联 姻 ， 促 良 缘 盛 事 ， 神 州 六 骏 梦 同 圆\n    ====================\n    src:有 风 伏 热\n    output: 无 月 无 香\n    target: 无 雨 冬 晴\n    ====================\n    src:彩 凤 鸣 岗 ， 人 文 故 里 翻 新 韵\n    output: 春 风 焕 月 ， 喜 风 风 风 月 国 情\n    target: 金 鸡 报 晓 ， 礼 乐 源 头 唱 好 春\n    ====================\n    src:大 漠 燃 情 ， 大 道 飞 花 ， 新 程 共 享 千 秋 岁\n    output: 新 来 大 日 ， ， 春 春 国 ， ， 春 春 万 万 里 春\n    target: 天 山 起 舞 ， 天 池 溢 彩 ， 疆 域 同 歌 万 里 春\n    ====================\n    src:竹 影 摇 明 月\n    output: 风 风 落 白 风\n    target: 柳 枝 舞 劲 风\n    ====================\n    src:汽 笛 啸 云 端 ， 拓 破 天 荒 ， 畅 驭 金 龙 腾 屋 脊\n    output: 大 山 扬 大 气 ， ， 春 春 国 ， ， 风 风 气 展 春 春\n    target: 春 风 融 雪 域 ， 舒 通 国 脉 ， 平 铺 铁 轨 逼 珠 峰\n    ====================\n    src:无 多 风 雨 闲 敲 句\n    output: 无 有 风 风 有 有 情\n    target: 小 有 壶 觞 可 对 花\n    ====================\n    src:有 弟 为 僧 ， 眉 头 常 聚 神 仙 气\n    output: 无 心 有 月 ， 不 不 无 心 月 人 心\n    target: 欲 之 同 道 ， 心 里 难 抛 儿 女 情\n    Evaluate model. Step: 2000, score: 0.000000, loss: 57.791187\n    Saving model. Step: 2100, loss: 56.984802\n    ******************************\n    src: 江 风 送 我 帆 尤 疾\n    output: 月 月 无 心 月 有 多\n    target: 春 色 迷 人 眼 欲 花\n    Saving model. Step: 2200, loss: 55.672489\n    ******************************\n    src: 秋 日 融 融 ， 麻 将 数 桌 ， 小 赌 怡 情 真 惬 意\n    output: 风 风 一 月 ， 一 秋 春 里 ， 一 来 风 月 醉 清 风\n    target: 晚 风 爽 爽 ， 欢 歌 几 曲 ， 畅 抒 顺 气 好 开 心\n    Saving model. Step: 2300, loss: 55.791397\n    ******************************\n    src: 虎 落 平 阳 ， 鸡 飞 狗 走\n    output: 春 开 玉 月 ， 大 展 春 香\n    target: 龙 临 小 岛 ， 燕 舞 莺 歌\n    Saving model. Step: 2400, loss: 54.259680\n    ******************************\n    src: 百 座 琼 楼 临 水 畔\n    output: 一 帘 秋 水 醉 风 诗\n    target: 几 间 陋 室 傍 山 隈\n    Saving model. Step: 2500, loss: 54.634726\n    ******************************\n    src: 柳 岸 蜿 蜒 濡 绿 带\n    output: 梅 花 入 水 醉 红 红\n    target: 词 风 凄 婉 漫 红 楼\n    Saving model. Step: 2600, loss: 54.626771\n    ******************************\n    src: 非 常 之 举 ， 莫 名 其 妙\n    output: 不 不 不 心 ， 不 有 不 生\n    target: 特 别 无 言 ， 不 懂 何 因\n    Saving model. Step: 2700, loss: 54.737775\n    ******************************\n    src: 折 柳 问 君 春 几 许\n    output: 寒 花 似 月 月 三 秋\n    target: 临 梅 把 酒 月 三 更\n    Saving model. Step: 2800, loss: 54.746768\n    ******************************\n    src: 心 诚 自 有 门 前 客\n    output: 世 在 常 无 世 里 人\n    target: 酒 好 定 来 座 上 宾\n    Saving model. Step: 2900, loss: 54.268380\n    ******************************\n    src: 仰 社 稷 嘉 风 ， 登 高 以 远\n    output: 看 心 心 大 气 ， 大 业 无 来\n    target: 标 乾 坤 正 气 ， 鼎 盛 而 昌\n    Saving model. Step: 3000, loss: 53.481878\n    ******************************\n    src: 凡 颖 自 殊 擢 秀 日\n    output: 春 心 不 有 作 天 天\n    target: 道 人 独 饿 填 空 肠\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:和 谐 世 博 园 ， 广 厦 万 间 先 得 月\n    output: 大 大 春 春 气 ， 千 秋 一 里 不 成 天\n    target: 奥 秘 空 中 站 ， 神 舟 八 号 试 飞 天\n    ====================\n    src:舜 地 铺 春 ， 又 是 一 年 盛 景\n    output: 新 风 逐 梦 ， 共 来 万 里 新\n    target: 农 民 创 富 ， 还 依 四 代 雄 风\n    ====================\n    src:落 雪 梨 花 一 地 香 ， 甚 喜\n    output: 清 风 柳 月 无 一 曲 ， 春 流\n    target: 接 天 荷 叶 无 穷 碧 ， 壮 观\n    ====================\n    src:白 醙 香 浮 蝶 恋 花 ， 密 密 细 窥 涤 器\n    output: 青 风 落 落 风 风 水 ， 清 来 一 点 成 风\n    target: 青 帘 影 动 莺 穿 柳 ， 声 声 高 叫 提 壶\n    ====================\n    src:硬 齿 先 亡 ， 柔 舌 后 已 ， 千 古 几 人 明 道 理\n    output: 清 风 不 在 ， 一 千 秋 一 ， 一 秋 一 里 醉 人 人\n    target: 苍 鹰 断 翅 ， 麻 雀 回 窝 ， 不 同 各 异 度 生 涯\n    ====================\n    src:不 惧 身 残 ， 心 灯 更 比 华 灯 亮\n    output: 无 来 意 意 ， 人 意 无 来 不 意 多\n    target: 当 惊 志 壮 ， 盲 道 咸 同 锦 道 宽\n    ====================\n    src:春 风 盛 世 花 千 树\n    output: 岁 气 和 风 岁 万 年\n    target: 骏 业 豪 情 酒 一 杯\n    Evaluate model. Step: 3000, score: 0.265031, loss: 53.481878\n    Saving model. Step: 3100, loss: 54.348823\n    ******************************\n    src: 常 从 曹 宪 识 难 字\n    output: 不 是 高 心 不 自 人\n    target: 喜 与 王 充 释 异 书\n    Saving model. Step: 3200, loss: 53.547293\n    ******************************\n    src: 一 寸 光 阴 二 寸 金 ， 年 年 看 涨\n    output: 一 年 山 上 千 千 风 ， 月 月 长 来\n    target: 千 般 风 雨 万 般 事 ， 比 比 皆 非\n    Saving model. Step: 3300, loss: 52.863027\n    ******************************\n    src: 零 落 雨 中 花 ， 春 梦 惊 回 栖 凤 宅\n    output: 长 春 风 雅 水 ， 春 风 一 在 醉 云 香\n    target: 绸 缪 天 下 事 ， 壮 心 销 尽 石 鱼 斋\n    Saving model. Step: 3400, loss: 53.100193\n    ******************************\n    src: 抛 诗 泼 墨 楹 联 热\n    output: 放 月 开 心 月 月 香\n    target: 挥 毫 铺 萱 宋 词 寒\n    Saving model. Step: 3500, loss: 52.764975\n    ******************************\n    src: 云 龙 高 昂 ， 拖 一 脉 秀 水\n    output: 春 花 画 ， ， 一 一 重 新 人\n    target: 佛 光 普 照 ， 保 四 方 黎 民\n    Saving model. Step: 3600, loss: 52.881379\n    ******************************\n    src: 拖 罗 一 饼 香 天 下\n    output: 绿 子 千 中 入 古 中\n    target: 钟 记 独 家 誉 岭 南\n    Saving model. Step: 3700, loss: 52.608606\n    ******************************\n    src: 华 夏 空 中 ， 千 龙 飞 舞\n    output: 春 龙 大 下 ， 万 海 腾 飞\n    target: 复 兴 路 上 ， 万 马 奔 腾\n    Saving model. Step: 3800, loss: 52.226515\n    ******************************\n    src: 原 汁 原 味 农 家 饭\n    output: 大 世 清 人 大 世 人\n    target: 糊 里 糊 涂 老 板 鞋\n    Saving model. Step: 3900, loss: 51.557316\n    ******************************\n    src: 比 年 爱 读 漆 园 书 ， 喜 闻 天 上 大 椿 ， 植 根 曾 阅 八 千 岁\n    output: 把 世 生 情 来 大 地 ， 看 梦 春 山 春 水 ， 我 水 同 来 一 万 年\n    target: 此 地 尽 容 征 士 隐 ， 好 趁 篱 东 黄 菊 ， 漉 酒 连 倾 三 百 杯\n    Saving model. Step: 4000, loss: 51.337827\n    ******************************\n    src: 明 月 探 花 鸡 尾 酒\n    output: 春 风 落 柳 柳 飞 花\n    target: 春 风 拂 柳 片 儿 汤\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:绣 幕\n    output: 书 机\n    target: 珠 帘\n    ====================\n    src:要 使 见 闻 多 ， 一 席 满 时 ， 尽 勾 留 北 调 英 雄 ， 南 腔 儿 女\n    output: 不 来 真 旧 梦 ， 看 一 年 ， 一 年 ， 一 年 大 世 ， 大 世 大 人 人\n    target: 莫 教 风 月 老 ， 十 分 好 处 ， 但 献 出 壶 中 美 酒 ， 袖 里 鲜 花\n    ====================\n    src:车 间 作 舞 台 ， 演 出 人 生 灿 烂\n    output: 世 里 同 新 梦 ， 欣 来 世 界 同 荣\n    target: 机 器 为 歌 手 ， 讴 成 曲 调 悠 扬\n    ====================\n    src:快 意 恩 仇 终 了 断\n    output: 清 风 不 事 不 无 人\n    target: 忧 思 悲 喜 总 无 常\n    ====================\n    src:纵 使 有 钱 难 买 命\n    output: 不 能 无 事 不 为 人\n    target: 须 知 无 药 可 医 贫\n    ====================\n    src:遗 策\n    output: 重 孙\n    target: 秘 笈\n    ====================\n    src:名 山 千 古 ， 名 宦 千 古\n    output: 大 世 一 州 ， 人 人 一 州\n    target: 斯 人 在 兹 ， 斯 文 在 兹\n    Evaluate model. Step: 4000, score: 0.411435, loss: 51.337827\n    Saving model. Step: 4100, loss: 50.553307\n    ******************************\n    src: 几 度 朔 风 云 化 雨\n    output: 一 轮 秋 雨 月 含 春\n    target: 一 场 夜 雪 水 成 冰\n    Saving model. Step: 4200, loss: 51.389370\n    ******************************\n    src: 门 前 鞍 马 稀 ， 不 行 乍 办\n    output: 世 上 梅 人 子 ， 何 可 相 知\n    target: 舍 下 贵 人 少 ， 最 好 能 来\n    Saving model. Step: 4300, loss: 50.467492\n    ******************************\n    src: 旷 世 英 才 ， 民 族 英 雄 ， 戬 鬼 舞 长 缨 ， 碧 血 丹 心 真 铁 汉\n    output: 今 山 国 地 ， 民 华 ， ， ， 民 风 天 水 水 ， 青 心 大 水 耀 东 山\n    target: 滇 军 猛 士 ， 中 华 猛 将 ， 抛 头 射 落 日 ， 远 山 近 水 满 江 红\n    Saving model. Step: 4400, loss: 51.451502\n    ******************************\n    src: 檐 下 雨 帘 檐 上 雾\n    output: 月 中 春 子 月 中 花\n    target: 水 中 人 影 水 边 情\n    Saving model. Step: 4500, loss: 51.797554\n    ******************************\n    src: 身 居 梁 上 曾 为 客\n    output: 心 在 人 中 不 是 人\n    target: 名 播 天 涯 本 有 星\n    Saving model. Step: 4600, loss: 50.252081\n    ******************************\n    src: 薄 酒 三 杯 酬 贵 客\n    output: 清 花 一 缕 醉 清 天\n    target: 红 花 两 朵 戴 新 人\n    Saving model. Step: 4700, loss: 50.528886\n    ******************************\n    src: 他 石 攻 玉\n    output: 今 地 为 金\n    target: 上 台 出 联\n    Saving model. Step: 4800, loss: 50.704376\n    ******************************\n    src: 蝶 梦 翩 翩 花 共 月\n    output: 梅 香 淡 舞 月 如 春\n    target: 箫 声 阵 阵 曲 含 春\n    Saving model. Step: 4900, loss: 49.776188\n    ******************************\n    src: 直 上 楼 头 呵 月 饮\n    output: 闲 游 水 外 见 诗 飞\n    target: 偏 从 砚 底 读 梅 开\n    Saving model. Step: 5000, loss: 50.832852\n    ******************************\n    src: 泪 眼 观 ， 山 河 破 碎 ， 走 南 闯 北 救 危 难\n    output: 文 康 下 ， 上 上 无 人 ， 把 我 中 中 上 后 雄\n    target: 赤 心 在 ， 生 死 全 抛 ， 经 夏 历 冬 存 英 名\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:海 颂 悲 歌 思 勤 劳 ， 慈 父 淳 朴 四 季\n    output: 民 生 大 世 无 民 德 ， 神 州 一 代 三 家\n    target: 山 披 素 带 挽 俭 节 ， 老 农 忠 厚 一 生\n    ====================\n    src:两 语 三 言 敷 衍 话\n    output: 一 生 一 片 乐 成 心\n    target: 千 叮 万 嘱 赤 诚 谈\n    ====================\n    src:未 曾 秋 至 鬓 先 老\n    output: 不 有 人 生 梦 不 寒\n    target: 写 到 春 来 心 不 甘\n    ====================\n    src:竹 杖 芒 鞋 ， 伛 步 云 峰 ， 抚 膺 览 四 海\n    output: 春 风 玉 幄 ， 看 风 风 月 ， 引 笔 满 三 春\n    target: 菊 篱 岩 户 ， 悠 歌 雾 涧 ， 抒 志 焕 三 光\n    ====================\n    src:风 花 雪 月 无 风 骨 ， 何 必 吟 风 弄 月\n    output: 月 雨 春 风 不 醉 情 ， 只 知 有 梦 如 风\n    target: 礼 易 诗 书 有 礼 节 ， 应 当 达 礼 知 书\n    ====================\n    src:大 雪 翩 翩 铺 锦 笺 ， 碧 柳 添 诗 ， 飞 禽 绘 画\n    output: 春 风 浩 暖 红 红 ， 春 风 映 玉 水 ， 碧 水 飘 香\n    target: 白 银 冽 冽 妆 原 野 ， 红 梅 增 艳 ， 走 曽 描 花\n    ====================\n    src:声 声 敲 碧 玉\n    output: 月 色 映 青 山\n    target: 岁 岁 乐 丰 年\n    ====================\n    src:倦 倚 枫 亭 ， 紫 箫 语 断 惊 云 雁\n    output: 闲 开 柳 影 ， 柳 影 轻 香 醉 柳 花\n    target: 闲 行 柳 岸 ， 碧 水 情 长 映 月 宫\n    ====================\n    src:格 物 致 知 真 学 问\n    output: 文 风 不 是 大 人 心\n    target: 扬 明 发 彩 见 襟 怀\n    ====================\n    src:华 夏 启 新 航 ， 继 往 开 来 ， 金 蛇 狂 舞 岐 江 韵\n    output: 春 风 开 盛 世 ， 喜 今 盛 世 ， 盛 世 同 开 盛 世 图\n    target: 中 山 承 厚 德 ， 修 身 敬 业 ， 紫 燕 轻 裁 桂 岭 春\n    Evaluate model. Step: 5000, score: 0.405460, loss: 50.832852\n    Saving model. Step: 5100, loss: 49.901008\n    ******************************\n    src: 和 风 牵 柳 臂 ， 澍 雨 吻 桃 腮 ， 偕 看 千 里 湖 山 谁 走 秀\n    output: 明 雨 入 青 香 ， 看 花 流 翠 水 ， 喜 看 一 湖 春 水 我 飘 天\n    target: 紫 燕 逐 花 潮 ， 黄 莺 嬉 麦 浪 ， 共 赏 三 春 郊 野 绿 登 台\n    Saving model. Step: 5200, loss: 50.370654\n    ******************************\n    src: 田 边 排 立 ， 白 鹭 似 谙 耕 事 苦\n    output: 月 上 不 ， ， 青 心 不 有 梦 州 高\n    target: 墙 脚 急 鸣 ， 促 织 尤 懂 九 天 寒\n    Saving model. Step: 5300, loss: 50.063563\n    ******************************\n    src: 沽 水 桅 樯 ， 揽 京 韵 杭 风 ， 撑 起 千 年 画 卷\n    output: 和 山 春 子 ， 听 人 生 大 展 ， 迎 一 万 代 文\n    target: 河 东 儿 女 ， 秉 人 文 科 技 ， 创 新 两 个 课 题\n    Saving model. Step: 5400, loss: 50.017901\n    ******************************\n    src: 登 楼 弹 月 ， 兰 雪 堂 除 ， 西 望 长 安 千 古 地\n    output: 入 雨 飞 诗 ， 风 花 入 上 ， 天 风 共 在 万 江 春\n    target: 把 酒 吟 风 ， 杏 花 村 渡 ， 中 流 永 济 一 方 天\n    Saving model. Step: 5500, loss: 49.584595\n    ******************************\n    src: 时 逆 潮 流 ， 我 行 僻 径\n    output: 我 开 月 去 ， 我 上 清 台\n    target: 笑 逐 野 水 ， 双 宿 兰 舟\n    Saving model. Step: 5600, loss: 49.290235\n    ******************************\n    src: 真 人 大 意 ， 有 一 冰 壶 失 手\n    output: 无 政 无 花 ， 无 无 地 子 无 心\n    target: 智 者 细 心 ， 派 双 龙 子 托 之\n    Saving model. Step: 5700, loss: 49.065030\n    ******************************\n    src: 夕 照 松 间 林 黛 玉\n    output: 春 流 水 上 柳 青 红\n    target: 春 风 陌 上 李 香 君\n    Saving model. Step: 5800, loss: 48.838600\n    ******************************\n    src: 听 海 笑 人 欢 ， 喜 唱 九 州 永 泰\n    output: 迎 春 开 国 舞 ， 迎 来 万 海 新 新\n    target: 看 盐 丰 鱼 跃 ， 高 歌 四 域 长 春\n    Saving model. Step: 5900, loss: 48.563482\n    ******************************\n    src: 须 效 灵 鲲 搏 瀚 海\n    output: 不 将 明 马 仰 青 天\n    target: 欲 栽 大 木 柱 长 天\n    Saving model. Step: 6000, loss: 48.982730\n    ******************************\n    src: 嫩 蕊\n    output: 青 花\n    target: 枯 杈\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:果 大 江 气 韵 ， 千 载 人 文 ， 辉 煌 不 负 新 时 代\n    output: 看 大 地 新 风 ， 一 生 春 色 ， 春 色 同 描 大 地 人\n    target: 究 多 景 风 情 ， 一 城 绮 梦 ， 锦 绣 重 开 北 固 楼\n    ====================\n    src:当 年 岁 月 燃 烧 ， 粮 山 棉 海 迎 红 日\n    output: 有 我 春 风 满 夜 ， 春 色 风 流 入 碧 阳\n    target: 此 际 城 乡 焕 彩 ， 锦 路 华 堂 庆 小 康\n    ====================\n    src:惇 德 秉 义 ， 英 雄 进 步 三 国 志\n    output: 大 国 创 民 ， 国 粹 、 一 代 民 民\n    target: 智 水 仁 山 ， 胜 景 抒 怀 千 亩 坪\n    ====================\n    src:娇 娃 血 性 浑 闲 事\n    output: 白 水 风 光 不 是 心\n    target: 壮 士 心 红 留 志 丹\n    ====================\n    src:征 人 折 箭 恨\n    output: 对 月 伴 花 香\n    target: 闺 妇 卷 帘 愁\n    Evaluate model. Step: 6000, score: 0.672440, loss: 48.982730\n    Saving model. Step: 6100, loss: 48.831857\n    ******************************\n    src: 临 风 诗 送 韵\n    output: 对 月 月 吟 情\n    target: 邀 月 酒 生 香\n    Saving model. Step: 6200, loss: 48.589330\n    ******************************\n    src: 镇 河 山 而 稳 固 ， 藉 万 仞 九 嶷 ， 堪 为 砥 柱\n    output: 兴 史 子 、 以 歌 ， 看 九 千 万 脉 ， 共 是 英 图\n    target: 播 孝 德 以 馨 香 ， 溯 三 湘 一 带 ， 自 有 渊 源\n    Saving model. Step: 6300, loss: 47.900101\n    ******************************\n    src: 背 倚 省 垣 ， 铸 辉 煌 不 惟 地 利\n    output: 心 怀 国 世 ， 扬 德 想 不 以 天 生\n    target: 时 当 盛 世 ， 求 发 展 全 仗 人 和\n    Saving model. Step: 6400, loss: 48.684890\n    ******************************\n    src: 丝 弦 缱 绻 轻 盈 曲\n    output: 柳 影 轻 飞 不 落 心\n    target: 笔 墨 疏 狂 桀 骜 心\n    Saving model. Step: 6500, loss: 48.858344\n    ******************************\n    src: 坐 下 来 养 养 精 神 ， 听 亭 中 南 腔 北 调\n    output: 看 西 上 来 开 大 下 ， 把 古 地 古 云 来 香\n    target: 攀 上 去 开 开 眼 界 ， 望 天 外 风 卷 云 舒\n    Saving model. Step: 6600, loss: 48.173863\n    ******************************\n    src: 俗 世 无 边 多 际 遇\n    output: 春 生 有 意 有 真 心\n    target: 人 生 有 限 欠 机 缘\n    Saving model. Step: 6700, loss: 48.330225\n    ******************************\n    src: 新 朋 旧 友 喜 临 门 ， 齐 夸 烹 饪 好\n    output: 大 地 新 山 新 好 地 ， 不 看 画 春 香\n    target: 海 味 山 珍 皆 入 馅 ， 远 胜 菜 根 香\n    Saving model. Step: 6800, loss: 48.219211\n    ******************************\n    src: 生 财 猪 拱 户\n    output: 报 富 业 迎 春\n    target: 致 富 燕 迎 春\n    Saving model. Step: 6900, loss: 48.083381\n    ******************************\n    src: 皆 道 小 风 周 末 好\n    output: 不 知 新 道 水 中 高\n    target: 可 知 方 竹 月 头 空\n    Saving model. Step: 7000, loss: 47.817075\n    ******************************\n    src: 一 世 烟 花 空 落 寞\n    output: 几 杯 月 泪 不 成 遥\n    target: 半 生 纨 绔 自 逍 遥\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:海 气 百 重 楼 ， 岂 独 浮 云 能 蔽 日\n    output: 山 山 三 万 里 ， 不 能 一 月 不 如 春\n    target: 文 章 千 古 事 ， 萧 条 异 代 不 同 时\n    ====================\n    src:遗 像 肃 清 高 ， 有 陨 自 天 ， 电 戟 霜 戈 沉 浩 气\n    output: 忠 贞 无 古 学 ， 无 私 不 负 ， 英 雄 俎 豆 壮 雄 风\n    target: 新 祠 严 壮 烈 ， 持 忠 入 地 ， 疾 风 劲 草 识 纯 臣\n    ====================\n    src:空 溪 猿 狙 愁 啼 暮\n    output: 一 夜 春 风 月 落 香\n    target: 古 道 雁 行 倦 戏 秋\n    ====================\n    src:言 志 抒 情 ， 两 行 文 字 千 秋 业\n    output: 文 章 作 意 ， 一 代 文 章 一 代 心\n    target: 挥 毫 泼 墨 ， 五 岳 烟 霞 万 副 联\n    ====================\n    src:慕 君 有 志 蹈 火 赴 汤 ， 总 为 中 华 崛 起 ， 何 求 其 勋 昭 日 月\n    output: 为 我 为 人 死 以 忠 烈 ， 不 能 为 国 之 民 ， 不 能 为 国 壮 乾 坤\n    target: 教 我 无 私 忧 国 虑 民 ， 但 图 神 州 兴 旺 ， 敢 将 赤 胆 鉴 乾 坤\n    ====================\n    src:高 人 往 往 行 低 调\n    output: 大 地 欣 来 爱 大 人\n    target: 俗 辈 常 常 做 佞 臣\n    ====================\n    src:尚 有 闲 情 摩 古 画\n    output: 欲 无 好 酒 醉 新 诗\n    target: 坤 崇 厚 德 播 春 风\n    ====================\n    src:稀 客 不 加 菜\n    output: 小 人 不 是 人\n    target: 熟 人 忙 递 烟\n    ====================\n    src:寂 寞 花 开 难 自 主\n    output: 清 明 月 冷 不 知 人\n    target: 相 思 泪 落 已 无 由\n    ====================\n    src:聊 斋 故 事 多 ， 老 鬼 出 山 犹 托 梦\n    output: 不 奈 春 风 去 ， 清 风 入 酒 不 知 人\n    target: 国 粹 知 音 少 ， 秋 风 听 雨 不 成 声\n    ====================\n    src:清 风 入 卷 书 生 气\n    output: 明 月 无 情 月 色 情\n    target: 暮 蔼 凝 怀 老 朽 心\n    Evaluate model. Step: 7000, score: 0.840510, loss: 47.817075\n    Saving model. Step: 7100, loss: 48.333594\n    ******************************\n    src: 人 生 来 去 本 虚 幻\n    output: 世 事 不 须 不 了 愁\n    target: 世 事 无 常 莫 较 真\n    Saving model. Step: 7200, loss: 47.943479\n    ******************************\n    src: 梨 落 香 肩 衣 上 雪\n    output: 花 开 红 酒 酒 中 花\n    target: 絮 萦 素 手 指 间 云\n    Saving model. Step: 7300, loss: 47.429941\n    ******************************\n    src: 国 正 天 心 顺\n    output: 国 高 国 富 兴\n    target: 官 清 民 自 安\n    Saving model. Step: 7400, loss: 47.439896\n    ******************************\n    src: 几 髭 吟 断 敲 奇 句\n    output: 一 曲 吟 成 作 老 心\n    target: 五 典 翻 残 乞 僻 辞\n    Saving model. Step: 7500, loss: 46.708812\n    ******************************\n    src: 心 游 书 海 香 无 染\n    output: 身 有 人 头 意 有 然\n    target: 情 系 笔 端 字 有 神\n    Saving model. Step: 7600, loss: 46.868555\n    ******************************\n    src: 勤 能 得 业 为 良 友\n    output: 自 可 为 人 是 大 人\n    target: 有 益 身 心 在 好 书\n    Saving model. Step: 7700, loss: 46.917361\n    ******************************\n    src: 空 喊 加 油 白 使 劲\n    output: 不 开 入 地 风 知 香\n    target: 偷 窥 上 瘾 不 甘 心\n    Saving model. Step: 7800, loss: 46.962515\n    ******************************\n    src: 临 风 排 酒 闲 听 海\n    output: 把 月 听 弦 自 入 天\n    target: 邀 月 推 枰 欲 赋 诗\n    Saving model. Step: 7900, loss: 47.313086\n    ******************************\n    src: 竹 影 涟 漪 迎 墨 客\n    output: 荷 香 淡 烂 醉 春 花\n    target: 菊 香 灿 烂 报 金 秋\n    Saving model. Step: 8000, loss: 46.684709\n    ******************************\n    src: 举 世 无 伦 清 醒 者 ， 青 史 铭 忠 ， 屈 子 岂 输 比 干 少\n    output: 为 秋 有 片 有 心 心 ， 青 波 有 影 ， 人 山 犹 是 是 天 长\n    target: 千 秋 一 样 断 肠 人 ， 碧 潮 卷 恨 ， 汨 罗 犹 胜 海 塘 多\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:兰 襟 楚 楚 散 芳 泽\n    output: 竹 树 荷 花 醉 故 人\n    target: 玉 颜 亭 亭 与 花 双\n    ====================\n    src:举 杯 歌 幸 福\n    output: 迎 春 庆 宏 图\n    target: 向 党 报 恩 情\n    ====================\n    src:别 酒 心 先 醉\n    output: 清 风 意 自 闲\n    target: 去 舟 泪 已 潸\n    ====================\n    src:炮 竹 响 亮 冲 九 宇\n    output: 龙 龙 腾 舞 耀 千 秋\n    target: 中 国 声 音 冠 全 球\n    ====================\n    src:江 天 一 色 轰 轰 烈\n    output: 海 海 千 年 共 自 难\n    target: 杨 柳 千 丝 寸 寸 柔\n    ====================\n    src:风 送 清 馨 盈 槛 外\n    output: 雨 摇 绿 水 醉 江 边\n    target: 心 仪 澄 碧 落 江 中\n    ====================\n    src:终 一 统 河 山 ， 天 授 甲 图 铭 甲 意\n    output: 万 千 年 国 粹 ， 民 生 国 富 展 宏 图\n    target: 念 同 根 血 脉 ， 时 逢 申 岁 沸 申 江\n    ====================\n    src:莲 叶 开 盘 ， 倏 尔 珍 珠 熠 熠\n    output: 荷 花 绕 影 ， 何 妨 玉 骨 悠 闲\n    target: 荷 花 醉 酒 ， 依 然 风 度 翩 翩\n    ====================\n    src:贬 斥 失 德 ， 雷 锋 标 尺 身 前 立\n    output: 弘 扬 立 志 ， 社 稷 宏 图 国 外 兴\n    target: 宏 扬 正 义 ， 模 范 言 行 事 后 评\n    ====================\n    src:羊 毫 作 笔 ， 书 华 夏 辉 煌 业 绩\n    output: 猴 岁 迎 春 ， 展 宏 图 壮 画 图 篇\n    target: 猴 棒 化 针 ， 绣 祖 国 美 丽 江 山\n    ====================\n    src:锥 ， 锥 ， 锥 ， 锥 出 穷 鬼 去\n    output: 啐 ， 狗 噪 - ， - 出 上 头 来\n    target: 拉 ， 拉 ， 拉 ， 拉 进 财 神 来\n    ====================\n    src:月 泻 清 辉 ， 风 送 桂 香 来 小 院\n    output: 花 开 绿 野 ， 雨 摇 绿 水 润 长 天\n    target: 湖 生 丽 景 ， 烟 笼 柳 色 舞 长 堤\n    ====================\n    src:一 日 有 夫 一 日 贵\n    output: 三 分 无 日 半 天 春\n    target: 半 年 辛 苦 半 年 闲\n    ====================\n    src:扬 眉 吐 气 ， 让 神 州 增 光 添 彩\n    output: 举 路 迎 春 ， 看 万 里 春 风 送 春\n    target: 飒 爽 英 姿 ， 为 祖 国 争 金 夺 银\n    ====================\n    src:多 彩 田 园 谁 绘 出\n    output: 一 生 春 色 我 来 来\n    target: 无 穷 景 色 网 传 来\n    Evaluate model. Step: 8000, score: 0.786046, loss: 46.684709\n    Saving model. Step: 8100, loss: 46.606567\n    ******************************\n    src: 红 叶 新 诗 霜 后 寄\n    output: 绿 花 白 竹 水 中 流\n    target: 玉 衣 金 缕 土 中 埋\n    Saving model. Step: 8200, loss: 47.552244\n    ******************************\n    src: 寿 岁 有 涯 ， 安 心 即 乐\n    output: 春 年 无 处 ， 大 气 无 春\n    target: 嘉 名 长 在 ， 遗 墨 犹 温\n    Saving model. Step: 8300, loss: 47.340977\n    ******************************\n    src: 既 悖 既 纯 ， 永 作 宪 矩\n    output: 其 诚 其 己 ， 可 为 其 头\n    target: 克 忠 克 力 ， 当 陟 台 阶\n    Saving model. Step: 8400, loss: 46.666164\n    ******************************\n    src: 餐 风 宿 露 寻 芳 渚\n    output: 入 水 清 云 入 玉 园\n    target: 破 雾 穿 云 洗 旧 尘\n    Saving model. Step: 8500, loss: 45.909014\n    ******************************\n    src: 动 物 猴 聪 颖\n    output: 迎 生 虎 自 车\n    target: 人 灵 我 笨 拙\n    Saving model. Step: 8600, loss: 45.818120\n    ******************************\n    src: 政 善 人 和 ， 华 夏 皆 追 中 国 梦\n    output: 民 生 国 阜 ， 春 风 永 上 小 康 风\n    target: 民 丰 物 富 ， 陵 城 满 是 小 康 花\n    Saving model. Step: 8700, loss: 45.595134\n    ******************************\n    src: 国 华 家 业 天 天 好\n    output: 国 丽 人 情 岁 色 新\n    target: 美 伴 柔 情 月 月 长\n    Saving model. Step: 8800, loss: 46.131187\n    ******************************\n    src: 放 舟 去 揽 丰 都 梦\n    output: 踏 马 欣 开 大 发 春\n    target: 携 笛 来 吟 巴 国 春\n    Saving model. Step: 8900, loss: 45.774760\n    ******************************\n    src: 房 新 院 美 人 常 乐\n    output: 户 暮 人 清 景 更 香\n    target: 日 丽 风 清 鸟 伴 歌\n    Saving model. Step: 9000, loss: 45.743536\n    ******************************\n    src: 老 道 缚 云 闲 扫 观\n    output: 新 山 流 月 不 吟 游\n    target: 山 翁 邀 鹤 静 纹 枰\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:硬 齿 先 亡 ， 柔 舌 后 已 ， 千 古 几 人 明 道 理\n    output: 清 风 不 染 ， 清 风 一 片 ， 一 人 一 片 有 人 情\n    target: 苍 鹰 断 翅 ， 麻 雀 回 窝 ， 不 同 各 异 度 生 涯\n    ====================\n    src:岸 随 春 水 阔\n    output: 风 到 水 光 凉\n    target: 舟 横 野 津 湾\n    ====================\n    src:林 樱 古 乐 膺 银 奖\n    output: 玉 宇 新 辉 耀 白 云\n    target: 艺 术 新 团 奏 玉 筝\n    ====================\n    src:儒 道 同 源 ， 诸 君 勉 力 国 方 治\n    output: 文 章 共 爱 ， 此 代 风 流 日 月 增\n    target: 慎 勤 为 要 ， 良 吏 敦 行 古 有 闻\n    ====================\n    src:恨 容 偏 落 泪\n    output: 无 恨 不 伤 情\n    target: 娇 态 欲 沉 春\n    ====================\n    src:负 手 千 山 寻 大 道\n    output: 开 心 一 曲 见 新 人\n    target: 泛 舟 一 叶 入 闲 云\n    Evaluate model. Step: 9000, score: 0.699609, loss: 45.743536\n    Saving model. Step: 9100, loss: 45.982395\n    ******************************\n    src: 花 亚 深 秋 悲 冷 色\n    output: 月 飞 旧 梦 醉 新 情\n    target: 兰 交 同 气 契 真 情\n    Saving model. Step: 9200, loss: 46.162248\n    ******************************\n    src: 一 曲 相 思 吟 到 老\n    output: 三 秋 寂 落 共 来 今\n    target: 千 般 落 寞 赋 成 诗\n    Saving model. Step: 9300, loss: 45.730751\n    ******************************\n    src: 吞 一 万 里 长 江 ， 吐 八 百 里 洞 庭 ， 敢 教 天 下 波 涛 ， 尽 为 我 用\n    output: 观 千 千 年 日 韵 ， 看 千 载 人 华 ， ， 千 此 三 山 一 景 ， 共 在 人 来\n    target: 复 几 千 年 古 迹 ， 展 卅 余 年 画 卷 ， 饱 览 巴 陵 胜 概 ， 还 有 何 忧\n    Saving model. Step: 9400, loss: 46.646062\n    ******************************\n    src: 小 诗 试 拟 孟 东 野\n    output: 大 笔 当 忘 小 南 人\n    target: 高 吟 不 减 谢 宣 城\n    Saving model. Step: 9500, loss: 46.041625\n    ******************************\n    src: 好 汉 惜 好 汉\n    output: 老 君 是 真 人\n    target: 见 家 识 见 家\n    Saving model. Step: 9600, loss: 45.647432\n    ******************************\n    src: 心 态 平 和 遗 憾 少\n    output: 心 情 浩 转 好 然 多\n    target: 风 姿 绰 约 自 如 多\n    Saving model. Step: 9700, loss: 45.687586\n    ******************************\n    src: 佳 月 四 时 有 ， 更 把 浮 荣 喻 生 灭\n    output: 清 心 一 路 无 ， 无 能 一 念 念 心 尘\n    target: 归 舟 一 叶 轻 ， 不 将 真 性 染 埃 尘\n    Saving model. Step: 9800, loss: 45.655432\n    ******************************\n    src: 共 赋 新 诗 发 宫 徵\n    output: 不 将 佳 韵 醉 山 天\n    target: 已 闻 清 乐 动 云 韵\n    Saving model. Step: 9900, loss: 44.503411\n    ******************************\n    src: 西 极 燕 游 王 母 乐\n    output: 南 人 风 上 子 孙 欢\n    target: 后 宫 氏 族 子 夫 微\n    Saving model. Step: 10000, loss: 45.255381\n    ******************************\n    src: 瑞 气 盈 门 人 财 旺\n    output: 春 云 接 户 春 地 新\n    target: 祥 光 满 堂 福 寿 兴\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:梅 意 若 痴 春 可 解\n    output: 柳 枝 如 绿 月 如 烟\n    target: 柔 情 似 水 月 能 知\n    ====================\n    src:旧 街 建 旧 街 、 建 古 建 今 、 今 日 旧 街 街 不 旧\n    output: 大 地 迎 春 色 、 新 春 、 新 年 、 新 年 喜 梦 更 新\n    target: 新 集 修 新 集 、 修 前 修 后 、 后 期 新 集 集 常 新\n    ====================\n    src:田 翁 杖 酒 还 如 梦\n    output: 柳 岸 飞 花 不 似 情\n    target: 朝 士 归 帆 且 趁 秋\n    ====================\n    src:赊 副 醉 联 还 酒 债\n    output: 吟 来 雅 韵 醉 诗 情\n    target: 编 些 鬼 话 送 人 情\n    ====================\n    src:青 鬓 玉 颜 长 似 旧\n    output: 红 尘 玉 笛 不 如 春\n    target: 鸳 鸯 翡 翠 两 争 新\n    ====================\n    src:万 户 王 侯 尽 尘 土\n    output: 一 生 大 道 是 人 心\n    target: 千 秋 风 月 自 精 神\n    ====================\n    src:金 果 洇 秋 寺\n    output: 金 光 照 古 今\n    target: 霜 花 净 梵 钟\n    ====================\n    src:电 脑 游 玩 无 益 处\n    output: 人 心 不 老 不 知 人\n    target: 手 机 辛 苦 有 功 德\n    ====================\n    src:贞 子 重 来 妖 魅 减\n    output: 神 州 不 必 老 文 明\n    target: 乃 哥 依 旧 赌 徒 多\n    ====================\n    src:马 上 江 山 鞭 指 点\n    output: 人 间 世 界 步 蹄 头\n    target: 壶 中 岁 月 酒 消 磨\n    ====================\n    src:三 尺 长 剑 平 天 下\n    output: 一 片 清 风 入 画 中\n    target: 两 寸 妙 笔 著 春 秋\n    Evaluate model. Step: 10000, score: 1.037887, loss: 45.255381\n    Saving model. Step: 10100, loss: 44.965087\n    ******************************\n    src: 日 落 江 天 红 一 抹\n    output: 风 牵 月 地 乐 千 边\n    target: 心 思 祖 国 梦 无 垠\n    Saving model. Step: 10200, loss: 44.754994\n    ******************************\n    src: 稽 文 考 献 ， 新 编 山 史\n    output: 崇 赫 励 笔 ， 大 地 民 香\n    target: 扬 风 摧 雅 ， 大 启 词 场\n    Saving model. Step: 10300, loss: 45.373895\n    ******************************\n    src: 九 域 牢 基 固 本 ， 涌 绿 摇 红 ， 何 人 培 沃 土\n    output: 一 秋 大 气 兴 春 ， 和 金 立 后 ， 此 我 有 春 花\n    target: 千 村 浚 道 引 流 ， 洗 穷 去 白 ， 我 党 化 源 泉\n    Saving model. Step: 10400, loss: 44.786909\n    ******************************\n    src: 龙 舟 划 破 千 江 水\n    output: 蛇 曲 闲 来 一 里 情\n    target: 一 醉 方 休 万 事 空\n    Saving model. Step: 10500, loss: 44.728842\n    ******************************\n    src: 墨 点 兰 花 诗 味 重\n    output: 诗 摇 柳 岸 墨 情 浓\n    target: 风 开 柳 絮 性 情 真\n    Saving model. Step: 10600, loss: 45.089859\n    ******************************\n    src: 陶 公 山 静 卧 ， 树 影 苍 茫 ， 紫 气 千 重 隆 福 地\n    output: 天 北 水 清 安 ， 风 光 永 漫 ， 青 天 一 彩 映 天 台\n    target: 南 渡 水 平 流 ， 日 华 烂 漫 ， 卿 云 五 色 映 坡 仑\n    Saving model. Step: 10700, loss: 44.844246\n    ******************************\n    src: 白 水 滩 滩 水 白 ， 滩 水 如 飞 雪\n    output: 青 花 水 畔 水 中 ， 花 川 似 画 云\n    target: 红 茶 山 山 茶 红 ， 山 茶 似 披 霞\n    Saving model. Step: 10800, loss: 44.887577\n    ******************************\n    src: 时 代 更 新 ， 教 化 与 文 明 共 进\n    output: 人 生 共 后 ， 共 神 共 睦 共 同 赢\n    target: 民 生 向 上 ， 精 神 和 物 质 双 赢\n    Saving model. Step: 10900, loss: 44.876297\n    ******************************\n    src: 东 江 湖 畔 ， 博 学 深 思 ， 园 丁 志 励 新 猷 美\n    output: 南 水 楼 中 ， 风 躬 济 后 ， 社 子 人 中 大 业 多\n    target: 黄 草 镇 中 ， 劳 神 尽 责 ， 伯 乐 心 诚 大 器 宏\n    Saving model. Step: 11000, loss: 44.080705\n    ******************************\n    src: 赏 菊 游 园 ， 侵 衣 玉 露 还 留 醉\n    output: 临 风 赏 月 ， 落 酒 风 风 不 作 来\n    target: 寻 幽 探 胜 ， 吻 面 清 风 不 觉 寒\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:走 马 灯 看 龙 虎 榜\n    output: 飞 龙 虎 跃 虎 龙 门\n    target: 杀 猪 佬 卖 狗 皮 膏\n    ====================\n    src:岁 月 流 金 ， 剩 却 归 舟 几 许 ？ 空 有 一 江 聆 晚 唱\n    output: 天 涯 落 日 ， 何 曾 落 叶 无 边 ？ 何 来 万 里 落 红 尘\n    target: 伊 人 隔 岸 ， 犹 如 落 梦 三 千 ， 恨 无 双 翼 逐 斜 阳\n    ====================\n    src:浮 浪 青 萍 ， 踏 碎 繁 星 编 玉 带\n    output: 飞 云 碧 水 ， 飞 飞 碧 水 映 红 霞\n    target: 妩 媚 红 荷 ， 笑 迎 素 月 理 娇 妆\n    ====================\n    src:百 年 名 校 树 嘉 声 ， 喜 前 程 似 锦 ， 正 圆 美 梦\n    output: 万 里 春 风 和 和 韵 ， 喜 大 业 宏 图 ， 永 绘 宏 图\n    target: 一 路 高 歌 追 胜 境 ， 当 大 步 流 星 ， 直 上 青 云\n    ====================\n    src:何 妨 一 醉 溪 山 卧\n    output: 不 见 千 年 水 水 流\n    target: 但 得 千 杯 泉 石 听\n    ====================\n    src:喜 爆 千 声 歌 盛 世\n    output: 春 风 万 户 庆 新 春\n    target: 金 鸡 三 遍 报 新 春\n    ====================\n    src:五 城 同 创 ， 百 福 倶 臻 ， 美 丽 荆 州 圆 梦 想\n    output: 万 里 同 歌 ， 千 秋 共 庆 ， 和 谐 社 会 展 宏 图\n    target: 万 众 共 吟 ， 八 音 齐 奏 ， 悠 扬 楚 韵 颂 文 明\n    ====================\n    src:何 处 夕 阳 堪 忆 旧\n    output: 此 时 春 色 不 思 多\n    target: 桥 边 红 药 又 飘 香\n    ====================\n    src:杜 宇 声 声 啼 旧 梦\n    output: 桃 花 朵 朵 醉 新 春\n    target: 芙 蓉 朵 朵 促 新 诗\n    ====================\n    src:风 送 清 馨 盈 槛 外\n    output: 雨 滋 绿 水 润 江 南\n    target: 心 仪 澄 碧 落 江 中\n    Evaluate model. Step: 11000, score: 1.038232, loss: 44.080705\n    Saving model. Step: 11100, loss: 44.420513\n    ******************************\n    src: 攀 登 难 遂 青 云 志\n    output: 放 酒 犹 吟 白 雪 诗\n    target: 把 酒 且 吟 白 首 诗\n    Saving model. Step: 11200, loss: 44.854592\n    ******************************\n    src: 雪 笺 柳 笔 待 风 画\n    output: 雪 影 梅 丝 邀 月 歌\n    target: 竹 管 丝 弦 和 鸟 鸣\n    Saving model. Step: 11300, loss: 44.464183\n    ******************************\n    src: 敬 惜 资 源 ， 循 世 态 谋 篇 ， 繁 荣 相 土 宜 低 碳\n    output: 和 除 国 力 ， 喜 民 生 国 发 ， 盛 起 宏 图 壮 大 球\n    target: 提 升 实 力 ， 惠 民 生 布 局 ， 拓 展 新 天 赖 转 型\n    Saving model. Step: 11400, loss: 45.404420\n    ******************************\n    src: 敲 句 须 惜 一 寸 墨\n    output: 临 诗 不 负 两 年 书\n    target: 赋 诗 莫 吝 十 年 功\n    Saving model. Step: 11500, loss: 45.142768\n    ******************************\n    src: 萦 情 芳 草 无 涯 ， 何 处 探 春 寻 旧 约\n    output: 有 意 青 山 有 梦 ， 此 乡 有 我 有 新 颜\n    target: 极 目 江 山 如 画 ， 故 应 为 我 发 新 诗\n    Saving model. Step: 11600, loss: 44.351148\n    ******************************\n    src: 行 云 不 恋 青 峰 栈\n    output: 落 鹤 常 知 白 带 人\n    target: 游 子 总 思 玉 树 家\n    Saving model. Step: 11700, loss: 43.785803\n    ******************************\n    src: 孝 义 节\n    output: 文 心 心\n    target: 思 志 诚\n    Saving model. Step: 11800, loss: 44.631718\n    ******************************\n    src: 奥 运 百 年 开 领 域\n    output: 春 风 一 片 共 春 心\n    target: 秋 风 一 夜 瘦 容 颜\n    Saving model. Step: 11900, loss: 44.390913\n    ******************************\n    src: 海 经 烛 物 辞 非 诞\n    output: 天 上 人 心 乐 有 真\n    target: 木 客 能 歌 诗 亦 神\n    Saving model. Step: 12000, loss: 43.633223\n    ******************************\n    src: 渔 岛 之 子 、 扬 帆 、 试 航 ， 乘 风 破 浪\n    output: 山 雄 、 子 、 大 腐 、 大 业 ， 立 凤 、 潮\n    target: 英 雄 儿 女 、 拼 搏 、 创 业 ， 耕 云 播 雨\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:玉 笛 凌 波 ， 一 曲 红 尘 远\n    output: 青 山 入 地 ， 千 秋 古 道 新\n    target: 春 风 得 意 ， 满 山 绿 叶 浓\n    ====================\n    src:扬 名 天 井 红 ， 缘 于 大 梦\n    output: 引 古 今 风 雅 ， 醉 作 中 华\n    target: 图 志 芦 溪 山 ， 为 富 瑶 民\n    ====================\n    src:疏 通 竹 径 将 迎 月\n    output: 淡 淡 荷 花 不 染 尘\n    target: 远 爱 春 波 正 满 湖\n    ====================\n    src:烽 烟 接 塞 外\n    output: 风 月 照 人 间\n    target: 铁 马 过 冰 河\n    ====================\n    src:太 空 漫 步 ， 神 州 九 万 里 灰 霾 ， 奔 来 眼 底\n    output: 一 世 相 逢 ， 一 片 一 轮 中 外 梦 ， 醉 在 心 头\n    target: 寰 宇 聚 焦 ， 极 地 千 平 方 臭 氧 ， 惊 醒 议 程\n    ====================\n    src:水 调 歌 头 春 润 泽\n    output: 花 开 树 上 韵 飘 香\n    target: 木 兰 花 令 梦 香 甜\n    ====================\n    src:卧 佛 几 时 梦 醒\n    output: 寻 心 一 世 心 悲\n    target: 云 门 哪 日 天 开\n    Evaluate model. Step: 12000, score: 0.878220, loss: 43.633223\n    Saving model. Step: 12100, loss: 44.088389\n    ******************************\n    src: 银 钩 墨 尚 新 ， 书 得 凤 笺 无 限 事\n    output: 玉 阁 楼 有 秀 ， 风 流 竹 色 有 多 尘\n    target: 画 楼 帘 卷 翠 ， 烟 凝 象 口 疑 吹 香\n    Saving model. Step: 12200, loss: 44.110955\n    ******************************\n    src: 遥 看 汉 水 鸭 头 绿\n    output: 不 得 桃 涯 笔 上 寒\n    target: 莫 取 天 津 桥 上 春\n    Saving model. Step: 12300, loss: 44.179104\n    ******************************\n    src: 四 季 星 云 印 证 ， 芳 踪 岩 洞 无 双 处\n    output: 一 秋 诗 韵 浮 神 ， 碧 迹 人 声 第 一 时\n    target: 千 秋 水 月 传 真 ， 胜 迹 泉 山 数 一 樵\n    Saving model. Step: 12400, loss: 44.182534\n    ******************************\n    src: 梦 里 花 开 香 透 枕\n    output: 窗 中 月 暖 月 盈 杯\n    target: 酒 中 月 满 韵 盈 杯\n    Saving model. Step: 12500, loss: 44.000129\n    ******************************\n    src: 听 天 由 命 知 天 命\n    output: 得 面 为 功 作 世 心\n    target: 扑 地 接 球 抱 地 球\n    Saving model. Step: 12600, loss: 43.873758\n    ******************************\n    src: 浴 水 鸳 鸯 皆 对 对\n    output: 临 花 月 蝶 又 飞 飞\n    target: 采 花 蝴 蝶 尽 双 双\n    Saving model. Step: 12700, loss: 43.534413\n    ******************************\n    src: 桃 红 李 白 柳 三 变\n    output: 月 艳 花 红 花 一 青\n    target: 菊 黄 草 青 杨 玉 环\n    Saving model. Step: 12800, loss: 44.534225\n    ******************************\n    src: 龙 战 当 年 ， 巨 星 遽 殒 天 无 色\n    output: 龙 鸣 此 处 ？ 大 义 犹 留 我 有 风\n    target: 鹤 归 何 处 ？ 忠 骨 长 埋 土 亦 香\n    Saving model. Step: 12900, loss: 43.645827\n    ******************************\n    src: 松 声 竹 声 钟 磬 声 ， 声 声 自 应\n    output: 水 色 水 色 水 花 月 ， 色 色 相 空\n    target: 山 色 水 色 烟 霞 色 ， 色 色 皆 空\n    Saving model. Step: 13000, loss: 44.228569\n    ******************************\n    src: 武 汉 包 头 防 日 照\n    output: 英 山 上 后 对 人 山\n    target: 秦 皇 即 墨 颂 江 邮\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:明 月 临 池 ， 传 书 鸿 雁 浮 孤 影\n    output: 春 风 入 户 ， 醉 醉 清 风 入 画 屏\n    target: 春 风 曲 水 ， 漾 柳 龙 宫 托 紫 笺\n    ====================\n    src:人 勤 春 早 绘 新 景\n    output: 政 善 民 安 庆 大 年\n    target: 民 乐 年 丰 展 宏 图\n    ====================\n    src:江 山 一 统 腾 龙 日\n    output: 山 水 千 寻 逐 梦 人\n    target: 岁 月 三 春 入 虎 年\n    ====================\n    src:天 宝 物 华 ， 千 里 莺 啼 千 里 绿\n    output: 春 秋 秋 暖 ， 一 方 春 满 一 枝 红\n    target: 年 丰 人 寿 ， 万 家 雀 跃 万 家 歌\n    ====================\n    src:岁 月 如 诗 ， 谁 人 独 秉 生 花 笔\n    output: 人 生 似 梦 ， 我 辈 同 吟 发 子 歌\n    target: 城 乡 似 锦 ， 我 辈 同 耕 创 业 篇\n    ====================\n    src:千 载 奇 逢 ， 只 在 好 书 良 友\n    output: 一 生 正 气 ， 不 能 不 老 佳 人\n    target: 一 生 清 福 ， 无 如 坐 茂 临 流\n    ====================\n    src:丰 草 虎 蹲 秋 没 石\n    output: 清 风 柳 绿 柳 垂 丝\n    target: 华 林 马 射 夜 张 灯\n    Evaluate model. Step: 13000, score: 1.054799, loss: 44.228569\n    Saving model. Step: 13100, loss: 43.743906\n    ******************************\n    src: 靠 科 学 ， 富 裕 临 门 早\n    output: 迎 人 明 ， 和 风 满 人 新\n    target: 讲 文 明 ， 春 风 及 第 先\n    Saving model. Step: 13200, loss: 43.857819\n    ******************************\n    src: 明 湖 映 日 月 ， 繁 星 做 证\n    output: 大 山 开 天 明 ， 大 年 成 真\n    target: 趵 突 涌 文 化 ， 百 姓 认 同\n    Saving model. Step: 13300, loss: 43.833594\n    ******************************\n    src: 一 心 可 换 十 分 亮\n    output: 两 手 难 开 万 里 金\n    target: 双 手 擦 来 万 步 新\n    Saving model. Step: 13400, loss: 45.017147\n    ******************************\n    src: 旋 攀 龙 脊 招 鸣 鹤\n    output: 不 看 春 中 看 醉 虹\n    target: 且 坐 阁 端 抚 彩 云\n    Saving model. Step: 13500, loss: 44.209497\n    ******************************\n    src: 谢 王 子 夸 ， 谢 董 郎 骂\n    output: 汉 女 夫 辣 ， - 女 女 汤\n    target: 吃 药 蛋 美 ， 吃 绿 豆 香\n    Saving model. Step: 13600, loss: 43.382341\n    ******************************\n    src: 一 邪 教 骗 万 民 心 ， 用 意 险 恶\n    output: 千 载 不 为 千 丈 毒 ， 为 善 知 生\n    target: 千 堆 雪 砌 百 罗 汉 ， 真 相 大 白\n    Saving model. Step: 13700, loss: 43.796750\n    ******************************\n    src: 下 界 红 尘 飞 不 到\n    output: 中 中 白 气 到 无 来\n    target: 上 清 紫 府 迥 非 凡\n    Saving model. Step: 13800, loss: 43.560072\n    ******************************\n    src: 武 胜 街 尚 武 ， 江 山 永 固\n    output: 文 明 日 长 名 ， 日 李 长 春\n    target: 文 化 路 崇 文 ， 桃 李 长 春\n    Saving model. Step: 13900, loss: 43.184391\n    ******************************\n    src: 岚 影 清 波 云 棹 月\n    output: 柳 光 明 雪 水 云 人\n    target: 星 光 白 露 雾 迷 花\n    Saving model. Step: 14000, loss: 43.422277\n    ******************************\n    src: 心 惊 胆 战\n    output: 志 动 心 磨\n    target: 舌 敝 唇 焦\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:雁 阵 书 人 字\n    output: 花 香 醉 客 心\n    target: 星 空 绘 几 何\n    ====================\n    src:兴 银 邑 ， 筑 银 山 ， 聚 财 永 固 千 秋 业\n    output: 树 金 牌 ， 传 赤 子 ， 创 业 都 安 万 里 春\n    target: 沐 春 晖 ， 荣 春 色 ， 纳 税 常 怀 寸 草 心\n    ====================\n    src:两 岸 青 山 卧 江 底\n    output: 一 轮 碧 水 绕 云 间\n    target: 一 行 白 鹭 上 蓝 天\n    ====================\n    src:疏 通 竹 径 将 迎 月\n    output: 淡 泊 梅 花 自 在 天\n    target: 远 爱 春 波 正 满 湖\n    ====================\n    src:勤 俭 持 家 远\n    output: 勤 俭 为 国 强\n    target: 诗 书 继 世 长\n    ====================\n    src:金 匾 生 辉 ， 茶 兼 四 百 人 文 味\n    output: 金 牌 献 德 ， 德 耀 千 秋 世 界 春\n    target: 骏 图 竞 彩 ， 堂 共 十 年 天 地 春\n    ====================\n    src:青 石 路 ， 木 牌 坊 ， 歙 地 明 珠 嵌 画 卷\n    output: 绿 水 山 ， 山 水 水 ， 新 风 美 景 醉 诗 情\n    target: 古 城 楼 ， 深 街 巷 ， 徽 州 灿 玉 缀 诗 篇\n    ====================\n    src:牛 刀 笔 小 试 牛 刀 ， 厥 词 大 放\n    output: 凤 凰 阁 上 游 凤 尾 ， 其 志 无 边\n    target: 马 掌 柜 专 钉 马 掌 ， 生 意 不 多\n    ====================\n    src:人 杰 地 灵 ， 系 宋 元 时 八 闽 客 家 首 府\n    output: 人 和 风 雅 ， 和 谐 万 户 、 文 化 古 今 人\n    target: 物 华 天 宝 ， 为 近 现 代 九 州 历 史 名 城\n    Evaluate model. Step: 14000, score: 1.220398, loss: 43.422277\n    Saving model. Step: 14100, loss: 43.528501\n    ******************************\n    src: 干 正 事\n    output: 小 真 心\n    target: 为 公 民\n    Saving model. Step: 14200, loss: 42.959012\n    ******************************\n    src: 月 光 清 朗 千 江 现\n    output: 风 里 长 森 万 宇 空\n    target: 万 象 森 罗 玉 镜 含\n    Saving model. Step: 14300, loss: 42.606996\n    ******************************\n    src: 车 夫 非 有 意 ， 赔 钱 足 矣\n    output: 世 口 不 无 人 ， 得 口 之 乎\n    target: 狗 主 故 施 威 ， 缺 德 悲 哉\n    Saving model. Step: 14400, loss: 43.034098\n    ******************************\n    src: 粒 我 蒸 民 ， 三 时 不 害\n    output: 为 人 报 事 ， 一 姓 难 香\n    target: 诞 降 嘉 种 ， 百 谷 用 成\n    Saving model. Step: 14500, loss: 42.901307\n    ******************************\n    src: 廿 年 创 业 篇 ， 五 光 十 色 皆 成 画\n    output: 万 姓 争 春 景 ， 一 里 千 红 总 是 春\n    target: 百 卉 争 荣 日 ， 万 紫 千 红 总 是 春\n    Saving model. Step: 14600, loss: 43.444734\n    ******************************\n    src: 若 想 降 龙 伏 虎\n    output: 不 如 入 凤 蝶 蜂\n    target: 休 来 引 蝶 招 蜂\n    Saving model. Step: 14700, loss: 43.427128\n    ******************************\n    src: 文 学 一 科 冠 夫 子\n    output: 文 香 万 里 醉 英 京\n    target: 馨 香 千 古 颂 南 人\n    Saving model. Step: 14800, loss: 42.671368\n    ******************************\n    src: 岛 似 太 阳 ， 看 大 江 浪 涌\n    output: 人 如 碧 色 ， 看 天 穹 天 飞\n    target: 湾 如 月 亮 ， 鉴 苍 昊 云 飞\n    Saving model. Step: 14900, loss: 42.561817\n    ******************************\n    src: 野 渡 舟 为 开 路 者\n    output: 山 山 月 在 是 花 人\n    target: 山 居 萤 是 点 灯 人\n    Saving model. Step: 15000, loss: 42.682297\n    ******************************\n    src: 书 中 凝 智 慧\n    output: 笔 内 悟 真 坤\n    target: 字 里 有 乾 坤\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:日 月 同 明 子 女 好\n    output: 人 生 不 老 人 生 悲\n    target: 人 言 并 信 武 文 斌\n    ====================\n    src:孝 心 本 是 爱 心 ， 悉 心 向 善 留 佳 话\n    output: 大 道 常 存 善 义 ， 立 德 为 人 做 大 家\n    target: 义 举 常 襄 盛 举 ， 美 举 明 德 传 惠 风\n    ====================\n    src:天 光 起 瑞 ， 宜 雨 宜 晴 ， 是 圆 梦 地 ， 居 来 钓 月 餐 霞 客\n    output: 春 色 盈 怀 ， 春 风 拂 柳 ， 喜 喜 春 风 ， 醉 醉 清 风 入 画 图\n    target: 海 气 连 家 ， 濯 缨 濯 志 ， 看 弄 潮 儿 ， 登 上 生 金 产 玉 舟\n    ====================\n    src:巨 柱 擎 天 ， 漳 水 流 金 ， 宝 刹 千 秋 泽 宇 内\n    output: 神 仙 化 地 ， 金 龙 起 舞 ， 神 仙 万 里 仰 禅 中\n    target: 云 龙 驽 驾 ， 雄 狮 护 法 ， 佛 光 万 道 耀 山 中\n    ====================\n    src:尽 孝 常 来 看 望\n    output: 无 情 不 必 不 知\n    target: 献 忠 何 惧 捐 躯\n    ====================\n    src:金 花 五 朵 生 银 子\n    output: 玉 树 千 枝 映 碧 珠\n    target: 寡 汉 一 条 没 老 婆\n    ====================\n    src:蜗 居 不 止 八 零 后\n    output: 猴 跃 常 存 四 海 中\n    target: 挨 冻 正 当 三 九 天\n    ====================\n    src:舀 江 水 一 瓢 焙 茗 ， 明 目 纵 观 新 上 海\n    output: 看 山 河 千 载 清 风 ， 清 风 漫 过 大 中 华\n    target: 借 春 茶 几 叶 搭 台 ， 开 篇 即 是 大 文 章\n    Evaluate model. Step: 15000, score: 1.328084, loss: 42.682297\n    Saving model. Step: 15100, loss: 42.780795\n    ******************************\n    src: 赋 来 诗 句 无 闲 语\n    output: 醉 去 梅 花 有 旧 阳\n    target: 醉 折 荷 花 想 艳 妆\n    Saving model. Step: 15200, loss: 42.774936\n    ******************************\n    src: 三 巡 酒 过 堪 回 味\n    output: 一 点 风 深 不 醉 肠\n    target: 一 曲 情 终 欲 断 魂\n    Saving model. Step: 15300, loss: 42.222690\n    ******************************\n    src: 宜 一 家 进 安 世 乐\n    output: 喜 万 年 同 大 家 春\n    target: 为 二 人 述 相 逢 行\n    Saving model. Step: 15400, loss: 42.486417\n    ******************************\n    src: 月 遮 白 雪 夜 织 女\n    output: 风 照 红 花 花 落 人\n    target: 日 照 红 楼 花 袭 人\n    Saving model. Step: 15500, loss: 42.602255\n    ******************************\n    src: 国 庆 连 家 庆 ， 贴 红 联 ， 挂 红 灯 ， 红 日 子 红 红 火 火\n    output: 春 联 共 国 ， ， 迎 酒 彩 ， 歌 春 酒 ， 黄 人 年 绿 满 蓝 舟\n    target: 婚 期 逢 假 期 ， 酿 美 酒 ， 迎 美 女 ， 美 前 程 美 美 甜 甜\n    Saving model. Step: 15600, loss: 43.171983\n    ******************************\n    src: 施 大 爱 救 人 舍 己\n    output: 开 清 生 作 世 生 心\n    target: 引 诸 方 挂 肚 牵 肠\n    Saving model. Step: 15700, loss: 42.757233\n    ******************************\n    src: 山 山 皆 出 麻 石 磨\n    output: 山 月 长 出 水 鱼 衣\n    target: 水 水 乏 冰 食 包 饱\n    Saving model. Step: 15800, loss: 42.622533\n    ******************************\n    src: 一 夜 梅 花 香 醉 也\n    output: 半 般 春 禄 喜 还 乎\n    target: 万 家 福 字 梦 翩 然\n    Saving model. Step: 15900, loss: 42.887848\n    ******************************\n    src: 一 座 城 心 ， 自 古 珠 光 盈 宝 气\n    output: 三 千 人 道 ， 无 来 天 物 展 新 香\n    target: 二 郎 庙 会 ， 从 来 人 聚 胜 天 香\n    Saving model. Step: 16000, loss: 42.786601\n    ******************************\n    src: 十 年 树 木 耕 耘 苦\n    output: 一 岁 风 经 爱 慧 高\n    target: 百 代 传 名 智 慧 深\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:静 伴 闲 云 ， 灏 气 含 元 蟠 柱 础\n    output: 闲 观 明 月 ， 清 风 拂 壁 醉 江 南\n    target: 乐 随 野 鹤 ， 灵 光 抱 一 绕 宫 墙\n    ====================\n    src:荷 塘 一 隅 ， 春 池 凫 双 鹜\n    output: 柳 荫 千 里 ， 柳 岸 燕 双 飞\n    target: 青 烟 幂 处 ， 碧 海 飞 孤 帆\n    ====================\n    src:快 马 加 鞭 ， 指 日 同 圆 中 国 梦\n    output: 春 风 送 暖 ， 同 心 共 绘 小 康 图\n    target: 神 舟 伴 月 ， 巡 天 遥 看 九 州 春\n    ====================\n    src:岁 星 仙 气 原 方 朔\n    output: 天 地 仙 人 自 古 今\n    target: 璧 月 新 词 是 义 山\n    ====================\n    src:春 风 送 暖 ， 人 寿 年 丰 常 惬 意\n    output: 瑞 雪 兆 丰 ， 民 生 福 寿 永 盈 门\n    target: 秋 叶 映 辉 ， 善 行 厚 德 自 开 心\n    ====================\n    src:揽 胜 登 楼 ， 问 贤 圣 凭 谁 俯 仰\n    output: 登 高 望 远 ， 看 天 天 共 仰 先 驱\n    target: 临 风 追 梦 ， 放 江 山 入 我 襟 怀\n    ====================\n    src:种 德 人 福 ， 惜 花 春 起 早\n    output: 和 谐 社 会 ， 喜 庆 福 盈 门\n    target: 干 国 栋 家 ， 爱 月 夜 眠 迟\n    ====================\n    src:晋 水 流 丹 ， 白 玉 兰 香 ， 岁 月 长 怀 家 国 梦\n    output: 春 风 化 雨 ， 春 风 秋 雨 ， 春 风 遍 润 世 风 情\n    target: 江 风 摇 碧 ， 菩 提 树 美 ， 山 河 不 泯 古 今 情\n    Evaluate model. Step: 16000, score: 1.276550, loss: 42.786601\n    Saving model. Step: 16100, loss: 42.271011\n    ******************************\n    src: 浪 荡 扁 舟 波 愈 碎\n    output: 风 翻 大 浪 水 犹 流\n    target: 潮 冲 顽 石 角 将 圆\n    Saving model. Step: 16200, loss: 42.202940\n    ******************************\n    src: 五 夜 漏 声 催 晓 箭\n    output: 一 州 春 色 入 春 人\n    target: 九 重 春 色 醉 仙 桃\n    Saving model. Step: 16300, loss: 43.472992\n    ******************************\n    src: 剖 腹\n    output: 咬 眉\n    target: 画 魂\n    Saving model. Step: 16400, loss: 41.504525\n    ******************************\n    src: 邻 家 姊 妹 ， 赤 足 浣 纱 心 不 在\n    output: 大 水 青 山 ， 白 身 飞 水 意 相 逢\n    target: 隔 岸 叔 侄 ， 牵 牛 荷 耜 笑 相 闻\n    Saving model. Step: 16500, loss: 42.750397\n    ******************************\n    src: 人 生 多 笑 语\n    output: 世 月 有 清 穷\n    target: 岁 月 自 无 情\n    Saving model. Step: 16600, loss: 42.013652\n    ******************************\n    src: 盛\n    output: 迟\n    target: 昌\n    Saving model. Step: 16700, loss: 42.674585\n    ******************************\n    src: 雀 舌 翻 腾 伏 浅 碧\n    output: 龙 毫 飞 祥 展 清 华\n    target: 猴 魁 呈 瑞 醉 中 秋\n    Saving model. Step: 16800, loss: 41.628075\n    ******************************\n    src: 献 桃 贺 岁 ， 舞 棒 金 猴 飞 燕 岭\n    output: 迎 福 迎 春 ， 迎 金 玉 柳 舞 春 城\n    target: 接 福 迎 春 ， 镶 霞 红 对 写 龙 章\n    Saving model. Step: 16900, loss: 41.418076\n    ******************************\n    src: 满 市 新 容 ， 满 世 新 风 ， 情 牵 唐 洞 是 当 往\n    output: 一 方 春 景 ， 一 方 春 酒 ， 客 似 人 风 似 自 来\n    target: 一 湖 好 水 ， 一 壶 好 酒 ， 心 醉 东 江 胡 不 归\n    Saving model. Step: 17000, loss: 41.686647\n    ******************************\n    src: 天 风 弄 笛 兰 亭 里\n    output: 月 雨 衔 居 画 苑 中\n    target: 春 燕 安 琴 杏 雨 中\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:落 叶 纷 纷 ， 方 醉 看 窗 前 雾 色\n    output: 轻 舟 寂 寂 ， 更 思 来 月 下 风 光\n    target: 清 风 淡 淡 ， 亦 拂 过 水 面 湖 光\n    ====================\n    src:越 岭 涉 江 ， 求 索 人 生 无 注 脚\n    output: 中 原 有 路 ， 不 忘 世 界 有 真 情\n    target: 问 天 顿 地 ， 忠 贞 岁 月 有 骚 文\n    ====================\n    src:诗 与 我 ， 联 赠 君 ， 未 解 心 愁 能 下 笔\n    output: 人 为 人 ， 心 如 画 ， 不 如 月 色 不 成 诗\n    target: 酒 传 情 ， 琴 会 友 ， 尽 明 雅 趣 可 修 身\n    ====================\n    src:初 窥 风 月 羞 遮 眼\n    output: 不 负 春 光 不 动 心\n    target: 远 别 沙 尘 笑 展 颜\n    ====================\n    src:阳 光 为 墨 天 作 纸 ， 描 中 华 美 景\n    output: 春 色 为 人 面 是 花 ， 绘 锦 绣 春 光\n    target: 岁 月 似 弦 地 如 琴 ， 奏 盛 世 强 音\n    ====================\n    src:怜 月 常 于 人 寂 处\n    output: 伤 心 不 过 梦 中 时\n    target: 思 乡 每 在 夜 深 时\n    ====================\n    src:德 政 布 寰 中 ， 八 骏 嘶 风 传 捷 报\n    output: 春 光 开 画 里 ， 九 州 焕 彩 展 宏 猷\n    target: 春 雷 鸣 广 宇 ， 五 羊 跳 跃 展 新 图\n    Evaluate model. Step: 17000, score: 1.180818, loss: 41.686647\n    Saving model. Step: 17100, loss: 42.016766\n    ******************************\n    src: 高 雅 非 高 傲\n    output: 清 愚 不 是 人\n    target: 老 成 似 老 拙\n    Saving model. Step: 17200, loss: 41.938587\n    ******************************\n    src: 安 居 平 五 路\n    output: 不 壁 一 千 风\n    target: 赤 壁 借 东 风\n    Saving model. Step: 17300, loss: 42.255912\n    ******************************\n    src: 名 岳 腾 龙 皆 佛 性\n    output: 清 云 悟 水 有 禅 机\n    target: 白 云 流 水 是 禅 心\n    Saving model. Step: 17400, loss: 41.921644\n    ******************************\n    src: 手 气\n    output: 心 肠\n    target: 腰 酸\n    Saving model. Step: 17500, loss: 42.507713\n    ******************************\n    src: 处 处 风 情 好\n    output: 年 年 月 步 高\n    target: 喧 喧 车 马 驰\n    Saving model. Step: 17600, loss: 43.035639\n    ******************************\n    src: 天 上 ， 飞 来 三 羊 开 泰\n    output: 人 中 ， 迎 起 五 谷 争 高\n    target: 地 下 ， 钻 出 五 子 登 科\n    Saving model. Step: 17700, loss: 42.337670\n    ******************************\n    src: 芳 菲 前 途 无 量\n    output: 清 滚 中 世 人 情\n    target: 蔚 起 后 继 有 人\n    Saving model. Step: 17800, loss: 42.085011\n    ******************************\n    src: 忘 却 天 涯 烟 草 路\n    output: 不 成 天 界 水 星 春\n    target: 修 来 世 上 寿 长 生\n    Saving model. Step: 17900, loss: 42.498506\n    ******************************\n    src: 水 凭 容 器 鉴 清 浊 ， 行 倚 道 德 分 善 恶\n    output: 人 为 精 精 严 重 心 ， 培 育 人 模 育 精 生\n    target: 木 以 准 绳 正 曲 直 ， 法 引 规 矩 定 方 圆\n    Saving model. Step: 18000, loss: 41.797288\n    ******************************\n    src: 挚 意 挚 情 似 故 友\n    output: 清 人 如 意 有 今 朋\n    target: 问 寒 问 暖 如 亲 人\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:梅 作 清 词 无 俗 句\n    output: 竹 为 雅 趣 有 清 风\n    target: 菊 呈 傲 骨 有 冰 心\n    ====================\n    src:其 生 也 荣 ， 其 死 也 哀 ， 雨 露 雷 霆 皆 主 德\n    output: 其 人 不 死 ， 不 言 不 死 ， 风 流 尘 世 有 哀 思\n    target: 臣 门 如 市 ， 臣 心 如 水 ， 皇 天 后 土 鉴 愚 衷\n    ====================\n    src:湖 北\n    output: 山 东\n    target: 海 南\n    ====================\n    src:彩 屏 如 画 ， 望 秀 美 崤 函 ， 花 团 锦 簇\n    output: 春 色 如 诗 ， 看 春 风 化 雨 ， 凤 舞 龙 腾\n    target: 短 信 报 春 ， 喜 和 谐 社 会 ， 物 阜 民 康\n    ====================\n    src:雾 列 牌 坊 ， 长 对 三 河 征 战 地\n    output: 风 流 世 界 ， 长 留 一 代 仰 英 才\n    target: 风 传 捷 报 ， 犹 听 八 面 凯 歌 声\n    Evaluate model. Step: 18000, score: 1.431464, loss: 41.797288\n    Saving model. Step: 18100, loss: 41.925583\n    ******************************\n    src: 忽 惊 水 上 光 华 满\n    output: 不 有 人 间 气 德 宽\n    target: 始 觉 人 间 道 路 长\n    Saving model. Step: 18200, loss: 41.556684\n    ******************************\n    src: 山 山 海 海 ， 山 海 关 ， 雄 关 镇 山 海\n    output: 古 月 月 光 ， 月 月 月 ， 明 风 照 水 星\n    target: 日 日 月 月 ， 日 月 潭 ， 清 潭 映 日 月\n    Saving model. Step: 18300, loss: 41.673686\n    ******************************\n    src: 万 丈 雄 心 图 破 壁\n    output: 一 轮 妙 力 在 成 威\n    target: 一 番 斗 志 好 扬 帆\n    Saving model. Step: 18400, loss: 42.208757\n    ******************************\n    src: 若 蝶 金 叶 翩 翩 舞\n    output: 如 玉 银 笛 渐 款 来\n    target: 如 镜 玉 轮 款 款 升\n    Saving model. Step: 18500, loss: 41.937235\n    ******************************\n    src: 四 面 岚 烟 竹 榭 隐\n    output: 一 池 秋 月 水 歌 飞\n    target: 一 襟 风 月 鸟 山 空\n    Saving model. Step: 18600, loss: 41.577994\n    ******************************\n    src: 流 水 高 山 鸣 古 乐\n    output: 清 风 明 雨 送 新 猷\n    target: 栉 风 沐 雨 立 新 功\n    Saving model. Step: 18700, loss: 42.061477\n    ******************************\n    src: 青 衫 破 尽 乡 愁 老\n    output: 白 路 归 来 天 事 空\n    target: 前 路 望 来 故 梦 遥\n    Saving model. Step: 18800, loss: 41.802187\n    ******************************\n    src: 花 雨 轻 霏 ， 结 青 莲 世 界\n    output: 风 烟 淡 郁 ， 看 绿 发 精 思\n    target: 云 峰 郁 起 ， 现 白 毫 相 光\n    Saving model. Step: 18900, loss: 41.077972\n    ******************************\n    src: 亭 以 桥 名 ， 永 年 致 颂\n    output: 水 如 国 后 ， 大 后 风 闻\n    target: 功 成 夏 季 ， 过 客 咸 宜\n    Saving model. Step: 19000, loss: 41.779770\n    ******************************\n    src: 天 涯 共 此 时 ， 祝 福 乡 音 传 千 里\n    output: 天 岛 迎 春 日 ， 腾 今 风 日 誉 一 家\n    target: 海 内 同 今 日 ， 拜 年 吉 语 进 万 家\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:五 枝 锦 树 荣 今 代\n    output: 一 片 红 楼 耀 古 今\n    target: 百 秩 仙 筹 萃 一 门\n    ====================\n    src:钗 头 凤 句 最 悲 伤 ， 棒 打 鸳 鸯 散\n    output: 水 畔 花 香 如 故 土 ， 情 牵 玉 簟 香\n    target: 广 耜 斋 文 犹 歉 岁 ， 鹤 鸣 绿 水 寒\n    ====================\n    src:笔 端 流 畅 意\n    output: 心 底 悟 真 情\n    target: 诗 尾 泻 舒 心\n    Evaluate model. Step: 19000, score: 1.364269, loss: 41.779770\n    Saving model. Step: 19100, loss: 41.837460\n    ******************************\n    src: 篱 边 菊 已 经 霜 瘦\n    output: 窗 上 花 香 伴 日 斜\n    target: 岭 上 枫 方 向 日 红\n    Saving model. Step: 19200, loss: 41.641705\n    ******************************\n    src: 周 游 学 海 追 先 哲\n    output: 后 鲁 龙 山 仰 后 昆\n    target: 同 步 书 山 励 后 生\n    Saving model. Step: 19300, loss: 41.307513\n    ******************************\n    src: 一 杯 浊 酒 伤 行 色\n    output: 几 缕 清 风 乱 耳 愁\n    target: 几 管 秋 声 动 客 愁\n    Saving model. Step: 19400, loss: 41.562441\n    ******************************\n    src: 莺 声 燕 语 留 春 住\n    output: 燕 阔 龙 高 伴 月 来\n    target: 海 誓 山 盟 待 汝 归\n    Saving model. Step: 19500, loss: 41.651700\n    ******************************\n    src: 心 静 气 闲 ， 四 季 风 光 皆 入 眼\n    output: 情 高 地 淡 ， 一 多 风 色 总 怡 诗\n    target: 天 高 云 淡 ， 几 程 山 水 尽 成 诗\n    Saving model. Step: 19600, loss: 41.954834\n    ******************************\n    src: 陋 室 书 香 常 醉 客\n    output: 春 窗 月 韵 好 迷 人\n    target: 素 墙 墨 趣 总 迷 人\n    Saving model. Step: 19700, loss: 41.691529\n    ******************************\n    src: 入 山 不 记 来 时 路\n    output: 入 户 当 登 上 意 人\n    target: 出 寨 要 留 得 令 条\n    Saving model. Step: 19800, loss: 41.072585\n    ******************************\n    src: 几 曲 清 箫 吹 小 院\n    output: 一 帘 热 字 上 长 头\n    target: 一 支 天 籁 入 心 窗\n    Saving model. Step: 19900, loss: 41.453941\n    ******************************\n    src: 国 圆 大 梦\n    output: 民 乐 小 康\n    target: 民 步 小 康\n    Saving model. Step: 20000, loss: 41.626828\n    ******************************\n    src: 三 斗 白 云 君 纳 否\n    output: 一 杯 明 海 我 为 之\n    target: 一 瓢 沧 海 我 收 之\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:四 海 同 元 服\n    output: 一 山 共 月 圆\n    target: 三 加 进 达 樽\n    ====================\n    src:任 呼 茂 叔 穷 禅 客\n    output: 不 让 清 风 是 故 人\n    target: 早 判 公 羊 卖 饼 家\n    ====================\n    src:云 眸 落 泪 千 山 雨\n    output: 月 影 随 心 万 里 情\n    target: 海 嘴 喷 油 百 库 钱\n    ====================\n    src:金 龙 掣 电 倾 盆 雨\n    output: 玉 兔 开 花 报 喜 春\n    target: 疾 鼓 追 风 阴 晦 天\n    ====================\n    src:多 付 闲 情 招 祸 水\n    output: 不 知 俗 念 问 人 人\n    target: 少 生 欲 念 惹 红 颜\n    ====================\n    src:至 圣\n    output: 新 贤\n    target: 真 如\n    ====================\n    src:人 才 观\n    output: 我 不 知\n    target: 开 发 区\n    Evaluate model. Step: 20000, score: 1.511780, loss: 41.626828\n    Saving model. Step: 20100, loss: 40.952841\n    ******************************\n    src: 永 济 一 方 天 ， 信 为 人 脉 时 时 用\n    output: 长 华 千 古 事 ， 心 信 风 源 处 处 来\n    target: 中 华 千 古 梦 ， 诚 化 财 源 处 处 生\n    Saving model. Step: 20200, loss: 40.988183\n    ******************************\n    src: 反 腐 倡 廉 ， 准 则 频 送 清 心 剂\n    output: 秉 廉 养 气 ， 廉 岛 不 留 大 日 钟\n    target: 鉴 人 正 己 ， 条 例 长 鸣 警 世 钟\n    Saving model. Step: 20300, loss: 41.002979\n    ******************************\n    src: 风 含 翠 竹 娟 娟 净\n    output: 雨 润 清 花 自 冉 香\n    target: 雨 浥 红 莲 冉 冉 香\n    Saving model. Step: 20400, loss: 41.903536\n    ******************************\n    src: 聚 浦 分 廉 天 府 地\n    output: 中 山 独 水 古 山 山\n    target: 登 山 涉 海 电 雷 诗\n    Saving model. Step: 20500, loss: 40.842911\n    ******************************\n    src: 月 映 西 樵 珠 履 梦\n    output: 风 归 北 海 玉 河 心\n    target: 人 钦 南 海 大 同 书\n    Saving model. Step: 20600, loss: 41.354539\n    ******************************\n    src: 犹 记 当 年 青 草 地\n    output: 不 忘 此 个 旧 桃 梢\n    target: 难 忘 那 片 绿 杨 林\n    Saving model. Step: 20700, loss: 41.279942\n    ******************************\n    src: 画 地 为 牢 ， 蒲 团 暖 座\n    output: 书 街 有 路 ， 水 娆 高 人\n    target: 当 头 一 棒 ， 妖 孽 甭 逃\n    Saving model. Step: 20800, loss: 41.400214\n    ******************************\n    src: 希 贤 希 圣 希 天 ， 尚 友 诗 书 ， 其 揆 则 一\n    output: 学 德 为 德 在 德 ， 人 牌 名 豆 ， 其 愧 之 多\n    target: 立 言 立 功 立 德 ， 名 山 俎 豆 ， 不 朽 者 三\n    Saving model. Step: 20900, loss: 41.481511\n    ******************************\n    src: 携 药 品 登 门 ， 治 病 健 身 ， 边 远 医 生 美\n    output: 有 人 年 作 眼 ， 和 书 论 志 ， 文 微 世 国 多\n    target: 接 少 儿 到 校 ， 读 书 明 礼 ， 贫 乡 教 育 馨\n    Saving model. Step: 21000, loss: 41.619497\n    ******************************\n    src: 荷 香 缕 缕 寻 月 色\n    output: 柳 色 摇 悠 醉 春 烟\n    target: 柳 韵 悠 悠 觅 云 姿\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:嫡 子 拉 风 ， 庶 几 忘 我\n    output: 王 公 作 序 ， 一 一 一 人\n    target: 彼 人 吹 水 ， 其 实 无 他\n    ====================\n    src:万 户 金 鸡 歌 盛 世\n    output: 千 年 玉 兔 报 春 风\n    target: 九 霄 玉 兔 报 新 春\n    ====================\n    src:头 中 点 戒 招 蜂 刺\n    output: 脚 下 无 私 打 马 拉\n    target: 鬓 上 插 花 惹 蝶 追\n    ====================\n    src:学 子 辛 勤 成 大 器\n    output: 文 章 精 品 有 高 才\n    target: 园 丁 努 力 育 高 才\n    ====================\n    src:今 朝 大 款 携 二 奶\n    output: 此 日 新 年 出 一 人\n    target: 历 来 美 酒 伴 风 流\n    ====================\n    src:生 活 恰 如 鱼 饮 水\n    output: 和 谐 当 似 凤 栖 凰\n    target: 进 修 浑 似 燕 衔 泥\n    ====================\n    src:千 里 梦 ， 咫 尺 情 ， 渡 头 月 色 天 涯 望\n    output: 一 生 情 ， 心 头 梦 ， 心 里 情 怀 世 上 来\n    target: 峰 峦 移 ， 舟 棹 静 ， 帆 面 风 声 耳 际 聆\n    ====================\n    src:一 枕 残 书 深 夜 读\n    output: 几 分 雅 韵 满 天 香\n    target: 半 窗 明 月 异 乡 吟\n    ====================\n    src:肯 唱 戏 是 阴 功 ， 谁 靠 菩 萨 吃 饭\n    output: 不 知 心 非 苦 苦 ， 我 为 大 道 为 人\n    target: 要 上 台 充 角 色 ， 须 看 时 景 穿 衣\n    ====================\n    src:午 时 已 到 开 餐 否\n    output: 今 日 何 妨 对 酒 来\n    target: 肚 子 不 成 闹 事 啦\n    ====================\n    src:玩 不 出 花 样\n    output: 心 无 不 苦 心\n    target: 白 浪 费 时 间\n    ====================\n    src:八 百 里 洞 庭 ， 凭 岳 阳 壮 阔\n    output: 五 千 年 古 韵 ， 任 古 韵 悠 悠\n    target: 七 二 峰 螺 岛 ， 萃 天 下 灵 奇\n    Evaluate model. Step: 21000, score: 1.395569, loss: 41.619497\n    Saving model. Step: 21100, loss: 41.410087\n    ******************************\n    src: 捧 爱 兴 邦 ， 奶 奶 奉 她 全 部 爱\n    output: 开 怀 贺 爱 ， 民 君 有 国 万 生 歌\n    target: 放 歌 颂 党 ， 妈 妈 教 我 一 支 歌\n    Saving model. Step: 21200, loss: 41.077808\n    ******************************\n    src: 孝 父 母 天 赐 长 寿\n    output: 文 人 人 国 庆 大 福\n    target: 敬 双 亲 荣 神 益 人\n    Saving model. Step: 21300, loss: 41.807328\n    ******************************\n    src: 填 书 塞 典 文 博 士\n    output: 放 赋 人 人 学 俗 人\n    target: 曲 线 救 国 歹 汉 奸\n    Saving model. Step: 21400, loss: 40.760749\n    ******************************\n    src: 祝 亚 运 顺 风 ， 旗 开 得 胜\n    output: 看 新 华 圆 锦 ， 国 跃 成 功\n    target: 盼 中 华 夺 锦 ， 马 到 成 功\n    Saving model. Step: 21500, loss: 41.652135\n    ******************************\n    src: 启 南 湖 一 叶 舟 ， 血 荐 轩 辕 ， 党 救 万 民 功 盖 世\n    output: 仰 北 国 千 秋 梦 ， 功 飞 天 宇 ， 人 昭 北 域 史 凌 诗\n    target: 绘 中 国 千 秋 画 ， 云 蒸 海 岳 ， 旗 辉 九 秩 景 如 虹\n    Saving model. Step: 21600, loss: 41.405142\n    ******************************\n    src: 美 誉\n    output: 奇 宾\n    target: 嘉 褒\n    Saving model. Step: 21700, loss: 41.340503\n    ******************************\n    src: 李 杜 题 诗 ， 捭 阖 星 云 日 月 书 青 史\n    output: 金 髦 入 道 ， 文 横 天 古 画 坤 壮 宏 图\n    target: 时 空 主 笔 ， 纵 横 今 古 乾 坤 展 鸿 图\n    Saving model. Step: 21800, loss: 40.836730\n    ******************************\n    src: 风 轻 酒 暖 邀 云 醉\n    output: 月 暖 月 闲 任 月 明\n    target: 水 冷 心 寒 待 月 明\n    Saving model. Step: 21900, loss: 40.840516\n    ******************************\n    src: 读 书 有 味 千 回 少\n    output: 对 句 无 言 一 品 空\n    target: 对 客 无 情 一 句 多\n    Saving model. Step: 22000, loss: 40.738073\n    ******************************\n    src: 廉 泉 原 鉴 千 秋 洁\n    output: 清 海 长 添 万 世 通\n    target: 宦 海 平 添 一 脉 香\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:红 袖 影 翻 江 上 月\n    output: 白 云 霞 映 水 中 天\n    target: 紫 箫 声 逐 岭 头 云\n    ====================\n    src:愁 如 逝 水 不 堪 问\n    output: 恨 似 流 云 不 可 寻\n    target: 酒 似 春 风 一 任 斟\n    ====================\n    src:实 施 人 口 均 衡 发 展\n    output: 大 展 宏 图 大 展 宏 图\n    target: 应 对 老 龄 快 速 增 加\n    ====================\n    src:巫 山 有 梦 留 残 照\n    output: 山 水 无 声 入 故 人\n    target: 帝 子 乘 风 下 翠 微\n    ====================\n    src:奇 葩 男 、 奇 葩 女 ， 春 风 几 度\n    output: 新 人 家 、 新 人 家 ， 大 地 长 新\n    target: 乱 世 人 、 乱 世 狗 ， 等 闲 一 般\n    Evaluate model. Step: 22000, score: 1.534753, loss: 40.738073\n    Saving model. Step: 22100, loss: 40.692065\n    ******************************\n    src: 呓 语 低 吟 碧 落 ， 抱 月 清 幽 ， 无 畏 根 深 冷 处\n    output: 春 躅 淡 曳 红 尘 ， 听 风 落 暖 ， 有 情 梦 淡 时 时\n    target: 娉 婷 摇 曳 红 尘 ， 迎 风 温 婉 ， 有 怜 香 浅 秋 时\n    Saving model. Step: 22200, loss: 41.069920\n    ******************************\n    src: 革 命 尚 未 成 功\n    output: 为 心 同 是 为 力\n    target: 同 志 仍 需 努 力\n    Saving model. Step: 22300, loss: 41.656791\n    ******************************\n    src: 归 处 何 依 ？ 此 身 自 与 沧 州 老\n    output: 何 时 共 乐 ， 此 脉 常 同 天 气 风\n    target: 酬 国 之 祚 ， 一 世 愿 同 紫 塞 寒\n    Saving model. Step: 22400, loss: 41.325904\n    ******************************\n    src: 得 意 算 盘 ， 三 下 五 除 二\n    output: 开 心 卖 卖 ， 一 里 二 二 八\n    target: 招 工 相 面 ， 万 中 百 选 一\n    Saving model. Step: 22500, loss: 41.307732\n    ******************************\n    src: 过 闻 喜 闻 过 则 喜\n    output: 来 来 乐 者 乐 者 和\n    target: 来 和 顺 和 来 而 顺\n    Saving model. Step: 22600, loss: 41.669844\n    ******************************\n    src: 峰 峦 崇 岱 岳 ， 嶒 嶝 峥 嵘 峙 峻 岭\n    output: 山 海 涌 津 沽 ， 潆 洄 澎 湃 汇 洪 流\n    target: 河 海 润 津 沽 ， 潆 洄 澎 湃 汇 洪 流\n    Saving model. Step: 22700, loss: 41.233244\n    ******************************\n    src: 树 生 渡 口 天 然 好\n    output: 心 顶 山 山 日 处 多\n    target: 山 到 江 边 分 外 明\n    Saving model. Step: 22800, loss: 40.147790\n    ******************************\n    src: 映 雪 囊 萤 凿 壁 借\n    output: 飞 天 作 月 落 云 行\n    target: 负 薪 挂 角 带 经 锄\n    Saving model. Step: 22900, loss: 40.343244\n    ******************************\n    src: 山 中 土 养 天 然 味\n    output: 海 上 人 开 大 态 情\n    target: 席 上 肉 兴 生 态 风\n    Saving model. Step: 23000, loss: 40.675579\n    ******************************\n    src: 碧 瓦 朱 甍 照 城 郭\n    output: 青 云 玉 水 映 江 台\n    target: 浅 黄 轻 绿 映 楼 台\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:羊 毫 作 笔 ， 书 华 夏 辉 煌 业 绩\n    output: 猴 棒 挥 毫 ， 写 华 章 锦 绣 文 章\n    target: 猴 棒 化 针 ， 绣 祖 国 美 丽 江 山\n    ====================\n    src:礼 成 二 戴 ， 名 耀 八 方 ， 儒 家 经 典 千 秋 颂\n    output: 德 泽 三 门 ， 德 昭 四 海 ， 华 夏 文 明 万 代 传\n    target: 脚 踏 棉 山 ， 眼 观 粮 海 ， 红 日 光 芒 万 古 留\n    ====================\n    src:千 尺 廊 桥 千 尺 画\n    output: 一 江 水 水 一 江 诗\n    target: 一 湖 风 月 一 湖 诗\n    ====================\n    src:可 珍 土 地 忙 中 乐\n    output: 不 见 人 间 乐 外 欢\n    target: 有 味 诗 书 苦 后 甜\n    ====================\n    src:烈 日 当 空 ， 车 驰 山 路 救 灾 送 水\n    output: 春 风 化 雨 ， 花 绽 花 开 蝶 恋 花 香\n    target: 真 情 济 世 ， 爱 献 乡 民 动 地 感 天\n    ====================\n    src:杜 宇 声 声 啼 旧 梦\n    output: 桃 花 朵 朵 映 新 春\n    target: 芙 蓉 朵 朵 促 新 诗\n    ====================\n    src:巢 落 莺 哥 走\n    output: 山 高 月 下 来\n    target: 门 开 燕 子 归\n    ====================\n    src:一 杯 菊 酒 醉 秋 月\n    output: 几 缕 春 风 拂 柳 烟\n    target: 两 袖 清 风 伴 落 花\n    ====================\n    src:烛 灭\n    output: 花 开\n    target: 香 消\n    ====================\n    src:赝 品 时 呈 ， 乱 花 渐 欲 迷 人 眼\n    output: 新 风 乍 起 ， 明 月 还 须 照 我 心\n    target: 金 睛 明 察 ， 狗 嘴 何 曾 长 象 牙\n    ====================\n    src:狮 舞 雄 风 ， 荟 天 下 英 豪 ， 争 王 争 霸\n    output: 龙 腾 盛 世 ， 喜 中 华 大 地 ， 盛 世 扬 帆\n    target: 羊 开 泰 运 ， 壮 樵 山 文 翰 ， 入 梦 入 诗\n    Evaluate model. Step: 23000, score: 1.626593, loss: 40.675579\n    Saving model. Step: 23100, loss: 40.563663\n    ******************************\n    src: 遥 望 银 河 思 浪 漫\n    output: 闲 钟 碧 榭 叹 徘 柔\n    target: 独 居 月 殿 可 温 馨\n    Saving model. Step: 23200, loss: 40.285375\n    ******************************\n    src: 泼 墨 慕 文 成 巨 卷\n    output: 挥 情 作 善 是 高 文\n    target: 痴 书 弘 毅 聚 斯 斋\n    Saving model. Step: 23300, loss: 40.952627\n    ******************************\n    src: 开 国 精 神 元 不 老\n    output: 中 公 德 月 自 先 贤\n    target: 抡 才 岁 月 冠 群 伦\n    Saving model. Step: 23400, loss: 40.808578\n    ******************************\n    src: 昔 年 黯 影 今 朝 尽\n    output: 今 岁 新 心 一 日 来\n    target: 旧 岁 烦 痕 昨 夜 消\n    Saving model. Step: 23500, loss: 40.824891\n    ******************************\n    src: 顷 刻 驰 驱 千 里 外\n    output: 山 今 风 业 万 时 中\n    target: 古 今 事 业 一 霄 中\n    Saving model. Step: 23600, loss: 40.318340\n    ******************************\n    src: 携 唐 宋 诗 词 奔 涌 而 来 ， 赋 墨 客 文 人 ， 千 年 不 老 中 秋 月\n    output: 看 古 园 风 女 相 怀 在 见 ， 看 天 歌 楚 别 ， 一 里 长 思 北 子 诗\n    target: 是 家 山 儿 女 情 思 所 系 ， 问 悲 欢 离 合 ， 万 里 遥 听 游 子 吟\n    Saving model. Step: 23700, loss: 40.963029\n    ******************************\n    src: 逐 北\n    output: 东 南\n    target: 平 南\n    Saving model. Step: 23800, loss: 40.535250\n    ******************************\n    src: 月 影 初 临 空 朗 朗\n    output: 风 光 犹 杳 更 潺 飕\n    target: 风 声 已 定 水 湉 湉\n    Saving model. Step: 23900, loss: 40.171656\n    ******************************\n    src: 喷 壁 四 时 雨\n    output: 开 怀 万 里 风\n    target: 寄 声 千 里 风\n    Saving model. Step: 24000, loss: 40.912145\n    ******************************\n    src: 润 物 体 乾 培 国 运\n    output: 扬 功 济 道 铸 民 流\n    target: 成 思 端 致 见 风 怀\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:一 心 一 意 ， 只 予 有 缘 人 作 对\n    output: 无 事 无 非 ， 无 非 无 事 事 无 求\n    target: 几 字 几 词 ， 欣 获 臻 美 句 成 联\n    ====================\n    src:拜 佛 何 须 来 寺 庙\n    output: 修 身 自 是 上 山 门\n    target: 打 坐 当 然 要 空 心\n    ====================\n    src:几 人 登 顶 悟 真 道\n    output: 一 世 无 心 悟 佛 陀\n    target: 今 世 有 缘 拜 祖 师\n    ====================\n    src:目 曾 瞻 壮 景 ， 人 皆 仰 貌 聆 声 ， 惊 势 动 情 ， 诗 兴 荡 崖 亲 兴 上\n    output: 心 正 有 灵 犀 ， 气 爽 清 风 ， 清 风 入 韵 ， 风 情 ， 风 流 韵 韵 中 来\n    target: 身 复 隐 奇 帘 ， 我 竟 凝 神 探 趣 ， 运 思 成 句 ， 瀑 流 溶 伍 巨 流 中\n    ====================\n    src:才 斟 米 酒 撩 春 意\n    output: 欲 把 诗 书 寄 雅 情\n    target: 又 舞 羊 毫 咏 杜 鹃\n    ====================\n    src:舍 慢 持 净 界\n    output: 心 静 见 真 情\n    target: 以 法 化 众 生\n    ====================\n    src:慷 慨 视 别 剑\n    output: 萧 萧 听 鸣 琴\n    target: 凄 清 流 雅 音\n    ====================\n    src:苏 武 留 胡 节 不 辱\n    output: 王 公 有 主 子 如 来\n    target: 欧 文 赴 美 愿 成 空\n    ====================\n    src:樽 前 备 墨 时 邀 月\n    output: 笔 底 挥 毫 欲 赋 诗\n    target: 笔 下 生 花 静 有 香\n    ====================\n    src:上 天 堂 ， 凌 绝 顶 ， 腾 云 驾 雾 着 霓 裳 ， 只 觉 身 临 仙 境\n    output: 上 山 水 ， 观 山 河 ， 观 水 观 山 观 水 月 ， 何 妨 画 卷 诗 人\n    target: 观 瀑 布 ， 赏 奇 花 ， 涉 水 爬 山 穿 锦 绣 ， 方 知 美 在 人 间\n    Evaluate model. Step: 24000, score: 1.719163, loss: 40.912145\n    Saving model. Step: 24100, loss: 40.421879\n    ******************************\n    src: 夫 妻 上 擂 台 ， 成 双 作 对\n    output: 老 女 出 出 汉 ， 对 士 相 军\n    target: 男 女 下 武 池 ， 逞 独 耍 单\n    Saving model. Step: 24200, loss: 39.747900\n    ******************************\n    src: 别 让 星 星 流 眼 泪\n    output: 不 将 风 色 动 心 情\n    target: 管 教 月 月 好 心 情\n    Saving model. Step: 24300, loss: 39.477560\n    ******************************\n    src: 山 货 店 ， 竹 艺 店 ， 大 溪 开 店\n    output: 水 市 路 ， 新 画 路 ， 大 州 腾 家\n    target: 旺 铺 街 ， 财 富 街 ， 九 龙 兴 街\n    Saving model. Step: 24400, loss: 39.646577\n    ******************************\n    src: 独 怜 秋 叶 飘 摇 落\n    output: 不 忆 春 风 落 洒 来\n    target: 更 爱 春 光 潇 洒 歌\n    Saving model. Step: 24500, loss: 39.168146\n    ******************************\n    src: 秋 雨 如 丝 滋 故 土\n    output: 春 风 似 火 染 新 人\n    target: 西 风 似 墨 染 归 心\n    Saving model. Step: 24600, loss: 39.595880\n    ******************************\n    src: 创 会 辛 勤 ， 建 祠 修 谱 光 先 祖\n    output: 弘 官 济 远 ， 教 业 扬 魂 启 后 人\n    target: 为 人 清 正 ， 伟 绩 忠 魂 荫 后 昆\n    Saving model. Step: 24700, loss: 40.020131\n    ******************************\n    src: 寒 山 何 以 瘦\n    output: 浊 树 不 成 迟\n    target: 老 子 自 然 清\n    Saving model. Step: 24800, loss: 39.171483\n    ******************************\n    src: 不 作 寻 常 风 月 咏\n    output: 且 看 无 事 古 云 闲\n    target: 再 思 往 惜 水 茶 情\n    Saving model. Step: 24900, loss: 39.790902\n    ******************************\n    src: 百 会 征 联 文 集 雅\n    output: 一 军 对 对 字 传 新\n    target: 三 年 应 句 辑 编 优\n    Saving model. Step: 25000, loss: 39.693747\n    ******************************\n    src: 审 无 私 ， 甘 作 财 经 卫 士\n    output: 心 有 道 ， 不 扬 善 道 为 章\n    target: 计 有 道 ， 弘 扬 公 德 文 明\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:灯 下 吟 诗 人 自 雅\n    output: 花 间 酌 酒 酒 尤 香\n    target: 山 中 吃 肉 虎 真 凶\n    ====================\n    src:五 福 临 场 ， 莺 鸣 燕 舞 ， 舜 诵 南 风 ， 四 面 层 峦 来 紫 气\n    output: 三 春 送 暖 ， 春 满 花 开 ， 花 开 锦 绣 ， 千 秋 大 业 展 宏 图\n    target: 八 方 奏 乐 ， 水 唱 山 歌 ， 尧 耕 广 漠 ， 万 荣 飞 阁 誉 龙 头\n    ====================\n    src:月 笼 寒 水 清 凉 境\n    output: 风 过 泸 州 浪 漫 时\n    target: 雾 锁 修 竹 幽 雅 居\n    ====================\n    src:长 守 山 林 ， 甘 守 平 凡 ， 但 闻 世 人 知 伯 乐\n    output: 常 怀 故 里 ， 长 留 诗 句 ， 不 知 诗 酒 赋 诗 诗\n    target: 不 观 皮 相 ， 唯 观 筋 骨 ， 独 开 慧 眼 识 良 驹\n    ====================\n    src:十 天 四 节 何 时 有\n    output: 一 日 一 心 无 处 无\n    target: 七 日 一 周 哪 月 无\n    ====================\n    src:霜 冷 花 黄 ， 杯 中 有 酒 神 仙 妒\n    output: 风 清 月 白 ， 眼 底 无 声 诗 客 吟\n    target: 夜 阑 月 静 ， 亭 下 无 思 鬼 魅 愁\n    ====================\n    src:雾 列 牌 坊 ， 长 对 三 河 征 战 地\n    output: 风 流 大 地 ， 再 开 四 海 锦 程 图\n    target: 风 传 捷 报 ， 犹 听 八 面 凯 歌 声\n    ====================\n    src:一 庭 春 色 无 关 我\n    output: 几 度 秋 声 不 待 人\n    target: 对 面 娇 花 才 诱 人\n    ====================\n    src:吟 诗 最 喜 遇 佳 句\n    output: 对 月 常 思 对 对 联\n    target: 修 道 何 须 拜 圣 人\n    ====================\n    src:春 荣 渝 水 三 千 树\n    output: 风 送 桃 花 第 一 枝\n    target: 梦 染 丰 都 七 彩 图\n    ====================\n    src:留 下 清 廉 当 后 路\n    output: 打 开 新 纪 上 高 楼\n    target: 何 来 权 贵 鉴 前 车\n    ====================\n    src:一 团 开 处 春 无 价\n    output: 万 里 归 来 梦 有 缘\n    target: 七 泡 尝 来 梦 亦 香\n    ====================\n    src:屏 小 云 气 山 开 通\n    output: 月 满 月 光 月 满 轮\n    target: 树 里 檐 声 雨 满 堂\n    Evaluate model. Step: 25000, score: 1.796805, loss: 39.693747\n    Saving model. Step: 25100, loss: 40.349395\n    ******************************\n    src: 入 梦 赏 荷 依 柳 岸\n    output: 临 心 对 月 落 诗 弦\n    target: 无 眠 看 雨 奏 窗 台\n    Saving model. Step: 25200, loss: 39.946146\n    ******************************\n    src: 春 风 十 里 柔 情 ， 怎 奈 何 青 山 招 不 来 ， 明 日 留 难 住\n    output: 明 事 一 时 大 道 ， 莫 知 他 一 海 ， 多 ， ， 老 花 不 我 知\n    target: 世 事 一 场 大 梦 ， 始 信 得 皮 袋 非 真 我 ， 梅 花 是 故 人\n    Saving model. Step: 25300, loss: 39.704413\n    ******************************\n    src: 鬼 佬 恃 才 无 恶 意\n    output: 公 翁 有 句 有 愁 情\n    target: 醉 翁 对 酒 忒 钟 情\n    Saving model. Step: 25400, loss: 39.431829\n    ******************************\n    src: 策 杖 巫 山 ， 齐 谁 眉 目\n    output: 横 歌 海 海 ， 任 我 胸 胸\n    target: 放 舟 沧 海 ， 荡 我 心 胸\n    Saving model. Step: 25500, loss: 38.963491\n    ******************************\n    src: 香 火 千 年 ， 照 亮 迷 途 生 自 在\n    output: 清 风 万 里 ， 迎 开 梦 梦 梦 成 悲\n    target: 清 风 万 缕 ， 吹 开 好 梦 化 慈 悲\n    Saving model. Step: 25600, loss: 39.339904\n    ******************************\n    src: 渴 鹿 趋 阳 焰\n    output: 灵 鸿 舞 海 星\n    target: 飞 蛾 赴 火 光\n    Saving model. Step: 25700, loss: 39.591200\n    ******************************\n    src: 过 界 红 兵 图 杀 将\n    output: 中 天 紫 土 不 归 身\n    target: 卧 槽 黑 子 已 抽 车\n    Saving model. Step: 25800, loss: 39.805213\n    ******************************\n    src: 人 生 本 就 难 一 论\n    output: 世 化 不 须 不 不 言\n    target: 文 字 何 尝 真 万 能\n    Saving model. Step: 25900, loss: 39.375774\n    ******************************\n    src: 上 海 海 带 带 苦 味\n    output: 西 地 北 中 开 美 人\n    target: 天 津 津 贴 贴 穷 人\n    Saving model. Step: 26000, loss: 40.428164\n    ******************************\n    src: 柳 下 堆 烟 绿\n    output: 花 中 映 月 明\n    target: 水 中 卧 月 明\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:贸 易 兴 隆 盈 万 利\n    output: 和 谐 社 会 乐 千 家\n    target: 春 风 得 意 纳 千 祥\n    ====================\n    src:归 心 切 切 ， 雪 风 难 阻 返 乡 路\n    output: 举 步 望 穿 ， 云 雾 难 寻 入 梦 乡\n    target: 去 意 殷 殷 ， 金 玉 畅 通 出 国 途\n    ====================\n    src:会 压 洪 波 先 得 路\n    output: 难 寻 大 道 不 关 情\n    target: 催 沽 美 酒 敢 辞 贫\n    ====================\n    src:华 夏 春 光 传 马 赛\n    output: 中 华 大 业 展 鹏 程\n    target: 人 间 美 景 在 羊 城\n    ====================\n    src:苦 辣 酸 甜 ， 遭 遇 一 生 应 不 少\n    output: 酸 甜 苦 辣 ， 品 尝 几 度 乐 无 穷\n    target: 诗 书 画 印 ， 兼 擅 四 绝 已 无 多\n    ====================\n    src:一 部 茶 经 ， 七 千 锦 语 ， 字 字 珠 玑 香 国 饮\n    output: 三 分 景 色 ， 四 季 春 风 ， 人 文 荟 萃 壮 山 河\n    target: 八 方 雅 士 ， 四 路 欢 声 ， 纷 纷 车 马 向 天 门\n    ====================\n    src:幽 谷 梅 花 先 试 雪\n    output: 长 江 月 色 总 流 金\n    target: 岭 南 杨 柳 早 知 春\n    ====================\n    src:好 在 品 尝 方 得 体\n    output: 常 于 心 事 总 关 情\n    target: 勤 于 炒 作 却 无 名\n    ====================\n    src:名 花 无 好 主\n    output: 老 板 有 高 朋\n    target: 小 草 有 高 怀\n    ====================\n    src:舀 江 水 一 瓢 焙 茗 ， 明 目 纵 观 新 上 海\n    output: 看 山 山 万 里 山 山 ， 高 山 独 秀 古 今 山\n    target: 借 春 茶 几 叶 搭 台 ， 开 篇 即 是 大 文 章\n    ====================\n    src:那 风 那 雨 携 春 味\n    output: 此 地 无 人 惹 夏 风\n    target: 此 字 此 文 带 土 香\n    Evaluate model. Step: 26000, score: 1.797901, loss: 40.428164\n    Saving model. Step: 26100, loss: 40.160984\n    ******************************\n    src: 天 道 酬 勤 ， 金 榜 题 名 终 遂 愿\n    output: 人 龙 献 志 ， 金 江 送 路 不 知 人\n    target: 蛟 龙 得 水 ， 长 风 作 浪 不 由 云\n    Saving model. Step: 26200, loss: 39.514300\n    ******************************\n    src: 年 轮 铭 故 事\n    output: 岁 色 忆 新 怀\n    target: 月 夜 会 情 人\n    Saving model. Step: 26300, loss: 39.337964\n    ******************************\n    src: 雨 绿 春 山 风 送 暖\n    output: 花 开 柳 院 月 谈 欢\n    target: 花 香 醉 客 鸟 投 怀\n    Saving model. Step: 26400, loss: 38.800344\n    ******************************\n    src: 勤 劳 能 致 富\n    output: 和 气 可 生 财\n    target: 和 睦 可 发 财\n    Saving model. Step: 26500, loss: 39.283952\n    ******************************\n    src: 夜 幕 斜 垂 ， 雾 气 千 山 风 卷 去\n    output: 春 光 滚 荡 ， 烟 声 万 里 月 飞 来\n    target: 波 流 暗 落 ， 潮 音 万 里 浪 涛 空\n    Saving model. Step: 26600, loss: 39.319360\n    ******************************\n    src: 说 短 道 长 凭 胆 量\n    output: 打 空 心 要 要 精 情\n    target: 测 凉 试 热 靠 心 肠\n    Saving model. Step: 26700, loss: 39.792177\n    ******************************\n    src: 厌 闻 百 怪 千 奇 事\n    output: 醉 看 三 生 一 福 人\n    target: 喜 看 三 侠 五 义 书\n    Saving model. Step: 26800, loss: 38.765344\n    ******************************\n    src: 富 乐 裹 葱 茏 ， 杜 圣 忘 忧 ， 径 剪 山 光 包 韵 脚\n    output: 春 蓉 开 水 韵 ， 清 仙 醉 舞 ， 风 邀 月 月 醉 春 情\n    target: 芙 蓉 流 洌 澈 ， 谪 仙 载 酒 ， 直 掬 水 色 润 诗 肠\n    Saving model. Step: 26900, loss: 39.182064\n    ******************************\n    src: 归 心 已 逐 轻 桡 ， 几 夜 湖 山 生 梦 寐\n    output: 举 梦 又 有 春 节 ， 一 枝 风 竹 落 风 江\n    target: 晚 香 犹 有 佳 处 ， 一 区 松 菊 老 湘 滨\n    Saving model. Step: 27000, loss: 39.469606\n    ******************************\n    src: 口 伐 声 中 交 伐 咬\n    output: 眼 星 月 里 自 光 明\n    target: 日 光 城 里 月 光 明\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:绿 岛 轻 舟 人 入 画\n    output: 青 山 绿 水 我 吟 诗\n    target: 椰 风 海 水 浪 弹 琴\n    ====================\n    src:百 感 交 于 无 意 处\n    output: 一 生 不 尽 有 时 时\n    target: 千 情 自 在 不 言 中\n    ====================\n    src:人 无 远 虑 忧 多 近\n    output: 心 有 灵 犀 意 不 平\n    target: 帐 有 盈 余 庆 不 亏\n    ====================\n    src:粉 饰\n    output: 麻 秸\n    target: 装 潢\n    ====================\n    src:七 月 兰 盆 施 大 德\n    output: 一 朝 春 色 满 中 华\n    target: 一 筵 水 陆 度 幽 魂\n    ====================\n    src:野 渡 孤 心 ， 钓 月 上 船 听 故 事\n    output: 山 山 有 意 ， 临 风 把 盏 问 仙 人\n    target: 风 流 千 载 ， 持 觞 买 醉 梦 沧 桑\n    ====================\n    src:寂 寞 花 开 难 自 主\n    output: 相 思 月 落 不 成 愁\n    target: 相 思 泪 落 已 无 由\n    ====================\n    src:西 天 晓 月 枝 头 挂\n    output: 北 海 风 光 岭 上 飞\n    target: 北 海 春 潮 眼 里 生\n    ====================\n    src:乐 教 梓 楠 同 受 范\n    output: 不 知 风 月 不 成 诗\n    target: 喜 观 桃 李 广 成 材\n    ====================\n    src:佛 当 敬 ， 神 当 敬 ， 非 贤 毋 敬\n    output: 天 所 为 ， 地 无 私 ， 有 德 无 私\n    target: 钱 也 捐 ， 物 也 捐 ， 唯 德 不 捐\n    Evaluate model. Step: 27000, score: 1.690031, loss: 39.469606\n    Saving model. Step: 27100, loss: 39.927331\n    ******************************\n    src: 彩 笔 传 情 歌 伟 业\n    output: 丹 心 焕 志 展 英 恩\n    target: 丹 霞 达 意 颂 党 恩\n    Saving model. Step: 27200, loss: 39.121264\n    ******************************\n    src: 岂 可 无 梅 撑 画 骨\n    output: 须 经 有 月 入 梅 心\n    target: 曾 经 有 雪 映 诗 魂\n    Saving model. Step: 27300, loss: 39.961609\n    ******************************\n    src: 共 祝 党 与 天 齐 寿\n    output: 同 喜 人 同 日 同 春\n    target: 更 愿 民 同 地 永 宁\n    Saving model. Step: 27400, loss: 39.327849\n    ******************************\n    src: 残 阳 一 笛 花 村 入\n    output: 明 水 千 树 画 子 飞\n    target: 碧 山 千 江 燕 子 归\n    Saving model. Step: 27500, loss: 39.037501\n    ******************************\n    src: 虎 拜 仁 慈 大\n    output: 龙 吟 福 德 高\n    target: 龙 行 道 德 高\n    Saving model. Step: 27600, loss: 39.662881\n    ******************************\n    src: 看 我 非 我 ， 我 看 我 ， 我 也 非 我\n    output: 任 谁 是 谁 ？ 谁 是 谁 ， 谁 谁 谁 谁\n    target: 演 谁 像 谁 ， 谁 演 谁 ， 谁 就 像 谁\n    Saving model. Step: 27700, loss: 38.890640\n    ******************************\n    src: 予 贫 残 稚 弱 以 无 边 大 爱 ， 捐 资 相 助 ， 倾 力 而 扶 ， 仁 行 善 举 福 桑 梓\n    output: 有 大 米 风 薇 以 以 胞 之 天 ， 大 者 不 容 ， 以 辛 不 在 ， 大 作 甘 风 泽 栋 梁\n    target: 愿 樗 栎 楩 楠 享 同 片 蓝 天 ， 困 厄 不 辞 ， 艰 难 自 任 ， 化 雨 春 风 育 栋 梁\n    Saving model. Step: 27800, loss: 39.933081\n    ******************************\n    src: 天 寒 勤 问 酒\n    output: 月 富 好 思 书\n    target: 学 浅 多 读 书\n    Saving model. Step: 27900, loss: 39.427221\n    ******************************\n    src: 大 粮 飞 雪 ， 金 峰 缠 带 ， 大 爱 延 长 ， 直 向 民 心 铺 富 路\n    output: 大 镇 爱 钟 ， 玉 歌 兴 衢 ， 红 村 焕 目 ， 常 凭 国 想 展 豪 风\n    target: 乡 间 鸣 笛 ， 高 速 通 车 ， 乡 程 缩 短 ， 更 追 梦 想 畅 春 风\n    Saving model. Step: 28000, loss: 38.738341\n    ******************************\n    src: 十 里 荷 风 流 倩 影\n    output: 一 江 柳 色 醉 迷 人\n    target: 一 川 月 色 总 宜 人\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:临 水 乍 凝 双 袖 翠\n    output: 临 风 犹 带 一 枝 春\n    target: 凭 栏 偶 惹 一 身 香\n    ====================\n    src:与 百 姓 有 缘 ， 才 能 到 此\n    output: 替 一 心 一 念 ， 不 可 忘 形\n    target: 期 寸 心 无 愧 ， 不 负 斯 民\n    ====================\n    src:夕 夕 多 潮 朝 水 汐\n    output: 春 风 得 意 满 江 南\n    target: 山 山 出 矿 广 石 岩\n    ====================\n    src:玉 路 长 通 争 跃 马\n    output: 春 风 又 起 好 开 花\n    target: 金 瓯 永 固 不 亡 羊\n    ====================\n    src:松 下 云 闲 窥 落 子\n    output: 山 中 月 老 醉 游 人\n    target: 场 中 舞 秀 看 超 男\n    ====================\n    src:灵 猴 值 岁 春 光 满\n    output: 瑞 雪 迎 梅 瑞 气 盈\n    target: 骏 马 嘶 风 志 气 高\n    ====================\n    src:秀 阁 美 人 开 秀 阁\n    output: 青 山 绿 水 绕 青 山\n    target: 香 江 风 味 溢 香 江\n    ====================\n    src:大 肚 ， 能 容 天 下 之 事\n    output: 真 心 ， 不 让 人 间 不 言\n    target: 笑 脸 ， 迎 接 十 方 来 客\n    ====================\n    src:星 辉 南 极 岁 之 始\n    output: 日 照 东 西 风 不 来\n    target: 雨 足 西 江 云 自 闲\n    ====================\n    src:狗 皮 膏 药 牛 皮 癣\n    output: 牛 嘴 皮 皮 狗 嘴 皮\n    target: 人 血 馒 头 猪 血 汤\n    ====================\n    src:凭 草 木 传 神 ， 赋 彩 诗 篇 三 百 首\n    output: 把 文 明 作 笔 ， 挥 毫 墨 笔 五 千 年\n    target: 就 形 容 动 态 ， 涵 濡 造 化 万 千 姿\n    ====================\n    src:净 瓶 杨 柳 枝 ， 洒 点 点 风 调 雨 顺\n    output: 红 杏 红 花 朵 ， 点 点 点 点 染 花 红\n    target: 紫 竹 白 莺 哥 ， 叫 声 声 国 泰 民 安\n    ====================\n    src:清 江 滋 宿 草\n    output: 碧 水 泛 清 波\n    target: 细 雨 润 苞 花\n    Evaluate model. Step: 28000, score: 1.827040, loss: 38.738341\n    Saving model. Step: 28100, loss: 39.280227\n    ******************************\n    src: 寄 寓 客 官 ， 守 宿 ， 寒 窗 空 寂 寞\n    output: 吟 行 老 竹 ， 兴 民 ， 老 口 更 清 寥\n    target: 节 茶 芸 英 ， 荫 荷 ， 苦 苑 获 芙 蓉\n    Saving model. Step: 28200, loss: 40.120201\n    ******************************\n    src: 山 闲 云 影 静\n    output: 水 静 寺 光 明\n    target: 寺 古 佛 光 灵\n    Saving model. Step: 28300, loss: 38.932563\n    ******************************\n    src: 联 入 千 家 ， 张 张 笑 脸 泛 春 意\n    output: 联 腾 万 海 ， 锣 业 联 歌 动 党 康\n    target: 龙 荫 四 季 ， 事 事 欢 心 度 小 康\n    Saving model. Step: 28400, loss: 39.142739\n    ******************************\n    src: 无 一 丝 傲 气 ， 是 大 师 亦 是 大 哥 ， 怎 奈 何 谋 面 于 春 、 断 肠 在 夏\n    output: 几 几 更 青 心 ， 看 大 意 、 添 心 底 ， 怎 不 得 人 间 、 眼 、 不 火 为 天\n    target: 有 三 绝 殊 荣 ， 倾 心 血 更 倾 心 智 ， 长 留 这 瓷 魂 洁 白 、 炉 火 纯 青\n    Saving model. Step: 28500, loss: 39.402030\n    ******************************\n    src: 红 尘 碧 海 痴 情 种\n    output: 碧 道 清 山 好 意 人\n    target: 古 佛 青 灯 失 意 人\n    Saving model. Step: 28600, loss: 39.225309\n    ******************************\n    src: 航 海 史 中 航 海 使\n    output: 华 书 山 上 赋 书 人\n    target: 读 书 廊 上 读 书 郎\n    Saving model. Step: 28700, loss: 39.267307\n    ******************************\n    src: 龙 睛 不 点 恐 飞 去\n    output: 燕 鼓 无 鸣 不 自 来\n    target: 锣 鼓 常 敲 却 舞 来\n    Saving model. Step: 28800, loss: 38.852876\n    ******************************\n    src: 使 者 领 班 ， 百 年 礼 乐 行 三 献\n    output: 学 儿 出 曳 ， 一 片 风 声 唱 九 床\n    target: 女 仙 摇 佩 ， 一 派 箫 韶 起 半 空\n    Saving model. Step: 28900, loss: 39.312928\n    ******************************\n    src: 国 泰 民 安 歌 盛 世\n    output: 风 香 鸟 语 颂 新 春\n    target: 花 香 鸟 语 庆 新 春\n    Saving model. Step: 29000, loss: 39.763332\n    ******************************\n    src: 书 径 深 深 通 远 古\n    output: 书 山 峻 峻 仰 高 贤\n    target: 德 山 矗 矗 仰 先 贤\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:美 园 美 焕 古 今 情 ， 喜 远 养 禅 风 ， 近 养 仁 风 ， 远 近 无 双 朝 锦 绣\n    output: 大 地 长 流 天 下 梦 ， 喜 人 文 天 下 ， 同 心 气 象 ， 长 存 一 代 正 气 歌\n    target: 荣 氏 荣 开 荆 楚 梦 ， 有 天 之 道 业 ， 人 之 德 业 ， 天 人 合 一 阅 辉 煌\n    ====================\n    src:以 油 瓶 投 深 水 者 ， 瓶 破 瓦 沉 ， 其 油 浮 上\n    output: 于 天 井 点 点 红 尘 ， 花 开 花 落 ， 其 梦 在 乎\n    target: 若 业 识 于 命 终 时 ， 识 迁 身 坏 ， 彼 业 现 前\n    ====================\n    src:正 待 桃 红 春 上 路\n    output: 长 留 柳 绿 燕 穿 梭\n    target: 恰 逢 柳 绿 燕 回 门\n    ====================\n    src:登 高 能 望 远\n    output: 入 世 可 知 贫\n    target: 历 久 可 知 心\n    ====================\n    src:书 似 长 梯 ， 送 我 攀 登 知 识 峰 顶\n    output: 心 如 大 海 ， 看 他 俯 首 看 看 江 山\n    target: 学 如 航 船 ， 带 人 漫 步 真 理 海 洋\n    ====================\n    src:人 情 义 理 当 兼 顾\n    output: 世 事 风 流 不 可 求\n    target: 世 故 伦 常 得 互 融\n    ====================\n    src:灯 下 千 针 慈 母 线 ， 青 丝 成 皓 首\n    output: 门 前 一 片 红 尘 路 ， 绿 水 映 红 颜\n    target: 榻 边 百 馔 孝 儿 心 ， 寸 草 报 春 晖\n    ====================\n    src:星 稀 月 秀 斋 常 静\n    output: 水 远 山 高 气 自 闲\n    target: 雨 过 山 青 心 最 闲\n    ====================\n    src:右 翼 抬 头 ， 军 国 幽 灵 再 现\n    output: 中 华 圆 梦 ， 人 民 富 贵 无 疆\n    target: 小 泉 当 道 ， 东 条 魔 影 重 来\n    ====================\n    src:夜 雨 扫 凉 灯 下 影\n    output: 晨 风 吹 绿 柳 头 风\n    target: 鸡 声 啼 破 枕 边 书\n    ====================\n    src:鼓 大 河 风 ， 嵩 岳 迎 春 先 起 势\n    output: 兴 新 岁 月 ， 龙 城 焕 彩 更 扬 眉\n    target: 圆 中 国 梦 ， 愚 公 矢 志 再 移 山\n    ====================\n    src:有 意 打 工 ， 风 平 倚 树 觅 新 枝 ， 三 生 大 幸\n    output: 无 心 无 意 ， 心 系 一 生 忧 乐 在 ， 一 路 平 安\n    target: 无 辜 下 岗 ， 夜 静 凭 栏 寻 旧 梦 ， 五 味 俱 全\n    Evaluate model. Step: 29000, score: 1.921451, loss: 39.763332\n    Saving model. Step: 29100, loss: 38.770399\n    ******************************\n    src: 静 读 不 虚 兰 蕙 质\n    output: 闲 游 不 在 竹 云 风\n    target: 神 游 自 带 碧 螺 春\n    Saving model. Step: 29200, loss: 38.861765\n    ******************************\n    src: 赏 月 同 喝 团 圆 酒\n    output: 观 春 共 饮 大 一 年\n    target: 迎 春 共 毓 统 一 花\n    Saving model. Step: 29300, loss: 39.182710\n    ******************************\n    src: 人 行 中 路 月 生 海\n    output: 我 立 天 篱 风 上 天\n    target: 松 拂 疏 窗 竹 映 阑\n    Saving model. Step: 29400, loss: 38.583780\n    ******************************\n    src: 掬 水 天 人 真 合 一\n    output: 寻 杯 月 色 好 同 双\n    target: 举 杯 月 我 恰 成 三\n    Saving model. Step: 29500, loss: 38.896584\n    ******************************\n    src: 听 沽 水 新 声 ， 报 萃 精 华 ， 雅 士 咸 集 吟 好 韵\n    output: 喜 江 门 大 苑 ， 情 联 雅 事 ， 新 风 更 续 展 中 风\n    target: 看 津 门 联 苑 ， 楹 传 喜 庆 ， 高 阶 再 迈 振 雄 风\n    Saving model. Step: 29600, loss: 39.248295\n    ******************************\n    src: 老 梅 据 地 争 今 古\n    output: 老 马 飞 天 戏 水 天\n    target: 一 鹤 湍 风 放 水 云\n    Saving model. Step: 29700, loss: 38.578786\n    ******************************\n    src: 若 可 知 心 成 莫 逆\n    output: 不 如 得 意 得 人 杯\n    target: 何 妨 尽 兴 到 杯 干\n    Saving model. Step: 29800, loss: 38.985366\n    ******************************\n    src: 花 红 藕 白 莲 蓬 绿\n    output: 柳 绿 茶 黄 竹 叶 青\n    target: 果 黝 松 苍 树 柢 黄\n    Saving model. Step: 29900, loss: 39.133929\n    ******************************\n    src: 半 蛋 如 舟 ， 满 载 黄 金 白 玉\n    output: 一 生 似 水 ， 一 闻 玉 黛 白 莲\n    target: 一 盘 似 镜 ， 尽 收 粉 黛 青 红\n    Saving model. Step: 30000, loss: 39.228512\n    ******************************\n    src: 言 行 一 致 称 高 尚\n    output: 心 辱 三 曾 是 大 夫\n    target: 荣 辱 不 惊 大 丈 夫\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:腾 飞 上 铁 ， 锐 意 改 革 谋 发 展 ， 勇 当 千 里 马\n    output: 奋 进 中 华 ， 和 谐 发 展 和 谐 兴 ， 更 上 一 层 楼\n    target: 和 谐 南 供 ， 安 全 送 电 保 畅 通 ， 争 做 领 头 羊\n    ====================\n    src:自 命 清 高 装 隐 士\n    output: 人 生 淡 雅 醉 骚 人\n    target: 难 能 可 贵 是 虚 心\n    ====================\n    src:三 春 桃 李 东 皇 染\n    output: 一 片 丹 心 北 斗 星\n    target: 万 里 风 云 北 极 生\n    ====================\n    src:今 朝 旖 旎 风 光 秀\n    output: 此 际 玲 珑 气 象 新\n    target: 往 昔 峥 嵘 岁 月 稠\n    ====================\n    src:金 秋 炫 彩 秋 菊 艳\n    output: 玉 宇 飞 花 春 雨 香\n    target: 丹 桂 飘 香 桂 月 明\n    ====================\n    src:眼 前 漫 瞩 ， 惟 灵 气 所 钟 ， 神 仙 所 宅\n    output: 心 底 清 清 ， 有 风 云 不 涸 ， 天 地 之 灵\n    target: 楼 上 长 思 ， 是 朱 公 之 道 ， 项 子 之 心\n    ====================\n    src:一 炉 香 来 皆 蓬 岛\n    output: 三 月 月 出 是 仙 宫\n    target: 三 尺 剑 去 尽 葛 藤\n    Evaluate model. Step: 30000, score: 2.074309, loss: 39.228512\n    Saving model. Step: 30100, loss: 39.111912\n    ******************************\n    src: 江 南 联 友 ， 期 常 江 常 过 长 江 ， 常 讲 讲 对 联 常 识\n    output: 天 上 人 情 ， 看 天 山 而 游 远 海 ， 更 吟 听 诗 韵 长 来\n    target: 岭 北 道 家 ， 隐 远 岭 远 来 苑 岭 ， 远 聆 聆 同 道 远 谋\n    Saving model. Step: 30200, loss: 39.609224\n    ******************************\n    src: 一 湾 绿 水 渔 村 小\n    output: 满 里 青 山 日 祖 宽\n    target: 万 里 青 山 佛 寺 幽\n    Saving model. Step: 30300, loss: 38.878098\n    ******************************\n    src: 做 事 莫 嫌 难 ， 天 下 无 难 事\n    output: 为 民 须 有 爱 ， 世 间 有 少 心\n    target: 为 人 应 有 善 ， 世 间 多 善 人\n    Saving model. Step: 30400, loss: 38.445125\n    ******************************\n    src: 陇 中 名 邑 ， 山 卧 平 湖 峰 积 翠\n    output: 海 外 仙 南 ， 水 流 古 水 水 流 红\n    target: 塞 上 江 南 ， 风 流 曲 径 院 飞 红\n    Saving model. Step: 30500, loss: 38.229547\n    ******************************\n    src: 侬 本 多 情 ， 不 借 梅 花 不 借 月\n    output: 我 能 有 慨 ， 且 凭 流 水 不 流 风\n    target: 胸 怀 慷 慨 ， 但 随 逝 水 但 随 风\n    Saving model. Step: 30600, loss: 38.830921\n    ******************************\n    src: 反 哺 农 乡 ， 开 千 秋 惠 政\n    output: 欢 谐 社 会 ， 唱 万 代 新 明\n    target: 和 谐 社 会 ， 启 万 代 文 明\n    Saving model. Step: 30700, loss: 38.880846\n    ******************************\n    src: 茶 香 秋 梦 后\n    output: 月 雅 月 杯 中\n    target: 诗 韵 酒 酣 时\n    Saving model. Step: 30800, loss: 38.733662\n    ******************************\n    src: 壬 子 功 勋 ， 芝 云 齐 绚 彩\n    output: 中 春 事 业 ， 桃 树 更 辉 阳\n    target: 三 民 伟 业 ， 葵 日 共 朝 阳\n    Saving model. Step: 30900, loss: 38.047046\n    ******************************\n    src: 因 古 诗 享 誉 ， 飘 香 万 里 杜 康 酒\n    output: 为 天 联 添 香 ， 焕 愧 千 秋 锦 甸 城\n    target: 以 佳 话 留 名 ， 不 老 千 年 伊 洛 河\n    Saving model. Step: 31000, loss: 39.017662\n    ******************************\n    src: 大 雪 飞 ， 萧 风 瑟 ， 本 该 对 句 难 出 手\n    output: 春 心 夜 ， 明 情 深 ， 心 愿 人 乐 总 关 心\n    target: 良 宵 短 ， 友 情 长 ， 但 求 欢 娱 总 开 心\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:动 车 穿 洞 过 桥 ， 瑶 寨 迎 来 千 里 马\n    output: 举 步 赏 春 去 景 ， 春 风 吹 到 九 州 花\n    target: 河 水 上 坡 越 岭 ， 山 民 饮 到 幸 福 泉\n    ====================\n    src:一 纸 春 诗 梅 作 序\n    output: 半 帘 秋 月 柳 弹 琴\n    target: 两 束 艳 朵 骨 存 心\n    ====================\n    src:孝 逾 陈 妇 ， 义 抗 桓 嫠 ， 勤 踵 敬 姜 ， 严 符 陶 母\n    output: 德 耀 古 今 ， 德 昭 日 月 ， 崇 高 尚 德 ， 永 祀 英 才\n    target: 乐 献 飞 琼 ， 尊 开 少 府 ， 彩 娱 菜 子 ， 文 愧 震 川\n    ====================\n    src:砍 头 未 惧 英 雄 胆\n    output: 举 步 皆 为 霸 主 鞭\n    target: 诛 寇 常 挥 正 义 师\n    ====================\n    src:聚 会\n    output: 开 怀\n    target: 悲 欢\n    ====================\n    src:民 富 国 强 ， 国 泰 国 安 迎 国 庆\n    output: 人 和 地 利 ， 民 安 物 阜 颂 中 华\n    target: 人 勤 春 早 ， 春 风 春 雨 闹 春 耕\n    ====================\n    src:力 守 边 关 ， 稽 古 屈 功 推 李 广\n    output: 功 垂 天 地 ， 扬 帆 破 浪 驾 龙 腾\n    target: 情 牵 天 下 ， 凭 谁 设 榻 效 陈 蕃\n    ====================\n    src:妙 句 出 炉 须 百 炼\n    output: 清 风 明 月 是 三 更\n    target: 名 星 耀 眼 只 一 春\n    ====================\n    src:白 城 描 绘 上 河 景\n    output: 赤 县 腾 飞 中 国 龙\n    target: 红 日 施 加 如 意 章\n    ====================\n    src:梦 回 魂 已 断\n    output: 心 醉 泪 难 潸\n    target: 酒 醒 脑 犹 昏\n    ====================\n    src:千 年 寺 庙 千 年 塔\n    output: 万 里 江 山 万 里 天\n    target: 万 代 江 山 万 代 人\n    ====================\n    src:走 马 看 花 牵 走 狗\n    output: 飞 花 落 雁 落 空 花\n    target: 偷 天 换 日 扯 偷 心\n    ====================\n    src:狗 皮 膏 药 牛 皮 癣\n    output: 狗 仔 哈 皮 狗 蛋 皮\n    target: 人 血 馒 头 猪 血 汤\n    ====================\n    src:步 马 后 尘 威 不 减\n    output: 望 人 前 路 路 难 寻\n    target: 趋 羊 头 领 劲 更 足\n    Evaluate model. Step: 31000, score: 1.865657, loss: 39.017662\n    Saving model. Step: 31100, loss: 39.286089\n    ******************************\n    src: 难 言 之 隐 有 毛 病\n    output: 不 解 不 然 无 古 心\n    target: 不 打 自 招 说 冒 牌\n    Saving model. Step: 31200, loss: 37.925992\n    ******************************\n    src: 生 机 妙 发\n    output: 大 气 真 工\n    target: 意 趣 天 成\n    Saving model. Step: 31300, loss: 38.635538\n    ******************************\n    src: 交 易 中 却 无 市 气\n    output: 人 谈 后 有 有 人 声\n    target: 笑 谈 里 还 带 书 香\n    Saving model. Step: 31400, loss: 38.790219\n    ******************************\n    src: 道 相 当 何 见 于 人 ？ 维 大 巧 曰 亏 、 虚 明 自 照\n    output: 道 为 尽 不 来 不 法 ， 是 无 心 、 悟 、 不 不 何 空\n    target: 行 不 得 返 求 诸 己 ， 若 此 心 可 验 、 彼 境 而 知\n    Saving model. Step: 31500, loss: 39.419196\n    ******************************\n    src: 颇 知 人 ， 颇 知 我 ， 亲 疏 莫 论\n    output: 不 听 墨 ， 不 听 琴 ， 书 淡 无 歌\n    target: 闲 弄 墨 ， 闲 弄 琴 ， 淡 雅 如 初\n    Saving model. Step: 31600, loss: 38.959037\n    ******************************\n    src: 欢 声 笑 语 开 心 到\n    output: 笑 意 联 新 入 气 来\n    target: 辞 旧 迎 春 财 富 来\n    Saving model. Step: 31700, loss: 38.760586\n    ******************************\n    src: 沧 海 入 怀 消 块 垒\n    output: 风 风 入 地 落 风 埃\n    target: 劲 风 扫 地 荡 尘 嚣\n    Saving model. Step: 31800, loss: 38.641555\n    ******************************\n    src: 庚 辰 兴 旺 ， 龙 魂 披 彩 普 天 庆\n    output: 巳 亥 欢 平 ， 国 岁 腾 春 万 地 歌\n    target: 辛 巳 升 平 ， 蛇 序 迎 新 遍 地 春\n    Saving model. Step: 31900, loss: 38.465037\n    ******************************\n    src: 不 忘 悯 农 诗 ， 便 是 感 恩 心 一 片\n    output: 不 知 成 苦 读 ， 何 妨 无 命 意 千 条\n    target: 未 经 吃 苦 事 ， 何 来 求 学 路 千 条\n    Saving model. Step: 32000, loss: 38.666858\n    ******************************\n    src: 景 深 孚 甲 含 胎 际\n    output: 心 暖 人 家 到 阜 中\n    target: 春 在 人 心 物 性 间\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:和 谐 兴 旺 ， 党 引 春 风 添 虎 翼\n    output: 科 学 发 展 ， 民 臻 大 业 振 龙 威\n    target: 富 裕 平 安 ， 国 描 特 色 绘 龙 图\n    ====================\n    src:细 雨 流 光 ， 孕 石 涧 春 芽 ， 竹 影 松 风 菊 露 ， 听 涛 阁 上 回 桥 月\n    output: 清 风 拂 面 ， 听 晨 钟 暮 鼓 ， 钟 声 声 响 响 声 ， 醉 客 心 中 醉 梦 人\n    target: 苍 台 弃 履 ， 调 犀 杯 酒 色 ， 梅 妻 鹤 子 鸥 盟 ， 洗 砚 池 头 浸 墨 花\n    ====================\n    src:楼 阁 淡 疏 烟 ， 炉 中 百 和 添 香 兽\n    output: 山 林 开 画 境 ， 画 里 千 姿 映 画 屏\n    target: 河 汉 湛 秋 碧 ， 云 天 万 里 看 归 鸿\n    ====================\n    src:一 树 绿 到 半 天 顶\n    output: 万 里 红 于 万 里 春\n    target: 万 山 青 过 双 溪 头\n    ====================\n    src:居 有 楼 ， 行 有 车 ， 户 户 微 机 ， 网 上 喜 开 致 富 路\n    output: 人 无 私 ， 事 无 畏 ， 无 非 事 业 ， 人 间 喜 做 大 家 庭\n    target: 农 无 税 ， 学 无 费 ， 家 家 医 保 ， 梦 中 笑 醒 种 田 人\n    ====================\n    src:秀 食 驰 名 三 姐 店\n    output: 金 樽 饮 酒 一 杯 茶\n    target: 香 厨 享 誉 五 星 衔\n    ====================\n    src:将 勤 补 拙 ， 开 卷 观 兴 替\n    output: 以 德 为 邻 ， 举 杯 敬 古 今\n    target: 以 智 化 愚 ， 挥 毫 写 乐 忧\n    ====================\n    src:治\n    output: 收\n    target: 戡\n    ====================\n    src:诗 情 画 意 ， 墨 客 笔 端 生 妙 趣\n    output: 诗 赋 诗 词 ， 诗 词 酒 里 赋 新 诗\n    target: 民 心 乡 俗 ， 作 家 书 简 尽 淋 漓\n    ====================\n    src:拜 佛 何 须 来 寺 庙\n    output: 出 门 不 必 上 天 堂\n    target: 打 坐 当 然 要 空 心\n    ====================\n    src:奉 使 纪 浮 楂 ， 早 遣 天 骄 识 麟 凤\n    output: 传 承 传 世 泽 ， 长 教 大 地 起 鲲 鹏\n    target: 抡 才 忆 围 棘 ， 曾 同 伯 乐 辨 骊 黄\n    ====================\n    src:冬 日 观 灯 雪 妖 娆\n    output: 春 风 拂 柳 柳 婆 娑\n    target: 春 夜 听 歌 雨 缠 绵\n    ====================\n    src:溪 畔 流 光 ， 缀 得 千 花 秀\n    output: 山 中 吐 瑞 ， 迎 来 万 象 新\n    target: 枕 边 云 气 ， 梦 登 九 宇 烟\n    ====================\n    src:节 约 办 婚 事 ， 亲 友 皆 欢 喜\n    output: 和 谐 共 和 谐 ， 和 谐 共 和 谐\n    target: 勤 俭 建 家 园 ， 夫 妻 更 和 睦\n    ====================\n    src:梅 心 惊 破 春 情 意\n    output: 柳 眼 羞 成 夏 意 思\n    target: 桂 影 筛 出 月 横 波\n    Evaluate model. Step: 32000, score: 1.974521, loss: 38.666858\n    Saving model. Step: 32100, loss: 38.381612\n    ******************************\n    src: 梦 醒 春 声 脆\n    output: 风 宽 月 子 斜\n    target: 心 怡 燕 影 娇\n    Saving model. Step: 32200, loss: 38.457991\n    ******************************\n    src: 桂 花 松 子 有 仙 意\n    output: 梅 石 松 山 无 老 心\n    target: 葛 岭 孤 山 无 俗 人\n    Saving model. Step: 32300, loss: 38.189169\n    ******************************\n    src: 一 纸 秋 声 无 落 处\n    output: 几 般 月 意 有 愁 时\n    target: 千 般 枉 顾 是 当 时\n    Saving model. Step: 32400, loss: 38.737712\n    ******************************\n    src: 眠 花 宿 柳 登 徒 子\n    output: 落 月 飞 风 醉 子 思\n    target: 正 坐 危 襟 惠 相 公\n    Saving model. Step: 32500, loss: 38.937122\n    ******************************\n    src: 刚 日 读 经 ， 柔 日 读 史\n    output: 春 州 作 诗 ， 妙 学 赋 诗\n    target: 神 至 赋 笔 ， 兴 至 赋 诗\n    Saving model. Step: 32600, loss: 38.967441\n    ******************************\n    src: 程 门 立 雪 求 学 问\n    output: 学 业 兴 学 要 文 西\n    target: 隆 中 谋 策 望 疆 图\n    Saving model. Step: 32700, loss: 38.481725\n    ******************************\n    src: 济 世 有 怀 ， 蜚 流 今 雪\n    output: 兴 人 无 语 ， 不 表 后 垂\n    target: 诲 人 不 倦 ， 师 范 永 存\n    Saving model. Step: 32800, loss: 38.711494\n    ******************************\n    src: 月 入 深 山 问 碧 溪 ， 飞 花 何 去\n    output: 风 飘 古 室 一 清 豆 ， 醉 水 自 来\n    target: 香 飘 陋 室 盈 红 袖 ， 流 韵 自 来\n    Saving model. Step: 32900, loss: 38.790181\n    ******************************\n    src: 山 川 四 望 是\n    output: 天 地 一 人 春\n    target: 天 地 一 壶 通\n    Saving model. Step: 33000, loss: 37.796535\n    ******************************\n    src: 秋 实 春 华 成 果 硕\n    output: 冬 和 国 盛 有 延 长\n    target: 家 兴 业 旺 福 源 长\n    INFO:tensorflow:Restoring parameters from ./models/output_couplet/model.ckpl\n    ====================\n    src:君 德 臣 贤 安 世 道\n    output: 人 文 德 德 振 家 声\n    target: 父 慈 子 孝 乐 天 伦\n    ====================\n    src:地 藏 寺 八 百 年 旧 迹 犹 存 ， 誓 度 众 生 、 誓 成 正 觉\n    output: 佛 法 寺 千 万 佛 慈 慈 普 济 ， 普 济 众 生 、 普 济 众 生\n    target: 昆 明 市 十 万 人 偕 乐 之 所 ， 如 游 化 雨 、 如 登 春 台\n    ====================\n    src:冬 泳 冷 观 众\n    output: 春 归 大 有 年\n    target: 北 漂 红 艺 人\n    ====================\n    src:左 派 右 倾 谈 政 治\n    output: 东 来 西 去 卖 东 西\n    target: 贫 僧 老 道 讲 德 行\n    ====================\n    src:字 湛 辞 新 ， 光 华 焕 发\n    output: 联 精 作 对 ， 联 语 抒 情\n    target: 行 芳 志 馥 ， 才 德 兼 长\n    ====================\n    src:雨 霁 阅 芙 蓉 ， 一 帘 至 美 水 天 画\n    output: 风 清 吟 柳 絮 ， 万 里 飘 香 风 月 诗\n    target: 花 香 开 丽 景 ， 八 面 尚 真 云 月 心\n    ====================\n    src:心 无 小 我 装 天 下\n    output: 胸 有 灵 犀 贯 古 今\n    target: 志 有 黎 民 载 永 生\n    ====================\n    src:命 由 天 定\n    output: 德 自 自 修\n    target: 事 在 人 为\n    ====================\n    src:诲 教 者 当 先 揽 镜\n    output: 为 人 人 以 慎 修 身\n    target: 简 言 之 趁 早 磨 刀\n    ====================\n    src:天 上 人 间 ， 无 猜 花 月 两 相 恨\n    output: 世 间 世 界 ， 不 尽 风 云 一 世 情\n    target: 古 往 今 来 ， 有 幸 芝 兰 交 错 结\n    ====================\n    src:玄 鸟 呈 祥 ， 封 商 兆 瑞\n    output: 吉 羊 献 瑞 ， 福 寿 延 年\n    target: 景 山 钟 秀 ， 迁 祖 荷 庥\n    Evaluate model. Step: 33000, score: 1.979254, loss: 37.796535\n    Saving model. Step: 33100, loss: 38.390072\n    ******************************\n    src: 风 移 金 谷 屋\n    output: 月 舞 玉 花 符\n    target: 乔 木 好 音 多\n    Saving model. Step: 33200, loss: 38.409471\n    ******************************\n    src: 草 秀 芳 花 艳\n    output: 风 深 翠 语 香\n    target: 林 深 鸟 语 甜\n    Saving model. Step: 33300, loss: 39.280556\n    ******************************\n    src: 孝 犹 感 物 竟 连 栗\n    output: 德 可 无 风 不 染 松\n    target: 德 自 生 香 不 独 芝\n    Saving model. Step: 33400, loss: 38.996744\n    ******************************\n    src: 雁 阵 横 排 ， 惹 动 思 乡 意\n    output: 莺 帆 漫 览 ， 飞 来 动 我 魂\n    target: 云 团 纵 起 ， 操 劳 了 梦 情\n    Saving model. Step: 33500, loss: 38.862225\n    ******************************\n    src: 曾 于 太 白 峰 前 住\n    output: 不 是 青 风 笔 里 来\n    target: 都 是 春 风 画 里 人\n    Saving model. Step: 33600, loss: 38.234724\n    ******************************\n    src: 诚 字 描 红 ， 圣 地 千 行 兴 骏 业\n    output: 春 诚 载 史 ， 春 黄 万 里 绘 春 风\n    target: 信 篇 写 灿 ， 炎 陵 万 户 沐 春 光\n    Saving model. Step: 33700, loss: 37.715899\n    ******************************\n    src: 玉 韫 珠 怀 ， 山 川 辉 媚\n    output: 龙 浆 玉 馥 ， 日 样 芬 神\n    target: 琼 滋 芝 秀 ， 花 草 精 神\n    Saving model. Step: 33800, loss: 38.911881\n    ******************************\n    src: 一 段 风 流 谁 似 尔\n    output: 三 分 寂 意 我 如 君\n    target: 十 年 诗 酒 爱 逢 君\n    \n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/2NLP_Basics2/lesson3/Chinese_text_analysis_and_processing/","content":"\n# 中文文本基本任务与处理\n\n\n### 1.分词\n对于中文和日文这样的特殊亚洲语系文本而言，字和字之间是紧密相连的，单纯从文本形态上无法区分具备独立含义的词（拉丁语系纯天然由空格分隔不同的word），而不同的词以不同的方式排布，可以表达不同的内容和情感，因此在很多中文任务中，我们需要做的第一个处理叫做**分词**。\n\n这是一个非常基础的功能，但是会较大程度影响下游任务(机器翻译、情感分析、文本理解)的效果。\n\n目前主流的分词方法主要是**基于词典匹配的分词方法(正向最大匹配法、逆向最大匹配法和双向匹配分词法等)**和**基于统计的分词方法(HMM、CRF、和深度学习)**；主流的分词工具库包括 中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库等。\n\n关于分词方法和工具库更多内容可以参考知乎讨论[有哪些比较好的中文分词方案](https://www.zhihu.com/question/19578687)\n\n![](../img/L3_word_seg.png)\n\n### 2.停用词与N-gram\n\n\n##### 停用词\n> 在自然语言处理的很多任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对统计方法带来一些干扰，我们把这类词叫做**停用词**，通常我们会用一个停用词表把它们过滤出来。比如英语当中的**定冠词/不定冠词**(a,an,the等)。\n\n> 中文当中常用到的停用词词表可以参见[中文常用停用词表](https://github.com/goto456/stopwords)\n\n>关于机器学习中停用词的产出与收集方法，大家可以参见知乎讨论[机器学习中如何收集停用词](https://www.zhihu.com/question/34939177)\n\n##### N-gram\n>N-gram在中文中叫做n元语法，指文本中连续出现的n个语词。n元语法模型是基于(n-1)阶马尔可夫链的一种概率语言模型，通过n个语词出现的概率来推断语句的结构。关于语言模型的更多内容，我们在后续的课程会详细提到。\n\n>大家可以简单理解成N-gram是N个词条组成的n元组，也可以阅读[自然语言处理中N-Gram模型介绍](https://zhuanlan.zhihu.com/p/32829048)了解更多内容。\n\n### 3.更多任务(词性标注、依赖分析、NER、关键词抽取)\n\n\n##### 词性标注\n> 词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n\n> 词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。\n\n> 词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。\n\n![](../img/L3_pos.png)\n\n|代码|名称|说明|举例| \n| :------ | :------ | :------ | :------ | \n| a | 形容词 | 取英语形容词adjective的第1个字母 | 最/d 大/a 的/u| \n| ad | 副形词 | 直接作状语的形容词.形容词代码a和副词代码d并在一起 | 一定/d 能够/v 顺利/ad 实现/v 。/w| \n| ag | 形语素 | 形容词性语素。形容词代码为a，语素代码ｇ前面置以a | 喜/v 煞/ag 人/n| \n| an | 名形词 | 具有名词功能的形容词。形容词代码a和名词代码n并在一起 | 人民/n 的/u 根本/a 利益/n 和/c 国家/n 的/u 安稳/an 。/w| \n| b | 区别词 | 取汉字“别”的声母 | 副/b 书记/n 王/nr 思齐/nr| \n| c | 连词 | 取英语连词conjunction的第1个字母 | 全军/n 和/c 武警/n 先进/a 典型/n 代表/n| \n| d | 副词 | 取adverb的第2个字母，因其第1个字母已用于形容词 | 两侧/f 台柱/n 上/ 分别/d 雄踞/v 着/u| \n| dg | 副语素 |  副词性语素。副词代码为d，语素代码ｇ前面置以d | 用/v 不/d 甚/dg 流利/a 的/u 中文/nz 主持/v 节目/n 。/w| \n| e | 叹词 | 取英语叹词exclamation的第1个字母 | 嗬/e ！/w| \n| f | 方位词 | 取汉字“方” 的声母 | 从/p 一/m 大/a 堆/q 档案/n 中/f 发现/v 了/u| \n| g | 语素 | 绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母 | 例如dg 或ag| \n| h | 前接成分 | 取英语head的第1个字母 | 目前/t 各种/r 非/h 合作制/n 的/u 农产品/n| \n| i | 成语 | 取英语成语idiom的第1个字母 | 提高/v 农民/n 讨价还价/i 的/u 能力/n 。/w| \n| j | 简称略语 | 取汉字“简”的声母 | 民主/ad 选举/v 村委会/j 的/u 工作/vn| \n| k | 后接成分 |   | 权责/n 明确/a 的/u 逐级/d 授权/v 制/k| \n| l | 习用语 | 习用语尚未成为成语，有点“临时性”，取“临”的声母 | 是/v 建立/v 社会主义/n 市场经济/n 体制/n 的/u 重要/a 组成部分/l 。/w| \n| m | 数词 | 取英语numeral的第3个字母，n，u已有他用 | 科学技术/n 是/v 第一/m 生产力/n| \n| n | 名词 | 取英语名词noun的第1个字母 | 希望/v 双方/n 在/p 市政/n 规划/vn| \n| ng | 名语素 | 名词性语素。名词代码为n，语素代码ｇ前面置以n | 就此/d 分析/v 时/Ng 认为/v| \n| nr | 人名 | 名词代码n和“人(ren)”的声母并在一起 | 建设部/nt 部长/n 侯/nr 捷/nr| \n| ns | 地名 | 名词代码n和处所词代码s并在一起 | 北京/ns 经济/n 运行/vn 态势/n 喜人/a| \n| nt | 机构团体 | “团”的声母为t，名词代码n和t并在一起 | [冶金/n 工业部/n 洛阳/ns 耐火材料/l 研究院/n]nt| \n| nx | 字母专名 |   | ＡＴＭ/nx 交换机/n| \n| nz | 其他专名 | “专”的声母的第1个字母为z，名词代码n和z并在一起 | 德士古/nz 公司/n| \n| o | 拟声词 | 取英语拟声词onomatopoeia的第1个字母 | 汩汩/o 地/u 流/v 出来/v| \n| p | 介词 | 取英语介词prepositional的第1个字母 | 往/p 基层/n 跑/v 。/w| \n| q | 量词 | 取英语quantity的第1个字母 | 不止/v 一/m 次/q 地/u 听到/v ，/w| \n| r | 代词 | 取英语代词pronoun的第2个字母,因p已用于介词 | 有些/r 部门/n| \n| s | 处所词 | 取英语space的第1个字母 | 移居/v 海外/s 。/w| \n| t | 时间词 | 取英语time的第1个字母 | 当前/t 经济/n 社会/n 情况/n| \n| tg | 时语素 | 时间词性语素。时间词代码为t,在语素的代码g前面置以t | 秋/Tg 冬/tg 连/d 旱/a| \n| u | 助词 | 取英语助词auxiliary 的第2个字母,因a已用于形容词 | 工作/vn 的/u 政策/n| \n| ud | 结构助词 |   | 有/v 心/n 栽/v 得/ud 梧桐树/n| \n| ug | 时态助词 |   | 你/r 想/v 过/ug 没有/v| \n| uj | 结构助词的 |   | 迈向/v 充满/v 希望/n 的/uj 新/a 世纪/n| \n| ul | 时态助词了 |   | 完成/v 了/ ul| \n| uv | 结构助词地 |   | 满怀信心/l 地/uv 开创/v 新/a 的/u 业绩/n| \n| uz | 时态助词着 |   | 眼看/v 着/uz| \n| v | 动词 |   | 举行/v 老/a 干部/n 迎春/vn 团拜会/n| \n| vd | 副动词 |   | 强调/vd 指出/v| \n| vg | 动语素 | 动词性语素。动词代码为v。在语素的代码g前面置以V | 做好/v 尊/vg 干/j 爱/v 兵/n 工作/vn| \n| vn | 名动词 |  指具有名词功能的动词。动词和名词的代码并在一起 | 股份制/n 这种/r 企业/n 组织/vn 形式/n ，/w| \n| w | 标点符号 |   | 生产/v 的/u ５Ｇ/nx 、/w ８Ｇ/nx 型/k 燃气/n 热水器/n| \n| x | 非语素字 | 非语素字只是一个符号，字母x通常用于代表未知数、符号 |  | \n| y | 语气词 | 取汉字“语”的声母 | 已经/d ３０/m 多/m 年/q 了/y 。/w| \n| z | 状态词 | 取汉字“状”的声母的前一个字母 | 势头/n 依然/z 强劲/a ；/w| \n\n##### 句法依存分析\n在很多复杂的NLP问题中，我们还需要完成句法分析的任务，更具体一点说，需要**确定句子的句法结构**，**确定句子中各词之间的依存关系。**下图为Stanford nlp parser解析得到的结果(对分完词后的句子)\n\n![](../img/L3_syntax.png)\n\n##### 命名实体识别\n命名实体识别（Named Entity Recognition, NER）是从一段非结构化文本中找出相关实体（triplet中的主词和宾词），并标注出其位置以及类型，它是NLP领域中一些复杂任务（如**关系抽取**、**信息检索**、**知识问答**、**知识图谱**等）的基础。\n![](../img/L3_ner.png)\n\n##### 关键词抽取\n文本关键词抽取，是对文本信息进行高度凝练的一种有效手段，通过3-5个词语准确概括文本的主题，帮助读者快速理解文本信息。是文本检索、文本摘要等许多下游文本挖掘任务的基础性和必要性的工作。\n![](../img/L3_keyword.png)\n\n### 4.jieba工具库使用\n\n\n#### a.基本分词函数与用法\njieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)\n\n**jieba.cut** 方法接受三个输入参数: \n* 需要分词的字符串\n* cut_all 参数用来控制是否采用全模式\n* HMM 参数用来控制是否使用 HMM 模型\n\n\n**jieba.cut_for_search** 方法接受两个参数\n* 需要分词的字符串\n* 是否使用 HMM 模型。\n\n该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细\n\n\n```python\n# encoding=utf-8\nimport jieba\n\nseg_list = jieba.cut(\"我在网易云课堂学习自然语言处理\", cut_all=True)\nprint(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n\nseg_list = jieba.cut(\"我在网易云课堂学习自然语言处理\", cut_all=False)\nprint(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n\nseg_list = jieba.cut(\"他毕业于北京航空航天大学，在百度深度学习研究院进行研究\")  # 默认是精确模式\nprint(\", \".join(seg_list))\n\nseg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在斯坦福大学深造\")  # 搜索引擎模式\nprint(\", \".join(seg_list))\n```\n\n    Building prefix dict from the default dictionary ...\n    Dumping model to file cache /tmp/jieba.cache\n    Loading model cost 1.774 seconds.\n    Prefix dict has been built succesfully.\n    \n\n    Full Mode: 我/ 在/ 网易/ 云/ 课堂/ 学习/ 自然/ 自然语言/ 语言/ 处理\n    Default Mode: 我/ 在/ 网易/ 云/ 课堂/ 学习/ 自然语言/ 处理\n    他, 毕业, 于, 北京航空航天大学, ，, 在, 百度, 深度, 学习, 研究院, 进行, 研究\n    小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 福大, 大学, 斯坦福, 斯坦福大学, 深造\n    \n\n可以使用**jieba.lcut**以及**jieba.lcut_for_search**直接返回 list\n\n\n```python\nprint(jieba.lcut(\"我在网易云课堂学习自然语言处理\"))\nprint(jieba.lcut_for_search(\"小明硕士毕业于中国科学院计算所，后在斯坦福大学深造\"))\n```\n\n    ['我', '在', '网易', '云', '课堂', '学习', '自然语言', '处理']\n    ['小明', '硕士', '毕业', '于', '中国', '科学', '学院', '科学院', '中国科学院', '计算', '计算所', '，', '后', '在', '福大', '大学', '斯坦福', '斯坦福大学', '深造']\n    \n\n**添加用户自定义字典**\n\n很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。\n* 1.可以用jieba.load_userdict(file_name)加载用户字典\n* 2.少量的词汇可以自己用下面方法手动添加：\n    * 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典\n    * 用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。\n\n\n```python\nprint('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))\n```\n\n    如果/放到/旧/字典/中将/出错/。\n    \n\n\n```python\njieba.suggest_freq(('中', '将'), True)\nprint('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))\n```\n\n    如果/放到/旧/字典/中/将/出错/。\n    \n\n#### b.词性标注\n\n\n```python\nimport jieba.posseg as pseg\nwords = pseg.cut(\"我在网易云课堂学习自然语言处理\")\nfor word, flag in words:\n    print('%s %s' % (word, flag))\n```\n\n    我 r\n    在 p\n    网易 n\n    云 ns\n    课堂 n\n    学习 v\n    自然语言 l\n    处理 v\n    \n\n#### c.关键词抽取\n- **基于 TF-IDF 算法的关键词抽取**\n    - import jieba.analyse\n\n    * jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n        * sentence 为待提取的文本\n        * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n        * withWeight 为是否一并返回关键词权重值，默认值为 False\n        * allowPOS 仅包括指定词性的词，默认值为空，即不筛选\n\n\n```python\nimport jieba.analyse as analyse\nlines = open('data/NBA.txt', encoding='utf8').read()\nprint(\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))\n```\n\n    韦少  杜兰特  全明星  全明星赛  MVP  威少  正赛  科尔  投篮  勇士  球员  斯布鲁克  更衣柜  NBA  三连庄  张卫平  西部  指导  雷霆  明星队\n    \n\n\n```python\nlines = open('data/novel.txt', encoding='utf8').read()\nprint(\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))\n```\n\n    行者  八戒  师父  三藏  唐僧  大圣  沙僧  妖精  菩萨  和尚  那怪  那里  长老  呆子  徒弟  怎么  不知  老孙  国王  一个\n    \n\n* **关于TF-IDF 算法的关键词抽取补充**\n    * **阅读材料:[TF-IDF与关键词提取](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)**\n    * 关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径\n\n        * 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径\n            * 自定义语料库示例见[这里](https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big)\n            * 用法示例见[这里](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py)\n        * 关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径\n            * 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径\n            * 自定义语料库示例见[这里](https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt)\n            * 用法示例见[这里](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py)\n\n    * 关键词一并返回关键词权重值示例\n        * 用法示例见[这里](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py)\n\n* **基于 TextRank 算法的关键词抽取**\n    * jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n    * jieba.analyse.TextRank() 新建自定义 TextRank 实例\n\n* 算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n\n* 基本思想:\n    * 将待抽取关键词的文本进行分词\n    * 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n    * 计算图中节点的PageRank，注意是无向带权图\n* 阅读资料：\n    * [基于textrank的关键词抽取方法](https://blog.csdn.net/zhangf666/article/details/77841845)\n    * [pagerank算法核心思想](https://www.jianshu.com/p/f6d66ab97332)\n    * [浅析PageRank算法](http://blog.codinglabs.org/articles/intro-to-pagerank.html)\n\n\n```python\nimport jieba.analyse as analyse\nlines = open('data/NBA.txt', encoding='utf8').read()\nprint(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\nprint(\"---------------------我是分割线----------------\")\nprint(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n'))))\n```\n\n    全明星赛  勇士  正赛  指导  对方  投篮  球员  没有  出现  时间  威少  认为  看来  结果  相隔  助攻  现场  三连庄  介绍  嘉宾\n    ---------------------我是分割线----------------\n    勇士  正赛  全明星赛  指导  投篮  玩命  时间  对方  现场  结果  球员  嘉宾  时候  全队  主持人  全程  大伙  肥皂剧  照片  目标\n    \n\n\n```python\nlines = open('data/novel.txt', encoding='utf8').read()\nprint(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\n```\n\n    行者  师父  八戒  三藏  大圣  不知  菩萨  妖精  只见  长老  国王  却说  呆子  徒弟  小妖  出来  不得  不见  不能  师徒\n    \n\n### 5.python中文文本分析与可视化\n\n\n\n```python\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport jieba    #分词包\nimport numpy    #numpy计算包\nimport codecs   #codecs提供的open方法来指定打开的文件的语言编码，它会在读取的时候自动转换为内部unicode \nimport pandas as pd  \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\nfrom wordcloud import WordCloud#词云包\n```\n\n##### 读取数据\n\n\n```python\n# pandas读取数据\ndf = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8').dropna()\n# 转成list\ncontent=df[\"content\"].values.tolist()\n# 分词与统计词频\nsegment=[]\nfor line in content:\n    try:\n        segs=jieba.lcut(line)\n        for seg in segs:\n            if len(seg)>1 and seg!='\\r\\n':\n                segment.append(seg)\n    except:\n        print(line)\n        continue\n```\n\n##### 去停用词\n\n\n```python\nwords_df=pd.DataFrame({'segment':segment})\nstopwords=pd.read_csv(\"data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')#quoting=3全不引用\nwords_df=words_df[~words_df.segment.isin(stopwords.stopword)]\n```\n\n##### 统计排序\n\n\n```python\nwords_stat=words_df.groupby(by=['segment'])['segment'].agg({\"计数\":numpy.size})\nwords_stat=words_stat.reset_index().sort_values(by=[\"计数\"],ascending=False)\nwords_stat.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment</th>\n      <th>计数</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>60810</th>\n      <td>电影</td>\n      <td>10230</td>\n    </tr>\n    <tr>\n      <th>73264</th>\n      <td>观众</td>\n      <td>5574</td>\n    </tr>\n    <tr>\n      <th>8615</th>\n      <td>中国</td>\n      <td>5476</td>\n    </tr>\n    <tr>\n      <th>70480</th>\n      <td>节目</td>\n      <td>4398</td>\n    </tr>\n    <tr>\n      <th>33622</th>\n      <td>导演</td>\n      <td>4197</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n##### 构建词云\n\n\n```python\nmatplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\nwordcloud=WordCloud(font_path=\"data/simhei.ttf\",background_color=\"black\",max_font_size=80)\nword_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}\nwordcloud=wordcloud.fit_words(word_frequence)\nplt.imshow(wordcloud)\n```\n\n##### 自定义背景\n\n\n```python\nfrom scipy.misc import imread\nmatplotlib.rcParams['figure.figsize'] = (15.0, 15.0)\nfrom wordcloud import WordCloud,ImageColorGenerator\nbimg=imread('image/entertainment.jpeg')\nwordcloud=WordCloud(background_color=\"white\",mask=bimg,font_path='data/simhei.ttf',max_font_size=200)\nword_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}\nwordcloud=wordcloud.fit_words(word_frequence)\nbimgColors=ImageColorGenerator(bimg)\nplt.axis(\"off\")\nplt.imshow(wordcloud.recolor(color_func=bimgColors))\n```\n\n### 6.新闻关键词抽取\n\n\n#### TF-IDF\n\n\n```python\nimport jieba.analyse as analyse\nimport pandas as pd\ndf = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8').dropna()\nlines=df.content.values.tolist()\ncontent = \"\".join(lines)\nprint(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=()))\n```\n\n    ['用户', '2016', '互联网', '手机', '平台', '人工智能', '百度', '2017', '智能', '技术', '数据', '360', '服务', '直播', '产品', '企业', '安全', '视频', '移动', '应用', '网络', '行业', '游戏', '机器人', '电商', '内容', '中国', '领域', '通过', '发展']\n    \n\n\n```python\nimport jieba.analyse as analyse\nimport pandas as pd\ndf = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8').dropna()\nlines=df.content.values.tolist()\ncontent = \"\".join(lines)\nprint(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=()))\n```\n\n    ['航母', '训练', '海军', '中国', '官兵', '部队', '编队', '作战', '10', '任务', '美国', '导弹', '能力', '20', '2016', '军事', '无人机', '装备', '进行', '记者', '我们', '军队', '安全', '保障', '12', '战略', '军人', '日本', '南海', '战机']\n    \n\n#### TextRank\n\n\n```python\nimport jieba.analyse as analyse\nimport pandas as pd\ndf = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\ndf = df.dropna()\nlines=df.content.values.tolist()\ncontent = \"\".join(lines)\n\nprint(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')))\nprint(\"---------------------我是分割线----------------\")\nprint(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n')))\n```\n\n    ['中国', '海军', '训练', '美国', '部队', '进行', '官兵', '航母', '作战', '任务', '能力', '军事', '发展', '工作', '国家', '问题', '建设', '导弹', '编队', '记者']\n    ---------------------我是分割线----------------\n    ['中国', '海军', '美国', '部队', '官兵', '航母', '军事', '国家', '任务', '能力', '导弹', '技术', '问题', '日本', '军队', '编队', '装备', '系统', '记者', '战略']\n    \n\n![](../img/xiniu_neteasy.png)\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/6seq2seq_v2/3.seq2seq_application_step_by_step/","content":"\n# seq2seq项目说明\n\n\n### 1.seq2seq（序列到序列模型）应用\n在**聊天机器人，机器翻译，自动文摘，智能问答**等众多自然语言处理任务中都可能用到seq2seq模型，google在著名的[neural machine translation](https://arxiv.org/abs/1609.08144)中也使用过这个结构的模型(当然，现在因为效率等原因，可能不少应用项目迁移到transformer结构下了)，google在tensorflow的官方案例中给了一个[手把手训练seq2seq神经翻译系统](https://github.com/tensorflow/nmt)的github项目，下面我们就官方这个项目讲解一下如何应用tensorflow训练seq2seq的应用，并尝试用这个代码实现一个自动摘要的智能AI应用(机器翻译中的seq2seq应用会有专门的章节讲到)。\n\n参考资料:\n* [neural machine translation](https://arxiv.org/abs/1609.08144)\n* [手把手训练seq2seq神经翻译系统](https://github.com/tensorflow/nmt)\n\n## 0.说明\n\ngoogle的这个教程使用高版本tensorflow（TensorFlow 1.2+）的 seq2seq API完成，该API使seq2seq模型的构建过程干净、简单、易读，主要包括以下内容：\n\n- 使用 tf.data 中最新输入的管道对动态调整的输入序列进行预处理。\n- 使用批量填充和序列长度 bucketing，提高训练速度和推理速度。\n- 使用通用结构和训练时间表训练 seq2seq 模型，包括多种注意力机制和固定抽样。\n- 使用 in-graph 集束搜索在 seq2seq 模型中进行推理。\n- 优化 seq2seq 模型，以实现在多 GPU 设置中的模型训练。\n\n## 1.引言\n\n序列到序列（seq2seq）模型（Sutskever et al., 2014, Cho et al., 2014）在机器翻译、语音识别和文本摘要等任务上取得了巨大的成功。这里的教程内容致力于帮助读者全面掌握 seq2seq 模型，并且展示了如何从头开始构建一个强大的 seq2seq 模型。以下的讲解会注重神经机器翻译（NMT）任务，神经机器翻译是 seq2seq 模型很好的试验台，并且已经获得了广泛的成功。我们使用的代码是极其轻量、高质量、可投入生产并且结合了最新研究思路的实现。我们通过以下方式实现这一目标：\n\n1. 使用最新的解码器/attention wrapper API、TensorFlow 高版本数据迭代器。\n2. 结合了我们在构建循环型和 seq2seq 型模型的专业知识。\n3. 提供了可构建最好 NMT 模型的技巧，同时还复现了谷歌的 NMT（GNMT）系统。\n\n我们相信提供所有人都很容易复制的基准是非常重要的。因此，我们基于以下公开的数据集提供了全部的试验结果和预训练模型：\n\n1. 小规模数据集：TED 演讲的英语-越南语平行语料库（133K 个句子对），该数据集由 IWSLT Evaluation Campaign 提供。\n2. 大规模数据集：德语-英语平行语料库（4.5M 个句子对），该数据集由 WMT Evaluation Campaign 提供。\n\n我们首先需要了解用于 NMT 任务的 seq2seq 模型的基本知识，并需要理解如何构建和训练一个 vanilla NMT 模型。第二部分将更进一步详细地解释如何构建带注意力机制的强大神经机器翻译模型。然后我们会讨论构建更好神经机器翻译模型（翻译速度和质量）可能的技巧，例如 TensorFlow 最好的实践方法（batching, bucketing）、双向循环神经网络和集束搜索(beam search)等。\n\n## 1.基础\n\n**关于神经机器翻译**\n\n以词组为基础的传统翻译系统将源语言句子拆分成多个词块，然后进行词对词的翻译。这使得翻译输出结果流畅性大打折扣，远远不如人类译文。我们会通读整个源语言句子、了解句子含义，然后输出翻译结果。神经机器翻译（NMT）竟然可以模仿人类的翻译过程！\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/encdec.jpg)\n\n<center>图1. 编码器-解码器结构——神经机器翻译的通用方法实例。</center>\n\n具体来说，神经机器翻译系统首先使用编码器读取源语言句子，构建一个「上下文」向量(context vector)，即代表句义的数字化向量；然后使用解码器处理该内容，并输出翻译结果，如图1所示。这就是我们通常所说的编码器-解码器结构。神经机器翻译用这种方法解决以词组为基础的传统翻译系统遇到的翻译问题：神经机器翻译能够捕捉语言中的长距离依赖结构，如词性一致、句法结构等，然后输出流利度更高的翻译结果，正如谷歌神经机器翻译系统已经做到的那样。\n\nNMT 模型在具体的结构中会发生变化。对于序列数据而言，最好的选择是循环神经网络（RNN），这也被大多数 NMT 模型采用。通常情况下，编码器和解码器都可使用循环神经网络。但是，循环神经网络模型有多种选择：\n* （a）方向性（directionality），单向或双向；\n* （b）深度，单层或多层；\n* （c）类型，通常是 vanilla RNN、长短期记忆（Long Short-term Memory，LSTM），或门控循环单元（gated recurrent unit，GRU）。\n\n感兴趣的同学可打开该网址（https://colah.github.io/posts/2015-08-Understanding-LSTMs/）， 复习一下RNN 和 LSTM 的更多信息。\n\n这个教程中，我们将以单向的深度多层 RNN（deep multi-layer RNN）为例，它使用 LSTM 作为循环单元。模型实例如图 2 所示。我们在该实例中构建了一个模型，将源语言句子「I am a student」翻译成目标语言「Je suis étudiant」。该 NMT 模型包括两个循环神经网络：编码器 RNN，在不预测的情况下将输入的源语言单词进行编码；解码器，在预测下一个单词的条件下处理目标句子。\n\n若想参考更多信息，请查看论文 Luong（2016）（https://github.com/lmthang/thesis）。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/seq2seq.jpg)\n\n<center>图2. 神经机器翻译——一个深度循环结构实例</center>\n\n上图将源语言句子「I am a student」翻译成目标语言句子「Je suis étudiant」。此处，`「<s>」`代表解码过程开始，`「</s>」`代表解码过程结束。\n\n## 2.代码准备\n\n为了安装该教程，我们需要先安装 TensorFlow。建议使用新版本的TensorFlow。为了安装 TensorFlow，请按照以下安装指导：https://www.tensorflow.org/install/。\n\n在安装 TensorFlow 之后，我们需要运行以下命令拉取这个教程的源代码：\n\n```\ngit clone https://github.com/tensorflow/nmt/\n```\n\n\n```python\n!git clone https://github.com/tensorflow/nmt/\n```\n\n    Cloning into 'nmt'...\n    remote: Enumerating objects: 1247, done.\u001b[K\n    remote: Total 1247 (delta 0), reused 0 (delta 0), pack-reused 1247\u001b[K\n    Receiving objects: 100% (1247/1247), 1.22 MiB | 464.00 KiB/s, done.\n    Resolving deltas: 100% (893/893), done.\n    Checking connectivity... done.\n    \n\n## 3.训练-构建我们第一个 NMT 系统\n\n我们首先需要了解构建一个 NMT 模型具体代码的核心，我们会在图 2 中更详细地讲解。我们后面会介绍数据准备和全部的代码，这一部分是指 model.py 文件。\n\n在网络的底层，编码器和解码器 RNN 接收到以下输入：首先是原句子，然后是从编码到解码模式的过渡边界符号`「<s>」`，最后是目标语句。对于训练来说，我们将为系统提供以下张量，它们是以时间为主（time-major）的格式，并包括了单词索引：\n\n- encoder_inputs [max_encoder_time, batch_size]：源输入词。\n- decoder_inputs [max_decoder_time, batch_size]：目标输入词。\n- decoder_outputs [max_decoder_time, batch_size]：目标输出词，这些是 decoder_inputs 按一个时间步向左移动，并且在右边有句子结束符。\n\n为了更高的效率，我们一次用多个句子（batch_size）进行训练。测试略有不同，我们会在后面讨论。\n\n### 3.1.嵌入(embedding)\n\n给定单词的分类属性，模型首先必须查找词来源和目标嵌入以检索相应的词表征。为了令该嵌入层能够运行，我们首先需要为每一种语言选定一个词汇表。通常，选定词汇表大小 V，那么频率最高的 V 个词将视为唯一的。而所有其他的词将转换并打上「unknown」标志，因此所有的词将有相同的嵌入。我们通常在训练期间嵌入权重，并且每种语言都有一套。\n\n```python\n# Embedding\nembedding_encoder = variable_scope.get_variable(\n    \"embedding_encoder\", [src_vocab_size, embedding_size], ...)# Look up embedding:#   encoder_inputs: [max_time, batch_size]#   encoder_emp_inp: [max_time, batch_size, embedding_size]\nencoder_emb_inp = embedding_ops.embedding_lookup(\n    embedding_encoder, encoder_inputs)\n```\n\n我们同样可以构建 embedding_decoder 和 decoder_emb_inp。注意我们可以选择预训练的词表征如 word2vec 或 Glove vectors 初始化嵌入权重。通常给定大量的训练数据，我们能从头学习这些嵌入权重。\n\n### 3.2.编码器(encoder)\n\n一旦可以检索到，词嵌入就能作为输入馈送到主神经网络中。该网络有两个多层循环神经网络组成，一个是原语言的编码器，另一个是目标语言的解码器。这两个 RNN 原则上可以共享相同的权重，然而在实践中，我们通常使用两组不同的循环神经网络参数（这些模型在拟合大型训练数据集上做得更好）。解码器 RNN 使用零向量作为它的初始状态，并且可以使用如下代码构建：\n\n```python\n# Build RNN cell\nencoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n# Run Dynamic RNN#   encoder_outpus: [max_time, batch_size, num_units]#   encoder_state: [batch_size, num_units]\nencoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n    encoder_cell, encoder_emb_inp,\n    sequence_length=source_seqence_length, time_major=True)\n```\n\n注意语句有不同的长度以避免浪费计算力，因此我们会通过 source_seqence_length 告诉 dynamic_rnn 精确的句子长度。因为我们的输入是以时间为主（time major）的，我们需要设定 time_major=True。现在我们暂时只需要构建单层 LSTM、encoder_cell。我们后面会详细描述怎样构建多层 LSTM、添加 dropout 并使用注意力机制。\n\n### 3.3.解码器(decoder)\n\ndecoder 也需要访问源信息，一种简单的方式是用编码器最后的隐藏态 encoder_state 对其进行初始化。在图 2 中，我们将源词「student」中的隐藏态传递到了解码器。\n\n```python\n# Build RNN cell\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n```\n\n```python\n# Helper\nhelper = tf.contrib.seq2seq.TrainingHelper(\n    decoder_emb_inp, decoder_lengths, time_major=True)# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\nlogits = outputs.rnn_output\n```\n\n此处代码的核心是 BasicDecoder、获取 decoder_cell(类似于 encoder_cell) 的 decoder、helper 以及之前作为输入的 encoder_state。\n\n通过分离 decoders 和 helpers，我们能重复使用不同的代码库，例如 TrainingHelper 可由 GreedyEmbeddingHelper 进行替换，来做贪婪解码。\n\n最后，我们从未提到过的 projection_layer 是一个密集矩阵，将顶部的隐藏态转变为维度 V 的逻辑向量。我们在图 2 的上部展示了此过程。\n\n```python\nprojection_layer = layers_core.Dense(\n    tgt_vocab_size, use_bias=False)\n```\n\n### 3.4.损失构建\n\n给出以上的 logits，可计算训练损失：\n\n```python\ncrossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    labels=decoder_outputs, logits=logits)\ntrain_loss = (tf.reduce_sum(crossent * target_weights) /\n    batch_size)\n```\n\n以上代码中，target_weights 是一个与 decoder_outputs 大小一样的 0-1 矩阵。该矩阵将目标序列长度以外的其他位置填充为标量值 0。\n\n我们需要指出来的是，训练损失可以由 batch_size 分割，因此我们的超参数 batch_size 是「不变量」。也有些人将训练损失按照 batch_size * num_time_steps 分割，这样可以减少短句所造成的误差。更巧妙的，我们的超参数（应用于前面的方法）不能用于后面的方法。例如，如果两种方法都是用学习率为 1.0 的随机梯度下降，后面的方法将更有效地利用一个较小的学习率，即 1 / num_time_steps。\n\n### 3.5.梯度计算和优化器优化\n\n现在是时候定义我们的 NMT 模型的前向传播了。计算反向传播只需要写几行代码：\n\n```python\n# Calculate and clip gradients\nparameters = tf.trainable_variables()\ngradients = tf.gradients(train_loss, params)\nclipped_gradients, _ = tf.clip_by_global_norm(\n    gradients, max_gradient_norm)\n```\n\n训练 RNN 的一个重要步骤是梯度截断（gradient clipping）。这里，我们使用全局范数进行截断操作。最大值 max_gradient_norm 通常设置为 5 或 1。最后一步是选择优化器。Adam 优化器是最常见的选择。我们还要选择一个学习率，learning_rate 的值通常在 0.0001 和 0.001 之间，且可设置为随着训练进程逐渐减小。\n\n```python\n# Optimization\noptimizer = tf.train.AdamOptimizer(learning_rate)\nupdate_step = optimizer.apply_gradients(\n    zip(clipped_gradients, params))\n```\n\n在我们的实验中，我们使用标准的随机梯度下降（tf.train.GradientDescentOptimizer），并采用了递减的学习率方案，因此也就有更好的性能。\n\n### 3.6 开始训练 NMT 模型\n\n让我们开始训练第一个 NMT 模型，将越南语翻译为英语。代码的入口是** **nmt.py。\n\n我们将使用小规模的 Ted 演讲并行语料库（133k 的训练样本）进行训练。所有的数据都可从以下链接找到：https://nlp.stanford.edu/projects/nmt/。\n\n我们将使用 tst2012 作为开发数据集，tst 2013 作为测试数据集。运行以下命令行下载数据训练 NMT 模型：\n\n```shell\nnmt/scripts/download_iwslt15.sh /tmp/nmt_data\n```\n\n运行以下命令行开始训练：\n\n```python\nmkdir /tmp/nmt_model\npython -m nmt.nmt \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\n以上命令行训练一个 2 层的 LSTM seq2seq 模型，带有 128-dim 的隐藏单元和 12 个 epochs 的嵌入。我们使用 0.2（或然率为 0.8）的 dropout 值。如果没误差，在我们训练中随着降低混淆度，我们应该能看到类似于以下的 logs。\n\n```python\n# First evaluation, global step 0\n  eval dev: perplexity 17193.66\n  eval test: perplexity 17193.27\n# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017\n  sample train data:\n    src_reverse: </s> </s> Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .\n    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>\n  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00\n  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00\n  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00\n  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00\n  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00\n```\n\n更多细节，请查看：train.py。\n\n我们可以使用 Tensorboard 在训练过程中查看模型的summary：\n\n```shell\ntensorboard --port 22222 --logdir /tmp/nmt_model/\n```\n\n通过以下简单的变化，就能逆向完成英语到越南语的翻译。\n\n```shell\n--src=en --tgt=vi\n```\n\n### 3.7 预测(推理)与生成翻译结果\n\n当你训练你的 NMT 模型时（并且一旦你已经训练了模型），可以在给定之前不可见的源语句的情况下获得翻译。这一过程被称作推理。训练与推理之间有一个明确的区分（测试）：在推理时，我们只访问源语句，即 encoder_inputs。解码的方式有很多种，包括 greedy 解码、采样解码和束搜索解码（beam-search）。下面我们讨论一下 greedy 解码策略。\n\n其想法简单，我们将在图 3 中作说明：\n\n1. 在训练获取 encoder_state 的过程中，我们依然以相同方式编码源语句，并且 encoder_state 用于初始化解码器。\n2. 一旦解码器接收到开始符`<s>`（在我们的代码中指 tgt_sos_id），就开始解码处理（翻译）。\n3. 最大的单词，其 id 与最大的 logit 值相关联，正如被发出的词（这是 greedy 行为）。例如在图 3 中，单词 moi 在第一个解码步中具有最高的翻译概率。接着我们把这一单词作为输入馈送至下一个时间步。\n4. 这一过程会持续到这句话的终止符`「</s>」`，然后输出（在我们的代码中是 tgt_eos_id）。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/greedy_dec.jpg)\n\n<center>图 3. Greedy 解码实例</center>\n\n推理与训练的区别在于步骤 3。推理不总是馈送作为输入的正确目标词，而是使用被模型预测的单词。下面是实现 greedy 解码的代码。它与训练解码器非常相似。\n\n```python\n# Helper\nhelper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n    embedding_decoder,\n    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)\n# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder, maximum_iterations=maximum_iterations)\ntranslations = outputs.sample_id\n```\n\n我们在本文中使用了 GreedyEmbeddingHelper 而不是 TrainingHelper。由于无法提前知道目标语句的长度，我们使用 maximum_iterations 限制翻译的长度。一个启发是解码最多两倍的源语句长度。\n\n```python\nmaximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n```\n\n我们已经训练了一个模型，现在可以创建一个推理文件并翻译一些语句：\n\n```shell\ncat > /tmp/my_infer_file.vi# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)\n\npython -m nmt.nmt \\\n    --model_dir=/tmp/nmt_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_model/output_infer\n\ncat /tmp/nmt_model/output_infer # To view the inference as output\n```\n\n注意上述指令也可在模型被训练时运行，只要存在一个训练检查点。详见 inference.py。\n\n## 4.提升\n\n在训练了一些最基本的序列到序列模型之后，我们现在更进一步。为了打造当前最优的神经机器翻译系统，我们需要更多的秘诀：注意力机制。该机制由 Bahdanau 等人在 2015 年首次提出（https://arxiv.org/abs/1409.0473 ），稍后 Luong 等人和其他人完善了它，其核心思想是当我们翻译时通过「注意」相关的源内容，建立直接的短连接。注意力机制的一个很好副产品是源语句和目标语句之间的一个易于可视化的对齐矩阵（如图 4 所示）。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_vis.jpg)\n\n<center>图 4. 注意力可视化——源语句与目标语句之间对齐的实例。图片来自 2015 年 Bahdanau 等人的论文。</center>\n\n请记住在 vanilla 序列到序列模型中，当开始编码处理时，我们把最后的源状态从编码器传递到解码器。这对短、中长度的语句效果很好；对于长句子，单一固定大小的隐状态成为了信息瓶颈。注意力机制没有摈弃源 RNN 中计算的所有隐状态，而是提出了允许解码器窥探它们的方法（把它们看作是源信息的动态存储）。如此，注意力机制提升了长句的翻译质量。现在，注意力机制实至名归，已成功应用于其他诸多任务（比如语音识别）。\n\n### 4.1 注意力机制背景\n\n我们现在描述一下注意力机制的实例（Luong et al., 2015），它已经被应用到几个最新型的系统当中了，包括开源工具，比如 OpenNMT（http://opennmt.net/about/ ）和此教程中的 TF seq2seq API。我们还将会提供注意力机制相关变体的内容。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg)\n\n图 5. 注意力机制——基于注意力的 NMT 系统（Luong et al., 2015 中有具体描述）。\n\n我们重点详解注意力计算过程中的第一步。为了更加清晰，我们没有展示图（2）中的嵌入层和投影层。\n\n如图 5 所示，注意力计算发生在解码步骤中的每一步。它包含下列步骤：\n\n1. 当前目标隐蔽状态和所有源状态（source state）进行比较，以导出权重（weight），见图 4。\n2. 基于注意力权重，我们计算了一个背景向量（context vector），作为源状态的平均权值。\n3. 将背景向量与当前目标隐蔽态进行结合以生成最终的注意力向量。\n4. 此注意力向量将作为下一时序步骤的输入。前三个步骤可以由下列公式总结：\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg)\n\n这里，函数 score 用于将目标隐蔽状态 ht 和每一个源状态 hs 进行比较，结果会被标准化成生成式注意力权重（一个源位置的分布）。其实有很多种关于评分函数（scoring function）的选择；比较流行的评分函数包括公式（4）中给出的乘法与加法形式。一旦被计算，注意力向量 at 就会用于推导 softmax logit 和损失。这与 vanilla seq2seq 模型顶层的目标隐蔽态相似。函数 f 也可以利用其它形式。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg)\n\n注意力机制的多种实现方法可由以下链接获得：https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py 。\n\n注意力机制中有什么相关注意事项呢？\n\n上述公式表明注意力机制有很多种变体。这些变体依赖于评分函数（scoring function）和注意力函数（attention function）的形式，也依赖于前一状态 ht-1，而不依赖于开始建议的评分函数 ht（Bahdanau et al., 2015）。实际上我们发现的只有一些选择上的注意事项。\n* 一，注意力的基本形式，例如，目标和源之间的直接联系需要被呈现。\n* 二，把注意力向量输入给下一时间步骤，以把之前的注意力决策告知给网络（Luong et al., 2015）。\n\n评分函数的选择经常可以造成不同的性能表现。\n\n### 4.2 Attention Wrapper API\n\n在我们的 Attention Wrapper API 的实现中，借鉴了 Weston et al., 2015 在 onmemory network 工作中的术语。相比于拥有可读、可写的记忆，此教程中的 attention 机制仅是可读的记忆。特别是对隐藏态（或者隐藏态的变体，例如 $W\\overline{h}_s$ in Luong's scoring style or $W_2\\overline{h}_s$ ) 的设定，被认为是「记忆」。在每个时间步下，我们使用现有的目标隐藏态作为「query」决定读取哪一部分记忆。通常情况下，query 需要与单个记忆条相对应的 keys 进行对比。在上面对注意机制的演示中，我们偶然使用一套源隐藏态（或者其变体，例如$W_1h_t$）作为「key」。你们可以从这种记忆网络术语中获取灵感，找到其他形式的 attention。\n\n由于 attention wrapper，就不再需要扩展我们带有 attention 的 vanilla seq2seq 代码。这部分文件为 attention_model.py。\n\n首先，我们需要定义一种注意机制，例如采用 Luong et al., 2015 的研究。\n\n```python\n# attention_states: [batch_size, max_time, num_units]\nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n# Create an attention mechanism\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units, attention_states,\n    memory_sequence_length=source_sequence_length)\n```\n\n在之前的 Encoder 部分，encoder_outputs 是一套顶层的掩藏态源，形式为 [max_time, batch_size, num_units]（因为我们使用 dynamic_rnn with time_major 设定）。在注意机制上，我们需要保证通过的「memory」是批次为主的，所以需要调换 attention_states。我们通过 source_sequence_length 保证注意机制的权重有适当的规范化（只在 non-padding 的位置）。定义完注意机制之后，我们使用 AttentionWrapper 来包裹解码单元。\n\n```python\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    decoder_cell, attention_mechanism,\n    attention_layer_size=num_units)\n```\n\n剩下的代码基本和编码器一转样 (https://github.com/tensorflow/nmt#decoder)!\n\n### 4.3 上手—打造一个基于注意力的 NMT 模型\n\n为了使注意力发挥作用，我们需要用到 luong、scaled_luong、bahdanau 或 normed_bahdanau 其中的一个作为训练期间注意力标志（attention flag）的值。该标志指定了我们将要使用的注意力机制。除此之外，我们需要为注意力模型创建一个新目录，因此无需重新使用之前训练的基本 NMT 模型。\n\n运行以下指令开始训练：\n\n```shell\nmkdir /tmp/nmt_attention_model\n\npython -m nmt.nmt \\\n    --attention=scaled_luong \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_attention_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\n训练之后，我们可以使用带有新 model_dir 的相同推理指令进行推理：\n\n```shell\npython -m nmt.nmt \\\n    --model_dir=/tmp/nmt_attention_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_attention_model/output_infer\n```\n\n## 5.技巧和注意点\n\n### 5.1 构建训练、验证和测试图\n\n当我们使用 TensorFlow 单间一个机器学习模型的时候，最好构建三个分开的 graph：\n\n- 训练图，包括：\n  - Batches, buckets, 以及来自文件或外部输入的数据集的部分采样数据；\n  - 前向和反向传播的 ops；\n  - 创建 optimizer，以及添加训练 op；\n- 验证图，包括：\n  - Batches, buckets, 以及来自文件或外部输入的数据集输入数据\n  - 训练时的前向传播 op，以及要添加的 evaluation ops\n- 预测图，包括：\n  - 不需要批处理的输入数据\n  - 不需要 subsample 和 bucket 输入数据\n  - 从 placeholders 读取输入数据（数据可以通过 feed_dict 被图读取，或者使用 C++ TensorFlow serving binary）\n  - 模型前向传播的部分 ops，以及一些可能 [session.run](http://session.run/) 函数调用的所需要的额外的为存储状态（state）所需要的 inputs/outputs。\n\n现在比较棘手的一点是 ，如何在一个机器上让三个图共享这些 variables，这可以通过为每个图创建不同的 session 来解决。训练过程的session 周期性的保存 checkpoints，然后 eval session 和 inference session 就可以读取checkpoints。下面的例子展示了这两种方法的不同。\n\n#### 前一种方法：三个模型都在一个图里，并且共享一个 session。\n\n```python\nwith tf.variable_scope('root'):\n  train_inputs = tf.placeholder()\n  train_op, loss = BuildTrainModel(train_inputs)\n  initializer = tf.global_variables_initializer()\n\nwith tf.variable_scope('root', reuse=True):\n  eval_inputs = tf.placeholder()\n  eval_loss = BuildEvalModel(eval_inputs)\n\nwith tf.variable_scope('root', reuse=True):\n  infer_inputs = tf.placeholder()\n  inference_output = BuildInferenceModel(infer_inputs)\n\nsess = tf.Session()\n\nsess.run(initializer)\n\nfor i in itertools.count():\n  train_input_data = ...\n  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})\n\n  if i % EVAL_STEPS == 0:\n    while data_to_eval:\n      eval_input_data = ...\n      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})\n\n  if i % INFER_STEPS == 0:\n    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})\n```\n\n#### 后一种方法：三个模型在三个图里，三个 sessions 共享同样的 variables。\n\n```python\ntrain_graph = tf.Graph()\neval_graph = tf.Graph()\ninfer_graph = tf.Graph()\n\nwith train_graph.as_default():\n  train_iterator = ...\n  train_model = BuildTrainModel(train_iterator)\n  initializer = tf.global_variables_initializer()\n\nwith eval_graph.as_default():\n  eval_iterator = ...\n  eval_model = BuildEvalModel(eval_iterator)\n\nwith infer_graph.as_default():\n  infer_iterator, infer_inputs = ...\n  infer_model = BuildInferenceModel(infer_iterator)\n\ncheckpoints_path = \"/tmp/model/checkpoints\"\n\ntrain_sess = tf.Session(graph=train_graph)\neval_sess = tf.Session(graph=eval_graph)\ninfer_sess = tf.Session(graph=infer_graph)\n\ntrain_sess.run(initializer)\ntrain_sess.run(train_iterator.initializer)\n\nfor i in itertools.count():\n\n  train_model.train(train_sess)\n\n  if i % EVAL_STEPS == 0:\n    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n    eval_model.saver.restore(eval_sess, checkpoint_path)\n    eval_sess.run(eval_iterator.initializer)\n    while data_to_eval:\n      eval_model.eval(eval_sess)\n\n  if i % INFER_STEPS == 0:\n    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n    infer_model.saver.restore(infer_sess, checkpoint_path)\n    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})\n    while data_to_infer:\n      infer_model.infer(infer_sess)\n```\n\n注意后一种方法是如何被转换为分布式版本的。\n\n后种方法与前种方法的另一个不同在于，后者不用在 *session.sun* 调用时通过 *feed_dicts* 喂给数据，而是使用自带状态的 *iterator* 对象（stateful iterator objects）。不论在单机还是分布式集群上，这些 *iterators* 可以让输入管道（input pipeline）变得更容易。在下一小节，我们将使用新的数据输入管道（input data pipeline）。\n\n### 5.2 数据输入管道(Data Input Pipeline)\n\n在 TensorFlow 1.2版本之前，用户有两种把数据喂给 TensorFlow training 和 eval pipelines的方法：\n\n- 1. 在每次训练调用 *session.run* 时，通过 *feed_dict* 直接喂给数据；\n- 2. 使用 tf.train（例如 tf.train.batch）和 tf.contrib.train 中的队列机制（queueing machanisms）；\n- 3. 使用来自 helper 层级框架比如 tf.contrib.learn 或 tf.contrib.slim 的 helpers （这种方法是使用更高效的方法利用第二种方法）。\n\n第一种方法对不熟悉 TensorFlow 或需要做一些外部的数据修改（比如他们自己的 minibatch queueing）的用户来说更简单，这种方法只需用简单的 Python 语法就可实现。第二种和第三种方法更标准但也不那么灵活，他们需要开启多个 Python 线程（queue runners）。更重要的是，如果操作不当会导致死锁或难以查明的错误。尽管如此，队列的方法仍要比 *feed_dict* 的方法高效很多，并且也是单机和分布式训练的标准。\n\n从TensorFlow 1.2开始，有一种新的数据读取的方法可以使用： dataset iterators，其在 **tf.data**模块。Data iterators 非常灵活，易于使用和操作，并且利用 TensorFlow C++ runtime 实现了高效和多线程。\n\n我们可以使用一个 batch data Tensor，一个文件名，或者包含多个文件名的 Tensor 来创建一个 **dataset**。下面是一些例子：\n\n```python\n# Training dataset consists of multiple files.\ntrain_dataset = tf.data.TextLineDataset(train_files)\n\n# Evaluation dataset uses a single file, but we may\n# point to a different file for each evaluation round.\neval_file = tf.placeholder(tf.string, shape=())\neval_dataset = tf.data.TextLineDataset(eval_file)\n\n# For inference, feed input data to the dataset directly via feed_dict.\ninfer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))\ninfer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)\n```\n\n所有的数据都可以完成像数据预处理一样的处理方式，包括数据的 reading 和 cleaning，bucketing（在 training 和 eval 的时候），filtering 以及 batching。\n\n把每个句子转换为单词串的向量（vectors of word strings），那我们可以使用 dataset 的 map transformation:\n\n```python\ndataset = dataset.map(lambda string: tf.string_split([string]).values)\n```\n\n我们也可以把每个句子向量转换为包含向量与其动态长度的元组：\n\n```python\ndataset = dataset.map(lambda words: (words, tf.size(words))\n```\n\n最后，我们可以对每个句子应用 vocabulary lookup。给定一个 lookup 的 table，此 map 函数可以把元组的第一个元素从串向量转换为数字向量。（译者注：不好翻译，原文是：Finally, we can perform a vocabulary lookup on each sentence. Given a lookup table object table, this map converts the first tuple elements from a vector of strings to a vector of integers.）\n\n```python\ndataset = dataset.map(lambda words, size: (table.lookup(words), size))\n```\n\n合并两个 datasets 也非常简单，如果两个文件有行行对应的翻译，并且两个文件分别被不同的 dataset 读取，那么可以通过下面这种方式生成一个新的 dataset，这个新的 dataset 的内容是两种语言的翻译一一对应的元组。\n\n```python\nsource_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))\n```\n\nBatching 变长的句子实现起来也很直观。下边的代码从 *source_target_dataset* 中 batch 了 *batch_size* 个元素，并且分别为每个 batch 的源向量和目标向量 padding 到最长的源向量和目标向量的长度。\n\n```python\nbatched_dataset = source_target_dataset.padded_batch(\n        batch_size,\n        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size\n                        tf.TensorShape([])),     # size(source)\n                       (tf.TensorShape([None]),  # target vectors of unknown size\n                        tf.TensorShape([]))),    # size(target)\n        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id\n                         0),          # size(source) -- unused\n                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id\n                         0)))         # size(target) -- unused\n```\n\n从 dataset 拿到的数据会嵌套为元组，其 tensors 的最左边的维度是 batch_size. 其结构如下：\n\n- iterator[0][0] has the batched and padded source sentence matrices.\n- iterator[0][1] has the batched source size vectors.\n- iterator[1][0] has the batched and padded target sentence matrices.\n- iterator[1][1] has the batched target size vectors.\n\n最后，bucketing 多个 batch 的大小差不多的源句子也是可以的。更多的代码实现详见文件[utils/iterator_utils.py](https://github.com/tensorflow/nmt/tree/master/nmt/utils/iterator_utils.py)。\n\n从 dataset 中读取数据需要三行的代码：创建 iterator，取其值，初始化。\n\n```python\nbatched_iterator = batched_dataset.make_initializable_iterator()\n\n((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next()\n\n# At initialization time.\nsession.run(batched_iterator.initializer, feed_dict={...})\n```\n\n一旦 iterator 被初始化，那么 [session.run](http://session.run) 每一次调用 source 和 target ，都会从dataset中自动提取下一个 minibatch 的数据。\n\n### 5.3 让 NMT 模型更完美的其他技巧\n\n#### 双向RNN(Bidirectional RNN)\n\n一般来讲，encoder 的双向 RNNs 可以让模型表现更好（训练速度会下降，因为有更多的层需要计算）。这里，我们给出了构建一个单层双向层的 encoder 的简单代码：\n\n```python\n# Construct forward and backward cells\nforward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\nbackward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n\nbi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n    forward_cell, backward_cell, encoder_emb_inp,\n    sequence_length=source_sequence_length, time_major=True)\nencoder_outputs = tf.concat(bi_outputs, -1)\n```\n\n*encoder_outputs* 和 *encoder_state* 也可以使用 Encoder 小节的方法获取到。需要注意的是，如果要创建多层双向层，你需要修改一下 encoder_state，见 [model.py](https://github.com/tensorflow/nmt/tree/master/nmt/model.py) 的*_build_bidirectional_rnn()*方法。\n\n#### 集束搜索(Beam Search)\n\nGreedy decoding 可以给我们非常合理的翻译结果，但是 beam search decoding 可以让翻译结果更好。Beam search 的思想是，考虑我们可以选择的所有翻译结果的排名最靠前的几个候选的集合，我们探索其所有的可能翻译结果（大家也可以参考[知乎的beam search讨论](https://www.zhihu.com/question/54356960/answer/138990060)）。Beam 的这个 size 我们称为 *beam width*，一个较小的 beam width 比如说 10，就已经足够大了。我们推荐读者阅读 [Neubig, (2017)](https://arxiv.org/abs/1703.01619) 的 7.2.3 小节。这是 beam search 的一个例子：\n\n```python\n# Replicate encoder infos beam_width times\ndecoder_initial_state = tf.contrib.seq2seq.tile_batch(\n    encoder_state, multiplier=hparams.beam_width)\n\n# Define a beam-search decoder\ndecoder = tf.contrib.seq2seq.BeamSearchDecoder(\n        cell=decoder_cell,\n        embedding=embedding_decoder,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        initial_state=decoder_initial_state,\n        beam_width=beam_width,\n        output_layer=projection_layer,\n        length_penalty_weight=0.0)\n\n# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n```\n\n在 Decoder 小节，*dynamic_decode()* API 也被使用过。解码结束，我们就可以使用下面的代码得到翻译结果：\n\n```python\ntranslations = outputs.predicted_ids\n# Make sure translations shape is [batch_size, beam_width, time]\nif self.time_major:\n   translations = tf.transpose(translations, perm=[1, 2, 0])\n```\n\n更多细节，可查看 [model.py](https://github.com/tensorflow/nmt/tree/master/nmt/model.py), *_build_decoder()* 函数。\n\n#### 超参数(Hyperparameters)\n\n有一些超参数也可以供我们调节。这里，根据我们的实验，我们列举了几个超参数【你可以表示不认同，保留自己的看法】。\n\n- **optimizer**：对于“不太常见”的网络结构，Adam 可能可以给出一个较合理的结果，如果你用 SGD 进行训练，那么 SGD 往往可以取得更好的结果。\n- **Attention**：Bahdanau 类型的 attention，encoder 需要双向结构才能表现很好；同时 Luong 类型的 attention 需要其他的一些设置才能表现很好。在本教程中，我们推荐使用被改进的这两个类型的 attention：**scaled_luong** 和 **normed_bahdanau**。\n\n**多 GPU 训练 | Multi-GPU training**\n\n训练一个 NMT 模型可能需要几天的时间，我们可以把不同的 RNN layers 放在不同的 GPUs 进行训练可以加快训练速度。这里是使用多 GPUs 创建 RNN layers 的例子：\n\n```python\ncells = []\nfor i in range(num_layers):\n  cells.append(tf.contrib.rnn.DeviceWrapper(\n      tf.contrib.rnn.LSTMCell(num_units),\n      \"/gpu:%d\" % (num_layers % num_gpus)))\ncell = tf.contrib.rnn.MultiRNNCell(cells)\n```\n\n另外，我们还需要 tf.gradients 的 colocate_gradients_with_ops 参数来同步梯度的计算。\n\n你会发现，尽管我们使用了多个 GPUs，但是 attention-based NMT 模型的训练速度提升不大。问题的关键在于，在标准的 attention 模型中，在每个时间步，我们都需要用最后一层的输出去“查询”attention，这就意味着，每一个解码的时间步都需要等前面的时间步完全完成。因此，我们不能简单的通过在多 GPUs 上部署 RNN layers 来同步解码过程。\n\n[GNMT attention architecture](https://arxiv.org/pdf/1609.08144.pdf) 可以通过使用第一层的输出来查询 attention 的方法来同步 decoder 的计算。这样，解码器的每一步就可以在前一步的第一层和 attention 计算完成之后就可以进行解码了。我们的 API 实现了这个结构 [GNMTAttentionMultiCell](https://github.com/tensorflow/nmt/tree/master/nmt/gnmt_model.py)，其是*tf.contrib.rnn.MultiRNNCell*的子类。这里是使用 *GNMTAttentionMultiCell* 创建一个 decoder 的例子：\n\n```python\ncells = []\nfor i in range(num_layers):\n  cells.append(tf.contrib.rnn.DeviceWrapper(\n      tf.contrib.rnn.LSTMCell(num_units),\n      \"/gpu:%d\" % (num_layers % num_gpus)))\nattention_cell = cells.pop(0)\nattention_cell = tf.contrib.seq2seq.AttentionWrapper(\n    attention_cell,\n    attention_mechanism,\n    attention_layer_size=None,  # don't add an additional dense layer.\n    output_attention=False,)\ncell = GNMTAttentionMultiCell(attention_cell, cells)\n```\n\n### 结果与预测质量\n\nIWSLT 英语-越南语\n\n训练：133k 的样本，dev=tst2012，test=tst2013\n\nSystems | tst2012 (dev) | test2013 (test)\n--- | :---: | :---:\nNMT (greedy) | 23.2 | 25.5\nNMT (beam=10) | 23.8 | **26.1**\n[(Luong & Manning, 2015)](https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf) | - | 23.3\n\n*训练速度：在英伟达 K40m 上是 0.37s 的时间步、15.3k 的 wps，在 Titan X 上是 0.17 s 的时间步，32.2k 的 wps。*\n\nWMT 德语-英语\n\n训练：4.5M 的样本量，dev=newstest2013，test=newtest2015\n\nSystems | newstest2013 (dev) | newstest2015\n--- | :---: | :---:\nNMT (greedy) | 27.1 | 27.6\nNMT (beam=10) | 28.0 | 28.9\nNMT + GNMT attention (beam=10) | 29.0 | **29.9**\n[WMT SOTA](http://matrix.statmt.org/) | - | 29.3\n\n*训练速度：在英伟达 K40m 上是 2.1s 的时间步，3.4k 的 wps，在英伟达 Titan X 上是 0.7s 的时间步，8.7k 的 wps。*\n\n为了查看 GNMT 注意的加速度，我们只在 K40m 上做了基准测试：\n\nSystems | 1 gpu | 4 gpus | 8 gpus\n--- | :---: | :---: | :---:\nNMT (4 layers) | 2.2s, 3.4K | 1.9s, 3.9K | -\nNMT (8 layers) | 3.5s, 2.0K | - | 2.9s, 2.4K\nNMT + GNMT attention (4 layers) | 2.6s, 2.8K | 1.7s, 4.3K | -\nNMT + GNMT attention (8 layers) | 4.2s, 1.7K | - | 1.9s, 3.8K\n\n*WMT 英语-德语 全对比*\n\n第二行是我们 GNMT 注意模型：模型 1（4 层），模型 2（8 层）。\n\nSystems | newstest2014 | newstest2015\n--- | :---: | :---:\n*Ours* &mdash; NMT + GNMT attention (4 layers) | 23.7 | 26.5\n*Ours* &mdash; NMT + GNMT attention (8 layers) | 24.4 | **27.6**\n[WMT SOTA](http://matrix.statmt.org/) | 20.6 | 24.9\nOpenNMT [(Klein et al., 2017)](https://arxiv.org/abs/1701.02810) | 19.3 | -\ntf-seq2seq [(Britz et al., 2017)](https://arxiv.org/abs/1703.03906) | 22.2 | 25.2\nGNMT [(Wu et al., 2016)](https://research.google.com/pubs/pub45610.html) | **24.6** | -\n\n**其他资源**\n\n若想深入了解神经机器翻译和序列-序列模型，我们非常推荐以下资源：\n\n- Neural Machine Translation and Sequence-to-sequence Models: A Tutorial：https://arxiv.org/abs/1703.01619\n- Neural Machine Translation - Tutorial ACL 2016：https://sites.google.com/site/acl16nmt/\n- Thang Luong's Thesis on Neural Machine Translation：https://github.com/lmthang/thesis\n\n用于构建 seq2seq 模型的工具很多：\n\n- Stanford NMT https://nlp.stanford.edu/projects/nmt/ [Matlab] \n- tf-seq2seq https://github.com/google/seq2seq [TensorFlow] \n- Nemantus https://github.com/rsennrich/nematus [Theano] \n- OpenNMT http://opennmt.net/ [Torch]\n\n## 6.seq2seq构建的摘要生成应用\n我们来使用seq2seq框架完成一个文本摘要的任务，我给大家准备了一些新闻数据(新闻内容与对应的标题)，我们使用这份数据来构建文本摘要的AI应用。在此之前，我们先了解一下原有的翻译系统需要准备的语料格式，我们把中文数据处理成格式一致的形态。\n\n我们先拉取一份样例数据。\n\n\n```python\n!bash nmt/scripts/download_iwslt15.sh /tmp/nmt_data\n```\n\n    mkdir: created directory '/tmp/nmt_data'\n    Download training dataset train.en and train.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100 12.9M  100 12.9M    0     0  1952k      0  0:00:06  0:00:06 --:--:-- 2859k\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100 17.2M  100 17.2M    0     0  2682k      0  0:00:06  0:00:06 --:--:-- 4214k\n    Download dev dataset tst2012.en and tst2012.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  136k  100  136k    0     0  63286      0  0:00:02  0:00:02 --:--:-- 63289\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  183k  100  183k    0     0  74267      0  0:00:02  0:00:02 --:--:-- 74259\n    Download test dataset tst2013.en and tst2013.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  129k  100  129k    0     0  56802      0  0:00:02  0:00:02 --:--:-- 56814\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  179k  100  179k    0     0  85002      0  0:00:02  0:00:02 --:--:-- 85039\n    Download vocab file vocab.en and vocab.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  136k  100  136k    0     0  53845      0  0:00:02  0:00:02 --:--:-- 53829 0  40969      0  0:00:03  0:00:02  0:00:01 40960\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100 46767  100 46767    0     0  26664      0  0:00:01  0:00:01 --:--:-- 26678\n    \n\n**查看一下包含的文件**\n\n\n```python\n!ls /tmp/nmt_data\n```\n\n    train.en  tst2012.en  tst2013.en  vocab.en\n\n    train.vi  tst2012.vi  tst2013.vi  vocab.vi\n\n\n\n**看一下源语言与目标语言的格式，以及对应的数据量**\n\n可以看到都是做过tokenization之后的数据。\n\n\n```python\n!head -10 /tmp/nmt_data/train.en\n```\n\n    Rachel Pike : The science behind a climate headline\n\n    In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n\n    I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n\n    Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n\n    They are both two branches of the same field of atmospheric science .\n\n    Recently the headlines looked like this when the Intergovernmental Panel on Climate Change , or IPCC , put out their report on the state of understanding of the atmospheric system .\n\n    That report was written by 620 scientists from 40 countries .\n\n    They wrote almost a thousand pages on the topic .\n\n    And all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n\n    It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n\n\n\n\n```python\n!wc -l /tmp/nmt_data/train.en\n```\n\n    133317 /tmp/nmt_data/train.en\n\n\n\n**还需要准备好vocabulary词表**\n\n\n```python\n!head -10 /tmp/nmt_data/vocab.en\n```\n\n    <unk>\n\n    <s>\n\n    </s>\n\n    Rachel\n\n    :\n\n    The\n\n    science\n\n    behind\n\n    a\n\n    climate\n\n\n\n\n```python\n!wc -l /tmp/nmt_data/vocab.en\n```\n\n    17191 /tmp/nmt_data/vocab.en\n\n\n\n\n```python\n!wc -l /tmp/nmt_data/vocab.vi\n```\n\n    7709 /tmp/nmt_data/vocab.vi\n\n\n\n**我们来对我们的数据做一个处理，使得它符合seq2seq模型的输入格式**\n\n\n```python\nimport re\ndef get_vocab(in_f, out_f):\n    vocab_dic = {}\n    for line in open(in_f, encoding='utf-8'):\n        words = line.strip().split(\" \")\n        for word in words:\n            # 保留汉字内容\n            if not re.match(r\"[\\u4e00-\\u9fa5]+\", word):\n                continue\n            try:\n                vocab_dic[word] += 1\n            except:\n                vocab_dic[word] = 1\n    out = open(out_f, 'w', encoding='utf-8')\n    out.write(\"<unk>\\n<s>\\n</s>\\n\")\n    vocab = sorted(vocab_dic.items(),key = lambda x:x[1],reverse = True)\n    for word in [x[0] for x in vocab[:40000]]:\n        out.write(word)\n        out.write(\"\\n\")\n    out.close()\n```\n\n**构建训练集与测试集词表**\n\n\n```python\nin_file = \"./data/train.input\"\nout_file = \"./data/vocab.input\"\nget_vocab(in_file, out_file)\n```\n\n\n```python\nin_file = \"./data/train.input\"\nout_file = \"./data/vocab.output\"\nget_vocab(in_file, out_file)\n```\n\n**新建文件夹**\n\n\n```python\n!mkdir /tmp/nmt_attention_model\n```\n\n**训练摘要生成模型**\n\n\n```python\n!python3 -m nmt.nmt \\\n    --attention=scaled_luong \\\n    --src=input --tgt=output \\\n    --vocab_prefix=./data/vocab  \\\n    --train_prefix=./data/train \\\n    --dev_prefix=./data/val  \\\n    --test_prefix=./data/test \\\n    --out_dir=/tmp/nmt_attention_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\n    2018-12-26 15:55:34.569220: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n    2018-12-26 15:55:34.734832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n    2018-12-26 15:55:34.735921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n    name: Tesla M40 major: 5 minor: 2 memoryClockRate(GHz): 1.112\n    pciBusID: 0000:00:07.0\n    totalMemory: 11.93GiB freeMemory: 11.81GiB\n    2018-12-26 15:55:34.735982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n    2018-12-26 15:55:35.140033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n    2018-12-26 15:55:35.140119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n    2018-12-26 15:55:35.140139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n    2018-12-26 15:55:35.140543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11429 MB memory) -> physical GPU (device: 0, name: Tesla M40, pci bus id: 0000:00:07.0, compute capability: 5.2)\n    WARNING:tensorflow:From /notebooks/seq2seq/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n    2018-12-26 15:55:41.658974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n    2018-12-26 15:55:41.659074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n    2018-12-26 15:55:41.659112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n    2018-12-26 15:55:41.659148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n    2018-12-26 15:55:41.659435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11429 MB memory) -> physical GPU (device: 0, name: Tesla M40, pci bus id: 0000:00:07.0, compute capability: 5.2)\n    2018-12-26 15:55:41.659919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n    2018-12-26 15:55:41.659995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n    2018-12-26 15:55:41.660028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n    2018-12-26 15:55:41.660051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n    2018-12-26 15:55:41.660290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11429 MB memory) -> physical GPU (device: 0, name: Tesla M40, pci bus id: 0000:00:07.0, compute capability: 5.2)\n    2018-12-26 15:55:41.660684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n    2018-12-26 15:55:41.660749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n    2018-12-26 15:55:41.660778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n    2018-12-26 15:55:41.660801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n    2018-12-26 15:55:41.661050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11429 MB memory) -> physical GPU (device: 0, name: Tesla M40, pci bus id: 0000:00:07.0, compute capability: 5.2)\n      eval dev: perplexity 39972.22, time 103s, Wed Dec 26 15:57:27 2018.\n      eval test: perplexity 39980.85, time 48s, Wed Dec 26 15:58:16 2018.\n    2018-12-26 15:58:16.392671: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 15:58:16.392673: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 15:58:16.393881: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 15:58:28.732436: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:98] Filling up shuffle buffer (this may take a while): 97073 of 128000\n    2018-12-26 15:58:31.867989: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:136] Shuffle buffer filled.\n    2018-12-26 16:03:11.026422: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:03:11.027613: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:03:11.027806: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:03:11.301212: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:03:11.301246: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:03:11.302525: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      eval dev: perplexity 128.75, time 104s, Wed Dec 26 16:04:56 2018.\n      eval test: perplexity 1208.21, time 49s, Wed Dec 26 16:05:45 2018.\n    2018-12-26 16:08:07.854876: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:08:07.854896: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:08:07.856102: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:08:08.088732: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:08:08.088732: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:08:08.089873: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      done, num sentences 26983, num translations per input 1, time 182s, Wed Dec 26 16:11:10 2018.\n      done, num sentences 20000, num translations per input 1, time 109s, Wed Dec 26 16:13:10 2018.\n    2018-12-26 16:13:25.812099: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:98] Filling up shuffle buffer (this may take a while): 97236 of 128000\n    2018-12-26 16:13:29.000803: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:136] Shuffle buffer filled.\n    2018-12-26 16:15:41.279316: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:15:41.280466: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:15:41.281552: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:15:42.232244: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:15:42.234559: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:15:42.234801: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      eval dev: perplexity 91.48, time 107s, Wed Dec 26 16:17:29 2018.\n      eval test: perplexity 985.22, time 49s, Wed Dec 26 16:18:19 2018.\n    2018-12-26 16:22:34.548027: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:22:34.548708: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:22:34.548746: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:22:34.850419: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:22:34.851449: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:22:34.852502: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n      eval dev: perplexity 66.91, time 103s, Wed Dec 26 16:24:18 2018.\n      eval test: perplexity 768.29, time 49s, Wed Dec 26 16:25:08 2018.\n    2018-12-26 16:25:40.531038: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:25:40.532240: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:25:40.533340: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:25:40.825590: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:25:40.825597: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:25:40.826816: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      done, num sentences 26983, num translations per input 1, time 267s, Wed Dec 26 16:30:08 2018.\n      done, num sentences 20000, num translations per input 1, time 167s, Wed Dec 26 16:33:10 2018.\n    2018-12-26 16:33:27.960171: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:98] Filling up shuffle buffer (this may take a while): 98403 of 128000\n    2018-12-26 16:33:30.966868: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:136] Shuffle buffer filled.\n    2018-12-26 16:37:31.908563: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:37:31.909736: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:37:31.909785: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:37:32.241936: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:37:32.243089: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:37:32.244209: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n      eval dev: perplexity 72.49, time 105s, Wed Dec 26 16:39:17 2018.\n      eval test: perplexity 706.67, time 49s, Wed Dec 26 16:40:06 2018.\n    2018-12-26 16:43:02.147477: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:43:02.148608: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:43:02.148619: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:43:02.443770: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:43:02.444003: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:43:02.444912: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      done, num sentences 26983, num translations per input 1, time 309s, Wed Dec 26 16:48:12 2018.\n      done, num sentences 20000, num translations per input 1, time 198s, Wed Dec 26 16:51:45 2018.\n    2018-12-26 16:52:04.791782: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:98] Filling up shuffle buffer (this may take a while): 98170 of 128000\n    2018-12-26 16:52:07.839030: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:136] Shuffle buffer filled.\n    2018-12-26 16:53:47.283489: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:53:47.284614: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:53:47.284625: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:53:48.291710: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:53:48.292651: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:53:48.292892: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      eval dev: perplexity 70.18, time 105s, Wed Dec 26 16:55:33 2018.\n      eval test: perplexity 630.23, time 49s, Wed Dec 26 16:56:23 2018.\n    2018-12-26 16:56:23.516704: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:56:23.517882: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:56:23.518960: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:56:24.384298: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n    2018-12-26 16:56:24.384344: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 16:56:24.385497: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2018-12-26 17:02:58.613038: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.19GiB.  Current allocation summary follows.\n    2018-12-26 17:02:58.613765: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (256): \tTotal Chunks: 89, Chunks in use: 89. 22.2KiB allocated for chunks. 22.2KiB in use in bin. 353B client-requested in use in bin.\n    2018-12-26 17:02:58.613802: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.613834: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1024): \tTotal Chunks: 3, Chunks in use: 3. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 3.0KiB client-requested in use in bin.\n    2018-12-26 17:02:58.613870: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2048): \tTotal Chunks: 32, Chunks in use: 32. 64.0KiB allocated for chunks. 64.0KiB in use in bin. 64.0KiB client-requested in use in bin.\n    2018-12-26 17:02:58.613905: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.613934: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.613961: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.\n    2018-12-26 17:02:58.613987: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 32.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614016: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (65536): \tTotal Chunks: 1299, Chunks in use: 3. 103.05MiB allocated for chunks. 192.0KiB in use in bin. 192.0KiB client-requested in use in bin.\n    2018-12-26 17:02:58.614044: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (131072): \tTotal Chunks: 5, Chunks in use: 4. 737.2KiB allocated for chunks. 512.0KiB in use in bin. 512.0KiB client-requested in use in bin.\n    2018-12-26 17:02:58.614070: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614096: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (524288): \tTotal Chunks: 16, Chunks in use: 16. 9.00MiB allocated for chunks. 9.00MiB in use in bin. 9.00MiB client-requested in use in bin.\n    2018-12-26 17:02:58.614121: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614145: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614171: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4194304): \tTotal Chunks: 1298, Chunks in use: 1298. 6.19GiB allocated for chunks. 6.19GiB in use in bin. 6.19GiB client-requested in use in bin.\n    2018-12-26 17:02:58.614200: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8388608): \tTotal Chunks: 2, Chunks in use: 2. 21.09MiB allocated for chunks. 21.09MiB in use in bin. 20.28MiB client-requested in use in bin.\n    2018-12-26 17:02:58.614228: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16777216): \tTotal Chunks: 9, Chunks in use: 9. 175.79MiB allocated for chunks. 175.79MiB in use in bin. 175.79MiB client-requested in use in bin.\n    2018-12-26 17:02:58.614254: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614281: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 1. 102.83MiB allocated for chunks. 102.83MiB in use in bin. 102.83MiB client-requested in use in bin.\n    2018-12-26 17:02:58.614306: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614332: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 4.57GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n    2018-12-26 17:02:58.614357: I tensorflow/core/common_runtime/bfc_allocator.cc:626] Bin for 6.19GiB was 256.00MiB, Chunk State: \n    2018-12-26 17:02:58.614390: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 4.57GiB | Requested Size: 324.5KiB | in_use: 0, prev:   Size: 102.83MiB | Requested Size: 102.83MiB | in_use: 1\n    2018-12-26 17:02:58.614422: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a600000 of size 1280\n    2018-12-26 17:02:58.614447: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a600500 of size 1280\n    2018-12-26 17:02:58.614470: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a600a00 of size 1280\n    2018-12-26 17:02:58.614492: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a600f00 of size 256\n    2018-12-26 17:02:58.614514: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a601000 of size 256\n    2018-12-26 17:02:58.614535: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a601100 of size 256\n    2018-12-26 17:02:58.614557: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a601200 of size 256\n    2018-12-26 17:02:58.614579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a601300 of size 2048\n    2018-12-26 17:02:58.614601: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a601b00 of size 256\n    2018-12-26 17:02:58.614622: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a601c00 of size 2048\n    2018-12-26 17:02:58.614645: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a602400 of size 256\n    2018-12-26 17:02:58.614666: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a602500 of size 256\n    2018-12-26 17:02:58.614687: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a602600 of size 2048\n    2018-12-26 17:02:58.614716: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a602e00 of size 256\n    2018-12-26 17:02:58.614739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a602f00 of size 2048\n    2018-12-26 17:02:58.614764: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a603700 of size 256\n    2018-12-26 17:02:58.614785: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a603800 of size 256\n    2018-12-26 17:02:58.614806: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a603900 of size 256\n    2018-12-26 17:02:58.614828: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a603a00 of size 65536\n    2018-12-26 17:02:58.614850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0a613a00 of size 20481536\n    2018-12-26 17:02:58.614922: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0b99c000 of size 131072\n    2018-12-26 17:02:58.614950: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0b9bc000 of size 256\n    2018-12-26 17:02:58.614972: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0b9bc100 of size 2048\n    2018-12-26 17:02:58.614995: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0b9bc900 of size 786432\n    2018-12-26 17:02:58.615018: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0ba7c900 of size 2048\n    2018-12-26 17:02:58.615040: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0ba7d100 of size 524288\n    2018-12-26 17:02:58.615062: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0bafd100 of size 20481536\n    2018-12-26 17:02:58.615083: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0ce85700 of size 2048\n    2018-12-26 17:02:58.615104: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0ce85f00 of size 524288\n    2018-12-26 17:02:58.615125: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0cf05f00 of size 2048\n    2018-12-26 17:02:58.615146: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0cf06700 of size 524288\n    2018-12-26 17:02:58.615170: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0cf86700 of size 20481536\n    2018-12-26 17:02:58.615192: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30ed00 of size 256\n    2018-12-26 17:02:58.615213: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30ee00 of size 256\n    2018-12-26 17:02:58.615235: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30ef00 of size 256\n    2018-12-26 17:02:58.615256: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30f000 of size 256\n    2018-12-26 17:02:58.615278: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30f100 of size 2048\n    2018-12-26 17:02:58.615299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30f900 of size 256\n    2018-12-26 17:02:58.615322: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e30fa00 of size 2048\n    2018-12-26 17:02:58.615343: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e310200 of size 256\n    2018-12-26 17:02:58.615364: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e310300 of size 2048\n    2018-12-26 17:02:58.615385: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e310b00 of size 256\n    2018-12-26 17:02:58.615407: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e310c00 of size 2048\n    2018-12-26 17:02:58.615428: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e311400 of size 256\n    2018-12-26 17:02:58.615448: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e311500 of size 256\n    2018-12-26 17:02:58.615470: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e311600 of size 256\n    2018-12-26 17:02:58.615491: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e311700 of size 256\n    2018-12-26 17:02:58.615511: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0e311800 of size 20481536\n    2018-12-26 17:02:58.615532: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb0f699e00 of size 20481536\n    2018-12-26 17:02:58.615553: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10a22400 of size 131072\n    2018-12-26 17:02:58.615575: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10a42400 of size 256\n    2018-12-26 17:02:58.615596: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10a42500 of size 2048\n    2018-12-26 17:02:58.615617: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10a42d00 of size 786432\n    2018-12-26 17:02:58.615639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10b02d00 of size 2048\n    2018-12-26 17:02:58.615660: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10b03500 of size 524288\n    2018-12-26 17:02:58.615681: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10b83500 of size 2048\n    2018-12-26 17:02:58.615710: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10b83d00 of size 524288\n    2018-12-26 17:02:58.615735: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10c03d00 of size 2048\n    2018-12-26 17:02:58.615757: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10c04500 of size 524288\n    2018-12-26 17:02:58.615778: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb10c84500 of size 20481536\n    2018-12-26 17:02:58.615799: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1200cb00 of size 65536\n    2018-12-26 17:02:58.615823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201cb00 of size 256\n    2018-12-26 17:02:58.615844: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201cc00 of size 256\n    2018-12-26 17:02:58.615865: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201cd00 of size 256\n    2018-12-26 17:02:58.615886: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201ce00 of size 256\n    2018-12-26 17:02:58.615906: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201cf00 of size 256\n    2018-12-26 17:02:58.615927: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d000 of size 256\n    2018-12-26 17:02:58.615948: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d100 of size 256\n    2018-12-26 17:02:58.616014: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d200 of size 256\n    2018-12-26 17:02:58.616040: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d300 of size 256\n    2018-12-26 17:02:58.616063: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d400 of size 256\n    2018-12-26 17:02:58.616085: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d500 of size 256\n    2018-12-26 17:02:58.616107: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d600 of size 256\n    2018-12-26 17:02:58.616127: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201d700 of size 2048\n    2018-12-26 17:02:58.616148: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201df00 of size 256\n    2018-12-26 17:02:58.616169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201e000 of size 2048\n    2018-12-26 17:02:58.616192: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201e800 of size 256\n    2018-12-26 17:02:58.616214: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201e900 of size 256\n    2018-12-26 17:02:58.616234: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201ea00 of size 2048\n    2018-12-26 17:02:58.616255: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201f200 of size 256\n    2018-12-26 17:02:58.616275: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201f300 of size 2048\n    2018-12-26 17:02:58.616296: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201fb00 of size 256\n    2018-12-26 17:02:58.616316: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201fc00 of size 256\n    2018-12-26 17:02:58.616337: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201fd00 of size 256\n    2018-12-26 17:02:58.616357: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1201fe00 of size 65536\n    2018-12-26 17:02:58.616378: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1202fe00 of size 20481536\n    2018-12-26 17:02:58.616399: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb133b8400 of size 131072\n    2018-12-26 17:02:58.616421: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb133d8400 of size 256\n    2018-12-26 17:02:58.616441: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb133d8500 of size 2048\n    2018-12-26 17:02:58.616464: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb133d8d00 of size 786432\n    2018-12-26 17:02:58.616488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb13498d00 of size 2048\n    2018-12-26 17:02:58.616512: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb13499500 of size 524288\n    2018-12-26 17:02:58.616534: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb13519500 of size 20481536\n    2018-12-26 17:02:58.616555: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb148a1b00 of size 2048\n    2018-12-26 17:02:58.616576: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb148a2300 of size 524288\n    2018-12-26 17:02:58.616597: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb14922300 of size 2048\n    2018-12-26 17:02:58.616618: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb14922b00 of size 524288\n    2018-12-26 17:02:58.616641: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb149a2b00 of size 20481536\n    2018-12-26 17:02:58.616662: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b100 of size 256\n    2018-12-26 17:02:58.616683: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b200 of size 256\n    2018-12-26 17:02:58.616712: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b300 of size 256\n    2018-12-26 17:02:58.616736: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b400 of size 256\n    2018-12-26 17:02:58.616757: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b500 of size 256\n    2018-12-26 17:02:58.616778: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b600 of size 256\n    2018-12-26 17:02:58.616802: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b700 of size 256\n    2018-12-26 17:02:58.616823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b800 of size 256\n    2018-12-26 17:02:58.616844: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2b900 of size 256\n    2018-12-26 17:02:58.616865: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2ba00 of size 2048\n    2018-12-26 17:02:58.616885: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2c200 of size 256\n    2018-12-26 17:02:58.616906: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2c300 of size 2048\n    2018-12-26 17:02:58.616927: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2cb00 of size 256\n    2018-12-26 17:02:58.616950: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2cc00 of size 2048\n    2018-12-26 17:02:58.616971: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2d400 of size 256\n    2018-12-26 17:02:58.616992: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2d500 of size 2048\n    2018-12-26 17:02:58.617014: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2dd00 of size 256\n    2018-12-26 17:02:58.617035: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2de00 of size 256\n    2018-12-26 17:02:58.617057: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2df00 of size 256\n    2018-12-26 17:02:58.617078: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e000 of size 256\n    2018-12-26 17:02:58.617100: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e100 of size 256\n    2018-12-26 17:02:58.617120: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e200 of size 256\n    2018-12-26 17:02:58.617140: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e300 of size 256\n    2018-12-26 17:02:58.617161: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e400 of size 256\n    2018-12-26 17:02:58.617182: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e500 of size 256\n    2018-12-26 17:02:58.617203: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e600 of size 256\n    2018-12-26 17:02:58.617224: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e700 of size 256\n    2018-12-26 17:02:58.617246: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e800 of size 256\n    2018-12-26 17:02:58.617267: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15d2e900 of size 524288\n    2018-12-26 17:02:58.617290: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15dae900 of size 2048\n    2018-12-26 17:02:58.617311: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15daf100 of size 524288\n    2018-12-26 17:02:58.617331: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15e2f100 of size 2048\n    2018-12-26 17:02:58.617353: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15e2f900 of size 256\n    2018-12-26 17:02:58.617374: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15e2fa00 of size 256\n    2018-12-26 17:02:58.617397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15e2fb00 of size 786432\n    2018-12-26 17:02:58.617418: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15eefb00 of size 2048\n    2018-12-26 17:02:58.617440: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15ef0300 of size 524288\n    2018-12-26 17:02:58.617461: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f70300 of size 2048\n    2018-12-26 17:02:58.617483: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f70b00 of size 256\n    2018-12-26 17:02:58.617504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f70c00 of size 131072\n    2018-12-26 17:02:58.617526: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f90c00 of size 256\n    2018-12-26 17:02:58.617547: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f90d00 of size 256\n    2018-12-26 17:02:58.617570: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f90e00 of size 256\n    2018-12-26 17:02:58.617591: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f90f00 of size 256\n    2018-12-26 17:02:58.617612: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91000 of size 256\n    2018-12-26 17:02:58.617633: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91100 of size 256\n    2018-12-26 17:02:58.617654: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91200 of size 256\n    2018-12-26 17:02:58.617676: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91300 of size 256\n    2018-12-26 17:02:58.617697: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91400 of size 256\n    2018-12-26 17:02:58.617727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91500 of size 256\n    2018-12-26 17:02:58.617750: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91600 of size 256\n    2018-12-26 17:02:58.617770: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91700 of size 256\n    2018-12-26 17:02:58.617791: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91800 of size 256\n    2018-12-26 17:02:58.617812: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91900 of size 256\n    2018-12-26 17:02:58.617832: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91a00 of size 256\n    2018-12-26 17:02:58.617854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91b00 of size 256\n    2018-12-26 17:02:58.617875: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91c00 of size 256\n    2018-12-26 17:02:58.617897: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15f91d00 of size 256\n    2018-12-26 17:02:58.617919: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb15f91e00 of size 101632\n    2018-12-26 17:02:58.617941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15faab00 of size 16384\n    2018-12-26 17:02:58.617962: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb15faeb00 of size 230656\n    2018-12-26 17:02:58.617983: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb15fe7000 of size 11485184\n    2018-12-26 17:02:58.618004: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb16adb000 of size 32768\n    2018-12-26 17:02:58.618026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb16ae3000 of size 10633216\n    2018-12-26 17:02:58.618048: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb17507000 of size 5120512\n    2018-12-26 17:02:58.618071: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb179e9200 of size 5120512\n    2018-12-26 17:02:58.618093: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb17ecb400 of size 5120512\n    2018-12-26 17:02:58.618114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb183ad600 of size 83200\n    2018-12-26 17:02:58.618135: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb183c1b00 of size 5120512\n    2018-12-26 17:02:58.618156: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb188a3d00 of size 83200\n    2018-12-26 17:02:58.618177: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb188b8200 of size 5120512\n    2018-12-26 17:02:58.618198: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb18d9a400 of size 99840\n    2018-12-26 17:02:58.618219: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb18db2a00 of size 5120512\n    2018-12-26 17:02:58.618242: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb19294c00 of size 83200\n    2018-12-26 17:02:58.618264: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb192a9100 of size 5120512\n    2018-12-26 17:02:58.618285: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1978b300 of size 83200\n    2018-12-26 17:02:58.618306: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1979f800 of size 5120512\n    2018-12-26 17:02:58.618329: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb19c81a00 of size 83456\n    2018-12-26 17:02:58.618354: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb19c96000 of size 5120512\n    2018-12-26 17:02:58.618375: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1a178200 of size 83200\n    2018-12-26 17:02:58.618397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1a18c700 of size 5120512\n    2018-12-26 17:02:58.618420: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1a66e900 of size 83456\n    2018-12-26 17:02:58.618441: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1a682f00 of size 5120512\n    2018-12-26 17:02:58.618462: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1ab65100 of size 83200\n    2018-12-26 17:02:58.618483: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1ab79600 of size 5120512\n    2018-12-26 17:02:58.618504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1b05b800 of size 83200\n    2018-12-26 17:02:58.618524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1b06fd00 of size 5120512\n    2018-12-26 17:02:58.618546: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1b551f00 of size 83200\n    2018-12-26 17:02:58.618569: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1b566400 of size 5120512\n    2018-12-26 17:02:58.618590: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1ba48600 of size 83200\n    2018-12-26 17:02:58.618611: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1ba5cb00 of size 5120512\n    2018-12-26 17:02:58.618632: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1bf3ed00 of size 83200\n    2018-12-26 17:02:58.618653: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1bf53200 of size 5120512\n    2018-12-26 17:02:58.618674: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1c435400 of size 83200\n    2018-12-26 17:02:58.618695: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1c449900 of size 5120512\n    2018-12-26 17:02:58.618726: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1c92bb00 of size 83200\n    2018-12-26 17:02:58.618750: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1c940000 of size 5120512\n    2018-12-26 17:02:58.618770: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1ce22200 of size 83200\n    2018-12-26 17:02:58.618791: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1ce36700 of size 5120512\n    2018-12-26 17:02:58.618811: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1d318900 of size 83200\n    2018-12-26 17:02:58.618831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1d32ce00 of size 5120512\n    2018-12-26 17:02:58.618852: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1d80f000 of size 83200\n    2018-12-26 17:02:58.618871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1d823500 of size 5120512\n    2018-12-26 17:02:58.618891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1dd05700 of size 83200\n    2018-12-26 17:02:58.618912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1dd19c00 of size 5120512\n    2018-12-26 17:02:58.618932: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1e1fbe00 of size 83200\n    2018-12-26 17:02:58.618952: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1e210300 of size 5120512\n    2018-12-26 17:02:58.618972: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1e6f2500 of size 83200\n    2018-12-26 17:02:58.618992: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1e706a00 of size 5120512\n    2018-12-26 17:02:58.619012: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1ebe8c00 of size 83200\n    2018-12-26 17:02:58.619032: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1ebfd100 of size 5120512\n    2018-12-26 17:02:58.619053: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1f0df300 of size 83200\n    2018-12-26 17:02:58.619074: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1f0f3800 of size 5120512\n    2018-12-26 17:02:58.619093: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1f5d5a00 of size 83200\n    2018-12-26 17:02:58.619114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1f5e9f00 of size 5120512\n    2018-12-26 17:02:58.619135: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1facc100 of size 83200\n    2018-12-26 17:02:58.619155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1fae0600 of size 5120512\n    2018-12-26 17:02:58.619175: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb1ffc2800 of size 83200\n    2018-12-26 17:02:58.619195: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb1ffd6d00 of size 5120512\n    2018-12-26 17:02:58.619216: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb204b8f00 of size 83200\n    2018-12-26 17:02:58.619237: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb204cd400 of size 5120512\n    2018-12-26 17:02:58.619258: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb209af600 of size 83200\n    2018-12-26 17:02:58.619278: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb209c3b00 of size 5120512\n    2018-12-26 17:02:58.619299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb20ea5d00 of size 83200\n    2018-12-26 17:02:58.619319: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb20eba200 of size 5120512\n    2018-12-26 17:02:58.619339: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2139c400 of size 83200\n    2018-12-26 17:02:58.619359: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb213b0900 of size 5120512\n    2018-12-26 17:02:58.619380: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb21892b00 of size 83200\n    2018-12-26 17:02:58.619401: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb218a7000 of size 5120512\n    2018-12-26 17:02:58.619421: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb21d89200 of size 83200\n    2018-12-26 17:02:58.619441: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb21d9d700 of size 5120512\n    2018-12-26 17:02:58.619461: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2227f900 of size 83200\n    2018-12-26 17:02:58.619481: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb22293e00 of size 5120512\n    2018-12-26 17:02:58.619502: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb22776000 of size 83200\n    2018-12-26 17:02:58.619524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2278a500 of size 5120512\n    2018-12-26 17:02:58.619548: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb22c6c700 of size 83200\n    2018-12-26 17:02:58.619569: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb22c80c00 of size 5120512\n    2018-12-26 17:02:58.619590: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb23162e00 of size 83200\n    2018-12-26 17:02:58.619609: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb23177300 of size 5120512\n    2018-12-26 17:02:58.619629: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb23659500 of size 83200\n    2018-12-26 17:02:58.619650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2366da00 of size 5120512\n    2018-12-26 17:02:58.619670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb23b4fc00 of size 83200\n    2018-12-26 17:02:58.619690: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb23b64100 of size 5120512\n    2018-12-26 17:02:58.619718: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb24046300 of size 83200\n    2018-12-26 17:02:58.619741: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2405a800 of size 5120512\n    2018-12-26 17:02:58.619761: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2453ca00 of size 83200\n    2018-12-26 17:02:58.619782: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb24550f00 of size 5120512\n    2018-12-26 17:02:58.619802: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb24a33100 of size 83200\n    2018-12-26 17:02:58.619822: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb24a47600 of size 5120512\n    2018-12-26 17:02:58.619842: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb24f29800 of size 83200\n    2018-12-26 17:02:58.619862: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb24f3dd00 of size 5120512\n    2018-12-26 17:02:58.619882: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2541ff00 of size 83200\n    2018-12-26 17:02:58.619902: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb25434400 of size 5120512\n    2018-12-26 17:02:58.619925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb25916600 of size 83200\n    2018-12-26 17:02:58.619945: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2592ab00 of size 5120512\n    2018-12-26 17:02:58.619980: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb25e0cd00 of size 83200\n    2018-12-26 17:02:58.620005: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb25e21200 of size 5120512\n    2018-12-26 17:02:58.620026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb26303400 of size 83200\n    2018-12-26 17:02:58.620047: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb26317900 of size 5120512\n    2018-12-26 17:02:58.620068: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb267f9b00 of size 83200\n    2018-12-26 17:02:58.620089: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2680e000 of size 5120512\n    2018-12-26 17:02:58.620109: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb26cf0200 of size 83200\n    2018-12-26 17:02:58.620129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb26d04700 of size 5120512\n    2018-12-26 17:02:58.620149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb271e6900 of size 83200\n    2018-12-26 17:02:58.620169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb271fae00 of size 5120512\n    2018-12-26 17:02:58.620189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb276dd000 of size 83200\n    2018-12-26 17:02:58.620209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb276f1500 of size 5120512\n    2018-12-26 17:02:58.620231: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb27bd3700 of size 83200\n    2018-12-26 17:02:58.620253: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb27be7c00 of size 5120512\n    2018-12-26 17:02:58.620275: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb280c9e00 of size 83200\n    2018-12-26 17:02:58.620295: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb280de300 of size 5120512\n    2018-12-26 17:02:58.620315: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb285c0500 of size 83200\n    2018-12-26 17:02:58.620335: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb285d4a00 of size 5120512\n    2018-12-26 17:02:58.620355: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb28ab6c00 of size 83200\n    2018-12-26 17:02:58.620374: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb28acb100 of size 5120512\n    2018-12-26 17:02:58.620394: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb28fad300 of size 83200\n    2018-12-26 17:02:58.620414: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb28fc1800 of size 5120512\n    2018-12-26 17:02:58.620434: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb294a3a00 of size 83200\n    2018-12-26 17:02:58.620456: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb294b7f00 of size 5120512\n    2018-12-26 17:02:58.620475: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2999a100 of size 83200\n    2018-12-26 17:02:58.620495: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb299ae600 of size 5120512\n    2018-12-26 17:02:58.620516: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb29e90800 of size 83200\n    2018-12-26 17:02:58.620537: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb29ea4d00 of size 5120512\n    2018-12-26 17:02:58.620559: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2a386f00 of size 83200\n    2018-12-26 17:02:58.620579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2a39b400 of size 5120512\n    2018-12-26 17:02:58.620599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2a87d600 of size 83200\n    2018-12-26 17:02:58.620619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2a891b00 of size 5120512\n    2018-12-26 17:02:58.620639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2ad73d00 of size 83200\n    2018-12-26 17:02:58.620659: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2ad88200 of size 5120512\n    2018-12-26 17:02:58.620678: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2b26a400 of size 83200\n    2018-12-26 17:02:58.620698: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2b27e900 of size 5120512\n    2018-12-26 17:02:58.620728: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2b760b00 of size 83200\n    2018-12-26 17:02:58.620748: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2b775000 of size 5120512\n    2018-12-26 17:02:58.620768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2bc57200 of size 83200\n    2018-12-26 17:02:58.620788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2bc6b700 of size 5120512\n    2018-12-26 17:02:58.620808: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2c14d900 of size 83200\n    2018-12-26 17:02:58.620830: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2c161e00 of size 5120512\n    2018-12-26 17:02:58.620850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2c644000 of size 83200\n    2018-12-26 17:02:58.620870: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2c658500 of size 5120512\n    2018-12-26 17:02:58.620891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2cb3a700 of size 83200\n    2018-12-26 17:02:58.620911: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2cb4ec00 of size 5120512\n    2018-12-26 17:02:58.620931: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2d030e00 of size 83200\n    2018-12-26 17:02:58.620951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2d045300 of size 5120512\n    2018-12-26 17:02:58.620973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2d527500 of size 83200\n    2018-12-26 17:02:58.620993: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2d53ba00 of size 5120512\n    2018-12-26 17:02:58.621013: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2da1dc00 of size 83200\n    2018-12-26 17:02:58.621032: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2da32100 of size 5120512\n    2018-12-26 17:02:58.621052: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2df14300 of size 83200\n    2018-12-26 17:02:58.621073: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2df28800 of size 5120512\n    2018-12-26 17:02:58.621093: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2e40aa00 of size 83200\n    2018-12-26 17:02:58.621114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2e41ef00 of size 5120512\n    2018-12-26 17:02:58.621134: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2e901100 of size 83200\n    2018-12-26 17:02:58.621154: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2e915600 of size 5120512\n    2018-12-26 17:02:58.621175: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2edf7800 of size 83200\n    2018-12-26 17:02:58.621195: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2ee0bd00 of size 5120512\n    2018-12-26 17:02:58.621214: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2f2edf00 of size 83200\n    2018-12-26 17:02:58.621234: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2f302400 of size 5120512\n    2018-12-26 17:02:58.621254: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2f7e4600 of size 83200\n    2018-12-26 17:02:58.621274: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2f7f8b00 of size 5120512\n    2018-12-26 17:02:58.621294: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb2fcdad00 of size 83200\n    2018-12-26 17:02:58.621314: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb2fcef200 of size 5120512\n    2018-12-26 17:02:58.621335: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb301d1400 of size 83200\n    2018-12-26 17:02:58.621355: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb301e5900 of size 5120512\n    2018-12-26 17:02:58.621375: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb306c7b00 of size 83200\n    2018-12-26 17:02:58.621396: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb306dc000 of size 5120512\n    2018-12-26 17:02:58.621416: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb30bbe200 of size 83200\n    2018-12-26 17:02:58.621435: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb30bd2700 of size 5120512\n    2018-12-26 17:02:58.621456: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb310b4900 of size 83200\n    2018-12-26 17:02:58.621477: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb310c8e00 of size 5120512\n    2018-12-26 17:02:58.621497: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb315ab000 of size 83200\n    2018-12-26 17:02:58.621517: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb315bf500 of size 5120512\n    2018-12-26 17:02:58.621537: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb31aa1700 of size 83200\n    2018-12-26 17:02:58.621558: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb31ab5c00 of size 5120512\n    2018-12-26 17:02:58.621578: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb31f97e00 of size 83200\n    2018-12-26 17:02:58.621599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb31fac300 of size 5120512\n    2018-12-26 17:02:58.621619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3248e500 of size 83200\n    2018-12-26 17:02:58.621640: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb324a2a00 of size 5120512\n    2018-12-26 17:02:58.621660: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb32984c00 of size 83200\n    2018-12-26 17:02:58.621680: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb32999100 of size 5120512\n    2018-12-26 17:02:58.621706: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb32e7b300 of size 83200\n    2018-12-26 17:02:58.621729: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb32e8f800 of size 5120512\n    2018-12-26 17:02:58.621750: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb33371a00 of size 83200\n    2018-12-26 17:02:58.621771: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb33385f00 of size 5120512\n    2018-12-26 17:02:58.621792: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb33868100 of size 83200\n    2018-12-26 17:02:58.621815: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3387c600 of size 5120512\n    2018-12-26 17:02:58.621837: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb33d5e800 of size 83200\n    2018-12-26 17:02:58.621857: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb33d72d00 of size 5120512\n    2018-12-26 17:02:58.621878: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb34254f00 of size 83200\n    2018-12-26 17:02:58.621898: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb34269400 of size 5120512\n    2018-12-26 17:02:58.621919: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3474b600 of size 83200\n    2018-12-26 17:02:58.621939: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3475fb00 of size 5120512\n    2018-12-26 17:02:58.621959: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb34c41d00 of size 83200\n    2018-12-26 17:02:58.621979: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb34c56200 of size 5120512\n    2018-12-26 17:02:58.622002: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb35138400 of size 83200\n    2018-12-26 17:02:58.622022: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3514c900 of size 5120512\n    2018-12-26 17:02:58.622043: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3562eb00 of size 83200\n    2018-12-26 17:02:58.622063: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb35643000 of size 5120512\n    2018-12-26 17:02:58.622084: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb35b25200 of size 83200\n    2018-12-26 17:02:58.622106: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb35b39700 of size 5120512\n    2018-12-26 17:02:58.622127: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3601b900 of size 83200\n    2018-12-26 17:02:58.622150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3602fe00 of size 5120512\n    2018-12-26 17:02:58.622172: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb36512000 of size 83200\n    2018-12-26 17:02:58.622193: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb36526500 of size 5120512\n    2018-12-26 17:02:58.622215: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb36a08700 of size 83200\n    2018-12-26 17:02:58.622236: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb36a1cc00 of size 5120512\n    2018-12-26 17:02:58.622257: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb36efee00 of size 83200\n    2018-12-26 17:02:58.622278: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb36f13300 of size 5120512\n    2018-12-26 17:02:58.622298: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb373f5500 of size 83200\n    2018-12-26 17:02:58.622321: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb37409a00 of size 5120512\n    2018-12-26 17:02:58.622343: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb378ebc00 of size 83200\n    2018-12-26 17:02:58.622364: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb37900100 of size 5120512\n    2018-12-26 17:02:58.622385: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb37de2300 of size 83200\n    2018-12-26 17:02:58.622405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb37df6800 of size 5120512\n    2018-12-26 17:02:58.622426: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb382d8a00 of size 83200\n    2018-12-26 17:02:58.622447: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb382ecf00 of size 5120512\n    2018-12-26 17:02:58.622467: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb387cf100 of size 83200\n    2018-12-26 17:02:58.622489: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb387e3600 of size 5120512\n    2018-12-26 17:02:58.622511: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb38cc5800 of size 83200\n    2018-12-26 17:02:58.622532: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb38cd9d00 of size 5120512\n    2018-12-26 17:02:58.622552: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb391bbf00 of size 83200\n    2018-12-26 17:02:58.622572: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb391d0400 of size 5120512\n    2018-12-26 17:02:58.622592: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb396b2600 of size 83200\n    2018-12-26 17:02:58.622613: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb396c6b00 of size 5120512\n    2018-12-26 17:02:58.622632: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb39ba8d00 of size 83200\n    2018-12-26 17:02:58.622652: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb39bbd200 of size 5120512\n    2018-12-26 17:02:58.622673: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3a09f400 of size 83200\n    2018-12-26 17:02:58.622694: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3a0b3900 of size 5120512\n    2018-12-26 17:02:58.622723: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3a595b00 of size 83200\n    2018-12-26 17:02:58.622745: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3a5aa000 of size 5120512\n    2018-12-26 17:02:58.622766: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3aa8c200 of size 83200\n    2018-12-26 17:02:58.622786: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3aaa0700 of size 5120512\n    2018-12-26 17:02:58.622806: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3af82900 of size 83200\n    2018-12-26 17:02:58.622826: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3af96e00 of size 5120512\n    2018-12-26 17:02:58.622848: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3b479000 of size 83200\n    2018-12-26 17:02:58.622869: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3b48d500 of size 5120512\n    2018-12-26 17:02:58.622890: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3b96f700 of size 83200\n    2018-12-26 17:02:58.622910: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3b983c00 of size 5120512\n    2018-12-26 17:02:58.622930: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3be65e00 of size 83200\n    2018-12-26 17:02:58.622951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3be7a300 of size 5120512\n    2018-12-26 17:02:58.622972: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3c35c500 of size 83200\n    2018-12-26 17:02:58.622994: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3c370a00 of size 5120512\n    2018-12-26 17:02:58.623015: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3c852c00 of size 83200\n    2018-12-26 17:02:58.623036: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3c867100 of size 5120512\n    2018-12-26 17:02:58.623056: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3cd49300 of size 83200\n    2018-12-26 17:02:58.623076: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3cd5d800 of size 5120512\n    2018-12-26 17:02:58.623097: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3d23fa00 of size 83200\n    2018-12-26 17:02:58.623117: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3d253f00 of size 5120512\n    2018-12-26 17:02:58.623138: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3d736100 of size 83200\n    2018-12-26 17:02:58.623159: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3d74a600 of size 5120512\n    2018-12-26 17:02:58.623180: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3dc2c800 of size 83200\n    2018-12-26 17:02:58.623200: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3dc40d00 of size 5120512\n    2018-12-26 17:02:58.623221: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3e122f00 of size 83200\n    2018-12-26 17:02:58.623242: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3e137400 of size 5120512\n    2018-12-26 17:02:58.623262: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3e619600 of size 83200\n    2018-12-26 17:02:58.623283: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3e62db00 of size 5120512\n    2018-12-26 17:02:58.623303: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3eb0fd00 of size 83200\n    2018-12-26 17:02:58.623325: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3eb24200 of size 5120512\n    2018-12-26 17:02:58.623345: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3f006400 of size 83200\n    2018-12-26 17:02:58.623365: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3f01a900 of size 5120512\n    2018-12-26 17:02:58.623386: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3f4fcb00 of size 83200\n    2018-12-26 17:02:58.623406: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3f511000 of size 5120512\n    2018-12-26 17:02:58.623426: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3f9f3200 of size 83200\n    2018-12-26 17:02:58.623446: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3fa07700 of size 5120512\n    2018-12-26 17:02:58.623466: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb3fee9900 of size 83200\n    2018-12-26 17:02:58.623487: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb3fefde00 of size 5120512\n    2018-12-26 17:02:58.623507: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb403e0000 of size 83200\n    2018-12-26 17:02:58.623527: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb403f4500 of size 5120512\n    2018-12-26 17:02:58.623548: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb408d6700 of size 83200\n    2018-12-26 17:02:58.623568: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb408eac00 of size 5120512\n    2018-12-26 17:02:58.623590: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb40dcce00 of size 83200\n    2018-12-26 17:02:58.623610: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb40de1300 of size 5120512\n    2018-12-26 17:02:58.623630: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb412c3500 of size 83200\n    2018-12-26 17:02:58.623651: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb412d7a00 of size 5120512\n    2018-12-26 17:02:58.623673: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb417b9c00 of size 83200\n    2018-12-26 17:02:58.623695: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb417ce100 of size 5120512\n    2018-12-26 17:02:58.623725: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb41cb0300 of size 83200\n    2018-12-26 17:02:58.623747: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb41cc4800 of size 5120512\n    2018-12-26 17:02:58.623769: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb421a6a00 of size 83200\n    2018-12-26 17:02:58.623790: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb421baf00 of size 5120512\n    2018-12-26 17:02:58.623810: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4269d100 of size 83200\n    2018-12-26 17:02:58.623831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb426b1600 of size 5120512\n    2018-12-26 17:02:58.623851: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb42b93800 of size 83200\n    2018-12-26 17:02:58.623871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb42ba7d00 of size 5120512\n    2018-12-26 17:02:58.623891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb43089f00 of size 83200\n    2018-12-26 17:02:58.623912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4309e400 of size 5120512\n    2018-12-26 17:02:58.623933: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb43580600 of size 83200\n    2018-12-26 17:02:58.623955: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb43594b00 of size 5120512\n    2018-12-26 17:02:58.623990: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb43a76d00 of size 83200\n    2018-12-26 17:02:58.624012: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb43a8b200 of size 5120512\n    2018-12-26 17:02:58.624033: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb43f6d400 of size 83200\n    2018-12-26 17:02:58.624054: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb43f81900 of size 5120512\n    2018-12-26 17:02:58.624074: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb44463b00 of size 83200\n    2018-12-26 17:02:58.624094: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb44478000 of size 5120512\n    2018-12-26 17:02:58.624116: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4495a200 of size 83200\n    2018-12-26 17:02:58.624136: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4496e700 of size 5120512\n    2018-12-26 17:02:58.624157: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb44e50900 of size 83200\n    2018-12-26 17:02:58.624178: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb44e64e00 of size 5120512\n    2018-12-26 17:02:58.624198: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb45347000 of size 83200\n    2018-12-26 17:02:58.624218: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4535b500 of size 5120512\n    2018-12-26 17:02:58.624239: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4583d700 of size 83200\n    2018-12-26 17:02:58.624259: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb45851c00 of size 5120512\n    2018-12-26 17:02:58.624279: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb45d33e00 of size 83200\n    2018-12-26 17:02:58.624299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb45d48300 of size 5120512\n    2018-12-26 17:02:58.624320: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4622a500 of size 83200\n    2018-12-26 17:02:58.624340: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4623ea00 of size 5120512\n    2018-12-26 17:02:58.624360: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb46720c00 of size 83200\n    2018-12-26 17:02:58.624380: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb46735100 of size 5120512\n    2018-12-26 17:02:58.624403: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb46c17300 of size 83200\n    2018-12-26 17:02:58.624423: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb46c2b800 of size 5120512\n    2018-12-26 17:02:58.624443: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4710da00 of size 83200\n    2018-12-26 17:02:58.624464: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb47121f00 of size 5120512\n    2018-12-26 17:02:58.624484: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb47604100 of size 83200\n    2018-12-26 17:02:58.624504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb47618600 of size 5120512\n    2018-12-26 17:02:58.624524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb47afa800 of size 83200\n    2018-12-26 17:02:58.624545: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb47b0ed00 of size 5120512\n    2018-12-26 17:02:58.624566: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb47ff0f00 of size 83200\n    2018-12-26 17:02:58.624586: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb48005400 of size 5120512\n    2018-12-26 17:02:58.624609: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb484e7600 of size 83200\n    2018-12-26 17:02:58.624630: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb484fbb00 of size 5120512\n    2018-12-26 17:02:58.624650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb489ddd00 of size 83200\n    2018-12-26 17:02:58.624670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb489f2200 of size 5120512\n    2018-12-26 17:02:58.624691: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb48ed4400 of size 83200\n    2018-12-26 17:02:58.624720: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb48ee8900 of size 5120512\n    2018-12-26 17:02:58.624742: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb493cab00 of size 83200\n    2018-12-26 17:02:58.624763: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb493df000 of size 5120512\n    2018-12-26 17:02:58.624784: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb498c1200 of size 83200\n    2018-12-26 17:02:58.624804: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb498d5700 of size 5120512\n    2018-12-26 17:02:58.624824: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb49db7900 of size 83200\n    2018-12-26 17:02:58.624844: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb49dcbe00 of size 5120512\n    2018-12-26 17:02:58.624864: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4a2ae000 of size 83200\n    2018-12-26 17:02:58.624883: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4a2c2500 of size 5120512\n    2018-12-26 17:02:58.624899: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4a7a4700 of size 83200\n    2018-12-26 17:02:58.624921: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4a7b8c00 of size 5120512\n    2018-12-26 17:02:58.624942: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4ac9ae00 of size 83200\n    2018-12-26 17:02:58.624964: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4acaf300 of size 5120512\n    2018-12-26 17:02:58.624984: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4b191500 of size 83200\n    2018-12-26 17:02:58.625004: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4b1a5a00 of size 5120512\n    2018-12-26 17:02:58.625024: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4b687c00 of size 83200\n    2018-12-26 17:02:58.625044: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4b69c100 of size 5120512\n    2018-12-26 17:02:58.625063: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4bb7e300 of size 83200\n    2018-12-26 17:02:58.625083: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4bb92800 of size 5120512\n    2018-12-26 17:02:58.625104: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4c074a00 of size 83200\n    2018-12-26 17:02:58.625124: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4c088f00 of size 5120512\n    2018-12-26 17:02:58.625146: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4c56b100 of size 83200\n    2018-12-26 17:02:58.625166: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4c57f600 of size 5120512\n    2018-12-26 17:02:58.625185: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4ca61800 of size 83200\n    2018-12-26 17:02:58.625205: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4ca75d00 of size 5120512\n    2018-12-26 17:02:58.625226: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4cf57f00 of size 83200\n    2018-12-26 17:02:58.625246: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4cf6c400 of size 5120512\n    2018-12-26 17:02:58.625266: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4d44e600 of size 83200\n    2018-12-26 17:02:58.625286: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4d462b00 of size 5120512\n    2018-12-26 17:02:58.625305: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4d944d00 of size 83200\n    2018-12-26 17:02:58.625325: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4d959200 of size 5120512\n    2018-12-26 17:02:58.625347: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4de3b400 of size 83200\n    2018-12-26 17:02:58.625366: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4de4f900 of size 5120512\n    2018-12-26 17:02:58.625387: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4e331b00 of size 83200\n    2018-12-26 17:02:58.625407: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4e346000 of size 5120512\n    2018-12-26 17:02:58.625427: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4e828200 of size 83200\n    2018-12-26 17:02:58.625452: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4e83c700 of size 5120512\n    2018-12-26 17:02:58.625471: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4ed1e900 of size 83200\n    2018-12-26 17:02:58.625491: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4ed32e00 of size 5120512\n    2018-12-26 17:02:58.625512: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4f215000 of size 83200\n    2018-12-26 17:02:58.625531: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4f229500 of size 5120512\n    2018-12-26 17:02:58.625551: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4f70b700 of size 83200\n    2018-12-26 17:02:58.625571: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4f71fc00 of size 5120512\n    2018-12-26 17:02:58.625591: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb4fc01e00 of size 83200\n    2018-12-26 17:02:58.625611: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb4fc16300 of size 5120512\n    2018-12-26 17:02:58.625631: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb500f8500 of size 83200\n    2018-12-26 17:02:58.625650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5010ca00 of size 5120512\n    2018-12-26 17:02:58.625670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb505eec00 of size 83200\n    2018-12-26 17:02:58.625690: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb50603100 of size 5120512\n    2018-12-26 17:02:58.625716: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb50ae5300 of size 83200\n    2018-12-26 17:02:58.625738: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb50af9800 of size 5120512\n    2018-12-26 17:02:58.625758: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb50fdba00 of size 83200\n    2018-12-26 17:02:58.625779: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb50feff00 of size 5120512\n    2018-12-26 17:02:58.625800: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb514d2100 of size 83200\n    2018-12-26 17:02:58.625820: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb514e6600 of size 5120512\n    2018-12-26 17:02:58.625840: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb519c8800 of size 83200\n    2018-12-26 17:02:58.625861: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb519dcd00 of size 5120512\n    2018-12-26 17:02:58.625881: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb51ebef00 of size 83200\n    2018-12-26 17:02:58.625901: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb51ed3400 of size 5120512\n    2018-12-26 17:02:58.625921: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb523b5600 of size 83200\n    2018-12-26 17:02:58.625941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb523c9b00 of size 5120512\n    2018-12-26 17:02:58.625961: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb528abd00 of size 83200\n    2018-12-26 17:02:58.625981: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb528c0200 of size 5120512\n    2018-12-26 17:02:58.626001: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb52da2400 of size 83200\n    2018-12-26 17:02:58.626021: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb52db6900 of size 5120512\n    2018-12-26 17:02:58.626040: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb53298b00 of size 83200\n    2018-12-26 17:02:58.626061: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb532ad000 of size 5120512\n    2018-12-26 17:02:58.626080: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5378f200 of size 83200\n    2018-12-26 17:02:58.626103: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb537a3700 of size 5120512\n    2018-12-26 17:02:58.626123: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb53c85900 of size 83200\n    2018-12-26 17:02:58.626143: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb53c99e00 of size 5120512\n    2018-12-26 17:02:58.626163: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5417c000 of size 83200\n    2018-12-26 17:02:58.626183: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb54190500 of size 5120512\n    2018-12-26 17:02:58.626203: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb54672700 of size 83200\n    2018-12-26 17:02:58.626223: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb54686c00 of size 5120512\n    2018-12-26 17:02:58.626243: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb54b68e00 of size 83200\n    2018-12-26 17:02:58.626263: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb54b7d300 of size 5120512\n    2018-12-26 17:02:58.626283: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5505f500 of size 83200\n    2018-12-26 17:02:58.626303: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb55073a00 of size 5120512\n    2018-12-26 17:02:58.626322: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb55555c00 of size 83200\n    2018-12-26 17:02:58.626342: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5556a100 of size 5120512\n    2018-12-26 17:02:58.626362: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb55a4c300 of size 83200\n    2018-12-26 17:02:58.626382: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb55a60800 of size 5120512\n    2018-12-26 17:02:58.626402: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb55f42a00 of size 83200\n    2018-12-26 17:02:58.626423: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb55f56f00 of size 5120512\n    2018-12-26 17:02:58.626444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb56439100 of size 83200\n    2018-12-26 17:02:58.626464: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5644d600 of size 5120512\n    2018-12-26 17:02:58.626484: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5692f800 of size 83200\n    2018-12-26 17:02:58.626504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb56943d00 of size 5120512\n    2018-12-26 17:02:58.626524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb56e25f00 of size 83200\n    2018-12-26 17:02:58.626543: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb56e3a400 of size 5120512\n    2018-12-26 17:02:58.626563: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5731c600 of size 83200\n    2018-12-26 17:02:58.626583: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb57330b00 of size 5120512\n    2018-12-26 17:02:58.626603: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb57812d00 of size 83200\n    2018-12-26 17:02:58.626623: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb57827200 of size 5120512\n    2018-12-26 17:02:58.626643: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb57d09400 of size 83200\n    2018-12-26 17:02:58.626663: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb57d1d900 of size 5120512\n    2018-12-26 17:02:58.626682: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb581ffb00 of size 83200\n    2018-12-26 17:02:58.626709: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb58214000 of size 5120512\n    2018-12-26 17:02:58.626731: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb586f6200 of size 83200\n    2018-12-26 17:02:58.626754: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5870a700 of size 5120512\n    2018-12-26 17:02:58.626774: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb58bec900 of size 83200\n    2018-12-26 17:02:58.626794: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb58c00e00 of size 5120512\n    2018-12-26 17:02:58.626814: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb590e3000 of size 83200\n    2018-12-26 17:02:58.626834: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb590f7500 of size 5120512\n    2018-12-26 17:02:58.626854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb595d9700 of size 83200\n    2018-12-26 17:02:58.626874: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb595edc00 of size 5120512\n    2018-12-26 17:02:58.626894: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb59acfe00 of size 83200\n    2018-12-26 17:02:58.626913: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb59ae4300 of size 5120512\n    2018-12-26 17:02:58.626933: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb59fc6500 of size 83200\n    2018-12-26 17:02:58.626953: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb59fdaa00 of size 5120512\n    2018-12-26 17:02:58.626973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5a4bcc00 of size 83200\n    2018-12-26 17:02:58.626993: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5a4d1100 of size 5120512\n    2018-12-26 17:02:58.627013: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5a9b3300 of size 83200\n    2018-12-26 17:02:58.627032: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5a9c7800 of size 5120512\n    2018-12-26 17:02:58.627052: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5aea9a00 of size 83200\n    2018-12-26 17:02:58.627075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5aebdf00 of size 5120512\n    2018-12-26 17:02:58.627094: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5b3a0100 of size 83200\n    2018-12-26 17:02:58.627114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5b3b4600 of size 5120512\n    2018-12-26 17:02:58.627135: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5b896800 of size 83200\n    2018-12-26 17:02:58.627155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5b8aad00 of size 5120512\n    2018-12-26 17:02:58.627175: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5bd8cf00 of size 83200\n    2018-12-26 17:02:58.627195: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5bda1400 of size 5120512\n    2018-12-26 17:02:58.627215: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5c283600 of size 83200\n    2018-12-26 17:02:58.627235: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5c297b00 of size 5120512\n    2018-12-26 17:02:58.627255: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5c779d00 of size 83200\n    2018-12-26 17:02:58.627275: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5c78e200 of size 5120512\n    2018-12-26 17:02:58.627294: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5cc70400 of size 83200\n    2018-12-26 17:02:58.627314: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5cc84900 of size 5120512\n    2018-12-26 17:02:58.627335: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5d166b00 of size 83200\n    2018-12-26 17:02:58.627355: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5d17b000 of size 5120512\n    2018-12-26 17:02:58.627375: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5d65d200 of size 83200\n    2018-12-26 17:02:58.627397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5d671700 of size 5120512\n    2018-12-26 17:02:58.627417: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5db53900 of size 83200\n    2018-12-26 17:02:58.627438: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5db67e00 of size 5120512\n    2018-12-26 17:02:58.627458: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5e04a000 of size 83200\n    2018-12-26 17:02:58.627478: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5e05e500 of size 5120512\n    2018-12-26 17:02:58.627498: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5e540700 of size 83200\n    2018-12-26 17:02:58.627519: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5e554c00 of size 5120512\n    2018-12-26 17:02:58.627540: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5ea36e00 of size 83200\n    2018-12-26 17:02:58.627560: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5ea4b300 of size 5120512\n    2018-12-26 17:02:58.627580: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5ef2d500 of size 83200\n    2018-12-26 17:02:58.627599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5ef41a00 of size 5120512\n    2018-12-26 17:02:58.627620: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5f423c00 of size 83200\n    2018-12-26 17:02:58.627639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5f438100 of size 5120512\n    2018-12-26 17:02:58.627659: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5f91a300 of size 83200\n    2018-12-26 17:02:58.627679: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5f92e800 of size 5120512\n    2018-12-26 17:02:58.627705: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb5fe10a00 of size 83200\n    2018-12-26 17:02:58.627730: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb5fe24f00 of size 5120512\n    2018-12-26 17:02:58.627751: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb60307100 of size 83200\n    2018-12-26 17:02:58.627771: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6031b600 of size 5120512\n    2018-12-26 17:02:58.627791: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb607fd800 of size 83200\n    2018-12-26 17:02:58.627811: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb60811d00 of size 5120512\n    2018-12-26 17:02:58.627831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb60cf3f00 of size 83200\n    2018-12-26 17:02:58.627851: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb60d08400 of size 5120512\n    2018-12-26 17:02:58.627871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb611ea600 of size 83200\n    2018-12-26 17:02:58.627891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb611feb00 of size 5120512\n    2018-12-26 17:02:58.627911: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb616e0d00 of size 83200\n    2018-12-26 17:02:58.627930: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb616f5200 of size 5120512\n    2018-12-26 17:02:58.627951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb61bd7400 of size 83200\n    2018-12-26 17:02:58.627985: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb61beb900 of size 5120512\n    2018-12-26 17:02:58.628006: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb620cdb00 of size 83200\n    2018-12-26 17:02:58.628026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb620e2000 of size 5120512\n    2018-12-26 17:02:58.628046: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb625c4200 of size 83200\n    2018-12-26 17:02:58.628069: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb625d8700 of size 5120512\n    2018-12-26 17:02:58.628089: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb62aba900 of size 83200\n    2018-12-26 17:02:58.628109: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb62acee00 of size 5120512\n    2018-12-26 17:02:58.628129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb62fb1000 of size 83200\n    2018-12-26 17:02:58.628149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb62fc5500 of size 5120512\n    2018-12-26 17:02:58.628168: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb634a7700 of size 83200\n    2018-12-26 17:02:58.628191: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb634bbc00 of size 5120512\n    2018-12-26 17:02:58.628218: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6399de00 of size 83200\n    2018-12-26 17:02:58.628240: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb639b2300 of size 5120512\n    2018-12-26 17:02:58.628265: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb63e94500 of size 83200\n    2018-12-26 17:02:58.628287: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb63ea8a00 of size 5120512\n    2018-12-26 17:02:58.628302: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6438ac00 of size 83200\n    2018-12-26 17:02:58.628315: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6439f100 of size 5120512\n    2018-12-26 17:02:58.628329: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb64881300 of size 83200\n    2018-12-26 17:02:58.628342: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb64895800 of size 5120512\n    2018-12-26 17:02:58.628355: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb64d77a00 of size 83200\n    2018-12-26 17:02:58.628370: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb64d8bf00 of size 5120512\n    2018-12-26 17:02:58.628383: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6526e100 of size 83200\n    2018-12-26 17:02:58.628397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb65282600 of size 5120512\n    2018-12-26 17:02:58.628410: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb65764800 of size 83200\n    2018-12-26 17:02:58.628450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb65778d00 of size 5120512\n    2018-12-26 17:02:58.628476: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb65c5af00 of size 83200\n    2018-12-26 17:02:58.628496: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb65c6f400 of size 5120512\n    2018-12-26 17:02:58.628516: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb66151600 of size 83200\n    2018-12-26 17:02:58.628535: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb66165b00 of size 5120512\n    2018-12-26 17:02:58.628555: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb66647d00 of size 83200\n    2018-12-26 17:02:58.628574: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6665c200 of size 5120512\n    2018-12-26 17:02:58.628593: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb66b3e400 of size 83200\n    2018-12-26 17:02:58.628613: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb66b52900 of size 5120512\n    2018-12-26 17:02:58.628632: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb67034b00 of size 83200\n    2018-12-26 17:02:58.628646: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb67049000 of size 5120512\n    2018-12-26 17:02:58.628660: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6752b200 of size 83200\n    2018-12-26 17:02:58.628675: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6753f700 of size 5120512\n    2018-12-26 17:02:58.628688: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb67a21900 of size 83200\n    2018-12-26 17:02:58.628707: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb67a35e00 of size 5120512\n    2018-12-26 17:02:58.628723: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb67f18000 of size 83200\n    2018-12-26 17:02:58.628737: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb67f2c500 of size 5120512\n    2018-12-26 17:02:58.628751: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6840e700 of size 83200\n    2018-12-26 17:02:58.628764: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb68422c00 of size 5120512\n    2018-12-26 17:02:58.628778: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb68904e00 of size 83200\n    2018-12-26 17:02:58.628791: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb68919300 of size 5120512\n    2018-12-26 17:02:58.628804: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb68dfb500 of size 83200\n    2018-12-26 17:02:58.628818: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb68e0fa00 of size 5120512\n    2018-12-26 17:02:58.628831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb692f1c00 of size 83200\n    2018-12-26 17:02:58.628844: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb69306100 of size 5120512\n    2018-12-26 17:02:58.628858: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb697e8300 of size 83200\n    2018-12-26 17:02:58.628871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb697fc800 of size 5120512\n    2018-12-26 17:02:58.628884: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb69cdea00 of size 83200\n    2018-12-26 17:02:58.628899: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb69cf2f00 of size 5120512\n    2018-12-26 17:02:58.628912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6a1d5100 of size 83200\n    2018-12-26 17:02:58.628925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6a1e9600 of size 5120512\n    2018-12-26 17:02:58.628939: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6a6cb800 of size 83200\n    2018-12-26 17:02:58.628952: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6a6dfd00 of size 5120512\n    2018-12-26 17:02:58.628965: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6abc1f00 of size 83200\n    2018-12-26 17:02:58.628979: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6abd6400 of size 5120512\n    2018-12-26 17:02:58.628992: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6b0b8600 of size 83200\n    2018-12-26 17:02:58.629005: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6b0ccb00 of size 5120512\n    2018-12-26 17:02:58.629019: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6b5aed00 of size 83200\n    2018-12-26 17:02:58.629032: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6b5c3200 of size 5120512\n    2018-12-26 17:02:58.629045: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6baa5400 of size 83200\n    2018-12-26 17:02:58.629059: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6bab9900 of size 5120512\n    2018-12-26 17:02:58.629072: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6bf9bb00 of size 83200\n    2018-12-26 17:02:58.629085: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6bfb0000 of size 5120512\n    2018-12-26 17:02:58.629098: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6c492200 of size 83200\n    2018-12-26 17:02:58.629113: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6c4a6700 of size 5120512\n    2018-12-26 17:02:58.629127: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6c988900 of size 83200\n    2018-12-26 17:02:58.629140: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6c99ce00 of size 5120512\n    2018-12-26 17:02:58.629153: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6ce7f000 of size 83200\n    2018-12-26 17:02:58.629167: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6ce93500 of size 5120512\n    2018-12-26 17:02:58.629180: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6d375700 of size 83200\n    2018-12-26 17:02:58.629192: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6d389c00 of size 5120512\n    2018-12-26 17:02:58.629214: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6d86be00 of size 83200\n    2018-12-26 17:02:58.629242: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6d880300 of size 5120512\n    2018-12-26 17:02:58.629265: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6dd62500 of size 83200\n    2018-12-26 17:02:58.629293: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6dd76a00 of size 5120512\n    2018-12-26 17:02:58.629316: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6e258c00 of size 83200\n    2018-12-26 17:02:58.629337: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6e26d100 of size 5120512\n    2018-12-26 17:02:58.629357: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6e74f300 of size 83200\n    2018-12-26 17:02:58.629377: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6e763800 of size 5120512\n    2018-12-26 17:02:58.629396: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6ec45a00 of size 83200\n    2018-12-26 17:02:58.629419: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6ec59f00 of size 5120512\n    2018-12-26 17:02:58.629440: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6f13c100 of size 83200\n    2018-12-26 17:02:58.629460: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6f150600 of size 5120512\n    2018-12-26 17:02:58.629479: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6f632800 of size 83200\n    2018-12-26 17:02:58.629499: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6f646d00 of size 5120512\n    2018-12-26 17:02:58.629519: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb6fb28f00 of size 83200\n    2018-12-26 17:02:58.629539: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb6fb3d400 of size 5120512\n    2018-12-26 17:02:58.629559: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7001f600 of size 83200\n    2018-12-26 17:02:58.629579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb70033b00 of size 5120512\n    2018-12-26 17:02:58.629599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb70515d00 of size 83200\n    2018-12-26 17:02:58.629619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7052a200 of size 5120512\n    2018-12-26 17:02:58.629637: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb70a0c400 of size 83200\n    2018-12-26 17:02:58.629658: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb70a20900 of size 5120512\n    2018-12-26 17:02:58.629678: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb70f02b00 of size 83200\n    2018-12-26 17:02:58.629698: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb70f17000 of size 5120512\n    2018-12-26 17:02:58.629727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb713f9200 of size 83200\n    2018-12-26 17:02:58.629751: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7140d700 of size 5120512\n    2018-12-26 17:02:58.629772: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb718ef900 of size 83200\n    2018-12-26 17:02:58.629792: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb71903e00 of size 5120512\n    2018-12-26 17:02:58.629812: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb71de6000 of size 83200\n    2018-12-26 17:02:58.629832: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb71dfa500 of size 5120512\n    2018-12-26 17:02:58.629852: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb722dc700 of size 83200\n    2018-12-26 17:02:58.629872: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb722f0c00 of size 5120512\n    2018-12-26 17:02:58.629892: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb727d2e00 of size 83200\n    2018-12-26 17:02:58.629912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb727e7300 of size 5120512\n    2018-12-26 17:02:58.629932: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb72cc9500 of size 83200\n    2018-12-26 17:02:58.629952: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb72cdda00 of size 5120512\n    2018-12-26 17:02:58.629972: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb731bfc00 of size 83200\n    2018-12-26 17:02:58.629991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb731d4100 of size 5120512\n    2018-12-26 17:02:58.630011: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb736b6300 of size 83200\n    2018-12-26 17:02:58.630031: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb736ca800 of size 5120512\n    2018-12-26 17:02:58.630051: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb73baca00 of size 83200\n    2018-12-26 17:02:58.630094: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb73bc0f00 of size 5120512\n    2018-12-26 17:02:58.630120: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb740a3100 of size 83200\n    2018-12-26 17:02:58.630142: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb740b7600 of size 5120512\n    2018-12-26 17:02:58.630162: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb74599800 of size 83200\n    2018-12-26 17:02:58.630182: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb745add00 of size 5120512\n    2018-12-26 17:02:58.630202: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb74a8ff00 of size 83200\n    2018-12-26 17:02:58.630222: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb74aa4400 of size 5120512\n    2018-12-26 17:02:58.630242: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb74f86600 of size 83200\n    2018-12-26 17:02:58.630262: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb74f9ab00 of size 5120512\n    2018-12-26 17:02:58.630282: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7547cd00 of size 83200\n    2018-12-26 17:02:58.630302: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb75491200 of size 5120512\n    2018-12-26 17:02:58.630322: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb75973400 of size 83200\n    2018-12-26 17:02:58.630341: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb75987900 of size 5120512\n    2018-12-26 17:02:58.630361: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb75e69b00 of size 83200\n    2018-12-26 17:02:58.630381: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb75e7e000 of size 5120512\n    2018-12-26 17:02:58.630401: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb76360200 of size 83200\n    2018-12-26 17:02:58.630423: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb76374700 of size 5120512\n    2018-12-26 17:02:58.630444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb76856900 of size 83200\n    2018-12-26 17:02:58.630464: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7686ae00 of size 5120512\n    2018-12-26 17:02:58.630485: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb76d4d000 of size 83200\n    2018-12-26 17:02:58.630505: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb76d61500 of size 5120512\n    2018-12-26 17:02:58.630526: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb77243700 of size 83200\n    2018-12-26 17:02:58.630546: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb77257c00 of size 5120512\n    2018-12-26 17:02:58.630566: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb77739e00 of size 83200\n    2018-12-26 17:02:58.630587: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7774e300 of size 5120512\n    2018-12-26 17:02:58.630606: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb77c30500 of size 83200\n    2018-12-26 17:02:58.630627: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb77c44a00 of size 5120512\n    2018-12-26 17:02:58.630647: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb78126c00 of size 83200\n    2018-12-26 17:02:58.630667: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7813b100 of size 5120512\n    2018-12-26 17:02:58.630686: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7861d300 of size 83200\n    2018-12-26 17:02:58.630715: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb78631800 of size 5120512\n    2018-12-26 17:02:58.630738: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb78b13a00 of size 83200\n    2018-12-26 17:02:58.630761: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb78b27f00 of size 5120512\n    2018-12-26 17:02:58.630781: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7900a100 of size 83200\n    2018-12-26 17:02:58.630801: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7901e600 of size 5120512\n    2018-12-26 17:02:58.630821: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb79500800 of size 83200\n    2018-12-26 17:02:58.630842: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb79514d00 of size 5120512\n    2018-12-26 17:02:58.630865: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb799f6f00 of size 83200\n    2018-12-26 17:02:58.630885: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb79a0b400 of size 5120512\n    2018-12-26 17:02:58.630905: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb79eed600 of size 83200\n    2018-12-26 17:02:58.630925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb79f01b00 of size 5120512\n    2018-12-26 17:02:58.630945: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7a3e3d00 of size 83200\n    2018-12-26 17:02:58.630965: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7a3f8200 of size 5120512\n    2018-12-26 17:02:58.630984: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7a8da400 of size 83200\n    2018-12-26 17:02:58.631004: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7a8ee900 of size 5120512\n    2018-12-26 17:02:58.631024: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7add0b00 of size 83200\n    2018-12-26 17:02:58.631044: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7ade5000 of size 5120512\n    2018-12-26 17:02:58.631064: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7b2c7200 of size 83200\n    2018-12-26 17:02:58.631086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7b2db700 of size 5120512\n    2018-12-26 17:02:58.631106: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7b7bd900 of size 83200\n    2018-12-26 17:02:58.631127: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7b7d1e00 of size 5120512\n    2018-12-26 17:02:58.631146: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7bcb4000 of size 83200\n    2018-12-26 17:02:58.631166: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7bcc8500 of size 5120512\n    2018-12-26 17:02:58.631187: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7c1aa700 of size 83200\n    2018-12-26 17:02:58.631208: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7c1bec00 of size 5120512\n    2018-12-26 17:02:58.631230: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7c6a0e00 of size 83200\n    2018-12-26 17:02:58.631250: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7c6b5300 of size 5120512\n    2018-12-26 17:02:58.631270: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7cb97500 of size 83200\n    2018-12-26 17:02:58.631290: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7cbaba00 of size 5120512\n    2018-12-26 17:02:58.631310: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7d08dc00 of size 83200\n    2018-12-26 17:02:58.631330: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7d0a2100 of size 5120512\n    2018-12-26 17:02:58.631350: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7d584300 of size 83200\n    2018-12-26 17:02:58.631369: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7d598800 of size 5120512\n    2018-12-26 17:02:58.631389: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7da7aa00 of size 83200\n    2018-12-26 17:02:58.631411: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7da8ef00 of size 5120512\n    2018-12-26 17:02:58.631431: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7df71100 of size 83200\n    2018-12-26 17:02:58.631451: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7df85600 of size 5120512\n    2018-12-26 17:02:58.631471: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7e467800 of size 83200\n    2018-12-26 17:02:58.631491: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7e47bd00 of size 5120512\n    2018-12-26 17:02:58.631511: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7e95df00 of size 83200\n    2018-12-26 17:02:58.631531: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7e972400 of size 5120512\n    2018-12-26 17:02:58.631551: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7ee54600 of size 83200\n    2018-12-26 17:02:58.631571: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7ee68b00 of size 5120512\n    2018-12-26 17:02:58.631591: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7f34ad00 of size 83200\n    2018-12-26 17:02:58.631611: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7f35f200 of size 5120512\n    2018-12-26 17:02:58.631631: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7f841400 of size 83200\n    2018-12-26 17:02:58.631650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7f855900 of size 5120512\n    2018-12-26 17:02:58.631670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb7fd37b00 of size 83200\n    2018-12-26 17:02:58.631690: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb7fd4c000 of size 5120512\n    2018-12-26 17:02:58.631718: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8022e200 of size 83200\n    2018-12-26 17:02:58.631742: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb80242700 of size 5120512\n    2018-12-26 17:02:58.631762: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb80724900 of size 83200\n    2018-12-26 17:02:58.631783: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb80738e00 of size 5120512\n    2018-12-26 17:02:58.631803: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb80c1b000 of size 83200\n    2018-12-26 17:02:58.631823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb80c2f500 of size 5120512\n    2018-12-26 17:02:58.631842: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb81111700 of size 83200\n    2018-12-26 17:02:58.631862: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb81125c00 of size 5120512\n    2018-12-26 17:02:58.631882: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb81607e00 of size 83200\n    2018-12-26 17:02:58.631902: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8161c300 of size 5120512\n    2018-12-26 17:02:58.631922: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb81afe500 of size 83200\n    2018-12-26 17:02:58.631942: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb81b12a00 of size 5120512\n    2018-12-26 17:02:58.631976: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb81ff4c00 of size 83200\n    2018-12-26 17:02:58.631998: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb82009100 of size 5120512\n    2018-12-26 17:02:58.632017: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb824eb300 of size 83200\n    2018-12-26 17:02:58.632037: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb824ff800 of size 5120512\n    2018-12-26 17:02:58.632057: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb829e1a00 of size 83200\n    2018-12-26 17:02:58.632079: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb829f5f00 of size 5120512\n    2018-12-26 17:02:58.632099: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb82ed8100 of size 83200\n    2018-12-26 17:02:58.632119: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb82eec600 of size 5120512\n    2018-12-26 17:02:58.632139: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb833ce800 of size 83200\n    2018-12-26 17:02:58.632159: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb833e2d00 of size 5120512\n    2018-12-26 17:02:58.632179: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb838c4f00 of size 83200\n    2018-12-26 17:02:58.632199: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb838d9400 of size 5120512\n    2018-12-26 17:02:58.632219: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb83dbb600 of size 83200\n    2018-12-26 17:02:58.632239: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb83dcfb00 of size 5120512\n    2018-12-26 17:02:58.632260: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb842b1d00 of size 83200\n    2018-12-26 17:02:58.632280: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb842c6200 of size 5120512\n    2018-12-26 17:02:58.632300: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb847a8400 of size 83200\n    2018-12-26 17:02:58.632322: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb847bc900 of size 5120512\n    2018-12-26 17:02:58.632363: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb84c9eb00 of size 83200\n    2018-12-26 17:02:58.632386: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb84cb3000 of size 5120512\n    2018-12-26 17:02:58.632407: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb85195200 of size 83200\n    2018-12-26 17:02:58.632430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb851a9700 of size 5120512\n    2018-12-26 17:02:58.632450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8568b900 of size 83200\n    2018-12-26 17:02:58.632470: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8569fe00 of size 5120512\n    2018-12-26 17:02:58.632490: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb85b82000 of size 83200\n    2018-12-26 17:02:58.632510: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb85b96500 of size 5120512\n    2018-12-26 17:02:58.632530: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb86078700 of size 83200\n    2018-12-26 17:02:58.632550: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8608cc00 of size 5120512\n    2018-12-26 17:02:58.632570: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8656ee00 of size 83200\n    2018-12-26 17:02:58.632590: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb86583300 of size 5120512\n    2018-12-26 17:02:58.632610: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb86a65500 of size 83200\n    2018-12-26 17:02:58.632630: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb86a79a00 of size 5120512\n    2018-12-26 17:02:58.632650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb86f5bc00 of size 83200\n    2018-12-26 17:02:58.632670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb86f70100 of size 5120512\n    2018-12-26 17:02:58.632690: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb87452300 of size 83200\n    2018-12-26 17:02:58.632718: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb87466800 of size 5120512\n    2018-12-26 17:02:58.632739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb87948a00 of size 83200\n    2018-12-26 17:02:58.632762: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8795cf00 of size 5120512\n    2018-12-26 17:02:58.632782: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb87e3f100 of size 83200\n    2018-12-26 17:02:58.632801: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb87e53600 of size 5120512\n    2018-12-26 17:02:58.632821: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb88335800 of size 83200\n    2018-12-26 17:02:58.632841: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb88349d00 of size 5120512\n    2018-12-26 17:02:58.632861: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8882bf00 of size 83200\n    2018-12-26 17:02:58.632881: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb88840400 of size 5120512\n    2018-12-26 17:02:58.632901: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb88d22600 of size 83200\n    2018-12-26 17:02:58.632921: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb88d36b00 of size 5120512\n    2018-12-26 17:02:58.632940: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb89218d00 of size 83200\n    2018-12-26 17:02:58.632963: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8922d200 of size 5120512\n    2018-12-26 17:02:58.632984: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8970f400 of size 83200\n    2018-12-26 17:02:58.633005: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb89723900 of size 5120512\n    2018-12-26 17:02:58.633025: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb89c05b00 of size 83200\n    2018-12-26 17:02:58.633045: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb89c1a000 of size 5120512\n    2018-12-26 17:02:58.633064: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8a0fc200 of size 83200\n    2018-12-26 17:02:58.633086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8a110700 of size 5120512\n    2018-12-26 17:02:58.633107: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8a5f2900 of size 83200\n    2018-12-26 17:02:58.633128: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8a606e00 of size 5120512\n    2018-12-26 17:02:58.633149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8aae9000 of size 83200\n    2018-12-26 17:02:58.633169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8aafd500 of size 5120512\n    2018-12-26 17:02:58.633189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8afdf700 of size 83200\n    2018-12-26 17:02:58.633209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8aff3c00 of size 5120512\n    2018-12-26 17:02:58.633229: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8b4d5e00 of size 83200\n    2018-12-26 17:02:58.633249: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8b4ea300 of size 5120512\n    2018-12-26 17:02:58.633269: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8b9cc500 of size 83200\n    2018-12-26 17:02:58.633289: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8b9e0a00 of size 5120512\n    2018-12-26 17:02:58.633309: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8bec2c00 of size 83200\n    2018-12-26 17:02:58.633328: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8bed7100 of size 5120512\n    2018-12-26 17:02:58.633349: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8c3b9300 of size 83200\n    2018-12-26 17:02:58.633369: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8c3cd800 of size 5120512\n    2018-12-26 17:02:58.633388: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8c8afa00 of size 83200\n    2018-12-26 17:02:58.633411: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8c8c3f00 of size 5120512\n    2018-12-26 17:02:58.633431: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8cda6100 of size 83200\n    2018-12-26 17:02:58.633450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8cdba600 of size 5120512\n    2018-12-26 17:02:58.633471: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8d29c800 of size 83200\n    2018-12-26 17:02:58.633491: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8d2b0d00 of size 5120512\n    2018-12-26 17:02:58.633514: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8d792f00 of size 83200\n    2018-12-26 17:02:58.633535: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8d7a7400 of size 5120512\n    2018-12-26 17:02:58.633555: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8dc89600 of size 83200\n    2018-12-26 17:02:58.633575: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8dc9db00 of size 5120512\n    2018-12-26 17:02:58.633595: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8e17fd00 of size 83200\n    2018-12-26 17:02:58.633614: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8e194200 of size 5120512\n    2018-12-26 17:02:58.633634: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8e676400 of size 83200\n    2018-12-26 17:02:58.633654: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8e68a900 of size 5120512\n    2018-12-26 17:02:58.633674: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8eb6cb00 of size 83200\n    2018-12-26 17:02:58.633693: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8eb81000 of size 5120512\n    2018-12-26 17:02:58.633722: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8f063200 of size 83200\n    2018-12-26 17:02:58.633746: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8f077700 of size 5120512\n    2018-12-26 17:02:58.633767: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8f559900 of size 83200\n    2018-12-26 17:02:58.633786: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8f56de00 of size 5120512\n    2018-12-26 17:02:58.633807: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8fa50000 of size 83200\n    2018-12-26 17:02:58.633826: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8fa64500 of size 5120512\n    2018-12-26 17:02:58.633846: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb8ff46700 of size 83200\n    2018-12-26 17:02:58.633866: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb8ff5ac00 of size 5120512\n    2018-12-26 17:02:58.633886: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9043ce00 of size 83200\n    2018-12-26 17:02:58.633907: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb90451300 of size 5120512\n    2018-12-26 17:02:58.633929: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb90933500 of size 83200\n    2018-12-26 17:02:58.633949: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb90947a00 of size 5120512\n    2018-12-26 17:02:58.633969: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb90e29c00 of size 83200\n    2018-12-26 17:02:58.633989: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb90e3e100 of size 5120512\n    2018-12-26 17:02:58.634009: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb91320300 of size 83200\n    2018-12-26 17:02:58.634028: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb91334800 of size 5120512\n    2018-12-26 17:02:58.634048: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb91816a00 of size 83200\n    2018-12-26 17:02:58.634070: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9182af00 of size 5120512\n    2018-12-26 17:02:58.634090: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb91d0d100 of size 83200\n    2018-12-26 17:02:58.634110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb91d21600 of size 5120512\n    2018-12-26 17:02:58.634130: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb92203800 of size 83200\n    2018-12-26 17:02:58.634150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb92217d00 of size 5120512\n    2018-12-26 17:02:58.634169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb926f9f00 of size 83200\n    2018-12-26 17:02:58.634189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9270e400 of size 5120512\n    2018-12-26 17:02:58.634209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb92bf0600 of size 83200\n    2018-12-26 17:02:58.634229: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb92c04b00 of size 5120512\n    2018-12-26 17:02:58.634248: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb930e6d00 of size 83200\n    2018-12-26 17:02:58.634268: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb930fb200 of size 5120512\n    2018-12-26 17:02:58.634288: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb935dd400 of size 83200\n    2018-12-26 17:02:58.634307: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb935f1900 of size 5120512\n    2018-12-26 17:02:58.634327: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb93ad3b00 of size 83200\n    2018-12-26 17:02:58.634346: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb93ae8000 of size 5120512\n    2018-12-26 17:02:58.634366: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb93fca200 of size 83200\n    2018-12-26 17:02:58.634388: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb93fde700 of size 5120512\n    2018-12-26 17:02:58.634408: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb944c0900 of size 83200\n    2018-12-26 17:02:58.634428: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb944d4e00 of size 5120512\n    2018-12-26 17:02:58.634448: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb949b7000 of size 83200\n    2018-12-26 17:02:58.634468: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb949cb500 of size 5120512\n    2018-12-26 17:02:58.634488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb94ead700 of size 83200\n    2018-12-26 17:02:58.634508: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb94ec1c00 of size 5120512\n    2018-12-26 17:02:58.634528: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb953a3e00 of size 83200\n    2018-12-26 17:02:58.634547: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb953b8300 of size 5120512\n    2018-12-26 17:02:58.634567: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9589a500 of size 83200\n    2018-12-26 17:02:58.634587: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb958aea00 of size 5120512\n    2018-12-26 17:02:58.634607: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb95d90c00 of size 83200\n    2018-12-26 17:02:58.634627: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb95da5100 of size 5120512\n    2018-12-26 17:02:58.634646: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb96287300 of size 83200\n    2018-12-26 17:02:58.634666: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9629b800 of size 5120512\n    2018-12-26 17:02:58.634686: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9677da00 of size 83200\n    2018-12-26 17:02:58.634715: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb96791f00 of size 5120512\n    2018-12-26 17:02:58.634737: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb96c74100 of size 83200\n    2018-12-26 17:02:58.634757: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb96c88600 of size 5120512\n    2018-12-26 17:02:58.634776: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9716a800 of size 83200\n    2018-12-26 17:02:58.634796: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9717ed00 of size 5120512\n    2018-12-26 17:02:58.634816: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb97660f00 of size 83200\n    2018-12-26 17:02:58.634836: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb97675400 of size 5120512\n    2018-12-26 17:02:58.634857: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb97b57600 of size 83200\n    2018-12-26 17:02:58.634874: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb97b6bb00 of size 5120512\n    2018-12-26 17:02:58.634895: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9804dd00 of size 83200\n    2018-12-26 17:02:58.634916: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb98062200 of size 5120512\n    2018-12-26 17:02:58.634936: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb98544400 of size 83200\n    2018-12-26 17:02:58.634955: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb98558900 of size 5120512\n    2018-12-26 17:02:58.634975: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb98a3ab00 of size 83200\n    2018-12-26 17:02:58.634995: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb98a4f000 of size 5120512\n    2018-12-26 17:02:58.635015: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb98f31200 of size 83200\n    2018-12-26 17:02:58.635038: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb98f45700 of size 5120512\n    2018-12-26 17:02:58.635058: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb99427900 of size 83200\n    2018-12-26 17:02:58.635078: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9943be00 of size 5120512\n    2018-12-26 17:02:58.635097: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9991e000 of size 83200\n    2018-12-26 17:02:58.635117: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb99932500 of size 5120512\n    2018-12-26 17:02:58.635137: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb99e14700 of size 83200\n    2018-12-26 17:02:58.635157: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb99e28c00 of size 5120512\n    2018-12-26 17:02:58.635177: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9a30ae00 of size 83200\n    2018-12-26 17:02:58.635197: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9a31f300 of size 5120512\n    2018-12-26 17:02:58.635216: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9a801500 of size 83200\n    2018-12-26 17:02:58.635236: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9a815a00 of size 5120512\n    2018-12-26 17:02:58.635256: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9acf7c00 of size 83200\n    2018-12-26 17:02:58.635276: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9ad0c100 of size 5120512\n    2018-12-26 17:02:58.635296: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9b1ee300 of size 83200\n    2018-12-26 17:02:58.635316: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9b202800 of size 5120512\n    2018-12-26 17:02:58.635336: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9b6e4a00 of size 83200\n    2018-12-26 17:02:58.635358: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9b6f8f00 of size 5120512\n    2018-12-26 17:02:58.635379: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9bbdb100 of size 83200\n    2018-12-26 17:02:58.635398: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9bbef600 of size 5120512\n    2018-12-26 17:02:58.635418: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9c0d1800 of size 83200\n    2018-12-26 17:02:58.635438: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9c0e5d00 of size 5120512\n    2018-12-26 17:02:58.635458: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9c5c7f00 of size 83200\n    2018-12-26 17:02:58.635478: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9c5dc400 of size 5120512\n    2018-12-26 17:02:58.635498: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9cabe600 of size 83200\n    2018-12-26 17:02:58.635518: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9cad2b00 of size 5120512\n    2018-12-26 17:02:58.635538: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9cfb4d00 of size 83200\n    2018-12-26 17:02:58.635557: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9cfc9200 of size 5120512\n    2018-12-26 17:02:58.635577: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9d4ab400 of size 83200\n    2018-12-26 17:02:58.635597: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9d4bf900 of size 5120512\n    2018-12-26 17:02:58.635616: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9d9a1b00 of size 83200\n    2018-12-26 17:02:58.635636: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9d9b6000 of size 5120512\n    2018-12-26 17:02:58.635656: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9de98200 of size 83200\n    2018-12-26 17:02:58.635677: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9deac700 of size 5120512\n    2018-12-26 17:02:58.635697: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9e38e900 of size 83200\n    2018-12-26 17:02:58.635726: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9e3a2e00 of size 5120512\n    2018-12-26 17:02:58.635747: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9e885000 of size 83200\n    2018-12-26 17:02:58.635767: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9e899500 of size 5120512\n    2018-12-26 17:02:58.635787: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9ed7b700 of size 83200\n    2018-12-26 17:02:58.635807: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9ed8fc00 of size 5120512\n    2018-12-26 17:02:58.635827: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9f271e00 of size 83200\n    2018-12-26 17:02:58.635847: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9f286300 of size 5120512\n    2018-12-26 17:02:58.635867: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9f768500 of size 83200\n    2018-12-26 17:02:58.635887: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9f77ca00 of size 5120512\n    2018-12-26 17:02:58.635907: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xb9fc5ec00 of size 83200\n    2018-12-26 17:02:58.635926: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xb9fc73100 of size 5120512\n    2018-12-26 17:02:58.635946: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba0155300 of size 83200\n    2018-12-26 17:02:58.635979: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba0169800 of size 5120512\n    2018-12-26 17:02:58.636001: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba064ba00 of size 83200\n    2018-12-26 17:02:58.636023: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba065ff00 of size 5120512\n    2018-12-26 17:02:58.636043: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba0b42100 of size 83456\n    2018-12-26 17:02:58.636063: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba0b56700 of size 5120512\n    2018-12-26 17:02:58.636083: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba1038900 of size 83200\n    2018-12-26 17:02:58.636103: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba104ce00 of size 5120512\n    2018-12-26 17:02:58.636126: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba152f000 of size 83200\n    2018-12-26 17:02:58.636146: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba1543500 of size 5120512\n    2018-12-26 17:02:58.636165: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba1a25700 of size 83200\n    2018-12-26 17:02:58.636185: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba1a39c00 of size 5120512\n    2018-12-26 17:02:58.636205: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba1f1be00 of size 83200\n    2018-12-26 17:02:58.636225: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba1f30300 of size 5120512\n    2018-12-26 17:02:58.636245: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba2412500 of size 83200\n    2018-12-26 17:02:58.636265: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba2426a00 of size 5120512\n    2018-12-26 17:02:58.636285: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba2908c00 of size 83200\n    2018-12-26 17:02:58.636304: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba291d100 of size 5120512\n    2018-12-26 17:02:58.636324: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba2dff300 of size 83200\n    2018-12-26 17:02:58.636346: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba2e13800 of size 5120512\n    2018-12-26 17:02:58.636366: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba32f5a00 of size 83200\n    2018-12-26 17:02:58.636385: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba3309f00 of size 5120512\n    2018-12-26 17:02:58.636405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba37ec100 of size 83200\n    2018-12-26 17:02:58.636425: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba3800600 of size 5120512\n    2018-12-26 17:02:58.636444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba3ce2800 of size 83200\n    2018-12-26 17:02:58.636464: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba3cf6d00 of size 5120512\n    2018-12-26 17:02:58.636484: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba41d8f00 of size 83200\n    2018-12-26 17:02:58.636504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba41ed400 of size 5120512\n    2018-12-26 17:02:58.636524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba46cf600 of size 83200\n    2018-12-26 17:02:58.636543: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba46e3b00 of size 5120512\n    2018-12-26 17:02:58.636563: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba4bc5d00 of size 83200\n    2018-12-26 17:02:58.636583: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba4bda200 of size 5120512\n    2018-12-26 17:02:58.636602: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba50bc400 of size 83200\n    2018-12-26 17:02:58.636622: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba50d0900 of size 5120512\n    2018-12-26 17:02:58.636641: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba55b2b00 of size 83200\n    2018-12-26 17:02:58.636663: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba55c7000 of size 5120512\n    2018-12-26 17:02:58.636683: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba5aa9200 of size 83200\n    2018-12-26 17:02:58.636711: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba5abd700 of size 5120512\n    2018-12-26 17:02:58.636733: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba5f9f900 of size 83200\n    2018-12-26 17:02:58.636754: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba5fb3e00 of size 5120512\n    2018-12-26 17:02:58.636773: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba6496000 of size 83200\n    2018-12-26 17:02:58.636794: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba64aa500 of size 5120512\n    2018-12-26 17:02:58.636813: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba698c700 of size 83200\n    2018-12-26 17:02:58.636834: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba69a0c00 of size 5120512\n    2018-12-26 17:02:58.636854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba6e82e00 of size 83200\n    2018-12-26 17:02:58.636873: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba6e97300 of size 5120512\n    2018-12-26 17:02:58.636893: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba7379500 of size 83200\n    2018-12-26 17:02:58.636912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba738da00 of size 5120512\n    2018-12-26 17:02:58.636932: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba786fc00 of size 83200\n    2018-12-26 17:02:58.636953: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba7884100 of size 5120512\n    2018-12-26 17:02:58.636973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba7d66300 of size 83200\n    2018-12-26 17:02:58.636995: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba7d7a800 of size 5120512\n    2018-12-26 17:02:58.637015: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba825ca00 of size 83200\n    2018-12-26 17:02:58.637035: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba8270f00 of size 5120512\n    2018-12-26 17:02:58.637055: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba8753100 of size 83200\n    2018-12-26 17:02:58.637075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba8767600 of size 5120512\n    2018-12-26 17:02:58.637094: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba8c49800 of size 83200\n    2018-12-26 17:02:58.637114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba8c5dd00 of size 5120512\n    2018-12-26 17:02:58.637134: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba913ff00 of size 83200\n    2018-12-26 17:02:58.637155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba9154400 of size 5120512\n    2018-12-26 17:02:58.637175: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba9636600 of size 83200\n    2018-12-26 17:02:58.637195: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba964ab00 of size 5120512\n    2018-12-26 17:02:58.637215: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xba9b2cd00 of size 83200\n    2018-12-26 17:02:58.637235: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xba9b41200 of size 5120512\n    2018-12-26 17:02:58.637254: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaa023400 of size 83200\n    2018-12-26 17:02:58.637275: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaa037900 of size 5120512\n    2018-12-26 17:02:58.637295: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaa519b00 of size 83200\n    2018-12-26 17:02:58.637317: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaa52e000 of size 5120512\n    2018-12-26 17:02:58.637337: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaaa10200 of size 83200\n    2018-12-26 17:02:58.637356: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaaa24700 of size 5120512\n    2018-12-26 17:02:58.637376: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaaf06900 of size 83200\n    2018-12-26 17:02:58.637396: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaaf1ae00 of size 5120512\n    2018-12-26 17:02:58.637416: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbab3fd000 of size 83200\n    2018-12-26 17:02:58.637437: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbab411500 of size 5120512\n    2018-12-26 17:02:58.637457: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbab8f3700 of size 83200\n    2018-12-26 17:02:58.637477: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbab907c00 of size 5120512\n    2018-12-26 17:02:58.637497: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbabde9e00 of size 83200\n    2018-12-26 17:02:58.637517: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbabdfe300 of size 5120512\n    2018-12-26 17:02:58.637536: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbac2e0500 of size 83200\n    2018-12-26 17:02:58.637556: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbac2f4a00 of size 5120512\n    2018-12-26 17:02:58.637576: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbac7d6c00 of size 83200\n    2018-12-26 17:02:58.637596: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbac7eb100 of size 5120512\n    2018-12-26 17:02:58.637616: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbacccd300 of size 83200\n    2018-12-26 17:02:58.637638: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbacce1800 of size 5120512\n    2018-12-26 17:02:58.637658: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbad1c3a00 of size 83200\n    2018-12-26 17:02:58.637678: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbad1d7f00 of size 5120512\n    2018-12-26 17:02:58.637698: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbad6ba100 of size 83200\n    2018-12-26 17:02:58.637727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbad6ce600 of size 5120512\n    2018-12-26 17:02:58.637748: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbadbb0800 of size 83200\n    2018-12-26 17:02:58.637768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbadbc4d00 of size 5120512\n    2018-12-26 17:02:58.637788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbae0a6f00 of size 83200\n    2018-12-26 17:02:58.637809: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbae0bb400 of size 5120512\n    2018-12-26 17:02:58.637828: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbae59d600 of size 83200\n    2018-12-26 17:02:58.637848: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbae5b1b00 of size 5120512\n    2018-12-26 17:02:58.637868: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaea93d00 of size 83200\n    2018-12-26 17:02:58.637888: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaeaa8200 of size 5120512\n    2018-12-26 17:02:58.637908: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaef8a400 of size 83200\n    2018-12-26 17:02:58.637927: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaef9e900 of size 5120512\n    2018-12-26 17:02:58.637947: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaf480b00 of size 83200\n    2018-12-26 17:02:58.637969: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaf495000 of size 5120512\n    2018-12-26 17:02:58.637989: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbaf977200 of size 83200\n    2018-12-26 17:02:58.638009: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbaf98b700 of size 5120512\n    2018-12-26 17:02:58.638029: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbafe6d900 of size 83200\n    2018-12-26 17:02:58.638049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbafe81e00 of size 5120512\n    2018-12-26 17:02:58.638068: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb0364000 of size 83200\n    2018-12-26 17:02:58.638088: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb0378500 of size 5120512\n    2018-12-26 17:02:58.638109: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb085a700 of size 83200\n    2018-12-26 17:02:58.638128: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb086ec00 of size 5120512\n    2018-12-26 17:02:58.638149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb0d50e00 of size 83200\n    2018-12-26 17:02:58.638169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb0d65300 of size 5120512\n    2018-12-26 17:02:58.638188: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb1247500 of size 83200\n    2018-12-26 17:02:58.638208: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb125ba00 of size 5120512\n    2018-12-26 17:02:58.638228: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb173dc00 of size 83200\n    2018-12-26 17:02:58.638247: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb1752100 of size 5120512\n    2018-12-26 17:02:58.638267: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb1c34300 of size 83200\n    2018-12-26 17:02:58.638288: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb1c48800 of size 5120512\n    2018-12-26 17:02:58.638308: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb212aa00 of size 83200\n    2018-12-26 17:02:58.638327: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb213ef00 of size 5120512\n    2018-12-26 17:02:58.638347: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb2621100 of size 83200\n    2018-12-26 17:02:58.638367: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb2635600 of size 5120512\n    2018-12-26 17:02:58.638386: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb2b17800 of size 83200\n    2018-12-26 17:02:58.638406: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb2b2bd00 of size 5120512\n    2018-12-26 17:02:58.638426: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb300df00 of size 83200\n    2018-12-26 17:02:58.638446: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb3022400 of size 5120512\n    2018-12-26 17:02:58.638465: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb3504600 of size 83200\n    2018-12-26 17:02:58.638485: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb3518b00 of size 5120512\n    2018-12-26 17:02:58.638504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb39fad00 of size 83200\n    2018-12-26 17:02:58.638524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb3a0f200 of size 5120512\n    2018-12-26 17:02:58.638543: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb3ef1400 of size 83200\n    2018-12-26 17:02:58.638563: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb3f05900 of size 5120512\n    2018-12-26 17:02:58.638582: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb43e7b00 of size 83200\n    2018-12-26 17:02:58.638604: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb43fc000 of size 5120512\n    2018-12-26 17:02:58.638623: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb48de200 of size 83200\n    2018-12-26 17:02:58.638642: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb48f2700 of size 5120512\n    2018-12-26 17:02:58.638662: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb4dd4900 of size 83200\n    2018-12-26 17:02:58.638682: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb4de8e00 of size 5120512\n    2018-12-26 17:02:58.638710: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb52cb000 of size 83200\n    2018-12-26 17:02:58.638732: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb52df500 of size 5120512\n    2018-12-26 17:02:58.638752: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb57c1700 of size 83200\n    2018-12-26 17:02:58.638772: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb57d5c00 of size 5120512\n    2018-12-26 17:02:58.638792: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb5cb7e00 of size 83200\n    2018-12-26 17:02:58.638811: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb5ccc300 of size 5120512\n    2018-12-26 17:02:58.638831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb61ae500 of size 83200\n    2018-12-26 17:02:58.638850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb61c2a00 of size 5120512\n    2018-12-26 17:02:58.638870: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb66a4c00 of size 83200\n    2018-12-26 17:02:58.638889: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb66b9100 of size 5120512\n    2018-12-26 17:02:58.638909: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb6b9b300 of size 83200\n    2018-12-26 17:02:58.638931: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb6baf800 of size 5120512\n    2018-12-26 17:02:58.638950: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb7091a00 of size 83200\n    2018-12-26 17:02:58.638970: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb70a5f00 of size 5120512\n    2018-12-26 17:02:58.638989: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb7588100 of size 83200\n    2018-12-26 17:02:58.639009: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb759c600 of size 5120512\n    2018-12-26 17:02:58.639029: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb7a7e800 of size 83200\n    2018-12-26 17:02:58.639048: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb7a92d00 of size 5120512\n    2018-12-26 17:02:58.639068: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb7f74f00 of size 83200\n    2018-12-26 17:02:58.639087: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb7f89400 of size 5120512\n    2018-12-26 17:02:58.639107: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb846b600 of size 83200\n    2018-12-26 17:02:58.639126: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb847fb00 of size 5120512\n    2018-12-26 17:02:58.639146: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb8961d00 of size 83200\n    2018-12-26 17:02:58.639165: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb8976200 of size 5120512\n    2018-12-26 17:02:58.639185: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb8e58400 of size 83200\n    2018-12-26 17:02:58.639204: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb8e6c900 of size 5120512\n    2018-12-26 17:02:58.639224: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb934eb00 of size 83200\n    2018-12-26 17:02:58.639245: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb9363000 of size 5120512\n    2018-12-26 17:02:58.639265: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb9845200 of size 83200\n    2018-12-26 17:02:58.639284: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb9859700 of size 5120512\n    2018-12-26 17:02:58.639304: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbb9d3b900 of size 83200\n    2018-12-26 17:02:58.639324: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbb9d4fe00 of size 5120512\n    2018-12-26 17:02:58.639344: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbba232000 of size 83200\n    2018-12-26 17:02:58.639363: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbba246500 of size 5120512\n    2018-12-26 17:02:58.639383: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbba728700 of size 83200\n    2018-12-26 17:02:58.639403: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbba73cc00 of size 5120512\n    2018-12-26 17:02:58.639422: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbac1ee00 of size 83200\n    2018-12-26 17:02:58.639442: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbac33300 of size 5120512\n    2018-12-26 17:02:58.639461: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbb115500 of size 83200\n    2018-12-26 17:02:58.639481: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbb129a00 of size 5120512\n    2018-12-26 17:02:58.639500: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbb60bc00 of size 83200\n    2018-12-26 17:02:58.639519: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbb620100 of size 5120512\n    2018-12-26 17:02:58.639539: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbbb02300 of size 83200\n    2018-12-26 17:02:58.639560: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbbb16800 of size 5120512\n    2018-12-26 17:02:58.639580: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbbff8a00 of size 83200\n    2018-12-26 17:02:58.639600: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbc00cf00 of size 5120512\n    2018-12-26 17:02:58.639620: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbc4ef100 of size 83200\n    2018-12-26 17:02:58.639639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbc503600 of size 5120512\n    2018-12-26 17:02:58.639659: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbc9e5800 of size 83200\n    2018-12-26 17:02:58.639679: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbc9f9d00 of size 5120512\n    2018-12-26 17:02:58.639705: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbcedbf00 of size 83200\n    2018-12-26 17:02:58.639728: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbcef0400 of size 5120512\n    2018-12-26 17:02:58.639748: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbd3d2600 of size 83200\n    2018-12-26 17:02:58.639768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbd3e6b00 of size 5120512\n    2018-12-26 17:02:58.639788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbd8c8d00 of size 83200\n    2018-12-26 17:02:58.639807: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbd8dd200 of size 5120512\n    2018-12-26 17:02:58.639827: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbddbf400 of size 83200\n    2018-12-26 17:02:58.639846: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbddd3900 of size 5120512\n    2018-12-26 17:02:58.639866: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbe2b5b00 of size 83200\n    2018-12-26 17:02:58.639887: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbe2ca000 of size 5120512\n    2018-12-26 17:02:58.639907: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbe7ac200 of size 83200\n    2018-12-26 17:02:58.639927: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbe7c0700 of size 5120512\n    2018-12-26 17:02:58.639946: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbeca2900 of size 83200\n    2018-12-26 17:02:58.639977: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbecb6e00 of size 5120512\n    2018-12-26 17:02:58.639999: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbf199000 of size 83200\n    2018-12-26 17:02:58.640018: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbf1ad500 of size 5120512\n    2018-12-26 17:02:58.640038: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbf68f700 of size 83200\n    2018-12-26 17:02:58.640057: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbf6a3c00 of size 5120512\n    2018-12-26 17:02:58.640077: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbbfb85e00 of size 83200\n    2018-12-26 17:02:58.640097: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbbfb9a300 of size 5120512\n    2018-12-26 17:02:58.640118: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc007c500 of size 83200\n    2018-12-26 17:02:58.640139: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc0090a00 of size 5120512\n    2018-12-26 17:02:58.640159: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc0572c00 of size 83200\n    2018-12-26 17:02:58.640178: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc0587100 of size 5120512\n    2018-12-26 17:02:58.640198: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc0a69300 of size 83200\n    2018-12-26 17:02:58.640221: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc0a7d800 of size 5120512\n    2018-12-26 17:02:58.640240: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc0f5fa00 of size 83200\n    2018-12-26 17:02:58.640260: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc0f73f00 of size 5120512\n    2018-12-26 17:02:58.640280: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc1456100 of size 83200\n    2018-12-26 17:02:58.640299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc146a600 of size 5120512\n    2018-12-26 17:02:58.640319: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc194c800 of size 83200\n    2018-12-26 17:02:58.640338: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc1960d00 of size 5120512\n    2018-12-26 17:02:58.640358: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc1e42f00 of size 83200\n    2018-12-26 17:02:58.640377: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc1e57400 of size 5120512\n    2018-12-26 17:02:58.640397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc2339600 of size 83200\n    2018-12-26 17:02:58.640417: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc234db00 of size 5120512\n    2018-12-26 17:02:58.640436: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc282fd00 of size 83200\n    2018-12-26 17:02:58.640455: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc2844200 of size 5120512\n    2018-12-26 17:02:58.640475: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc2d26400 of size 83200\n    2018-12-26 17:02:58.640494: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc2d3a900 of size 5120512\n    2018-12-26 17:02:58.640513: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc321cb00 of size 83200\n    2018-12-26 17:02:58.640535: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc3231000 of size 5120512\n    2018-12-26 17:02:58.640555: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc3713200 of size 83200\n    2018-12-26 17:02:58.640574: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc3727700 of size 5120512\n    2018-12-26 17:02:58.640594: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc3c09900 of size 83200\n    2018-12-26 17:02:58.640613: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc3c1de00 of size 5120512\n    2018-12-26 17:02:58.640632: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc4100000 of size 83200\n    2018-12-26 17:02:58.640652: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc4114500 of size 5120512\n    2018-12-26 17:02:58.640671: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc45f6700 of size 83200\n    2018-12-26 17:02:58.640691: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc460ac00 of size 5120512\n    2018-12-26 17:02:58.640719: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc4aece00 of size 83200\n    2018-12-26 17:02:58.640740: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc4b01300 of size 5120512\n    2018-12-26 17:02:58.640760: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc4fe3500 of size 83200\n    2018-12-26 17:02:58.640780: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc4ff7a00 of size 5120512\n    2018-12-26 17:02:58.640799: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc54d9c00 of size 83200\n    2018-12-26 17:02:58.640819: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc54ee100 of size 5120512\n    2018-12-26 17:02:58.640838: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc59d0300 of size 83200\n    2018-12-26 17:02:58.640860: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc59e4800 of size 5120512\n    2018-12-26 17:02:58.640879: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc5ec6a00 of size 83200\n    2018-12-26 17:02:58.640900: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc5edaf00 of size 5120512\n    2018-12-26 17:02:58.640922: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc63bd100 of size 83200\n    2018-12-26 17:02:58.640941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc63d1600 of size 5120512\n    2018-12-26 17:02:58.640961: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc68b3800 of size 83200\n    2018-12-26 17:02:58.640980: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc68c7d00 of size 5120512\n    2018-12-26 17:02:58.641000: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc6da9f00 of size 83200\n    2018-12-26 17:02:58.641020: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc6dbe400 of size 5120512\n    2018-12-26 17:02:58.641039: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc72a0600 of size 83200\n    2018-12-26 17:02:58.641059: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc72b4b00 of size 5120512\n    2018-12-26 17:02:58.641078: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc7796d00 of size 83200\n    2018-12-26 17:02:58.641098: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc77ab200 of size 5120512\n    2018-12-26 17:02:58.641117: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc7c8d400 of size 83200\n    2018-12-26 17:02:58.641137: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc7ca1900 of size 5120512\n    2018-12-26 17:02:58.641156: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc8183b00 of size 83200\n    2018-12-26 17:02:58.641178: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc8198000 of size 5120512\n    2018-12-26 17:02:58.641198: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc867a200 of size 83200\n    2018-12-26 17:02:58.641218: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc868e700 of size 5120512\n    2018-12-26 17:02:58.641238: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc8b70900 of size 83200\n    2018-12-26 17:02:58.641258: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc8b84e00 of size 5120512\n    2018-12-26 17:02:58.641281: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc9067000 of size 83200\n    2018-12-26 17:02:58.641301: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc907b500 of size 5120512\n    2018-12-26 17:02:58.641320: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc955d700 of size 83200\n    2018-12-26 17:02:58.641340: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc9571c00 of size 5120512\n    2018-12-26 17:02:58.641359: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc9a53e00 of size 83200\n    2018-12-26 17:02:58.641379: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc9a68300 of size 5120512\n    2018-12-26 17:02:58.641398: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbc9f4a500 of size 83200\n    2018-12-26 17:02:58.641418: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbc9f5ea00 of size 5120512\n    2018-12-26 17:02:58.641437: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbca440c00 of size 83200\n    2018-12-26 17:02:58.641457: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbca455100 of size 5120512\n    2018-12-26 17:02:58.641476: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbca937300 of size 83200\n    2018-12-26 17:02:58.641498: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbca94b800 of size 5120512\n    2018-12-26 17:02:58.641518: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcae2da00 of size 83200\n    2018-12-26 17:02:58.641538: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcae41f00 of size 5120512\n    2018-12-26 17:02:58.641557: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcb324100 of size 83200\n    2018-12-26 17:02:58.641577: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcb338600 of size 5120512\n    2018-12-26 17:02:58.641597: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcb81a800 of size 83200\n    2018-12-26 17:02:58.641616: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcb82ed00 of size 5120512\n    2018-12-26 17:02:58.641636: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcbd10f00 of size 83200\n    2018-12-26 17:02:58.641655: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcbd25400 of size 5120512\n    2018-12-26 17:02:58.641675: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcc207600 of size 83200\n    2018-12-26 17:02:58.641694: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcc21bb00 of size 5120512\n    2018-12-26 17:02:58.641722: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcc6fdd00 of size 83200\n    2018-12-26 17:02:58.641743: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcc712200 of size 5120512\n    2018-12-26 17:02:58.641762: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbccbf4400 of size 83200\n    2018-12-26 17:02:58.641782: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbccc08900 of size 5120512\n    2018-12-26 17:02:58.641801: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcd0eab00 of size 83200\n    2018-12-26 17:02:58.641823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcd0ff000 of size 5120512\n    2018-12-26 17:02:58.641842: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcd5e1200 of size 83200\n    2018-12-26 17:02:58.641862: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcd5f5700 of size 5120512\n    2018-12-26 17:02:58.641881: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcdad7900 of size 83200\n    2018-12-26 17:02:58.641901: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcdaebe00 of size 5120512\n    2018-12-26 17:02:58.641920: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcdfce000 of size 83200\n    2018-12-26 17:02:58.641940: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcdfe2500 of size 5120512\n    2018-12-26 17:02:58.641959: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbce4c4700 of size 83200\n    2018-12-26 17:02:58.641979: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbce4d8c00 of size 5120512\n    2018-12-26 17:02:58.641998: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbce9bae00 of size 83200\n    2018-12-26 17:02:58.642018: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbce9cf300 of size 5120512\n    2018-12-26 17:02:58.642038: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbceeb1500 of size 83200\n    2018-12-26 17:02:58.642057: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbceec5a00 of size 5120512\n    2018-12-26 17:02:58.642076: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcf3a7c00 of size 83200\n    2018-12-26 17:02:58.642096: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcf3bc100 of size 5120512\n    2018-12-26 17:02:58.642115: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcf89e300 of size 83200\n    2018-12-26 17:02:58.642137: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcf8b2800 of size 5120512\n    2018-12-26 17:02:58.642156: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbcfd94a00 of size 83200\n    2018-12-26 17:02:58.642176: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbcfda8f00 of size 5120512\n    2018-12-26 17:02:58.642195: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd028b100 of size 83200\n    2018-12-26 17:02:58.642215: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd029f600 of size 5120512\n    2018-12-26 17:02:58.642234: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd0781800 of size 83200\n    2018-12-26 17:02:58.642251: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd0795d00 of size 5120512\n    2018-12-26 17:02:58.642271: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd0c77f00 of size 83200\n    2018-12-26 17:02:58.642291: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd0c8c400 of size 5120512\n    2018-12-26 17:02:58.642310: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd116e600 of size 83200\n    2018-12-26 17:02:58.642330: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd1182b00 of size 5120512\n    2018-12-26 17:02:58.642350: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd1664d00 of size 83200\n    2018-12-26 17:02:58.642369: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd1679200 of size 5120512\n    2018-12-26 17:02:58.642388: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd1b5b400 of size 83200\n    2018-12-26 17:02:58.642408: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd1b6f900 of size 5120512\n    2018-12-26 17:02:58.642428: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd2051b00 of size 83200\n    2018-12-26 17:02:58.642450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd2066000 of size 5120512\n    2018-12-26 17:02:58.642469: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd2548200 of size 83200\n    2018-12-26 17:02:58.642488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd255c700 of size 5120512\n    2018-12-26 17:02:58.642508: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd2a3e900 of size 83200\n    2018-12-26 17:02:58.642528: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd2a52e00 of size 5120512\n    2018-12-26 17:02:58.642547: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd2f35000 of size 83200\n    2018-12-26 17:02:58.642567: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd2f49500 of size 5120512\n    2018-12-26 17:02:58.642586: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd342b700 of size 83200\n    2018-12-26 17:02:58.642606: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd343fc00 of size 5120512\n    2018-12-26 17:02:58.642625: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd3921e00 of size 83200\n    2018-12-26 17:02:58.642644: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd3936300 of size 5120512\n    2018-12-26 17:02:58.642664: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd3e18500 of size 83200\n    2018-12-26 17:02:58.642684: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd3e2ca00 of size 5120512\n    2018-12-26 17:02:58.642710: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd430ec00 of size 83200\n    2018-12-26 17:02:58.642732: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd4323100 of size 5120512\n    2018-12-26 17:02:58.642753: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd4805300 of size 83200\n    2018-12-26 17:02:58.642774: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd4819800 of size 5120512\n    2018-12-26 17:02:58.642794: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd4cfba00 of size 83200\n    2018-12-26 17:02:58.642814: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd4d0ff00 of size 5120512\n    2018-12-26 17:02:58.642834: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd51f2100 of size 83200\n    2018-12-26 17:02:58.642853: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd5206600 of size 5120512\n    2018-12-26 17:02:58.642873: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd56e8800 of size 83200\n    2018-12-26 17:02:58.642893: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd56fcd00 of size 5120512\n    2018-12-26 17:02:58.642912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd5bdef00 of size 83200\n    2018-12-26 17:02:58.642931: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd5bf3400 of size 5120512\n    2018-12-26 17:02:58.642951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd60d5600 of size 83200\n    2018-12-26 17:02:58.642970: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd60e9b00 of size 5120512\n    2018-12-26 17:02:58.642990: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd65cbd00 of size 83200\n    2018-12-26 17:02:58.643009: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd65e0200 of size 5120512\n    2018-12-26 17:02:58.643029: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd6ac2400 of size 83200\n    2018-12-26 17:02:58.643048: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd6ad6900 of size 5120512\n    2018-12-26 17:02:58.643068: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd6fb8b00 of size 83200\n    2018-12-26 17:02:58.643090: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd6fcd000 of size 5120512\n    2018-12-26 17:02:58.643110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd74af200 of size 83200\n    2018-12-26 17:02:58.643129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd74c3700 of size 5120512\n    2018-12-26 17:02:58.643149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd79a5900 of size 83200\n    2018-12-26 17:02:58.643169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd79b9e00 of size 5120512\n    2018-12-26 17:02:58.643189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd7e9c000 of size 83200\n    2018-12-26 17:02:58.643208: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd7eb0500 of size 5120512\n    2018-12-26 17:02:58.643228: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd8392700 of size 83200\n    2018-12-26 17:02:58.643247: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd83a6c00 of size 5120512\n    2018-12-26 17:02:58.643267: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd8888e00 of size 83200\n    2018-12-26 17:02:58.643287: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd889d300 of size 5120512\n    2018-12-26 17:02:58.643307: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd8d7f500 of size 83200\n    2018-12-26 17:02:58.643326: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd8d93a00 of size 5120512\n    2018-12-26 17:02:58.643346: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd9275c00 of size 83200\n    2018-12-26 17:02:58.643365: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd928a100 of size 5120512\n    2018-12-26 17:02:58.643386: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd976c300 of size 83200\n    2018-12-26 17:02:58.643407: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd9780800 of size 5120512\n    2018-12-26 17:02:58.643427: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbd9c62a00 of size 83200\n    2018-12-26 17:02:58.643446: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbd9c76f00 of size 5120512\n    2018-12-26 17:02:58.643466: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbda159100 of size 83200\n    2018-12-26 17:02:58.643486: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbda16d600 of size 5120512\n    2018-12-26 17:02:58.643505: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbda64f800 of size 83200\n    2018-12-26 17:02:58.643524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbda663d00 of size 5120512\n    2018-12-26 17:02:58.643544: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdab45f00 of size 83200\n    2018-12-26 17:02:58.643567: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdab5a400 of size 5120512\n    2018-12-26 17:02:58.643588: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdb03c600 of size 83200\n    2018-12-26 17:02:58.643607: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdb050b00 of size 5120512\n    2018-12-26 17:02:58.643626: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdb532d00 of size 83200\n    2018-12-26 17:02:58.643645: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdb547200 of size 5120512\n    2018-12-26 17:02:58.643656: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdba29400 of size 83200\n    2018-12-26 17:02:58.643673: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdba3d900 of size 5120512\n    2018-12-26 17:02:58.643687: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdbf1fb00 of size 83200\n    2018-12-26 17:02:58.643709: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdbf34000 of size 5120512\n    2018-12-26 17:02:58.643724: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdc416200 of size 83200\n    2018-12-26 17:02:58.643737: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdc42a700 of size 5120512\n    2018-12-26 17:02:58.643751: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdc90c900 of size 83200\n    2018-12-26 17:02:58.643764: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdc920e00 of size 5120512\n    2018-12-26 17:02:58.643788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdce03000 of size 83200\n    2018-12-26 17:02:58.643801: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdce17500 of size 5120512\n    2018-12-26 17:02:58.643814: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdd2f9700 of size 83200\n    2018-12-26 17:02:58.643827: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdd30dc00 of size 5120512\n    2018-12-26 17:02:58.643840: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdd7efe00 of size 83200\n    2018-12-26 17:02:58.643853: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdd804300 of size 5120512\n    2018-12-26 17:02:58.643866: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbddce6500 of size 83200\n    2018-12-26 17:02:58.643879: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbddcfaa00 of size 5120512\n    2018-12-26 17:02:58.643893: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbde1dcc00 of size 83200\n    2018-12-26 17:02:58.643906: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbde1f1100 of size 5120512\n    2018-12-26 17:02:58.643919: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbde6d3300 of size 83200\n    2018-12-26 17:02:58.643934: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbde6e7800 of size 5120512\n    2018-12-26 17:02:58.643947: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdebc9a00 of size 83200\n    2018-12-26 17:02:58.643973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdebddf00 of size 5120512\n    2018-12-26 17:02:58.643989: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdf0c0100 of size 83200\n    2018-12-26 17:02:58.644003: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdf0d4600 of size 5120512\n    2018-12-26 17:02:58.644016: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdf5b6800 of size 83200\n    2018-12-26 17:02:58.644029: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdf5cad00 of size 5120512\n    2018-12-26 17:02:58.644042: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdfaacf00 of size 83200\n    2018-12-26 17:02:58.644054: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdfac1400 of size 5120512\n    2018-12-26 17:02:58.644064: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbdffa3600 of size 83200\n    2018-12-26 17:02:58.644073: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbdffb7b00 of size 5120512\n    2018-12-26 17:02:58.644082: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe0499d00 of size 83200\n    2018-12-26 17:02:58.644091: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe04ae200 of size 5120512\n    2018-12-26 17:02:58.644101: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe0990400 of size 83200\n    2018-12-26 17:02:58.644110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe09a4900 of size 5120512\n    2018-12-26 17:02:58.644120: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe0e86b00 of size 83200\n    2018-12-26 17:02:58.644131: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe0e9b000 of size 5120512\n    2018-12-26 17:02:58.644144: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe137d200 of size 83200\n    2018-12-26 17:02:58.644158: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe1391700 of size 5120512\n    2018-12-26 17:02:58.644171: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe1873900 of size 83200\n    2018-12-26 17:02:58.644184: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe1887e00 of size 5120512\n    2018-12-26 17:02:58.644198: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe1d6a000 of size 83200\n    2018-12-26 17:02:58.644211: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe1d7e500 of size 5120512\n    2018-12-26 17:02:58.644224: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe2260700 of size 83200\n    2018-12-26 17:02:58.644237: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe2274c00 of size 5120512\n    2018-12-26 17:02:58.644250: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe2756e00 of size 83200\n    2018-12-26 17:02:58.644264: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe276b300 of size 5120512\n    2018-12-26 17:02:58.644277: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe2c4d500 of size 83200\n    2018-12-26 17:02:58.644286: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe2c61a00 of size 5120512\n    2018-12-26 17:02:58.644299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe3143c00 of size 83200\n    2018-12-26 17:02:58.644312: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe3158100 of size 5120512\n    2018-12-26 17:02:58.644325: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe363a300 of size 83200\n    2018-12-26 17:02:58.644339: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe364e800 of size 5120512\n    2018-12-26 17:02:58.644352: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe3b30a00 of size 83200\n    2018-12-26 17:02:58.644365: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe3b44f00 of size 5120512\n    2018-12-26 17:02:58.644379: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe4027100 of size 83200\n    2018-12-26 17:02:58.644392: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe403b600 of size 5120512\n    2018-12-26 17:02:58.644405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe451d800 of size 83200\n    2018-12-26 17:02:58.644417: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe4531d00 of size 5120512\n    2018-12-26 17:02:58.644430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe4a13f00 of size 83200\n    2018-12-26 17:02:58.644444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe4a28400 of size 5120512\n    2018-12-26 17:02:58.644457: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe4f0a600 of size 83200\n    2018-12-26 17:02:58.644470: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe4f1eb00 of size 5120512\n    2018-12-26 17:02:58.644483: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe5400d00 of size 83200\n    2018-12-26 17:02:58.644496: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe5415200 of size 5120512\n    2018-12-26 17:02:58.644509: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe58f7400 of size 83200\n    2018-12-26 17:02:58.644522: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe590b900 of size 5120512\n    2018-12-26 17:02:58.644535: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe5dedb00 of size 83200\n    2018-12-26 17:02:58.644549: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe5e02000 of size 5120512\n    2018-12-26 17:02:58.644562: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe62e4200 of size 83200\n    2018-12-26 17:02:58.644575: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe62f8700 of size 5120512\n    2018-12-26 17:02:58.644588: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe67da900 of size 83200\n    2018-12-26 17:02:58.644602: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe67eee00 of size 5120512\n    2018-12-26 17:02:58.644615: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe6cd1000 of size 83200\n    2018-12-26 17:02:58.644628: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe6ce5500 of size 5120512\n    2018-12-26 17:02:58.644641: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe71c7700 of size 83200\n    2018-12-26 17:02:58.644654: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe71dbc00 of size 5120512\n    2018-12-26 17:02:58.644667: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe76bde00 of size 83200\n    2018-12-26 17:02:58.644680: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe76d2300 of size 5120512\n    2018-12-26 17:02:58.644693: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe7bb4500 of size 83200\n    2018-12-26 17:02:58.644713: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe7bc8a00 of size 5120512\n    2018-12-26 17:02:58.644727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe80aac00 of size 83200\n    2018-12-26 17:02:58.644740: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe80bf100 of size 5120512\n    2018-12-26 17:02:58.644753: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe85a1300 of size 83200\n    2018-12-26 17:02:58.644768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe85b5800 of size 5120512\n    2018-12-26 17:02:58.644781: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe8a97a00 of size 83200\n    2018-12-26 17:02:58.644794: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe8aabf00 of size 5120512\n    2018-12-26 17:02:58.644807: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe8f8e100 of size 83200\n    2018-12-26 17:02:58.644820: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe8fa2600 of size 5120512\n    2018-12-26 17:02:58.644835: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe9484800 of size 83200\n    2018-12-26 17:02:58.644845: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe9498d00 of size 5120512\n    2018-12-26 17:02:58.644855: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe997af00 of size 83200\n    2018-12-26 17:02:58.644872: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe998f400 of size 5120512\n    2018-12-26 17:02:58.644890: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbe9e71600 of size 83200\n    2018-12-26 17:02:58.644916: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbe9e85b00 of size 5120512\n    2018-12-26 17:02:58.644940: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbea367d00 of size 83200\n    2018-12-26 17:02:58.644968: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbea37c200 of size 5120512\n    2018-12-26 17:02:58.644991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbea85e400 of size 83200\n    2018-12-26 17:02:58.645011: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbea872900 of size 5120512\n    2018-12-26 17:02:58.645031: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbead54b00 of size 83200\n    2018-12-26 17:02:58.645054: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbead69000 of size 5120512\n    2018-12-26 17:02:58.645073: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbeb24b200 of size 83200\n    2018-12-26 17:02:58.645093: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbeb25f700 of size 5120512\n    2018-12-26 17:02:58.645113: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbeb741900 of size 83200\n    2018-12-26 17:02:58.645133: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbeb755e00 of size 5120512\n    2018-12-26 17:02:58.645152: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbebc38000 of size 83200\n    2018-12-26 17:02:58.645172: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbebc4c500 of size 5120512\n    2018-12-26 17:02:58.645191: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbec12e700 of size 83200\n    2018-12-26 17:02:58.645211: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbec142c00 of size 5120512\n    2018-12-26 17:02:58.645231: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbec624e00 of size 83200\n    2018-12-26 17:02:58.645251: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbec639300 of size 5120512\n    2018-12-26 17:02:58.645270: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbecb1b500 of size 83200\n    2018-12-26 17:02:58.645290: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbecb2fa00 of size 5120512\n    2018-12-26 17:02:58.645310: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbed011c00 of size 83200\n    2018-12-26 17:02:58.645330: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbed026100 of size 5120512\n    2018-12-26 17:02:58.645349: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbed508300 of size 83200\n    2018-12-26 17:02:58.645371: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbed51c800 of size 5120512\n    2018-12-26 17:02:58.645391: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbed9fea00 of size 83200\n    2018-12-26 17:02:58.645411: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbeda12f00 of size 5120512\n    2018-12-26 17:02:58.645430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbedef5100 of size 83200\n    2018-12-26 17:02:58.645451: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbedf09600 of size 5120512\n    2018-12-26 17:02:58.645468: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbee3eb800 of size 83200\n    2018-12-26 17:02:58.645484: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbee3ffd00 of size 5120512\n    2018-12-26 17:02:58.645504: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbee8e1f00 of size 83200\n    2018-12-26 17:02:58.645524: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbee8f6400 of size 5120512\n    2018-12-26 17:02:58.645543: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbeedd8600 of size 83200\n    2018-12-26 17:02:58.645563: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbeedecb00 of size 5120512\n    2018-12-26 17:02:58.645583: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbef2ced00 of size 83200\n    2018-12-26 17:02:58.645602: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbef2e3200 of size 5120512\n    2018-12-26 17:02:58.645622: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbef7c5400 of size 83200\n    2018-12-26 17:02:58.645641: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbef7d9900 of size 5120512\n    2018-12-26 17:02:58.645661: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbefcbbb00 of size 83200\n    2018-12-26 17:02:58.645683: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbefcd0000 of size 5120512\n    2018-12-26 17:02:58.645710: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf01b2200 of size 83200\n    2018-12-26 17:02:58.645732: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf01c6700 of size 5120512\n    2018-12-26 17:02:58.645753: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf06a8900 of size 83200\n    2018-12-26 17:02:58.645773: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf06bce00 of size 5120512\n    2018-12-26 17:02:58.645796: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf0b9f000 of size 83200\n    2018-12-26 17:02:58.645815: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf0bb3500 of size 5120512\n    2018-12-26 17:02:58.645835: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf1095700 of size 83200\n    2018-12-26 17:02:58.645854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf10a9c00 of size 5120512\n    2018-12-26 17:02:58.645874: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf158be00 of size 83200\n    2018-12-26 17:02:58.645893: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf15a0300 of size 5120512\n    2018-12-26 17:02:58.645913: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf1a82500 of size 83200\n    2018-12-26 17:02:58.645932: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf1a96a00 of size 5120512\n    2018-12-26 17:02:58.645951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf1f78c00 of size 83200\n    2018-12-26 17:02:58.645971: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf1f8d100 of size 5120512\n    2018-12-26 17:02:58.645990: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf246f300 of size 83200\n    2018-12-26 17:02:58.646012: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf2483800 of size 5120512\n    2018-12-26 17:02:58.646031: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf2965a00 of size 83200\n    2018-12-26 17:02:58.646051: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf2979f00 of size 5120512\n    2018-12-26 17:02:58.646070: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf2e5c100 of size 83200\n    2018-12-26 17:02:58.646090: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf2e70600 of size 5120512\n    2018-12-26 17:02:58.646109: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf3352800 of size 83200\n    2018-12-26 17:02:58.646129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf3366d00 of size 5120512\n    2018-12-26 17:02:58.646148: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf3848f00 of size 83200\n    2018-12-26 17:02:58.646168: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf385d400 of size 5120512\n    2018-12-26 17:02:58.646188: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf3d3f600 of size 83200\n    2018-12-26 17:02:58.646208: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf3d53b00 of size 5120512\n    2018-12-26 17:02:58.646227: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf4235d00 of size 83200\n    2018-12-26 17:02:58.646246: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf424a200 of size 5120512\n    2018-12-26 17:02:58.646266: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf472c400 of size 83200\n    2018-12-26 17:02:58.646285: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf4740900 of size 5120512\n    2018-12-26 17:02:58.646305: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf4c22b00 of size 83200\n    2018-12-26 17:02:58.646327: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf4c37000 of size 5120512\n    2018-12-26 17:02:58.646346: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf5119200 of size 83200\n    2018-12-26 17:02:58.646366: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf512d700 of size 5120512\n    2018-12-26 17:02:58.646385: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf560f900 of size 83200\n    2018-12-26 17:02:58.646405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf5623e00 of size 5120512\n    2018-12-26 17:02:58.646425: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf5b06000 of size 83200\n    2018-12-26 17:02:58.646444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf5b1a500 of size 5120512\n    2018-12-26 17:02:58.646464: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf5ffc700 of size 83200\n    2018-12-26 17:02:58.646483: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf6010c00 of size 5120512\n    2018-12-26 17:02:58.646503: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf64f2e00 of size 83200\n    2018-12-26 17:02:58.646522: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf6507300 of size 5120512\n    2018-12-26 17:02:58.646542: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf69e9500 of size 83200\n    2018-12-26 17:02:58.646562: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf69fda00 of size 5120512\n    2018-12-26 17:02:58.646582: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf6edfc00 of size 83200\n    2018-12-26 17:02:58.646601: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf6ef4100 of size 5120512\n    2018-12-26 17:02:58.646621: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf73d6300 of size 83200\n    2018-12-26 17:02:58.646642: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf73ea800 of size 5120512\n    2018-12-26 17:02:58.646662: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf78cca00 of size 83200\n    2018-12-26 17:02:58.646681: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf78e0f00 of size 5120512\n    2018-12-26 17:02:58.646708: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf7dc3100 of size 83200\n    2018-12-26 17:02:58.646730: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf7dd7600 of size 5120512\n    2018-12-26 17:02:58.646751: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf82b9800 of size 83200\n    2018-12-26 17:02:58.646771: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf82cdd00 of size 5120512\n    2018-12-26 17:02:58.646791: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf87aff00 of size 83200\n    2018-12-26 17:02:58.646811: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf87c4400 of size 5120512\n    2018-12-26 17:02:58.646830: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf8ca6600 of size 83200\n    2018-12-26 17:02:58.646850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf8cbab00 of size 5120512\n    2018-12-26 17:02:58.646870: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf919cd00 of size 83200\n    2018-12-26 17:02:58.646889: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf91b1200 of size 5120512\n    2018-12-26 17:02:58.646909: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf9693400 of size 83200\n    2018-12-26 17:02:58.646929: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf96a7900 of size 5120512\n    2018-12-26 17:02:58.646949: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbf9b89b00 of size 83200\n    2018-12-26 17:02:58.646971: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbf9b9e000 of size 5120512\n    2018-12-26 17:02:58.646990: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfa080200 of size 83200\n    2018-12-26 17:02:58.647010: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfa094700 of size 5120512\n    2018-12-26 17:02:58.647030: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfa576900 of size 83200\n    2018-12-26 17:02:58.647050: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfa58ae00 of size 5120512\n    2018-12-26 17:02:58.647070: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfaa6d000 of size 83200\n    2018-12-26 17:02:58.647090: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfaa81500 of size 5120512\n    2018-12-26 17:02:58.647110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfaf63700 of size 83200\n    2018-12-26 17:02:58.647130: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfaf77c00 of size 5120512\n    2018-12-26 17:02:58.647149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfb459e00 of size 83200\n    2018-12-26 17:02:58.647169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfb46e300 of size 5120512\n    2018-12-26 17:02:58.647189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfb950500 of size 83200\n    2018-12-26 17:02:58.647209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfb964a00 of size 5120512\n    2018-12-26 17:02:58.647228: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfbe46c00 of size 83200\n    2018-12-26 17:02:58.647248: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfbe5b100 of size 5120512\n    2018-12-26 17:02:58.647268: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfc33d300 of size 83200\n    2018-12-26 17:02:58.647290: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfc351800 of size 5120512\n    2018-12-26 17:02:58.647310: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfc833a00 of size 83200\n    2018-12-26 17:02:58.647330: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfc847f00 of size 5120512\n    2018-12-26 17:02:58.647350: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfcd2a100 of size 83200\n    2018-12-26 17:02:58.647370: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfcd3e600 of size 5120512\n    2018-12-26 17:02:58.647390: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfd220800 of size 83200\n    2018-12-26 17:02:58.647410: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfd234d00 of size 5120512\n    2018-12-26 17:02:58.647429: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfd716f00 of size 83200\n    2018-12-26 17:02:58.647449: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfd72b400 of size 5120512\n    2018-12-26 17:02:58.647469: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfdc0d600 of size 83200\n    2018-12-26 17:02:58.647489: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfdc21b00 of size 5120512\n    2018-12-26 17:02:58.647508: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfe103d00 of size 83200\n    2018-12-26 17:02:58.647528: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfe118200 of size 5120512\n    2018-12-26 17:02:58.647547: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfe5fa400 of size 83200\n    2018-12-26 17:02:58.647567: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfe60e900 of size 5120512\n    2018-12-26 17:02:58.647587: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfeaf0b00 of size 83200\n    2018-12-26 17:02:58.647608: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfeb05000 of size 5120512\n    2018-12-26 17:02:58.647628: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbfefe7200 of size 83200\n    2018-12-26 17:02:58.647648: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbfeffb700 of size 5120512\n    2018-12-26 17:02:58.647668: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbff4dd900 of size 83200\n    2018-12-26 17:02:58.647688: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbff4f1e00 of size 5120512\n    2018-12-26 17:02:58.647715: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbff9d4000 of size 83200\n    2018-12-26 17:02:58.647737: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbff9e8500 of size 5120512\n    2018-12-26 17:02:58.647757: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xbffeca700 of size 83200\n    2018-12-26 17:02:58.647777: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xbffedec00 of size 5120512\n    2018-12-26 17:02:58.647797: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc003c0e00 of size 83200\n    2018-12-26 17:02:58.647817: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc003d5300 of size 5120512\n    2018-12-26 17:02:58.647837: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc008b7500 of size 83200\n    2018-12-26 17:02:58.647857: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc008cba00 of size 5120512\n    2018-12-26 17:02:58.647877: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc00dadc00 of size 83200\n    2018-12-26 17:02:58.647897: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc00dc2100 of size 5120512\n    2018-12-26 17:02:58.647916: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc012a4300 of size 83200\n    2018-12-26 17:02:58.647938: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc012b8800 of size 5120512\n    2018-12-26 17:02:58.648315: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0179aa00 of size 83200\n    2018-12-26 17:02:58.648351: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc017aef00 of size 5120512\n    2018-12-26 17:02:58.648372: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc01c91100 of size 83200\n    2018-12-26 17:02:58.648393: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc01ca5600 of size 5120512\n    2018-12-26 17:02:58.648413: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc02187800 of size 83200\n    2018-12-26 17:02:58.648433: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0219bd00 of size 5120512\n    2018-12-26 17:02:58.648452: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0267df00 of size 83200\n    2018-12-26 17:02:58.648472: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc02692400 of size 5120512\n    2018-12-26 17:02:58.648491: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc02b74600 of size 83200\n    2018-12-26 17:02:58.648511: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc02b88b00 of size 5120512\n    2018-12-26 17:02:58.648530: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0306ad00 of size 83200\n    2018-12-26 17:02:58.648550: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0307f200 of size 5120512\n    2018-12-26 17:02:58.648569: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc03561400 of size 83200\n    2018-12-26 17:02:58.648589: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc03575900 of size 5120512\n    2018-12-26 17:02:58.648608: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc03a57b00 of size 83200\n    2018-12-26 17:02:58.648628: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc03a6c000 of size 5120512\n    2018-12-26 17:02:58.648648: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc03f4e200 of size 83200\n    2018-12-26 17:02:58.648667: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc03f62700 of size 5120512\n    2018-12-26 17:02:58.648686: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc04444900 of size 83200\n    2018-12-26 17:02:58.648715: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc04458e00 of size 5120512\n    2018-12-26 17:02:58.648737: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0493b000 of size 83200\n    2018-12-26 17:02:58.648757: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0494f500 of size 5120512\n    2018-12-26 17:02:58.648776: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc04e31700 of size 83200\n    2018-12-26 17:02:58.648796: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc04e45c00 of size 5120512\n    2018-12-26 17:02:58.648814: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc05327e00 of size 83200\n    2018-12-26 17:02:58.648834: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0533c300 of size 5120512\n    2018-12-26 17:02:58.648853: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0581e500 of size 83200\n    2018-12-26 17:02:58.648872: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc05832a00 of size 5120512\n    2018-12-26 17:02:58.648891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc05d14c00 of size 83200\n    2018-12-26 17:02:58.648915: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc05d29100 of size 5120512\n    2018-12-26 17:02:58.648936: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0620b300 of size 83200\n    2018-12-26 17:02:58.648956: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0621f800 of size 5120512\n    2018-12-26 17:02:58.648976: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc06701a00 of size 83200\n    2018-12-26 17:02:58.648995: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc06715f00 of size 5120512\n    2018-12-26 17:02:58.649016: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc06bf8100 of size 83200\n    2018-12-26 17:02:58.649035: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc06c0c600 of size 5120512\n    2018-12-26 17:02:58.649055: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc070ee800 of size 83200\n    2018-12-26 17:02:58.649074: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc07102d00 of size 5120512\n    2018-12-26 17:02:58.649093: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc075e4f00 of size 83200\n    2018-12-26 17:02:58.649113: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc075f9400 of size 5120512\n    2018-12-26 17:02:58.649132: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc07adb600 of size 83200\n    2018-12-26 17:02:58.649152: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc07aefb00 of size 5120512\n    2018-12-26 17:02:58.649171: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc07fd1d00 of size 83200\n    2018-12-26 17:02:58.649190: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc07fe6200 of size 5120512\n    2018-12-26 17:02:58.649210: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc084c8400 of size 83200\n    2018-12-26 17:02:58.649229: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc084dc900 of size 5120512\n    2018-12-26 17:02:58.649249: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc089beb00 of size 83200\n    2018-12-26 17:02:58.649269: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc089d3000 of size 5120512\n    2018-12-26 17:02:58.649287: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc08eb5200 of size 83200\n    2018-12-26 17:02:58.649308: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc08ec9700 of size 5120512\n    2018-12-26 17:02:58.649328: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc093ab900 of size 83200\n    2018-12-26 17:02:58.649348: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc093bfe00 of size 5120512\n    2018-12-26 17:02:58.649367: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc098a2000 of size 83200\n    2018-12-26 17:02:58.649386: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc098b6500 of size 5120512\n    2018-12-26 17:02:58.649405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc09d98700 of size 83200\n    2018-12-26 17:02:58.649425: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc09dacc00 of size 5120512\n    2018-12-26 17:02:58.649444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0a28ee00 of size 83200\n    2018-12-26 17:02:58.649463: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0a2a3300 of size 5120512\n    2018-12-26 17:02:58.649482: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0a785500 of size 83200\n    2018-12-26 17:02:58.649502: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0a799a00 of size 5120512\n    2018-12-26 17:02:58.649521: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0ac7bc00 of size 83200\n    2018-12-26 17:02:58.649540: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0ac90100 of size 5120512\n    2018-12-26 17:02:58.649559: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0b172300 of size 83200\n    2018-12-26 17:02:58.649579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0b186800 of size 5120512\n    2018-12-26 17:02:58.649598: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0b668a00 of size 83200\n    2018-12-26 17:02:58.649618: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0b67cf00 of size 5120512\n    2018-12-26 17:02:58.649638: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0bb5f100 of size 83200\n    2018-12-26 17:02:58.649657: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0bb73600 of size 5120512\n    2018-12-26 17:02:58.649676: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0c055800 of size 83200\n    2018-12-26 17:02:58.649695: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0c069d00 of size 5120512\n    2018-12-26 17:02:58.649724: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0c54bf00 of size 83200\n    2018-12-26 17:02:58.649745: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0c560400 of size 5120512\n    2018-12-26 17:02:58.649764: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0ca42600 of size 83200\n    2018-12-26 17:02:58.649783: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0ca56b00 of size 5120512\n    2018-12-26 17:02:58.649803: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0cf38d00 of size 83200\n    2018-12-26 17:02:58.649823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0cf4d200 of size 5120512\n    2018-12-26 17:02:58.649842: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0d42f400 of size 83200\n    2018-12-26 17:02:58.649862: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0d443900 of size 5120512\n    2018-12-26 17:02:58.649881: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0d925b00 of size 83200\n    2018-12-26 17:02:58.649902: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0d93a000 of size 5120512\n    2018-12-26 17:02:58.649921: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0de1c200 of size 83200\n    2018-12-26 17:02:58.649941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0de30700 of size 5120512\n    2018-12-26 17:02:58.649961: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0e312900 of size 83200\n    2018-12-26 17:02:58.649981: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0e326e00 of size 5120512\n    2018-12-26 17:02:58.650003: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0e809000 of size 83200\n    2018-12-26 17:02:58.650023: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0e81d500 of size 5120512\n    2018-12-26 17:02:58.650043: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0ecff700 of size 83200\n    2018-12-26 17:02:58.650062: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0ed13c00 of size 5120512\n    2018-12-26 17:02:58.650082: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0f1f5e00 of size 83200\n    2018-12-26 17:02:58.650101: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0f20a300 of size 5120512\n    2018-12-26 17:02:58.650121: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0f6ec500 of size 83200\n    2018-12-26 17:02:58.650140: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0f700a00 of size 5120512\n    2018-12-26 17:02:58.650159: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc0fbe2c00 of size 83200\n    2018-12-26 17:02:58.650179: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc0fbf7100 of size 5120512\n    2018-12-26 17:02:58.650198: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc100d9300 of size 83200\n    2018-12-26 17:02:58.650218: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc100ed800 of size 5120512\n    2018-12-26 17:02:58.650237: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc105cfa00 of size 83200\n    2018-12-26 17:02:58.650256: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc105e3f00 of size 5120512\n    2018-12-26 17:02:58.650276: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc10ac6100 of size 83200\n    2018-12-26 17:02:58.650296: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc10ada600 of size 5120512\n    2018-12-26 17:02:58.650315: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc10fbc800 of size 83200\n    2018-12-26 17:02:58.650334: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc10fd0d00 of size 5120512\n    2018-12-26 17:02:58.650353: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc114b2f00 of size 83200\n    2018-12-26 17:02:58.650372: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc114c7400 of size 5120512\n    2018-12-26 17:02:58.650391: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc119a9600 of size 83200\n    2018-12-26 17:02:58.650410: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc119bdb00 of size 5120512\n    2018-12-26 17:02:58.650429: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc11e9fd00 of size 83200\n    2018-12-26 17:02:58.650449: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc11eb4200 of size 5120512\n    2018-12-26 17:02:58.650468: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc12396400 of size 83200\n    2018-12-26 17:02:58.650488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc123aa900 of size 5120512\n    2018-12-26 17:02:58.650507: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1288cb00 of size 83200\n    2018-12-26 17:02:58.650526: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc128a1000 of size 5120512\n    2018-12-26 17:02:58.650545: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc12d83200 of size 83200\n    2018-12-26 17:02:58.650565: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc12d97700 of size 5120512\n    2018-12-26 17:02:58.650584: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc13279900 of size 83200\n    2018-12-26 17:02:58.650604: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1328de00 of size 5120512\n    2018-12-26 17:02:58.650623: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc13770000 of size 83200\n    2018-12-26 17:02:58.650642: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc13784500 of size 5120512\n    2018-12-26 17:02:58.650661: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc13c66700 of size 83200\n    2018-12-26 17:02:58.650681: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc13c7ac00 of size 5120512\n    2018-12-26 17:02:58.650706: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1415ce00 of size 83200\n    2018-12-26 17:02:58.650729: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc14171300 of size 5120512\n    2018-12-26 17:02:58.650749: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc14653500 of size 83200\n    2018-12-26 17:02:58.650768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc14667a00 of size 5120512\n    2018-12-26 17:02:58.650788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc14b49c00 of size 83200\n    2018-12-26 17:02:58.650807: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc14b5e100 of size 5120512\n    2018-12-26 17:02:58.650826: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc15040300 of size 83200\n    2018-12-26 17:02:58.650846: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc15054800 of size 5120512\n    2018-12-26 17:02:58.650865: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc15536a00 of size 83200\n    2018-12-26 17:02:58.650885: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1554af00 of size 5120512\n    2018-12-26 17:02:58.650905: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc15a2d100 of size 83200\n    2018-12-26 17:02:58.650925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc15a41600 of size 5120512\n    2018-12-26 17:02:58.650944: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc15f23800 of size 83200\n    2018-12-26 17:02:58.650964: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc15f37d00 of size 5120512\n    2018-12-26 17:02:58.650983: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc16419f00 of size 83200\n    2018-12-26 17:02:58.651003: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1642e400 of size 5120512\n    2018-12-26 17:02:58.651023: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc16910600 of size 83200\n    2018-12-26 17:02:58.651042: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc16924b00 of size 5120512\n    2018-12-26 17:02:58.651062: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc16e06d00 of size 83200\n    2018-12-26 17:02:58.651082: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc16e1b200 of size 5120512\n    2018-12-26 17:02:58.651102: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc172fd400 of size 83200\n    2018-12-26 17:02:58.651122: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc17311900 of size 5120512\n    2018-12-26 17:02:58.651142: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc177f3b00 of size 83200\n    2018-12-26 17:02:58.651161: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc17808000 of size 5120512\n    2018-12-26 17:02:58.651181: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc17cea200 of size 83200\n    2018-12-26 17:02:58.651201: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc17cfe700 of size 5120512\n    2018-12-26 17:02:58.651221: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc181e0900 of size 83200\n    2018-12-26 17:02:58.651241: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc181f4e00 of size 5120512\n    2018-12-26 17:02:58.651261: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc186d7000 of size 83200\n    2018-12-26 17:02:58.651281: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc186eb500 of size 5120512\n    2018-12-26 17:02:58.651300: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc18bcd700 of size 83200\n    2018-12-26 17:02:58.651319: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc18be1c00 of size 5120512\n    2018-12-26 17:02:58.651339: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc190c3e00 of size 83200\n    2018-12-26 17:02:58.651359: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc190d8300 of size 5120512\n    2018-12-26 17:02:58.651378: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc195ba500 of size 83200\n    2018-12-26 17:02:58.651397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc195cea00 of size 5120512\n    2018-12-26 17:02:58.651417: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc19ab0c00 of size 83200\n    2018-12-26 17:02:58.651437: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc19ac5100 of size 5120512\n    2018-12-26 17:02:58.651457: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc19fa7300 of size 83200\n    2018-12-26 17:02:58.651476: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc19fbb800 of size 5120512\n    2018-12-26 17:02:58.651495: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1a49da00 of size 83200\n    2018-12-26 17:02:58.651515: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1a4b1f00 of size 5120512\n    2018-12-26 17:02:58.651534: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1a994100 of size 83200\n    2018-12-26 17:02:58.651554: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1a9a8600 of size 5120512\n    2018-12-26 17:02:58.651574: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1ae8a800 of size 83200\n    2018-12-26 17:02:58.651593: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1ae9ed00 of size 5120512\n    2018-12-26 17:02:58.651610: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1b380f00 of size 83200\n    2018-12-26 17:02:58.651630: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1b395400 of size 5120512\n    2018-12-26 17:02:58.651650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1b877600 of size 83200\n    2018-12-26 17:02:58.651670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1b88bb00 of size 5120512\n    2018-12-26 17:02:58.651690: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1bd6dd00 of size 83200\n    2018-12-26 17:02:58.651718: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1bd82200 of size 5120512\n    2018-12-26 17:02:58.651739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1c264400 of size 83200\n    2018-12-26 17:02:58.651759: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1c278900 of size 5120512\n    2018-12-26 17:02:58.651778: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1c75ab00 of size 83200\n    2018-12-26 17:02:58.651798: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1c76f000 of size 5120512\n    2018-12-26 17:02:58.651818: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1cc51200 of size 83200\n    2018-12-26 17:02:58.651838: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1cc65700 of size 5120512\n    2018-12-26 17:02:58.651858: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1d147900 of size 83200\n    2018-12-26 17:02:58.651879: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1d15be00 of size 5120512\n    2018-12-26 17:02:58.651899: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1d63e000 of size 83200\n    2018-12-26 17:02:58.651918: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1d652500 of size 5120512\n    2018-12-26 17:02:58.651938: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1db34700 of size 83200\n    2018-12-26 17:02:58.651957: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1db48c00 of size 5120512\n    2018-12-26 17:02:58.651991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1e02ae00 of size 83200\n    2018-12-26 17:02:58.652012: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1e03f300 of size 5120512\n    2018-12-26 17:02:58.652032: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1e521500 of size 83200\n    2018-12-26 17:02:58.652051: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1e535a00 of size 5120512\n    2018-12-26 17:02:58.652071: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1ea17c00 of size 83200\n    2018-12-26 17:02:58.652090: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1ea2c100 of size 5120512\n    2018-12-26 17:02:58.652110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1ef0e300 of size 83200\n    2018-12-26 17:02:58.652129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1ef22800 of size 5120512\n    2018-12-26 17:02:58.652149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1f404a00 of size 83200\n    2018-12-26 17:02:58.652168: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1f418f00 of size 5120512\n    2018-12-26 17:02:58.652188: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1f8fb100 of size 83200\n    2018-12-26 17:02:58.652207: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1f90f600 of size 5120512\n    2018-12-26 17:02:58.652227: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc1fdf1800 of size 83200\n    2018-12-26 17:02:58.652246: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc1fe05d00 of size 5120512\n    2018-12-26 17:02:58.652266: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc202e7f00 of size 83200\n    2018-12-26 17:02:58.652285: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc202fc400 of size 5120512\n    2018-12-26 17:02:58.652305: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc207de600 of size 83200\n    2018-12-26 17:02:58.652324: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc207f2b00 of size 5120512\n    2018-12-26 17:02:58.652344: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc20cd4d00 of size 83200\n    2018-12-26 17:02:58.652364: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc20ce9200 of size 5120512\n    2018-12-26 17:02:58.652384: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc211cb400 of size 83200\n    2018-12-26 17:02:58.652404: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc211df900 of size 5120512\n    2018-12-26 17:02:58.652423: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc216c1b00 of size 83200\n    2018-12-26 17:02:58.652443: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc216d6000 of size 5120512\n    2018-12-26 17:02:58.652463: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc21bb8200 of size 83200\n    2018-12-26 17:02:58.652483: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc21bcc700 of size 5120512\n    2018-12-26 17:02:58.652503: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc220ae900 of size 83200\n    2018-12-26 17:02:58.652523: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc220c2e00 of size 5120512\n    2018-12-26 17:02:58.652542: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc225a5000 of size 83200\n    2018-12-26 17:02:58.652561: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc225b9500 of size 5120512\n    2018-12-26 17:02:58.652579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc22a9b700 of size 83200\n    2018-12-26 17:02:58.652599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc22aafc00 of size 5120512\n    2018-12-26 17:02:58.652619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc22f91e00 of size 83200\n    2018-12-26 17:02:58.652638: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc22fa6300 of size 5120512\n    2018-12-26 17:02:58.652658: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc23488500 of size 83200\n    2018-12-26 17:02:58.652677: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2349ca00 of size 5120512\n    2018-12-26 17:02:58.652697: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2397ec00 of size 83200\n    2018-12-26 17:02:58.652726: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc23993100 of size 5120512\n    2018-12-26 17:02:58.652748: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc23e75300 of size 83200\n    2018-12-26 17:02:58.652768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc23e89800 of size 5120512\n    2018-12-26 17:02:58.652787: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2436ba00 of size 83200\n    2018-12-26 17:02:58.652806: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2437ff00 of size 5120512\n    2018-12-26 17:02:58.652826: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc24862100 of size 83200\n    2018-12-26 17:02:58.652846: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc24876600 of size 5120512\n    2018-12-26 17:02:58.652865: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc24d58800 of size 83200\n    2018-12-26 17:02:58.652885: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc24d6cd00 of size 5120512\n    2018-12-26 17:02:58.652905: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2524ef00 of size 83200\n    2018-12-26 17:02:58.652924: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc25263400 of size 5120512\n    2018-12-26 17:02:58.652943: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc25745600 of size 83200\n    2018-12-26 17:02:58.652963: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc25759b00 of size 5120512\n    2018-12-26 17:02:58.652983: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc25c3bd00 of size 83200\n    2018-12-26 17:02:58.653002: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc25c50200 of size 5120512\n    2018-12-26 17:02:58.653022: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc26132400 of size 83200\n    2018-12-26 17:02:58.653042: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc26146900 of size 5120512\n    2018-12-26 17:02:58.653062: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc26628b00 of size 83200\n    2018-12-26 17:02:58.653081: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2663d000 of size 5120512\n    2018-12-26 17:02:58.653100: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc26b1f200 of size 83200\n    2018-12-26 17:02:58.653120: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc26b33700 of size 5120512\n    2018-12-26 17:02:58.653139: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc27015900 of size 83200\n    2018-12-26 17:02:58.653158: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc27029e00 of size 5120512\n    2018-12-26 17:02:58.653178: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2750c000 of size 83200\n    2018-12-26 17:02:58.653197: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc27520500 of size 5120512\n    2018-12-26 17:02:58.653217: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc27a02700 of size 83200\n    2018-12-26 17:02:58.653236: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc27a16c00 of size 5120512\n    2018-12-26 17:02:58.653255: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc27ef8e00 of size 83200\n    2018-12-26 17:02:58.653276: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc27f0d300 of size 5120512\n    2018-12-26 17:02:58.653295: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc283ef500 of size 83200\n    2018-12-26 17:02:58.653315: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc28403a00 of size 5120512\n    2018-12-26 17:02:58.653335: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc288e5c00 of size 83200\n    2018-12-26 17:02:58.653355: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc288fa100 of size 5120512\n    2018-12-26 17:02:58.653374: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc28ddc300 of size 83200\n    2018-12-26 17:02:58.653394: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc28df0800 of size 5120512\n    2018-12-26 17:02:58.653413: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc292d2a00 of size 83200\n    2018-12-26 17:02:58.653433: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc292e6f00 of size 5120512\n    2018-12-26 17:02:58.653453: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc297c9100 of size 83200\n    2018-12-26 17:02:58.653473: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc297dd600 of size 5120512\n    2018-12-26 17:02:58.653492: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc29cbf800 of size 83200\n    2018-12-26 17:02:58.653512: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc29cd3d00 of size 5120512\n    2018-12-26 17:02:58.653531: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2a1b5f00 of size 83200\n    2018-12-26 17:02:58.653551: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2a1ca400 of size 5120512\n    2018-12-26 17:02:58.653571: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2a6ac600 of size 83200\n    2018-12-26 17:02:58.653590: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2a6c0b00 of size 5120512\n    2018-12-26 17:02:58.653610: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2aba2d00 of size 83200\n    2018-12-26 17:02:58.653630: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2abb7200 of size 5120512\n    2018-12-26 17:02:58.653649: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2b099400 of size 83200\n    2018-12-26 17:02:58.653668: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2b0ad900 of size 5120512\n    2018-12-26 17:02:58.653687: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2b58fb00 of size 83200\n    2018-12-26 17:02:58.653717: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2b5a4000 of size 5120512\n    2018-12-26 17:02:58.653739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2ba86200 of size 83200\n    2018-12-26 17:02:58.653759: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2ba9a700 of size 5120512\n    2018-12-26 17:02:58.653780: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2bf7c900 of size 83200\n    2018-12-26 17:02:58.653800: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2bf90e00 of size 5120512\n    2018-12-26 17:02:58.653820: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2c473000 of size 83200\n    2018-12-26 17:02:58.653839: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2c487500 of size 5120512\n    2018-12-26 17:02:58.653859: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2c969700 of size 83200\n    2018-12-26 17:02:58.653879: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2c97dc00 of size 5120512\n    2018-12-26 17:02:58.653899: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2ce5fe00 of size 83200\n    2018-12-26 17:02:58.653918: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2ce74300 of size 5120512\n    2018-12-26 17:02:58.653938: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2d356500 of size 83200\n    2018-12-26 17:02:58.653957: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2d36aa00 of size 5120512\n    2018-12-26 17:02:58.653976: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2d84cc00 of size 83200\n    2018-12-26 17:02:58.653996: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2d861100 of size 5120512\n    2018-12-26 17:02:58.654015: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2dd43300 of size 83200\n    2018-12-26 17:02:58.654035: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2dd57800 of size 5120512\n    2018-12-26 17:02:58.654055: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2e239a00 of size 83200\n    2018-12-26 17:02:58.654074: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2e24df00 of size 5120512\n    2018-12-26 17:02:58.654094: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2e730100 of size 83200\n    2018-12-26 17:02:58.654114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2e744600 of size 5120512\n    2018-12-26 17:02:58.654134: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2ec26800 of size 83200\n    2018-12-26 17:02:58.654153: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2ec3ad00 of size 5120512\n    2018-12-26 17:02:58.654173: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2f11cf00 of size 83200\n    2018-12-26 17:02:58.654193: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2f131400 of size 5120512\n    2018-12-26 17:02:58.654212: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2f613600 of size 83200\n    2018-12-26 17:02:58.654232: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2f627b00 of size 5120512\n    2018-12-26 17:02:58.654252: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc2fb09d00 of size 83200\n    2018-12-26 17:02:58.654271: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc2fb1e200 of size 5120512\n    2018-12-26 17:02:58.654291: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc30000400 of size 83200\n    2018-12-26 17:02:58.654311: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc30014900 of size 5120512\n    2018-12-26 17:02:58.654331: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc304f6b00 of size 83200\n    2018-12-26 17:02:58.654351: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3050b000 of size 5120512\n    2018-12-26 17:02:58.654371: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc309ed200 of size 83200\n    2018-12-26 17:02:58.654391: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc30a01700 of size 5120512\n    2018-12-26 17:02:58.654411: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc30ee3900 of size 83200\n    2018-12-26 17:02:58.654430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc30ef7e00 of size 5120512\n    2018-12-26 17:02:58.654450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc313da000 of size 83200\n    2018-12-26 17:02:58.654470: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc313ee500 of size 5120512\n    2018-12-26 17:02:58.654490: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc318d0700 of size 83200\n    2018-12-26 17:02:58.654509: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc318e4c00 of size 5120512\n    2018-12-26 17:02:58.654529: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc31dc6e00 of size 83200\n    2018-12-26 17:02:58.654548: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc31ddb300 of size 5120512\n    2018-12-26 17:02:58.654568: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc322bd500 of size 83200\n    2018-12-26 17:02:58.654587: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc322d1a00 of size 5120512\n    2018-12-26 17:02:58.654607: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc327b3c00 of size 83200\n    2018-12-26 17:02:58.654627: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc327c8100 of size 5120512\n    2018-12-26 17:02:58.654647: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc32caa300 of size 83200\n    2018-12-26 17:02:58.654666: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc32cbe800 of size 5120512\n    2018-12-26 17:02:58.654686: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc331a0a00 of size 83200\n    2018-12-26 17:02:58.654713: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc331b4f00 of size 5120512\n    2018-12-26 17:02:58.654735: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc33697100 of size 83200\n    2018-12-26 17:02:58.654755: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc336ab600 of size 5120512\n    2018-12-26 17:02:58.654776: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc33b8d800 of size 83200\n    2018-12-26 17:02:58.654795: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc33ba1d00 of size 5120512\n    2018-12-26 17:02:58.654815: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc34083f00 of size 83200\n    2018-12-26 17:02:58.654835: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc34098400 of size 5120512\n    2018-12-26 17:02:58.654854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3457a600 of size 83200\n    2018-12-26 17:02:58.654873: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3458eb00 of size 5120512\n    2018-12-26 17:02:58.654889: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc34a70d00 of size 83200\n    2018-12-26 17:02:58.654909: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc34a85200 of size 5120512\n    2018-12-26 17:02:58.654929: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc34f67400 of size 83200\n    2018-12-26 17:02:58.654949: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc34f7b900 of size 5120512\n    2018-12-26 17:02:58.654968: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3545db00 of size 83200\n    2018-12-26 17:02:58.654988: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc35472000 of size 5120512\n    2018-12-26 17:02:58.655008: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc35954200 of size 83200\n    2018-12-26 17:02:58.655027: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc35968700 of size 5120512\n    2018-12-26 17:02:58.655047: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc35e4a900 of size 83200\n    2018-12-26 17:02:58.655067: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc35e5ee00 of size 5120512\n    2018-12-26 17:02:58.655086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc36341000 of size 83200\n    2018-12-26 17:02:58.655106: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc36355500 of size 5120512\n    2018-12-26 17:02:58.655126: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc36837700 of size 83200\n    2018-12-26 17:02:58.655145: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3684bc00 of size 5120512\n    2018-12-26 17:02:58.655165: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc36d2de00 of size 83200\n    2018-12-26 17:02:58.655185: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc36d42300 of size 5120512\n    2018-12-26 17:02:58.655204: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc37224500 of size 83200\n    2018-12-26 17:02:58.655224: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc37238a00 of size 5120512\n    2018-12-26 17:02:58.655244: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3771ac00 of size 83200\n    2018-12-26 17:02:58.655264: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3772f100 of size 5120512\n    2018-12-26 17:02:58.655283: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc37c11300 of size 83200\n    2018-12-26 17:02:58.655302: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc37c25800 of size 5120512\n    2018-12-26 17:02:58.655322: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc38107a00 of size 83200\n    2018-12-26 17:02:58.655341: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3811bf00 of size 5120512\n    2018-12-26 17:02:58.655361: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc385fe100 of size 83200\n    2018-12-26 17:02:58.655380: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc38612600 of size 5120512\n    2018-12-26 17:02:58.655400: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc38af4800 of size 83200\n    2018-12-26 17:02:58.655419: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc38b08d00 of size 5120512\n    2018-12-26 17:02:58.655439: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc38feaf00 of size 83200\n    2018-12-26 17:02:58.655458: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc38fff400 of size 5120512\n    2018-12-26 17:02:58.655477: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc394e1600 of size 83200\n    2018-12-26 17:02:58.655496: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc394f5b00 of size 5120512\n    2018-12-26 17:02:58.655516: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc399d7d00 of size 83200\n    2018-12-26 17:02:58.655536: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc399ec200 of size 5120512\n    2018-12-26 17:02:58.655556: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc39ece400 of size 83200\n    2018-12-26 17:02:58.655575: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc39ee2900 of size 5120512\n    2018-12-26 17:02:58.655594: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3a3c4b00 of size 83200\n    2018-12-26 17:02:58.655614: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3a3d9000 of size 5120512\n    2018-12-26 17:02:58.655633: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3a8bb200 of size 83200\n    2018-12-26 17:02:58.655653: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3a8cf700 of size 5120512\n    2018-12-26 17:02:58.655672: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3adb1900 of size 83200\n    2018-12-26 17:02:58.655692: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3adc5e00 of size 5120512\n    2018-12-26 17:02:58.655720: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3b2a8000 of size 83200\n    2018-12-26 17:02:58.655742: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3b2bc500 of size 5120512\n    2018-12-26 17:02:58.655762: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3b79e700 of size 83200\n    2018-12-26 17:02:58.655781: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3b7b2c00 of size 5120512\n    2018-12-26 17:02:58.655801: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3bc94e00 of size 83200\n    2018-12-26 17:02:58.655820: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3bca9300 of size 5120512\n    2018-12-26 17:02:58.655840: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3c18b500 of size 83200\n    2018-12-26 17:02:58.655859: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3c19fa00 of size 5120512\n    2018-12-26 17:02:58.655879: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3c681c00 of size 83200\n    2018-12-26 17:02:58.655899: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3c696100 of size 5120512\n    2018-12-26 17:02:58.655919: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3cb78300 of size 83200\n    2018-12-26 17:02:58.655938: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3cb8c800 of size 5120512\n    2018-12-26 17:02:58.655958: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3d06ea00 of size 83200\n    2018-12-26 17:02:58.655991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3d082f00 of size 5120512\n    2018-12-26 17:02:58.656011: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3d565100 of size 83200\n    2018-12-26 17:02:58.656031: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3d579600 of size 5120512\n    2018-12-26 17:02:58.656050: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3da5b800 of size 83200\n    2018-12-26 17:02:58.656071: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3da6fd00 of size 5120512\n    2018-12-26 17:02:58.656090: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3df51f00 of size 83200\n    2018-12-26 17:02:58.656110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3df66400 of size 5120512\n    2018-12-26 17:02:58.656130: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3e448600 of size 83200\n    2018-12-26 17:02:58.656149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3e45cb00 of size 5120512\n    2018-12-26 17:02:58.656169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3e93ed00 of size 83200\n    2018-12-26 17:02:58.656188: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3e953200 of size 5120512\n    2018-12-26 17:02:58.656209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3ee35400 of size 83200\n    2018-12-26 17:02:58.656228: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3ee49900 of size 5120512\n    2018-12-26 17:02:58.656247: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3f32bb00 of size 83200\n    2018-12-26 17:02:58.656267: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3f340000 of size 5120512\n    2018-12-26 17:02:58.656286: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3f822200 of size 83200\n    2018-12-26 17:02:58.656306: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3f836700 of size 5120512\n    2018-12-26 17:02:58.656326: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc3fd18900 of size 83200\n    2018-12-26 17:02:58.656345: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc3fd2ce00 of size 5120512\n    2018-12-26 17:02:58.656366: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4020f000 of size 83200\n    2018-12-26 17:02:58.656387: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc40223500 of size 5120512\n    2018-12-26 17:02:58.656409: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc40705700 of size 83200\n    2018-12-26 17:02:58.656429: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc40719c00 of size 5120512\n    2018-12-26 17:02:58.656448: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc40bfbe00 of size 83200\n    2018-12-26 17:02:58.656468: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc40c10300 of size 5120512\n    2018-12-26 17:02:58.656487: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc410f2500 of size 83200\n    2018-12-26 17:02:58.656507: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc41106a00 of size 5120512\n    2018-12-26 17:02:58.656526: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc415e8c00 of size 83200\n    2018-12-26 17:02:58.656545: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc415fd100 of size 5120512\n    2018-12-26 17:02:58.656565: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc41adf300 of size 83200\n    2018-12-26 17:02:58.656584: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc41af3800 of size 5120512\n    2018-12-26 17:02:58.656604: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc41fd5a00 of size 83200\n    2018-12-26 17:02:58.656623: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc41fe9f00 of size 5120512\n    2018-12-26 17:02:58.656642: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc424cc100 of size 83200\n    2018-12-26 17:02:58.656661: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc424e0600 of size 5120512\n    2018-12-26 17:02:58.656680: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc429c2800 of size 83200\n    2018-12-26 17:02:58.656707: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc429d6d00 of size 5120512\n    2018-12-26 17:02:58.656730: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc42eb8f00 of size 83200\n    2018-12-26 17:02:58.656750: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc42ecd400 of size 5120512\n    2018-12-26 17:02:58.656769: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc433af600 of size 83200\n    2018-12-26 17:02:58.656790: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc433c3b00 of size 5120512\n    2018-12-26 17:02:58.656812: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc438a5d00 of size 83200\n    2018-12-26 17:02:58.656832: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc438ba200 of size 5120512\n    2018-12-26 17:02:58.656852: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc43d9c400 of size 83200\n    2018-12-26 17:02:58.656872: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc43db0900 of size 5120512\n    2018-12-26 17:02:58.656891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc44292b00 of size 83200\n    2018-12-26 17:02:58.656910: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc442a7000 of size 5120512\n    2018-12-26 17:02:58.656930: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc44789200 of size 83200\n    2018-12-26 17:02:58.656949: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4479d700 of size 5120512\n    2018-12-26 17:02:58.656969: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc44c7f900 of size 83200\n    2018-12-26 17:02:58.656988: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc44c93e00 of size 5120512\n    2018-12-26 17:02:58.657008: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc45176000 of size 83200\n    2018-12-26 17:02:58.657027: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4518a500 of size 5120512\n    2018-12-26 17:02:58.657047: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4566c700 of size 83200\n    2018-12-26 17:02:58.657067: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc45680c00 of size 5120512\n    2018-12-26 17:02:58.657086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc45b62e00 of size 83200\n    2018-12-26 17:02:58.657106: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc45b77300 of size 5120512\n    2018-12-26 17:02:58.657128: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc46059500 of size 83200\n    2018-12-26 17:02:58.657148: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4606da00 of size 5120512\n    2018-12-26 17:02:58.657168: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4654fc00 of size 83200\n    2018-12-26 17:02:58.657187: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc46564100 of size 5120512\n    2018-12-26 17:02:58.657206: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc46a46300 of size 83200\n    2018-12-26 17:02:58.657227: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc46a5a800 of size 5120512\n    2018-12-26 17:02:58.657247: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc46f3ca00 of size 83200\n    2018-12-26 17:02:58.657266: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc46f50f00 of size 5120512\n    2018-12-26 17:02:58.657285: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc47433100 of size 83200\n    2018-12-26 17:02:58.657305: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc47447600 of size 5120512\n    2018-12-26 17:02:58.657324: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc47929800 of size 83200\n    2018-12-26 17:02:58.657344: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4793dd00 of size 5120512\n    2018-12-26 17:02:58.657365: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc47e1ff00 of size 83200\n    2018-12-26 17:02:58.657384: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc47e34400 of size 5120512\n    2018-12-26 17:02:58.657403: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc48316600 of size 83200\n    2018-12-26 17:02:58.657423: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4832ab00 of size 5120512\n    2018-12-26 17:02:58.657442: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4880cd00 of size 83200\n    2018-12-26 17:02:58.657463: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc48821200 of size 5120512\n    2018-12-26 17:02:58.657482: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc48d03400 of size 83200\n    2018-12-26 17:02:58.657502: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc48d17900 of size 5120512\n    2018-12-26 17:02:58.657521: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc491f9b00 of size 83200\n    2018-12-26 17:02:58.657540: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4920e000 of size 5120512\n    2018-12-26 17:02:58.657560: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc496f0200 of size 83200\n    2018-12-26 17:02:58.657580: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc49704700 of size 5120512\n    2018-12-26 17:02:58.657600: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc49be6900 of size 83200\n    2018-12-26 17:02:58.657619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc49bfae00 of size 5120512\n    2018-12-26 17:02:58.657639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4a0dd000 of size 83200\n    2018-12-26 17:02:58.657660: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4a0f1500 of size 5120512\n    2018-12-26 17:02:58.657680: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4a5d3700 of size 83200\n    2018-12-26 17:02:58.657706: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4a5e7c00 of size 5120512\n    2018-12-26 17:02:58.657729: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4aac9e00 of size 83200\n    2018-12-26 17:02:58.657749: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4aade300 of size 5120512\n    2018-12-26 17:02:58.657768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4afc0500 of size 83200\n    2018-12-26 17:02:58.657788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4afd4a00 of size 5120512\n    2018-12-26 17:02:58.657808: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4b4b6c00 of size 83200\n    2018-12-26 17:02:58.657828: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4b4cb100 of size 5120512\n    2018-12-26 17:02:58.657848: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4b9ad300 of size 83200\n    2018-12-26 17:02:58.657868: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4b9c1800 of size 5120512\n    2018-12-26 17:02:58.657888: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4bea3a00 of size 83200\n    2018-12-26 17:02:58.657908: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4beb7f00 of size 5120512\n    2018-12-26 17:02:58.657928: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4c39a100 of size 83200\n    2018-12-26 17:02:58.657948: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4c3ae600 of size 5120512\n    2018-12-26 17:02:58.657967: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4c890800 of size 83200\n    2018-12-26 17:02:58.657987: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4c8a4d00 of size 5120512\n    2018-12-26 17:02:58.658007: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4cd86f00 of size 83200\n    2018-12-26 17:02:58.658026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4cd9b400 of size 5120512\n    2018-12-26 17:02:58.658046: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4d27d600 of size 83200\n    2018-12-26 17:02:58.658065: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4d291b00 of size 5120512\n    2018-12-26 17:02:58.658085: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4d773d00 of size 83200\n    2018-12-26 17:02:58.658105: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4d788200 of size 5120512\n    2018-12-26 17:02:58.658125: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4dc6a400 of size 83200\n    2018-12-26 17:02:58.658144: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4dc7e900 of size 5120512\n    2018-12-26 17:02:58.658164: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4e160b00 of size 83200\n    2018-12-26 17:02:58.658184: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4e175000 of size 5120512\n    2018-12-26 17:02:58.658203: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4e657200 of size 83200\n    2018-12-26 17:02:58.658223: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4e66b700 of size 5120512\n    2018-12-26 17:02:58.658243: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4eb4d900 of size 83200\n    2018-12-26 17:02:58.658263: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4eb61e00 of size 5120512\n    2018-12-26 17:02:58.658282: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4f044000 of size 83200\n    2018-12-26 17:02:58.658302: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4f058500 of size 5120512\n    2018-12-26 17:02:58.658322: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4f53a700 of size 83200\n    2018-12-26 17:02:58.658342: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4f54ec00 of size 5120512\n    2018-12-26 17:02:58.658361: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4fa30e00 of size 83200\n    2018-12-26 17:02:58.658380: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4fa45300 of size 5120512\n    2018-12-26 17:02:58.658399: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc4ff27500 of size 83200\n    2018-12-26 17:02:58.658419: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc4ff3ba00 of size 5120512\n    2018-12-26 17:02:58.658439: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5041dc00 of size 83200\n    2018-12-26 17:02:58.658459: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc50432100 of size 5120512\n    2018-12-26 17:02:58.658479: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc50914300 of size 83200\n    2018-12-26 17:02:58.658499: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc50928800 of size 5120512\n    2018-12-26 17:02:58.658518: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc50e0aa00 of size 83200\n    2018-12-26 17:02:58.658535: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc50e1ef00 of size 5120512\n    2018-12-26 17:02:58.658556: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc51301100 of size 83200\n    2018-12-26 17:02:58.658576: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc51315600 of size 5120512\n    2018-12-26 17:02:58.658596: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc517f7800 of size 83200\n    2018-12-26 17:02:58.658616: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5180bd00 of size 5120512\n    2018-12-26 17:02:58.658636: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc51cedf00 of size 83200\n    2018-12-26 17:02:58.658657: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc51d02400 of size 5120512\n    2018-12-26 17:02:58.658679: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc521e4600 of size 83200\n    2018-12-26 17:02:58.658705: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc521f8b00 of size 5120512\n    2018-12-26 17:02:58.658728: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc526dad00 of size 83200\n    2018-12-26 17:02:58.658750: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc526ef200 of size 5120512\n    2018-12-26 17:02:58.658769: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc52bd1400 of size 83200\n    2018-12-26 17:02:58.658789: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc52be5900 of size 5120512\n    2018-12-26 17:02:58.658809: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc530c7b00 of size 83200\n    2018-12-26 17:02:58.658829: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc530dc000 of size 5120512\n    2018-12-26 17:02:58.658849: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc535be200 of size 83200\n    2018-12-26 17:02:58.658870: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc535d2700 of size 5120512\n    2018-12-26 17:02:58.658891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc53ab4900 of size 83200\n    2018-12-26 17:02:58.658913: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc53ac8e00 of size 5120512\n    2018-12-26 17:02:58.658934: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc53fab000 of size 83200\n    2018-12-26 17:02:58.658953: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc53fbf500 of size 5120512\n    2018-12-26 17:02:58.658974: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc544a1700 of size 83200\n    2018-12-26 17:02:58.658993: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc544b5c00 of size 5120512\n    2018-12-26 17:02:58.659014: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc54997e00 of size 83200\n    2018-12-26 17:02:58.659035: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc549ac300 of size 5120512\n    2018-12-26 17:02:58.659055: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc54e8e500 of size 83200\n    2018-12-26 17:02:58.659075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc54ea2a00 of size 5120512\n    2018-12-26 17:02:58.659094: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc55384c00 of size 83200\n    2018-12-26 17:02:58.659114: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc55399100 of size 5120512\n    2018-12-26 17:02:58.659133: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5587b300 of size 83200\n    2018-12-26 17:02:58.659153: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5588f800 of size 5120512\n    2018-12-26 17:02:58.659173: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc55d71a00 of size 83200\n    2018-12-26 17:02:58.659193: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc55d85f00 of size 5120512\n    2018-12-26 17:02:58.659213: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc56268100 of size 83200\n    2018-12-26 17:02:58.659233: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5627c600 of size 5120512\n    2018-12-26 17:02:58.659253: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5675e800 of size 83200\n    2018-12-26 17:02:58.659273: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc56772d00 of size 5120512\n    2018-12-26 17:02:58.659292: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc56c54f00 of size 83200\n    2018-12-26 17:02:58.659312: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc56c69400 of size 5120512\n    2018-12-26 17:02:58.659331: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5714b600 of size 83200\n    2018-12-26 17:02:58.659351: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5715fb00 of size 5120512\n    2018-12-26 17:02:58.659370: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc57641d00 of size 83200\n    2018-12-26 17:02:58.659390: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc57656200 of size 5120512\n    2018-12-26 17:02:58.659410: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc57b38400 of size 83200\n    2018-12-26 17:02:58.659430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc57b4c900 of size 5120512\n    2018-12-26 17:02:58.659449: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5802eb00 of size 83200\n    2018-12-26 17:02:58.659469: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc58043000 of size 5120512\n    2018-12-26 17:02:58.659488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc58525200 of size 83200\n    2018-12-26 17:02:58.659507: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc58539700 of size 5120512\n    2018-12-26 17:02:58.659527: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc58a1b900 of size 83200\n    2018-12-26 17:02:58.659546: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc58a2fe00 of size 5120512\n    2018-12-26 17:02:58.659567: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc58f12000 of size 83200\n    2018-12-26 17:02:58.659586: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc58f26500 of size 5120512\n    2018-12-26 17:02:58.659606: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc59408700 of size 83200\n    2018-12-26 17:02:58.659626: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5941cc00 of size 5120512\n    2018-12-26 17:02:58.659645: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc598fee00 of size 83200\n    2018-12-26 17:02:58.659665: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc59913300 of size 5120512\n    2018-12-26 17:02:58.659685: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc59df5500 of size 83200\n    2018-12-26 17:02:58.659711: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc59e09a00 of size 5120512\n    2018-12-26 17:02:58.659734: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5a2ebc00 of size 83200\n    2018-12-26 17:02:58.659754: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5a300100 of size 5120512\n    2018-12-26 17:02:58.659774: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5a7e2300 of size 83200\n    2018-12-26 17:02:58.659794: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5a7f6800 of size 5120512\n    2018-12-26 17:02:58.659813: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5acd8a00 of size 83200\n\n    2018-12-26 17:02:58.659833: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5acecf00 of size 5120512\n\n    2018-12-26 17:02:58.659853: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5b1cf100 of size 83200\n\n    2018-12-26 17:02:58.659872: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5b1e3600 of size 5120512\n\n    2018-12-26 17:02:58.659892: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5b6c5800 of size 83200\n\n    2018-12-26 17:02:58.659912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5b6d9d00 of size 5120512\n\n    2018-12-26 17:02:58.659931: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5bbbbf00 of size 83200\n\n    2018-12-26 17:02:58.659951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5bbd0400 of size 5120512\n\n    2018-12-26 17:02:58.660042: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5c0b2600 of size 83200\n\n    2018-12-26 17:02:58.660064: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5c0c6b00 of size 5120512\n\n    2018-12-26 17:02:58.660084: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5c5a8d00 of size 83200\n\n    2018-12-26 17:02:58.660103: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5c5bd200 of size 5120512\n\n    2018-12-26 17:02:58.660123: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5ca9f400 of size 83200\n\n    2018-12-26 17:02:58.660143: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5cab3900 of size 5120512\n\n    2018-12-26 17:02:58.660163: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5cf95b00 of size 83200\n\n    2018-12-26 17:02:58.660183: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5cfaa000 of size 5120512\n\n    2018-12-26 17:02:58.660202: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5d48c200 of size 83200\n\n    2018-12-26 17:02:58.660223: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5d4a0700 of size 5120512\n\n    2018-12-26 17:02:58.660242: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5d982900 of size 83200\n\n    2018-12-26 17:02:58.660262: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5d996e00 of size 5120512\n\n    2018-12-26 17:02:58.660285: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5de79000 of size 83200\n\n    2018-12-26 17:02:58.660306: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5de8d500 of size 5120512\n\n    2018-12-26 17:02:58.660326: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5e36f700 of size 83200\n\n    2018-12-26 17:02:58.660346: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5e383c00 of size 5120512\n\n    2018-12-26 17:02:58.660365: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5e865e00 of size 83200\n\n    2018-12-26 17:02:58.660386: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5e87a300 of size 5120512\n\n    2018-12-26 17:02:58.660405: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5ed5c500 of size 83200\n\n    2018-12-26 17:02:58.660425: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5ed70a00 of size 5120512\n\n    2018-12-26 17:02:58.660444: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5f252c00 of size 83200\n\n    2018-12-26 17:02:58.660463: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5f267100 of size 5120512\n\n    2018-12-26 17:02:58.660483: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5f749300 of size 83200\n\n    2018-12-26 17:02:58.660503: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5f75d800 of size 5120512\n\n    2018-12-26 17:02:58.660522: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc5fc3fa00 of size 83200\n\n    2018-12-26 17:02:58.660542: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc5fc53f00 of size 5120512\n\n    2018-12-26 17:02:58.660562: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc60136100 of size 83200\n\n    2018-12-26 17:02:58.660582: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6014a600 of size 5120512\n\n    2018-12-26 17:02:58.660602: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6062c800 of size 83200\n\n    2018-12-26 17:02:58.660619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc60640d00 of size 5120512\n\n    2018-12-26 17:02:58.660639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc60b22f00 of size 83200\n\n    2018-12-26 17:02:58.660659: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc60b37400 of size 5120512\n\n    2018-12-26 17:02:58.660679: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc61019600 of size 83200\n\n    2018-12-26 17:02:58.660705: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6102db00 of size 5120512\n\n    2018-12-26 17:02:58.660729: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6150fd00 of size 83200\n\n    2018-12-26 17:02:58.660750: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc61524200 of size 5120512\n\n    2018-12-26 17:02:58.660771: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc61a06400 of size 83200\n\n    2018-12-26 17:02:58.660790: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc61a1a900 of size 5120512\n\n    2018-12-26 17:02:58.660810: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc61efcb00 of size 83200\n\n    2018-12-26 17:02:58.660831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc61f11000 of size 5120512\n\n    2018-12-26 17:02:58.660851: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc623f3200 of size 83200\n\n    2018-12-26 17:02:58.660871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc62407700 of size 5120512\n\n    2018-12-26 17:02:58.660891: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc628e9900 of size 83200\n\n    2018-12-26 17:02:58.660911: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc628fde00 of size 5120512\n\n    2018-12-26 17:02:58.660931: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc62de0000 of size 83200\n\n    2018-12-26 17:02:58.660951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc62df4500 of size 5120512\n\n    2018-12-26 17:02:58.660971: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc632d6700 of size 83200\n\n    2018-12-26 17:02:58.660990: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc632eac00 of size 5120512\n\n    2018-12-26 17:02:58.661010: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc637cce00 of size 83200\n\n    2018-12-26 17:02:58.661030: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc637e1300 of size 5120512\n\n    2018-12-26 17:02:58.661049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc63cc3500 of size 83200\n\n    2018-12-26 17:02:58.661069: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc63cd7a00 of size 5120512\n\n    2018-12-26 17:02:58.661088: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc641b9c00 of size 83200\n\n    2018-12-26 17:02:58.661108: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc641ce100 of size 5120512\n\n    2018-12-26 17:02:58.661130: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc646b0300 of size 83200\n\n    2018-12-26 17:02:58.661150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc646c4800 of size 5120512\n\n    2018-12-26 17:02:58.661169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc64ba6a00 of size 83200\n\n    2018-12-26 17:02:58.661189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc64bbaf00 of size 5120512\n\n    2018-12-26 17:02:58.661209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6509d100 of size 83200\n\n    2018-12-26 17:02:58.661229: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc650b1600 of size 5120512\n\n    2018-12-26 17:02:58.661250: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc65593800 of size 83200\n\n    2018-12-26 17:02:58.661270: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc655a7d00 of size 5120512\n\n    2018-12-26 17:02:58.661291: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc65a89f00 of size 83200\n\n    2018-12-26 17:02:58.661310: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc65a9e400 of size 5120512\n\n    2018-12-26 17:02:58.661330: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc65f80600 of size 83200\n\n    2018-12-26 17:02:58.661350: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc65f94b00 of size 5120512\n\n    2018-12-26 17:02:58.661370: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc66476d00 of size 83200\n\n    2018-12-26 17:02:58.661390: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6648b200 of size 5120512\n\n    2018-12-26 17:02:58.661410: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6696d400 of size 83200\n\n    2018-12-26 17:02:58.661430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc66981900 of size 5120512\n\n    2018-12-26 17:02:58.661452: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc66e63b00 of size 83200\n\n    2018-12-26 17:02:58.661473: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc66e78000 of size 5120512\n\n    2018-12-26 17:02:58.661493: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6735a200 of size 83200\n\n    2018-12-26 17:02:58.661513: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6736e700 of size 5120512\n\n    2018-12-26 17:02:58.661534: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc67850900 of size 83200\n\n    2018-12-26 17:02:58.661554: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc67864e00 of size 5120512\n\n    2018-12-26 17:02:58.661574: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc67d47000 of size 83200\n\n    2018-12-26 17:02:58.661595: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc67d5b500 of size 5120512\n\n    2018-12-26 17:02:58.661615: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6823d700 of size 83200\n\n    2018-12-26 17:02:58.661635: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc68251c00 of size 5120512\n\n    2018-12-26 17:02:58.661654: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc68733e00 of size 83200\n\n    2018-12-26 17:02:58.661674: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc68748300 of size 5120512\n\n    2018-12-26 17:02:58.661694: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc68c2a500 of size 83200\n\n    2018-12-26 17:02:58.661722: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc68c3ea00 of size 5120512\n\n    2018-12-26 17:02:58.661743: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc69120c00 of size 83200\n\n    2018-12-26 17:02:58.661762: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc69135100 of size 5120512\n\n    2018-12-26 17:02:58.661785: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc69617300 of size 83200\n\n    2018-12-26 17:02:58.661806: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6962b800 of size 5120512\n\n    2018-12-26 17:02:58.661826: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc69b0da00 of size 83200\n\n    2018-12-26 17:02:58.661847: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc69b21f00 of size 5120512\n\n    2018-12-26 17:02:58.661868: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6a004100 of size 83200\n\n    2018-12-26 17:02:58.661888: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6a018600 of size 5120512\n\n    2018-12-26 17:02:58.661908: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6a4fa800 of size 83200\n\n    2018-12-26 17:02:58.661927: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6a50ed00 of size 5120512\n\n    2018-12-26 17:02:58.661948: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6a9f0f00 of size 83200\n\n    2018-12-26 17:02:58.661967: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6aa05400 of size 5120512\n\n    2018-12-26 17:02:58.661987: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6aee7600 of size 83200\n\n    2018-12-26 17:02:58.662006: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6aefbb00 of size 5120512\n\n    2018-12-26 17:02:58.662026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6b3ddd00 of size 83200\n\n    2018-12-26 17:02:58.662046: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6b3f2200 of size 5120512\n\n    2018-12-26 17:02:58.662069: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6b8d4400 of size 83200\n\n    2018-12-26 17:02:58.662089: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6b8e8900 of size 5120512\n\n    2018-12-26 17:02:58.662110: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6bdcab00 of size 83200\n\n    2018-12-26 17:02:58.662129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6bddf000 of size 5120512\n\n    2018-12-26 17:02:58.662149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6c2c1200 of size 83200\n\n    2018-12-26 17:02:58.662169: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6c2d5700 of size 5120512\n\n    2018-12-26 17:02:58.662189: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6c7b7900 of size 83200\n\n    2018-12-26 17:02:58.662209: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6c7cbe00 of size 5120512\n\n    2018-12-26 17:02:58.662229: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6ccae000 of size 83200\n\n    2018-12-26 17:02:58.662249: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6ccc2500 of size 5120512\n\n    2018-12-26 17:02:58.662269: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6d1a4700 of size 83200\n\n    2018-12-26 17:02:58.662289: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6d1b8c00 of size 5120512\n\n    2018-12-26 17:02:58.662308: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6d69ae00 of size 83200\n\n    2018-12-26 17:02:58.662328: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6d6af300 of size 5120512\n\n    2018-12-26 17:02:58.662348: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6db91500 of size 83200\n\n    2018-12-26 17:02:58.662367: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6dba5a00 of size 5120512\n\n    2018-12-26 17:02:58.662389: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6e087c00 of size 83200\n\n    2018-12-26 17:02:58.662409: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6e09c100 of size 5120512\n\n    2018-12-26 17:02:58.662429: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6e57e300 of size 83200\n\n    2018-12-26 17:02:58.662448: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6e592800 of size 5120512\n\n    2018-12-26 17:02:58.662468: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6ea74a00 of size 83200\n\n    2018-12-26 17:02:58.662488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6ea88f00 of size 5120512\n\n    2018-12-26 17:02:58.662508: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6ef6b100 of size 83200\n\n    2018-12-26 17:02:58.662527: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6ef7f600 of size 5120512\n\n    2018-12-26 17:02:58.662547: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6f461800 of size 83200\n\n    2018-12-26 17:02:58.662576: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6f475d00 of size 5120512\n\n    2018-12-26 17:02:58.662597: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6f957f00 of size 83200\n\n    2018-12-26 17:02:58.662624: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6f96c400 of size 5120512\n\n    2018-12-26 17:02:58.662644: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc6fe4e600 of size 83200\n\n    2018-12-26 17:02:58.662673: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc6fe62b00 of size 5120512\n\n    2018-12-26 17:02:58.662690: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc70344d00 of size 83200\n\n    2018-12-26 17:02:58.662710: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc70359200 of size 5120512\n\n    2018-12-26 17:02:58.662727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7083b400 of size 83200\n\n    2018-12-26 17:02:58.662741: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7084f900 of size 5120512\n\n    2018-12-26 17:02:58.662755: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc70d31b00 of size 83200\n\n    2018-12-26 17:02:58.662768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc70d46000 of size 5120512\n\n    2018-12-26 17:02:58.662781: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc71228200 of size 83200\n\n    2018-12-26 17:02:58.662795: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7123c700 of size 5120512\n\n    2018-12-26 17:02:58.662809: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7171e900 of size 83200\n\n    2018-12-26 17:02:58.662823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc71732e00 of size 5120512\n\n    2018-12-26 17:02:58.662836: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc71c15000 of size 83200\n\n    2018-12-26 17:02:58.662849: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc71c29500 of size 5120512\n\n    2018-12-26 17:02:58.662863: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7210b700 of size 83200\n\n    2018-12-26 17:02:58.662876: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7211fc00 of size 5120512\n\n    2018-12-26 17:02:58.662889: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc72601e00 of size 83200\n\n    2018-12-26 17:02:58.662902: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc72616300 of size 5120512\n\n    2018-12-26 17:02:58.662916: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc72af8500 of size 83200\n\n    2018-12-26 17:02:58.662929: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc72b0ca00 of size 5120512\n\n    2018-12-26 17:02:58.662943: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc72feec00 of size 83200\n\n    2018-12-26 17:02:58.662956: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc73003100 of size 5120512\n\n    2018-12-26 17:02:58.678482: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc734e5300 of size 83200\n\n    2018-12-26 17:02:58.678547: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc734f9800 of size 5120512\n\n    2018-12-26 17:02:58.678569: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc739dba00 of size 83200\n\n    2018-12-26 17:02:58.678590: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc739eff00 of size 5120512\n\n    2018-12-26 17:02:58.678615: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc73ed2100 of size 83200\n\n    2018-12-26 17:02:58.678641: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc73ee6600 of size 5120512\n\n    2018-12-26 17:02:58.678668: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc743c8800 of size 83200\n\n    2018-12-26 17:02:58.678691: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc743dcd00 of size 5120512\n\n    2018-12-26 17:02:58.678726: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc748bef00 of size 83200\n\n    2018-12-26 17:02:58.678747: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc748d3400 of size 5120512\n\n    2018-12-26 17:02:58.678767: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc74db5600 of size 83200\n\n    2018-12-26 17:02:58.678788: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc74dc9b00 of size 5120512\n\n    2018-12-26 17:02:58.678809: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc752abd00 of size 83200\n\n    2018-12-26 17:02:58.678831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc752c0200 of size 5120512\n\n    2018-12-26 17:02:58.678857: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc757a2400 of size 83200\n\n    2018-12-26 17:02:58.678882: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc757b6900 of size 5120512\n\n    2018-12-26 17:02:58.678905: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc75c98b00 of size 83200\n\n    2018-12-26 17:02:58.678925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc75cad000 of size 5120512\n\n    2018-12-26 17:02:58.678946: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7618f200 of size 83200\n\n    2018-12-26 17:02:58.678966: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc761a3700 of size 5120512\n\n    2018-12-26 17:02:58.678986: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc76685900 of size 83200\n\n    2018-12-26 17:02:58.679007: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc76699e00 of size 5120512\n\n    2018-12-26 17:02:58.679026: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc76b7c000 of size 83200\n\n    2018-12-26 17:02:58.679049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc76b90500 of size 5120512\n\n    2018-12-26 17:02:58.679073: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc77072700 of size 83200\n\n    2018-12-26 17:02:58.679098: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc77086c00 of size 5120512\n\n    2018-12-26 17:02:58.679121: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc77568e00 of size 83200\n\n    2018-12-26 17:02:58.679142: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7757d300 of size 5120512\n\n    2018-12-26 17:02:58.679162: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc77a5f500 of size 83200\n\n    2018-12-26 17:02:58.679182: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc77a73a00 of size 5120512\n\n    2018-12-26 17:02:58.679200: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc77f55c00 of size 83200\n\n    2018-12-26 17:02:58.679219: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc77f6a100 of size 5120512\n\n    2018-12-26 17:02:58.679238: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7844c300 of size 83200\n\n    2018-12-26 17:02:58.679259: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc78460800 of size 5120512\n\n    2018-12-26 17:02:58.679284: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc78942a00 of size 83200\n\n    2018-12-26 17:02:58.679309: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc78956f00 of size 5120512\n\n    2018-12-26 17:02:58.679333: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc78e39100 of size 83200\n\n    2018-12-26 17:02:58.679358: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc78e4d600 of size 5120512\n\n    2018-12-26 17:02:58.679378: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7932f800 of size 83200\n\n    2018-12-26 17:02:58.679397: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc79343d00 of size 5120512\n\n    2018-12-26 17:02:58.679417: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc79825f00 of size 83200\n\n    2018-12-26 17:02:58.679437: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7983a400 of size 5120512\n\n    2018-12-26 17:02:58.679457: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc79d1c600 of size 83200\n\n    2018-12-26 17:02:58.679479: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc79d30b00 of size 5120512\n\n    2018-12-26 17:02:58.679505: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7a212d00 of size 83200\n\n    2018-12-26 17:02:58.679530: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7a227200 of size 5120512\n\n    2018-12-26 17:02:58.679553: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7a709400 of size 83200\n\n    2018-12-26 17:02:58.679573: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7a71d900 of size 5120512\n\n    2018-12-26 17:02:58.679592: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7abffb00 of size 83200\n\n    2018-12-26 17:02:58.679612: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7ac14000 of size 5120512\n\n    2018-12-26 17:02:58.679631: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7b0f6200 of size 83200\n\n    2018-12-26 17:02:58.679650: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7b10a700 of size 5120512\n\n    2018-12-26 17:02:58.679670: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7b5ec900 of size 83200\n\n    2018-12-26 17:02:58.679691: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7b600e00 of size 5120512\n\n    2018-12-26 17:02:58.679727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7bae3000 of size 83200\n\n    2018-12-26 17:02:58.679752: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7baf7500 of size 5120512\n\n    2018-12-26 17:02:58.679775: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7bfd9700 of size 83200\n\n    2018-12-26 17:02:58.679795: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7bfedc00 of size 5120512\n\n    2018-12-26 17:02:58.679815: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7c4cfe00 of size 83200\n\n    2018-12-26 17:02:58.679835: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7c4e4300 of size 5120512\n\n    2018-12-26 17:02:58.679855: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7c9c6500 of size 83200\n\n    2018-12-26 17:02:58.679875: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7c9daa00 of size 5120512\n\n    2018-12-26 17:02:58.679895: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7cebcc00 of size 83200\n\n    2018-12-26 17:02:58.679920: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7ced1100 of size 5120512\n\n    2018-12-26 17:02:58.679944: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7d3b3300 of size 83200\n\n    2018-12-26 17:02:58.679997: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7d3c7800 of size 5120512\n\n    2018-12-26 17:02:58.680022: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7d8a9a00 of size 83200\n\n    2018-12-26 17:02:58.680041: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7d8bdf00 of size 5120512\n\n    2018-12-26 17:02:58.680081: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7dda0100 of size 83200\n\n    2018-12-26 17:02:58.680104: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7ddb4600 of size 5120512\n\n    2018-12-26 17:02:58.680124: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7e296800 of size 83200\n\n    2018-12-26 17:02:58.680150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7e2aad00 of size 5120512\n\n    2018-12-26 17:02:58.680176: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7e78cf00 of size 83200\n\n    2018-12-26 17:02:58.680200: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7e7a1400 of size 5120512\n\n    2018-12-26 17:02:58.680221: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7ec83600 of size 83200\n\n    2018-12-26 17:02:58.680240: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7ec97b00 of size 5120512\n\n    2018-12-26 17:02:58.680260: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7f179d00 of size 83200\n\n    2018-12-26 17:02:58.680279: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7f18e200 of size 5120512\n\n    2018-12-26 17:02:58.680300: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7f670400 of size 83200\n\n    2018-12-26 17:02:58.680320: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7f684900 of size 5120512\n\n    2018-12-26 17:02:58.680340: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc7fb66b00 of size 83200\n\n    2018-12-26 17:02:58.680364: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc7fb7b000 of size 5120512\n\n    2018-12-26 17:02:58.680388: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8005d200 of size 83200\n\n    2018-12-26 17:02:58.680411: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc80071700 of size 5120512\n\n    2018-12-26 17:02:58.680432: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc80553900 of size 83200\n\n    2018-12-26 17:02:58.680451: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc80567e00 of size 5120512\n\n    2018-12-26 17:02:58.680472: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc80a4a000 of size 83200\n\n    2018-12-26 17:02:58.680491: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc80a5e500 of size 5120512\n\n    2018-12-26 17:02:58.680511: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc80f40700 of size 83200\n\n    2018-12-26 17:02:58.680531: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc80f54c00 of size 5120512\n\n    2018-12-26 17:02:58.680550: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc81436e00 of size 83200\n\n    2018-12-26 17:02:58.680573: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8144b300 of size 5120512\n\n    2018-12-26 17:02:58.680599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8192d500 of size 83200\n\n    2018-12-26 17:02:58.680624: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc81941a00 of size 5120512\n\n    2018-12-26 17:02:58.680646: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc81e23c00 of size 83200\n\n    2018-12-26 17:02:58.680666: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc81e38100 of size 5120512\n\n    2018-12-26 17:02:58.680686: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8231a300 of size 83200\n\n    2018-12-26 17:02:58.680716: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8232e800 of size 5120512\n\n    2018-12-26 17:02:58.680739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc82810a00 of size 83200\n\n    2018-12-26 17:02:58.680759: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc82824f00 of size 5120512\n\n    2018-12-26 17:02:58.680781: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc82d07100 of size 83200\n\n    2018-12-26 17:02:58.680806: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc82d1b600 of size 5120512\n\n    2018-12-26 17:02:58.680831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc831fd800 of size 83200\n\n    2018-12-26 17:02:58.680854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc83211d00 of size 5120512\n\n    2018-12-26 17:02:58.680875: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc836f3f00 of size 83200\n\n    2018-12-26 17:02:58.680894: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc83708400 of size 5120512\n\n    2018-12-26 17:02:58.680915: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc83bea600 of size 83200\n\n    2018-12-26 17:02:58.680936: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc83bfeb00 of size 5120512\n\n    2018-12-26 17:02:58.680956: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc840e0d00 of size 83200\n\n    2018-12-26 17:02:58.680976: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc840f5200 of size 5120512\n\n    2018-12-26 17:02:58.680997: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc845d7400 of size 83200\n\n    2018-12-26 17:02:58.681023: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc845eb900 of size 5120512\n\n    2018-12-26 17:02:58.681049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc84acdb00 of size 83200\n\n    2018-12-26 17:02:58.681074: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc84ae2000 of size 5120512\n\n    2018-12-26 17:02:58.681092: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc84fc4200 of size 83200\n\n    2018-12-26 17:02:58.681111: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc84fd8700 of size 5120512\n\n    2018-12-26 17:02:58.681131: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc854ba900 of size 83200\n\n    2018-12-26 17:02:58.681151: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc854cee00 of size 5120512\n\n    2018-12-26 17:02:58.681172: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc859b1000 of size 83200\n\n    2018-12-26 17:02:58.681190: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc859c5500 of size 5120512\n\n    2018-12-26 17:02:58.681213: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc85ea7700 of size 83200\n\n    2018-12-26 17:02:58.681238: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc85ebbc00 of size 5120512\n\n    2018-12-26 17:02:58.681265: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8639de00 of size 83200\n\n    2018-12-26 17:02:58.681287: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc863b2300 of size 5120512\n\n    2018-12-26 17:02:58.681307: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc86894500 of size 83200\n\n    2018-12-26 17:02:58.681327: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc868a8a00 of size 5120512\n\n    2018-12-26 17:02:58.681347: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc86d8ac00 of size 83200\n\n    2018-12-26 17:02:58.681368: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc86d9f100 of size 5120512\n\n    2018-12-26 17:02:58.681389: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc87281300 of size 83200\n\n    2018-12-26 17:02:58.681404: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc87295800 of size 5120512\n\n    2018-12-26 17:02:58.681424: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc87777a00 of size 83200\n\n    2018-12-26 17:02:58.681450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8778bf00 of size 5120512\n\n    2018-12-26 17:02:58.681476: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc87c6e100 of size 83200\n\n    2018-12-26 17:02:58.681500: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc87c82600 of size 5120512\n\n    2018-12-26 17:02:58.681520: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc88164800 of size 83200\n\n    2018-12-26 17:02:58.681541: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc88178d00 of size 5120512\n\n    2018-12-26 17:02:58.681561: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8865af00 of size 83200\n\n    2018-12-26 17:02:58.681581: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8866f400 of size 5120512\n\n    2018-12-26 17:02:58.681601: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc88b51600 of size 83200\n\n    2018-12-26 17:02:58.681621: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc88b65b00 of size 5120512\n\n    2018-12-26 17:02:58.681643: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc89047d00 of size 83200\n\n    2018-12-26 17:02:58.681668: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8905c200 of size 5120512\n\n    2018-12-26 17:02:58.681694: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8953e400 of size 83200\n\n    2018-12-26 17:02:58.681726: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc89552900 of size 5120512\n\n    2018-12-26 17:02:58.681747: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc89a34b00 of size 83200\n\n    2018-12-26 17:02:58.681766: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc89a49000 of size 5120512\n\n    2018-12-26 17:02:58.681785: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc89f2b200 of size 83200\n\n    2018-12-26 17:02:58.681802: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc89f3f700 of size 5120512\n\n    2018-12-26 17:02:58.681818: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8a421900 of size 83200\n\n    2018-12-26 17:02:58.681838: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8a435e00 of size 5120512\n\n    2018-12-26 17:02:58.681862: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8a918000 of size 83200\n\n    2018-12-26 17:02:58.681887: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8a92c500 of size 5120512\n\n    2018-12-26 17:02:58.681912: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8ae0e700 of size 83200\n\n    2018-12-26 17:02:58.681934: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8ae22c00 of size 5120512\n\n    2018-12-26 17:02:58.681954: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8b304e00 of size 83200\n\n    2018-12-26 17:02:58.681972: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8b319300 of size 5120512\n\n    2018-12-26 17:02:58.681991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8b7fb500 of size 83200\n\n    2018-12-26 17:02:58.682011: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8b80fa00 of size 5120512\n\n    2018-12-26 17:02:58.682031: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8bcf1c00 of size 83200\n\n    2018-12-26 17:02:58.682051: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8bd06100 of size 5120512\n\n    2018-12-26 17:02:58.682073: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8c1e8300 of size 83200\n\n    2018-12-26 17:02:58.682097: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8c1fc800 of size 5120512\n\n    2018-12-26 17:02:58.682121: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8c6dea00 of size 83200\n\n    2018-12-26 17:02:58.682144: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8c6f2f00 of size 5120512\n\n    2018-12-26 17:02:58.682164: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8cbd5100 of size 83200\n\n    2018-12-26 17:02:58.682180: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8cbe9600 of size 5120512\n\n    2018-12-26 17:02:58.682199: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8d0cb800 of size 83200\n\n    2018-12-26 17:02:58.682218: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8d0dfd00 of size 5120512\n\n    2018-12-26 17:02:58.682238: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8d5c1f00 of size 83200\n\n    2018-12-26 17:02:58.682258: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8d5d6400 of size 5120512\n\n    2018-12-26 17:02:58.682279: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8dab8600 of size 83200\n\n    2018-12-26 17:02:58.682306: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8daccb00 of size 5120512\n\n    2018-12-26 17:02:58.682331: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8dfaed00 of size 83200\n\n    2018-12-26 17:02:58.682354: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8dfc3200 of size 5120512\n\n    2018-12-26 17:02:58.682375: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8e4a5400 of size 83200\n\n    2018-12-26 17:02:58.682395: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8e4b9900 of size 5120512\n\n    2018-12-26 17:02:58.682414: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8e99bb00 of size 83200\n\n    2018-12-26 17:02:58.682433: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8e9b0000 of size 5120512\n\n    2018-12-26 17:02:58.682454: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8ee92200 of size 83200\n\n    2018-12-26 17:02:58.682474: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8eea6700 of size 5120512\n\n    2018-12-26 17:02:58.682497: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8f388900 of size 83200\n\n    2018-12-26 17:02:58.682523: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8f39ce00 of size 5120512\n\n    2018-12-26 17:02:58.682544: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8f87f000 of size 83200\n\n    2018-12-26 17:02:58.682565: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8f893500 of size 5120512\n\n    2018-12-26 17:02:58.682586: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc8fd75700 of size 83200\n\n    2018-12-26 17:02:58.682607: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc8fd89c00 of size 5120512\n\n    2018-12-26 17:02:58.682627: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9026be00 of size 83200\n\n    2018-12-26 17:02:58.682647: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc90280300 of size 5120512\n\n    2018-12-26 17:02:58.682667: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc90762500 of size 83200\n\n    2018-12-26 17:02:58.682687: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc90776a00 of size 5120512\n\n    2018-12-26 17:02:58.682715: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc90c58c00 of size 83200\n\n    2018-12-26 17:02:58.682743: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc90c6d100 of size 5120512\n\n    2018-12-26 17:02:58.682768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9114f300 of size 83200\n\n    2018-12-26 17:02:58.682791: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc91163800 of size 5120512\n\n    2018-12-26 17:02:58.682811: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc91645a00 of size 83200\n\n    2018-12-26 17:02:58.682831: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc91659f00 of size 5120512\n\n    2018-12-26 17:02:58.682850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc91b3c100 of size 83200\n\n    2018-12-26 17:02:58.682870: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc91b50600 of size 5120512\n\n    2018-12-26 17:02:58.682890: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc92032800 of size 83200\n\n    2018-12-26 17:02:58.682911: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc92046d00 of size 5120512\n\n    2018-12-26 17:02:58.682933: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc92528f00 of size 83200\n\n    2018-12-26 17:02:58.682958: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9253d400 of size 5120512\n\n    2018-12-26 17:02:58.682982: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc92a1f600 of size 83200\n\n    2018-12-26 17:02:58.683005: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc92a33b00 of size 5120512\n\n    2018-12-26 17:02:58.683025: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc92f15d00 of size 83200\n\n    2018-12-26 17:02:58.683046: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc92f2a200 of size 5120512\n\n    2018-12-26 17:02:58.683067: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9340c400 of size 83200\n\n    2018-12-26 17:02:58.683086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc93420900 of size 5120512\n\n    2018-12-26 17:02:58.683106: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc93902b00 of size 83200\n\n    2018-12-26 17:02:58.683126: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc93917000 of size 5120512\n\n    2018-12-26 17:02:58.683149: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc93df9200 of size 83200\n\n    2018-12-26 17:02:58.683174: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc93e0d700 of size 5120512\n\n    2018-12-26 17:02:58.683199: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc942ef900 of size 83200\n\n    2018-12-26 17:02:58.683221: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc94303e00 of size 5120512\n\n    2018-12-26 17:02:58.683241: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc947e6000 of size 83200\n\n    2018-12-26 17:02:58.683261: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc947fa500 of size 5120512\n\n    2018-12-26 17:02:58.683281: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc94cdc700 of size 83200\n\n    2018-12-26 17:02:58.683301: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc94cf0c00 of size 5120512\n\n    2018-12-26 17:02:58.683320: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc951d2e00 of size 83200\n\n    2018-12-26 17:02:58.683339: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc951e7300 of size 5120512\n\n    2018-12-26 17:02:58.683361: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc956c9500 of size 83200\n\n    2018-12-26 17:02:58.683385: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc956dda00 of size 5120512\n\n    2018-12-26 17:02:58.683408: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc95bbfc00 of size 83200\n\n    2018-12-26 17:02:58.683431: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc95bd4100 of size 5120512\n\n    2018-12-26 17:02:58.683450: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc960b6300 of size 83200\n\n    2018-12-26 17:02:58.683469: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc960ca800 of size 5120512\n\n    2018-12-26 17:02:58.683488: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc965aca00 of size 83200\n\n    2018-12-26 17:02:58.683507: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc965c0f00 of size 5120512\n\n    2018-12-26 17:02:58.683526: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc96aa3100 of size 83200\n\n    2018-12-26 17:02:58.683545: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc96ab7600 of size 5120512\n\n    2018-12-26 17:02:58.683564: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc96f99800 of size 83200\n\n    2018-12-26 17:02:58.683589: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc96fadd00 of size 5120512\n\n    2018-12-26 17:02:58.683613: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9748ff00 of size 83200\n\n    2018-12-26 17:02:58.683637: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc974a4400 of size 5120512\n\n    2018-12-26 17:02:58.683657: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc97986600 of size 83200\n\n    2018-12-26 17:02:58.683676: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9799ab00 of size 5120512\n\n    2018-12-26 17:02:58.683696: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc97e7cd00 of size 83200\n\n    2018-12-26 17:02:58.683723: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc97e91200 of size 5120512\n\n    2018-12-26 17:02:58.683743: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc98373400 of size 83200\n\n    2018-12-26 17:02:58.683762: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc98387900 of size 5120512\n\n    2018-12-26 17:02:58.683783: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc98869b00 of size 83200\n\n    2018-12-26 17:02:58.683808: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9887e000 of size 5120512\n\n    2018-12-26 17:02:58.683832: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc98d60200 of size 83200\n\n    2018-12-26 17:02:58.683855: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc98d74700 of size 5120512\n\n    2018-12-26 17:02:58.683875: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc99256900 of size 83200\n\n    2018-12-26 17:02:58.683894: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9926ae00 of size 5120512\n\n    2018-12-26 17:02:58.683913: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9974d000 of size 83200\n\n    2018-12-26 17:02:58.683932: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc99761500 of size 5120512\n\n    2018-12-26 17:02:58.683951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc99c43700 of size 83200\n\n    2018-12-26 17:02:58.684001: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc99c57c00 of size 5120512\n\n    2018-12-26 17:02:58.684023: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9a139e00 of size 83200\n\n    2018-12-26 17:02:58.684045: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9a14e300 of size 5120512\n\n    2018-12-26 17:02:58.684073: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9a630500 of size 83200\n\n    2018-12-26 17:02:58.684100: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9a644a00 of size 5120512\n\n    2018-12-26 17:02:58.684125: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9ab26c00 of size 83200\n\n    2018-12-26 17:02:58.684147: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9ab3b100 of size 5120512\n\n    2018-12-26 17:02:58.684167: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9b01d300 of size 83200\n\n    2018-12-26 17:02:58.684187: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9b031800 of size 5120512\n\n    2018-12-26 17:02:58.684207: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9b513a00 of size 83200\n\n    2018-12-26 17:02:58.684227: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9b527f00 of size 5120512\n\n    2018-12-26 17:02:58.684247: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9ba0a100 of size 83200\n\n    2018-12-26 17:02:58.684271: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9ba1e600 of size 5120512\n\n    2018-12-26 17:02:58.684299: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9bf00800 of size 83200\n\n    2018-12-26 17:02:58.684324: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9bf14d00 of size 5120512\n\n    2018-12-26 17:02:58.684348: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9c3f6f00 of size 83200\n\n    2018-12-26 17:02:58.684369: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9c40b400 of size 5120512\n\n    2018-12-26 17:02:58.684389: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9c8ed600 of size 83200\n\n    2018-12-26 17:02:58.684409: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9c901b00 of size 5120512\n\n    2018-12-26 17:02:58.684430: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9cde3d00 of size 83200\n\n    2018-12-26 17:02:58.684449: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9cdf8200 of size 5120512\n\n    2018-12-26 17:02:58.684468: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9d2da400 of size 83200\n\n    2018-12-26 17:02:58.684494: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9d2ee900 of size 5120512\n\n    2018-12-26 17:02:58.684521: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9d7d0b00 of size 83200\n\n    2018-12-26 17:02:58.684549: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9d7e5000 of size 5120512\n\n    2018-12-26 17:02:58.684573: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9dcc7200 of size 83200\n\n    2018-12-26 17:02:58.684594: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9dcdb700 of size 5120512\n\n    2018-12-26 17:02:58.684614: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9e1bd900 of size 83200\n\n    2018-12-26 17:02:58.684634: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9e1d1e00 of size 5120512\n\n    2018-12-26 17:02:58.684654: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9e6b4000 of size 83200\n\n    2018-12-26 17:02:58.684673: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9e6c8500 of size 5120512\n\n    2018-12-26 17:02:58.684693: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9ebaa700 of size 83200\n\n    2018-12-26 17:02:58.684732: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9ebbec00 of size 5120512\n\n    2018-12-26 17:02:58.684761: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9f0a0e00 of size 83200\n\n    2018-12-26 17:02:58.684785: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9f0b5300 of size 5120512\n\n    2018-12-26 17:02:58.684807: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9f597500 of size 83200\n\n    2018-12-26 17:02:58.684827: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9f5aba00 of size 5120512\n\n    2018-12-26 17:02:58.684845: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9fa8dc00 of size 83200\n\n    2018-12-26 17:02:58.684861: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9faa2100 of size 5120512\n\n    2018-12-26 17:02:58.684878: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xc9ff84300 of size 83200\n\n    2018-12-26 17:02:58.684902: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xc9ff98800 of size 5120512\n\n    2018-12-26 17:02:58.684924: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca047aa00 of size 83200\n\n    2018-12-26 17:02:58.684950: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca048ef00 of size 5120512\n\n    2018-12-26 17:02:58.684977: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca0971100 of size 83200\n\n    2018-12-26 17:02:58.685003: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca0985600 of size 5120512\n\n    2018-12-26 17:02:58.685030: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca0e67800 of size 83200\n\n    2018-12-26 17:02:58.685053: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca0e7bd00 of size 5120512\n\n    2018-12-26 17:02:58.685075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca135df00 of size 83200\n\n    2018-12-26 17:02:58.685095: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca1372400 of size 5120512\n\n    2018-12-26 17:02:58.685115: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca1854600 of size 83200\n\n    2018-12-26 17:02:58.685135: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca1868b00 of size 5120512\n\n    2018-12-26 17:02:58.685155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca1d4ad00 of size 83200\n\n    2018-12-26 17:02:58.685173: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca1d5f200 of size 5120512\n\n    2018-12-26 17:02:58.685225: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca2241400 of size 83200\n\n    2018-12-26 17:02:58.685250: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca2255900 of size 5120512\n\n    2018-12-26 17:02:58.685273: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca2737b00 of size 83200\n\n    2018-12-26 17:02:58.685293: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca274c000 of size 5120512\n\n    2018-12-26 17:02:58.685313: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca2c2e200 of size 83200\n\n    2018-12-26 17:02:58.685333: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca2c42700 of size 5120512\n\n    2018-12-26 17:02:58.685353: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca3124900 of size 83200\n\n    2018-12-26 17:02:58.685373: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca3138e00 of size 5120512\n\n    2018-12-26 17:02:58.685393: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca361b000 of size 83200\n\n    2018-12-26 17:02:58.685420: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca362f500 of size 5120512\n\n    2018-12-26 17:02:58.685447: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca3b11700 of size 83200\n\n    2018-12-26 17:02:58.685475: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca3b25c00 of size 5120512\n\n    2018-12-26 17:02:58.685497: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca4007e00 of size 83200\n\n    2018-12-26 17:02:58.685517: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca401c300 of size 5120512\n\n    2018-12-26 17:02:58.685537: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca44fe500 of size 83200\n\n    2018-12-26 17:02:58.685558: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca4512a00 of size 5120512\n\n    2018-12-26 17:02:58.685578: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca49f4c00 of size 83200\n\n    2018-12-26 17:02:58.685598: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca4a09100 of size 5120512\n\n    2018-12-26 17:02:58.685618: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca4eeb300 of size 83200\n\n    2018-12-26 17:02:58.685644: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca4eff800 of size 5120512\n\n    2018-12-26 17:02:58.685671: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca53e1a00 of size 83200\n\n    2018-12-26 17:02:58.685695: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca53f5f00 of size 5120512\n\n    2018-12-26 17:02:58.685726: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca58d8100 of size 83200\n\n    2018-12-26 17:02:58.685749: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca58ec600 of size 5120512\n\n    2018-12-26 17:02:58.685769: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca5dce800 of size 83200\n\n    2018-12-26 17:02:58.685789: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca5de2d00 of size 5120512\n\n    2018-12-26 17:02:58.685809: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca62c4f00 of size 83200\n\n    2018-12-26 17:02:58.685829: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca62d9400 of size 5120512\n\n    2018-12-26 17:02:58.685854: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca67bb600 of size 83200\n\n    2018-12-26 17:02:58.685881: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca67cfb00 of size 5120512\n\n    2018-12-26 17:02:58.685908: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca6cb1d00 of size 83200\n\n    2018-12-26 17:02:58.685951: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca6cc6200 of size 5120512\n\n    2018-12-26 17:02:58.685988: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca71a8400 of size 83200\n\n    2018-12-26 17:02:58.686010: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca71bc900 of size 5120512\n\n    2018-12-26 17:02:58.686030: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca769eb00 of size 83200\n\n    2018-12-26 17:02:58.686050: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca76b3000 of size 5120512\n\n    2018-12-26 17:02:58.686075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca7b95200 of size 83200\n\n    2018-12-26 17:02:58.686102: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca7ba9700 of size 5120512\n\n    2018-12-26 17:02:58.686128: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca808b900 of size 83200\n\n    2018-12-26 17:02:58.686153: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca809fe00 of size 5120512\n\n    2018-12-26 17:02:58.686174: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca8582000 of size 83200\n\n    2018-12-26 17:02:58.686193: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca8596500 of size 5120512\n\n    2018-12-26 17:02:58.686213: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca8a78700 of size 83200\n\n    2018-12-26 17:02:58.686233: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca8a8cc00 of size 5120512\n\n    2018-12-26 17:02:58.686253: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca8f6ee00 of size 83200\n\n    2018-12-26 17:02:58.686273: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca8f83300 of size 5120512\n\n    2018-12-26 17:02:58.686298: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca9465500 of size 83200\n\n    2018-12-26 17:02:58.686326: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca9479a00 of size 5120512\n\n    2018-12-26 17:02:58.686352: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xca995bc00 of size 83200\n\n    2018-12-26 17:02:58.686375: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca9970100 of size 5120512\n\n    2018-12-26 17:02:58.686398: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0xca9e52300 of size 107827456\n\n    2018-12-26 17:02:58.686419: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0xcb0527400 of size 4905176064\n\n    2018-12-26 17:02:58.686438: I tensorflow/core/common_runtime/bfc_allocator.cc:651]      Summary of in-use Chunks by size: \n\n    2018-12-26 17:02:58.686471: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 89 Chunks of size 256 totalling 22.2KiB\n\n    2018-12-26 17:02:58.686496: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 3 Chunks of size 1280 totalling 3.8KiB\n\n    2018-12-26 17:02:58.686524: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 32 Chunks of size 2048 totalling 64.0KiB\n\n    2018-12-26 17:02:58.686559: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 16384 totalling 16.0KiB\n\n    2018-12-26 17:02:58.686588: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 3 Chunks of size 65536 totalling 192.0KiB\n\n    2018-12-26 17:02:58.686612: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 131072 totalling 512.0KiB\n\n    2018-12-26 17:02:58.686635: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 12 Chunks of size 524288 totalling 6.00MiB\n\n    2018-12-26 17:02:58.686656: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 786432 totalling 3.00MiB\n\n    2018-12-26 17:02:58.686678: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1298 Chunks of size 5120512 totalling 6.19GiB\n\n    2018-12-26 17:02:58.686708: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 10633216 totalling 10.14MiB\n\n    2018-12-26 17:02:58.686740: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 11485184 totalling 10.95MiB\n\n    2018-12-26 17:02:58.686771: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 9 Chunks of size 20481536 totalling 175.79MiB\n\n    2018-12-26 17:02:58.686804: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 107827456 totalling 102.83MiB\n\n    2018-12-26 17:02:58.686829: I tensorflow/core/common_runtime/bfc_allocator.cc:658] Sum Total of in-use chunks: 6.49GiB\n\n    2018-12-26 17:02:58.686860: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Stats: \n\n    Limit:                 11984273408\n\n    InUse:                  6970970880\n\n    MaxInUse:              10953035776\n\n    NumAllocs:                64080678\n\n    MaxAllocSize:           5335440128\n\n    \n\n    2018-12-26 17:02:58.687138: W tensorflow/core/common_runtime/bfc_allocator.cc:275] ************************************************************________________________________________\n\n    2018-12-26 17:02:58.687223: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_array_ops.cc:674 : Resource exhausted: OOM when allocating tensor with shape[1298,32,40003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\n    Traceback (most recent call last):\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\n        return fn(*args)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\n        options, feed_dict, fetch_list, target_list, run_metadata)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\n        run_metadata)\n    tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1298,32,40003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n    \t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}} = TensorArrayGatherV3[dtype=DT_FLOAT, element_shape=[?,40003], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dynamic_seq2seq/decoder/decoder/TensorArray, dynamic_seq2seq/decoder/decoder/TensorArrayStack/range, dynamic_seq2seq/decoder/decoder/while/Exit_2)]]\n    Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n    \n    \t [[{{node dynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/All/_223}} = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_700_dynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/All\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/decoder/attention/assert_equal/Assert/Assert/data_0/_13)]]\n    Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n    \n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n        \"__main__\", mod_spec)\n      File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n        exec(code, run_globals)\n      File \"/notebooks/seq2seq/nmt/nmt.py\", line 703, in <module>\n        tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n        _sys.exit(main(argv))\n      File \"/notebooks/seq2seq/nmt/nmt.py\", line 696, in main\n        run_main(FLAGS, default_hparams, train_fn, inference_fn)\n      File \"/notebooks/seq2seq/nmt/nmt.py\", line 689, in run_main\n        train_fn(hparams, target_session=target_session)\n      File \"/notebooks/seq2seq/nmt/train.py\", line 592, in train\n        hparams, summary_writer)\n      File \"/notebooks/seq2seq/nmt/train.py\", line 176, in run_external_eval\n        avg_ckpts=avg_ckpts)\n      File \"/notebooks/seq2seq/nmt/train.py\", line 730, in _external_eval\n        infer_mode=hparams.infer_mode)\n      File \"/notebooks/seq2seq/nmt/utils/nmt_utils.py\", line 60, in decode_and_evaluate\n        nmt_outputs, _ = model.decode(sess)\n      File \"/notebooks/seq2seq/nmt/model.py\", line 688, in decode\n        output_tuple = self.infer(sess)\n      File \"/notebooks/seq2seq/nmt/model.py\", line 676, in infer\n        return sess.run(output_tuple)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 887, in run\n        run_metadata_ptr)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\n        feed_dict_tensor, options, run_metadata)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\n        run_metadata)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\n        raise type(e)(node_def, op, message)\n    tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1298,32,40003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n    \t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}} = TensorArrayGatherV3[dtype=DT_FLOAT, element_shape=[?,40003], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dynamic_seq2seq/decoder/decoder/TensorArray, dynamic_seq2seq/decoder/decoder/TensorArrayStack/range, dynamic_seq2seq/decoder/decoder/while/Exit_2)]]\n    Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n    \n    \t [[{{node dynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/All/_223}} = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_700_dynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/All\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/decoder/attention/assert_equal/Assert/Assert/data_0/_13)]]\n    Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n    \n    \n    Caused by op 'dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3', defined at:\n      File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n        \"__main__\", mod_spec)\n      File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n        exec(code, run_globals)\n      File \"/notebooks/seq2seq/nmt/nmt.py\", line 703, in <module>\n        tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n        _sys.exit(main(argv))\n      File \"/notebooks/seq2seq/nmt/nmt.py\", line 696, in main\n        run_main(FLAGS, default_hparams, train_fn, inference_fn)\n      File \"/notebooks/seq2seq/nmt/nmt.py\", line 689, in run_main\n        train_fn(hparams, target_session=target_session)\n      File \"/notebooks/seq2seq/nmt/train.py\", line 467, in train\n        infer_model = model_helper.create_infer_model(model_creator, hparams, scope)\n      File \"/notebooks/seq2seq/nmt/model_helper.py\", line 228, in create_infer_model\n        extra_args=extra_args)\n      File \"/notebooks/seq2seq/nmt/attention_model.py\", line 64, in __init__\n        extra_args=extra_args)\n      File \"/notebooks/seq2seq/nmt/model.py\", line 95, in __init__\n        res = self.build_graph(hparams, scope=scope)\n      File \"/notebooks/seq2seq/nmt/model.py\", line 393, in build_graph\n        self._build_decoder(self.encoder_outputs, encoder_state, hparams))\n      File \"/notebooks/seq2seq/nmt/model.py\", line 583, in _build_decoder\n        scope=decoder_scope)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 328, in dynamic_decode\n        final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 347, in map_structure\n        structure[0], [func(*x) for x in entries])\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 347, in <listcomp>\n        structure[0], [func(*x) for x in entries])\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 328, in <lambda>\n        final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 856, in stack\n        return self._implementation.stack(name=name)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 289, in stack\n        return self.gather(math_ops.range(0, self.size()), name=name)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 303, in gather\n        element_shape=element_shape)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6019, in tensor_array_gather_v3\n        flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n        op_def=op_def)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n        return func(*args, **kwargs)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n        op_def=op_def)\n      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n        self._traceback = tf_stack.extract_stack()\n    \n    ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1298,32,40003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n    \t [[{{node dynamic_seq2seq/decoder/decoder/TensorArrayStack/TensorArrayGatherV3}} = TensorArrayGatherV3[dtype=DT_FLOAT, element_shape=[?,40003], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dynamic_seq2seq/decoder/decoder/TensorArray, dynamic_seq2seq/decoder/decoder/TensorArrayStack/range, dynamic_seq2seq/decoder/decoder/while/Exit_2)]]\n    Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n    \n    \t [[{{node dynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/All/_223}} = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_700_dynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/All\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdynamic_seq2seq/decoder/decoder/while/BasicDecoderStep/decoder/attention/assert_equal/Assert/Assert/data_0/_13)]]\n    Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n    \n    \n    \n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/2.generative_chatbot/3_seq2seq_chatbot_step_by_step/","content":"\n# seq2seq项目说明\n\n\n### 1.seq2seq（序列到序列模型）应用\n在**聊天机器人，机器翻译，自动文摘，智能问答**等众多自然语言处理任务中都可能用到seq2seq模型，google在著名的[neural machine translation](https://arxiv.org/abs/1609.08144)中也使用过这个结构的模型(当然，现在因为效率等原因，可能不少应用项目迁移到transformer结构下了)，google在tensorflow的官方案例中给了一个[手把手训练seq2seq神经翻译系统](https://github.com/tensorflow/nmt)的github项目，下面我们就官方这个项目讲解一下如何应用tensorflow训练seq2seq的应用，并尝试用这个代码实现一个聊天机器人的智能AI应用。\n\n参考资料:\n* [neural machine translation](https://arxiv.org/abs/1609.08144)\n* [手把手训练seq2seq神经翻译系统](https://github.com/tensorflow/nmt)\n\n## 0.说明\n\ngoogle的这个教程使用高版本tensorflow（TensorFlow 1.2+）的 seq2seq API完成，该API使seq2seq模型的构建过程干净、简单、易读，主要包括以下内容：\n\n- 使用 tf.data 中最新输入的管道对动态调整的输入序列进行预处理。\n- 使用批量填充和序列长度 bucketing，提高训练速度和推理速度。\n- 使用通用结构和训练时间表训练 seq2seq 模型，包括多种注意力机制和固定抽样。\n- 使用 in-graph 集束搜索在 seq2seq 模型中进行推理。\n- 优化 seq2seq 模型，以实现在多 GPU 设置中的模型训练。\n\n## 1.引言\n\n序列到序列（seq2seq）模型（Sutskever et al., 2014, Cho et al., 2014）在机器翻译、语音识别和文本摘要等任务上取得了巨大的成功。这里的教程内容致力于帮助读者全面掌握 seq2seq 模型，并且展示了如何从头开始构建一个强大的 seq2seq 模型。以下的讲解会注重神经机器翻译（NMT）任务，神经机器翻译是 seq2seq 模型很好的试验台，并且已经获得了广泛的成功。我们使用的代码是极其轻量、高质量、可投入生产并且结合了最新研究思路的实现。我们通过以下方式实现这一目标：\n\n1. 使用最新的解码器/attention wrapper API、TensorFlow 高版本数据迭代器。\n2. 结合了我们在构建循环型和 seq2seq 型模型的专业知识。\n3. 提供了可构建最好 NMT 模型的技巧，同时还复现了谷歌的 NMT（GNMT）系统。\n\n我们相信提供所有人都很容易复制的基准是非常重要的。因此，我们基于以下公开的数据集提供了全部的试验结果和预训练模型：\n\n1. 小规模数据集：TED 演讲的英语-越南语平行语料库（133K 个句子对），该数据集由 IWSLT Evaluation Campaign 提供。\n2. 大规模数据集：德语-英语平行语料库（4.5M 个句子对），该数据集由 WMT Evaluation Campaign 提供。\n\n我们首先需要了解用于 NMT 任务的 seq2seq 模型的基本知识，并需要理解如何构建和训练一个 vanilla NMT 模型。第二部分将更进一步详细地解释如何构建带注意力机制的强大神经机器翻译模型。然后我们会讨论构建更好神经机器翻译模型（翻译速度和质量）可能的技巧，例如 TensorFlow 最好的实践方法（batching, bucketing）、双向循环神经网络和集束搜索(beam search)等。\n\n## 1.基础\n\n**关于神经机器翻译**\n\n以词组为基础的传统翻译系统将源语言句子拆分成多个词块，然后进行词对词的翻译。这使得翻译输出结果流畅性大打折扣，远远不如人类译文。我们会通读整个源语言句子、了解句子含义，然后输出翻译结果。神经机器翻译（NMT）竟然可以模仿人类的翻译过程！\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/encdec.jpg)\n\n<center>图1. 编码器-解码器结构——神经机器翻译的通用方法实例。</center>\n\n具体来说，神经机器翻译系统首先使用编码器读取源语言句子，构建一个「上下文」向量(context vector)，即代表句义的数字化向量；然后使用解码器处理该内容，并输出翻译结果，如图1所示。这就是我们通常所说的编码器-解码器结构。神经机器翻译用这种方法解决以词组为基础的传统翻译系统遇到的翻译问题：神经机器翻译能够捕捉语言中的长距离依赖结构，如词性一致、句法结构等，然后输出流利度更高的翻译结果，正如谷歌神经机器翻译系统已经做到的那样。\n\nNMT 模型在具体的结构中会发生变化。对于序列数据而言，最好的选择是循环神经网络（RNN），这也被大多数 NMT 模型采用。通常情况下，编码器和解码器都可使用循环神经网络。但是，循环神经网络模型有多种选择：\n* （a）方向性（directionality），单向或双向；\n* （b）深度，单层或多层；\n* （c）类型，通常是 vanilla RNN、长短期记忆（Long Short-term Memory，LSTM），或门控循环单元（gated recurrent unit，GRU）。\n\n感兴趣的同学可打开该网址(https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ， 复习一下RNN 和 LSTM 的更多信息。\n\n这个教程中，我们将以单向的深度多层 RNN（deep multi-layer RNN）为例，它使用 LSTM 作为循环单元。模型实例如图 2 所示。我们在该实例中构建了一个模型，将源语言句子「I am a student」翻译成目标语言「Je suis étudiant」。该 NMT 模型包括两个循环神经网络：编码器 RNN，在不预测的情况下将输入的源语言单词进行编码；解码器，在预测下一个单词的条件下处理目标句子。\n\n若想参考更多信息，请查看论文 Luong（2016）（https://github.com/lmthang/thesis）。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/seq2seq.jpg)\n\n<center>图2. 神经机器翻译——一个深度循环结构实例</center>\n\n上图将源语言句子「I am a student」翻译成目标语言句子「Je suis étudiant」。此处，`「<s>」`代表解码过程开始，`「</s>」`代表解码过程结束。\n\n## 2.代码准备\n\n为了安装该教程，我们需要先安装 TensorFlow。建议使用新版本的TensorFlow。为了安装 TensorFlow，请按照以下安装指导：https://www.tensorflow.org/install/。\n\n在安装 TensorFlow 之后，我们需要运行以下命令拉取这个教程的源代码：\n\n```\ngit clone https://github.com/tensorflow/nmt/\n```\n\n\n```python\n!git clone https://github.com/tensorflow/nmt/\n```\n\n    Cloning into 'nmt'...\n    remote: Enumerating objects: 1247, done.\u001b[K\n    remote: Total 1247 (delta 0), reused 0 (delta 0), pack-reused 1247\u001b[K\n    Receiving objects: 100% (1247/1247), 1.23 MiB | 15.70 MiB/s, done.\n    Resolving deltas: 100% (890/890), done.\n    \n\n## 3.训练-构建我们第一个 NMT 系统\n\n我们首先需要了解构建一个 NMT 模型具体代码的核心，我们会在图 2 中更详细地讲解。我们后面会介绍数据准备和全部的代码，这一部分是指 model.py 文件。\n\n在网络的底层，编码器和解码器 RNN 接收到以下输入：首先是原句子，然后是从编码到解码模式的过渡边界符号`「<s>」`，最后是目标语句。对于训练来说，我们将为系统提供以下张量，它们是以时间为主（time-major）的格式，并包括了单词索引：\n\n- encoder_inputs [max_encoder_time, batch_size]：源输入词。\n- decoder_inputs [max_decoder_time, batch_size]：目标输入词。\n- decoder_outputs [max_decoder_time, batch_size]：目标输出词，这些是 decoder_inputs 按一个时间步向左移动，并且在右边有句子结束符。\n\n为了更高的效率，我们一次用多个句子（batch_size）进行训练。测试略有不同，我们会在后面讨论。\n\n### 3.1.嵌入(embedding)\n\n给定单词的分类属性，模型首先必须查找词来源和目标嵌入以检索相应的词表征。为了令该嵌入层能够运行，我们首先需要为每一种语言选定一个词汇表。通常，选定词汇表大小 V，那么频率最高的 V 个词将视为唯一的。而所有其他的词将转换并打上「unknown」标志，因此所有的词将有相同的嵌入。我们通常在训练期间嵌入权重，并且每种语言都有一套。\n\n```python\n# Embedding\nembedding_encoder = variable_scope.get_variable(\n    \"embedding_encoder\", [src_vocab_size, embedding_size], ...)# Look up embedding:#   encoder_inputs: [max_time, batch_size]#   encoder_emp_inp: [max_time, batch_size, embedding_size]\nencoder_emb_inp = embedding_ops.embedding_lookup(\n    embedding_encoder, encoder_inputs)\n```\n\n我们同样可以构建 embedding_decoder 和 decoder_emb_inp。注意我们可以选择预训练的词表征如 word2vec 或 Glove vectors 初始化嵌入权重。通常给定大量的训练数据，我们能从头学习这些嵌入权重。\n\n### 3.2.编码器(encoder)\n\n一旦可以检索到，词嵌入就能作为输入馈送到主神经网络中。该网络有两个多层循环神经网络组成，一个是原语言的编码器，另一个是目标语言的解码器。这两个 RNN 原则上可以共享相同的权重，然而在实践中，我们通常使用两组不同的循环神经网络参数（这些模型在拟合大型训练数据集上做得更好）。解码器 RNN 使用零向量作为它的初始状态，并且可以使用如下代码构建：\n\n```python\n# Build RNN cell\nencoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n# Run Dynamic RNN#   encoder_outpus: [max_time, batch_size, num_units]#   encoder_state: [batch_size, num_units]\nencoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n    encoder_cell, encoder_emb_inp,\n    sequence_length=source_seqence_length, time_major=True)\n```\n\n注意语句有不同的长度以避免浪费计算力，因此我们会通过 source_seqence_length 告诉 dynamic_rnn 精确的句子长度。因为我们的输入是以时间为主（time major）的，我们需要设定 time_major=True。现在我们暂时只需要构建单层 LSTM、encoder_cell。我们后面会详细描述怎样构建多层 LSTM、添加 dropout 并使用注意力机制。\n\n### 3.3.解码器(decoder)\n\ndecoder 也需要访问源信息，一种简单的方式是用编码器最后的隐藏态 encoder_state 对其进行初始化。在图 2 中，我们将源词「student」中的隐藏态传递到了解码器。\n\n```python\n# Build RNN cell\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n```\n\n```python\n# Helper\nhelper = tf.contrib.seq2seq.TrainingHelper(\n    decoder_emb_inp, decoder_lengths, time_major=True)# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\nlogits = outputs.rnn_output\n```\n\n此处代码的核心是 BasicDecoder、获取 decoder_cell(类似于 encoder_cell) 的 decoder、helper 以及之前作为输入的 encoder_state。\n\n通过分离 decoders 和 helpers，我们能重复使用不同的代码库，例如 TrainingHelper 可由 GreedyEmbeddingHelper 进行替换，来做贪婪解码。\n\n最后，我们从未提到过的 projection_layer 是一个密集矩阵，将顶部的隐藏态转变为维度 V 的逻辑向量。我们在图 2 的上部展示了此过程。\n\n```python\nprojection_layer = layers_core.Dense(\n    tgt_vocab_size, use_bias=False)\n```\n\n### 3.4.损失构建\n\n给出以上的 logits，可计算训练损失：\n\n```python\ncrossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    labels=decoder_outputs, logits=logits)\ntrain_loss = (tf.reduce_sum(crossent * target_weights) /\n    batch_size)\n```\n\n以上代码中，target_weights 是一个与 decoder_outputs 大小一样的 0-1 矩阵。该矩阵将目标序列长度以外的其他位置填充为标量值 0。\n\n我们需要指出来的是，训练损失可以由 batch_size 分割，因此我们的超参数 batch_size 是「不变量」。也有些人将训练损失按照 batch_size * num_time_steps 分割，这样可以减少短句所造成的误差。更巧妙的，我们的超参数（应用于前面的方法）不能用于后面的方法。例如，如果两种方法都是用学习率为 1.0 的随机梯度下降，后面的方法将更有效地利用一个较小的学习率，即 1 / num_time_steps。\n\n### 3.5.梯度计算和优化器优化\n\n现在是时候定义我们的 NMT 模型的前向传播了。计算反向传播只需要写几行代码：\n\n```python\n# Calculate and clip gradients\nparameters = tf.trainable_variables()\ngradients = tf.gradients(train_loss, params)\nclipped_gradients, _ = tf.clip_by_global_norm(\n    gradients, max_gradient_norm)\n```\n\n训练 RNN 的一个重要步骤是梯度截断（gradient clipping）。这里，我们使用全局范数进行截断操作。最大值 max_gradient_norm 通常设置为 5 或 1。最后一步是选择优化器。Adam 优化器是最常见的选择。我们还要选择一个学习率，learning_rate 的值通常在 0.0001 和 0.001 之间，且可设置为随着训练进程逐渐减小。\n\n```python\n# Optimization\noptimizer = tf.train.AdamOptimizer(learning_rate)\nupdate_step = optimizer.apply_gradients(\n    zip(clipped_gradients, params))\n```\n\n在我们的实验中，我们使用标准的随机梯度下降（tf.train.GradientDescentOptimizer），并采用了递减的学习率方案，因此也就有更好的性能。\n\n### 3.6 开始训练 NMT 模型\n\n让我们开始训练第一个 NMT 模型，将越南语翻译为英语。代码的入口是nmt.py。\n\n我们将使用小规模的 Ted 演讲并行语料库（133k 的训练样本）进行训练。所有的数据都可从以下链接找到：https://nlp.stanford.edu/projects/nmt/。\n\n我们将使用 tst2012 作为开发数据集，tst 2013 作为测试数据集。运行以下命令行下载数据训练 NMT 模型：\n\n```shell\nnmt/scripts/download_iwslt15.sh /tmp/nmt_data\n```\n\n运行以下命令行开始训练：\n\n```python\nmkdir /tmp/nmt_model\npython -m nmt.nmt \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\n以上命令行训练一个 2 层的 LSTM seq2seq 模型，带有 128-dim 的隐藏单元和 12 个 epochs 的嵌入。我们使用 0.2（或然率为 0.8）的 dropout 值。如果没误差，在我们训练中随着降低混淆度，我们应该能看到类似于以下的 logs。\n\n```python\n# First evaluation, global step 0\n  eval dev: perplexity 17193.66\n  eval test: perplexity 17193.27\n# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017\n  sample train data:\n    src_reverse: </s> </s> Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .\n    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>\n  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00\n  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00\n  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00\n  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00\n  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00\n```\n\n更多细节，请查看：train.py。\n\n我们可以使用 Tensorboard 在训练过程中查看模型的summary：\n\n```shell\ntensorboard --port 22222 --logdir /tmp/nmt_model/\n```\n\n通过以下简单的变化，就能逆向完成英语到越南语的翻译。\n\n```shell\n--src=en --tgt=vi\n```\n\n### 3.7 预测(推理)与生成翻译结果\n\n当你训练你的 NMT 模型时（并且一旦你已经训练了模型），可以在给定之前不可见的源语句的情况下获得翻译。这一过程被称作推理。训练与推理之间有一个明确的区分（测试）：在推理时，我们只访问源语句，即 encoder_inputs。解码的方式有很多种，包括 greedy 解码、采样解码和束搜索解码（beam-search）。下面我们讨论一下 greedy 解码策略。\n\n其想法简单，我们将在图 3 中作说明：\n\n1. 在训练获取 encoder_state 的过程中，我们依然以相同方式编码源语句，并且 encoder_state 用于初始化解码器。\n2. 一旦解码器接收到开始符`<s>`（在我们的代码中指 tgt_sos_id），就开始解码处理（翻译）。\n3. 最大的单词，其 id 与最大的 logit 值相关联，正如被发出的词（这是 greedy 行为）。例如在图 3 中，单词 moi 在第一个解码步中具有最高的翻译概率。接着我们把这一单词作为输入馈送至下一个时间步。\n4. 这一过程会持续到这句话的终止符`「</s>」`，然后输出（在我们的代码中是 tgt_eos_id）。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/greedy_dec.jpg)\n\n<center>图 3. Greedy 解码实例</center>\n\n推理与训练的区别在于步骤 3。推理不总是馈送作为输入的正确目标词，而是使用被模型预测的单词。下面是实现 greedy 解码的代码。它与训练解码器非常相似。\n\n```python\n# Helper\nhelper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n    embedding_decoder,\n    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)\n# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder, maximum_iterations=maximum_iterations)\ntranslations = outputs.sample_id\n```\n\n我们在本文中使用了 GreedyEmbeddingHelper 而不是 TrainingHelper。由于无法提前知道目标语句的长度，我们使用 maximum_iterations 限制翻译的长度。一个启发是解码最多两倍的源语句长度。\n\n```python\nmaximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n```\n\n我们已经训练了一个模型，现在可以创建一个推理文件并翻译一些语句：\n\n```shell\ncat > /tmp/my_infer_file.vi# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)\n\npython -m nmt.nmt \\\n    --model_dir=/tmp/nmt_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_model/output_infer\n\ncat /tmp/nmt_model/output_infer # To view the inference as output\n```\n\n注意上述指令也可在模型被训练时运行，只要存在一个训练检查点。详见 inference.py。\n\n## 4.提升\n\n在训练了一些最基本的序列到序列模型之后，我们现在更进一步。为了打造当前最优的神经机器翻译系统，我们需要更多的秘诀：注意力机制。该机制由 Bahdanau 等人在 2015 年首次提出（https://arxiv.org/abs/1409.0473 ），稍后 Luong 等人和其他人完善了它，其核心思想是当我们翻译时通过「注意」相关的源内容，建立直接的短连接。注意力机制的一个很好副产品是源语句和目标语句之间的一个易于可视化的对齐矩阵（如图 4 所示）。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_vis.jpg)\n\n<center>图 4. 注意力可视化——源语句与目标语句之间对齐的实例。图片来自 2015 年 Bahdanau 等人的论文。</center>\n\n请记住在 vanilla 序列到序列模型中，当开始编码处理时，我们把最后的源状态从编码器传递到解码器。这对短、中长度的语句效果很好；对于长句子，单一固定大小的隐状态成为了信息瓶颈。注意力机制没有摈弃源 RNN 中计算的所有隐状态，而是提出了允许解码器窥探它们的方法（把它们看作是源信息的动态存储）。如此，注意力机制提升了长句的翻译质量。现在，注意力机制实至名归，已成功应用于其他诸多任务（比如语音识别）。\n\n### 4.1 注意力机制背景\n\n我们现在描述一下注意力机制的实例（Luong et al., 2015），它已经被应用到几个最新型的系统当中了，包括开源工具，比如 OpenNMT（http://opennmt.net/about/ ）和此教程中的 TF seq2seq API。我们还将会提供注意力机制相关变体的内容。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg)\n\n图 5. 注意力机制——基于注意力的 NMT 系统（Luong et al., 2015 中有具体描述）。\n\n我们重点详解注意力计算过程中的第一步。为了更加清晰，我们没有展示图（2）中的嵌入层和投影层。\n\n如图 5 所示，注意力计算发生在解码步骤中的每一步。它包含下列步骤：\n\n1. 当前目标隐蔽状态和所有源状态（source state）进行比较，以导出权重（weight），见图 4。\n2. 基于注意力权重，我们计算了一个背景向量（context vector），作为源状态的平均权值。\n3. 将背景向量与当前目标隐蔽态进行结合以生成最终的注意力向量。\n4. 此注意力向量将作为下一时序步骤的输入。前三个步骤可以由下列公式总结：\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg)\n\n这里，函数 score 用于将目标隐蔽状态 ht 和每一个源状态 hs 进行比较，结果会被标准化成生成式注意力权重（一个源位置的分布）。其实有很多种关于评分函数（scoring function）的选择；比较流行的评分函数包括公式（4）中给出的乘法与加法形式。一旦被计算，注意力向量 at 就会用于推导 softmax logit 和损失。这与 vanilla seq2seq 模型顶层的目标隐蔽态相似。函数 f 也可以利用其它形式。\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg)\n\n注意力机制的多种实现方法可由以下链接获得：https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py 。\n\n注意力机制中有什么相关注意事项呢？\n\n上述公式表明注意力机制有很多种变体。这些变体依赖于评分函数（scoring function）和注意力函数（attention function）的形式，也依赖于前一状态 ht-1，而不依赖于开始建议的评分函数 ht（Bahdanau et al., 2015）。实际上我们发现的只有一些选择上的注意事项。\n* 一，注意力的基本形式，例如，目标和源之间的直接联系需要被呈现。\n* 二，把注意力向量输入给下一时间步骤，以把之前的注意力决策告知给网络（Luong et al., 2015）。\n\n评分函数的选择经常可以造成不同的性能表现。\n\n### 4.2 Attention Wrapper API\n\n在我们的 Attention Wrapper API 的实现中，借鉴了 Weston et al., 2015 在 onmemory network 工作中的术语。相比于拥有可读、可写的记忆，此教程中的 attention 机制仅是可读的记忆。特别是对隐藏态（或者隐藏态的变体，例如 $W\\overline{h}_s$ in Luong's scoring style or $W_2\\overline{h}_s$ ) 的设定，被认为是「记忆」。在每个时间步下，我们使用现有的目标隐藏态作为「query」决定读取哪一部分记忆。通常情况下，query 需要与单个记忆条相对应的 keys 进行对比。在上面对注意机制的演示中，我们偶然使用一套源隐藏态（或者其变体，例如$W_1h_t$）作为「key」。你们可以从这种记忆网络术语中获取灵感，找到其他形式的 attention。\n\n由于 attention wrapper，就不再需要扩展我们带有 attention 的 vanilla seq2seq 代码。这部分文件为 attention_model.py。\n\n首先，我们需要定义一种注意机制，例如采用 Luong et al., 2015 的研究。\n\n```python\n# attention_states: [batch_size, max_time, num_units]\nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n# Create an attention mechanism\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units, attention_states,\n    memory_sequence_length=source_sequence_length)\n```\n\n在之前的 Encoder 部分，encoder_outputs 是一套顶层的掩藏态源，形式为 [max_time, batch_size, num_units]（因为我们使用 dynamic_rnn with time_major 设定）。在注意机制上，我们需要保证通过的「memory」是批次为主的，所以需要调换 attention_states。我们通过 source_sequence_length 保证注意机制的权重有适当的规范化（只在 non-padding 的位置）。定义完注意机制之后，我们使用 AttentionWrapper 来包裹解码单元。\n\n```python\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    decoder_cell, attention_mechanism,\n    attention_layer_size=num_units)\n```\n\n剩下的代码基本和编码器一转样 (https://github.com/tensorflow/nmt#decoder)!\n\n### 4.3 上手—打造一个基于注意力的 NMT 模型\n\n为了使注意力发挥作用，我们需要用到 luong、scaled_luong、bahdanau 或 normed_bahdanau 其中的一个作为训练期间注意力标志（attention flag）的值。该标志指定了我们将要使用的注意力机制。除此之外，我们需要为注意力模型创建一个新目录，因此无需重新使用之前训练的基本 NMT 模型。\n\n运行以下指令开始训练：\n\n```shell\nmkdir /tmp/nmt_attention_model\n\npython -m nmt.nmt \\\n    --attention=scaled_luong \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_attention_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\n训练之后，我们可以使用带有新 model_dir 的相同推理指令进行推理：\n\n```shell\npython -m nmt.nmt \\\n    --model_dir=/tmp/nmt_attention_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_attention_model/output_infer\n```\n\n## 5.技巧和注意点\n\n### 5.1 构建训练、验证和测试图\n\n当我们使用 TensorFlow 单间一个机器学习模型的时候，最好构建三个分开的 graph：\n\n- 训练图，包括：\n  - Batches, buckets, 以及来自文件或外部输入的数据集的部分采样数据；\n  - 前向和反向传播的 ops；\n  - 创建 optimizer，以及添加训练 op；\n- 验证图，包括：\n  - Batches, buckets, 以及来自文件或外部输入的数据集输入数据\n  - 训练时的前向传播 op，以及要添加的 evaluation ops\n- 预测图，包括：\n  - 不需要批处理的输入数据\n  - 不需要 subsample 和 bucket 输入数据\n  - 从 placeholders 读取输入数据（数据可以通过 feed_dict 被图读取，或者使用 C++ TensorFlow serving binary）\n  - 模型前向传播的部分 ops，以及一些可能 [session.run](http://session.run/) 函数调用的所需要的额外的为存储状态（state）所需要的 inputs/outputs。\n\n现在比较棘手的一点是 ，如何在一个机器上让三个图共享这些 variables，这可以通过为每个图创建不同的 session 来解决。训练过程的session 周期性的保存 checkpoints，然后 eval session 和 inference session 就可以读取checkpoints。下面的例子展示了这两种方法的不同。\n\n#### 前一种方法：三个模型都在一个图里，并且共享一个 session。\n\n```python\nwith tf.variable_scope('root'):\n  train_inputs = tf.placeholder()\n  train_op, loss = BuildTrainModel(train_inputs)\n  initializer = tf.global_variables_initializer()\n\nwith tf.variable_scope('root', reuse=True):\n  eval_inputs = tf.placeholder()\n  eval_loss = BuildEvalModel(eval_inputs)\n\nwith tf.variable_scope('root', reuse=True):\n  infer_inputs = tf.placeholder()\n  inference_output = BuildInferenceModel(infer_inputs)\n\nsess = tf.Session()\n\nsess.run(initializer)\n\nfor i in itertools.count():\n  train_input_data = ...\n  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})\n\n  if i % EVAL_STEPS == 0:\n    while data_to_eval:\n      eval_input_data = ...\n      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})\n\n  if i % INFER_STEPS == 0:\n    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})\n```\n\n#### 后一种方法：三个模型在三个图里，三个 sessions 共享同样的 variables。\n\n```python\ntrain_graph = tf.Graph()\neval_graph = tf.Graph()\ninfer_graph = tf.Graph()\n\nwith train_graph.as_default():\n  train_iterator = ...\n  train_model = BuildTrainModel(train_iterator)\n  initializer = tf.global_variables_initializer()\n\nwith eval_graph.as_default():\n  eval_iterator = ...\n  eval_model = BuildEvalModel(eval_iterator)\n\nwith infer_graph.as_default():\n  infer_iterator, infer_inputs = ...\n  infer_model = BuildInferenceModel(infer_iterator)\n\ncheckpoints_path = \"/tmp/model/checkpoints\"\n\ntrain_sess = tf.Session(graph=train_graph)\neval_sess = tf.Session(graph=eval_graph)\ninfer_sess = tf.Session(graph=infer_graph)\n\ntrain_sess.run(initializer)\ntrain_sess.run(train_iterator.initializer)\n\nfor i in itertools.count():\n\n  train_model.train(train_sess)\n\n  if i % EVAL_STEPS == 0:\n    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n    eval_model.saver.restore(eval_sess, checkpoint_path)\n    eval_sess.run(eval_iterator.initializer)\n    while data_to_eval:\n      eval_model.eval(eval_sess)\n\n  if i % INFER_STEPS == 0:\n    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n    infer_model.saver.restore(infer_sess, checkpoint_path)\n    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})\n    while data_to_infer:\n      infer_model.infer(infer_sess)\n```\n\n注意后一种方法是如何被转换为分布式版本的。\n\n后种方法与前种方法的另一个不同在于，后者不用在 *session.sun* 调用时通过 *feed_dicts* 喂给数据，而是使用自带状态的 *iterator* 对象（stateful iterator objects）。不论在单机还是分布式集群上，这些 *iterators* 可以让输入管道（input pipeline）变得更容易。在下一小节，我们将使用新的数据输入管道（input data pipeline）。\n\n### 5.2 数据输入管道(Data Input Pipeline)\n\n在 TensorFlow 1.2版本之前，用户有两种把数据喂给 TensorFlow training 和 eval pipelines的方法：\n\n- 1. 在每次训练调用 *session.run* 时，通过 *feed_dict* 直接喂给数据；\n- 2. 使用 tf.train（例如 tf.train.batch）和 tf.contrib.train 中的队列机制（queueing machanisms）；\n- 3. 使用来自 helper 层级框架比如 tf.contrib.learn 或 tf.contrib.slim 的 helpers （这种方法是使用更高效的方法利用第二种方法）。\n\n第一种方法对不熟悉 TensorFlow 或需要做一些外部的数据修改（比如他们自己的 minibatch queueing）的用户来说更简单，这种方法只需用简单的 Python 语法就可实现。第二种和第三种方法更标准但也不那么灵活，他们需要开启多个 Python 线程（queue runners）。更重要的是，如果操作不当会导致死锁或难以查明的错误。尽管如此，队列的方法仍要比 *feed_dict* 的方法高效很多，并且也是单机和分布式训练的标准。\n\n从TensorFlow 1.2开始，有一种新的数据读取的方法可以使用： dataset iterators，其在 **tf.data**模块。Data iterators 非常灵活，易于使用和操作，并且利用 TensorFlow C++ runtime 实现了高效和多线程。\n\n我们可以使用一个 batch data Tensor，一个文件名，或者包含多个文件名的 Tensor 来创建一个 **dataset**。下面是一些例子：\n\n```python\n# Training dataset consists of multiple files.\ntrain_dataset = tf.data.TextLineDataset(train_files)\n\n# Evaluation dataset uses a single file, but we may\n# point to a different file for each evaluation round.\neval_file = tf.placeholder(tf.string, shape=())\neval_dataset = tf.data.TextLineDataset(eval_file)\n\n# For inference, feed input data to the dataset directly via feed_dict.\ninfer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))\ninfer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)\n```\n\n所有的数据都可以完成像数据预处理一样的处理方式，包括数据的 reading 和 cleaning，bucketing（在 training 和 eval 的时候），filtering 以及 batching。\n\n把每个句子转换为单词串的向量（vectors of word strings），那我们可以使用 dataset 的 map transformation:\n\n```python\ndataset = dataset.map(lambda string: tf.string_split([string]).values)\n```\n\n我们也可以把每个句子向量转换为包含向量与其动态长度的元组：\n\n```python\ndataset = dataset.map(lambda words: (words, tf.size(words))\n```\n\n最后，我们可以对每个句子应用 vocabulary lookup。给定一个 lookup 的 table，此 map 函数可以把元组的第一个元素从串向量转换为数字向量。（译者注：不好翻译，原文是：Finally, we can perform a vocabulary lookup on each sentence. Given a lookup table object table, this map converts the first tuple elements from a vector of strings to a vector of integers.）\n\n```python\ndataset = dataset.map(lambda words, size: (table.lookup(words), size))\n```\n\n合并两个 datasets 也非常简单，如果两个文件有行行对应的翻译，并且两个文件分别被不同的 dataset 读取，那么可以通过下面这种方式生成一个新的 dataset，这个新的 dataset 的内容是两种语言的翻译一一对应的元组。\n\n```python\nsource_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))\n```\n\nBatching 变长的句子实现起来也很直观。下边的代码从 *source_target_dataset* 中 batch 了 *batch_size* 个元素，并且分别为每个 batch 的源向量和目标向量 padding 到最长的源向量和目标向量的长度。\n\n```python\nbatched_dataset = source_target_dataset.padded_batch(\n        batch_size,\n        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size\n                        tf.TensorShape([])),     # size(source)\n                       (tf.TensorShape([None]),  # target vectors of unknown size\n                        tf.TensorShape([]))),    # size(target)\n        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id\n                         0),          # size(source) -- unused\n                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id\n                         0)))         # size(target) -- unused\n```\n\n从 dataset 拿到的数据会嵌套为元组，其 tensors 的最左边的维度是 batch_size. 其结构如下：\n\n- iterator[0][0] has the batched and padded source sentence matrices.\n- iterator[0][1] has the batched source size vectors.\n- iterator[1][0] has the batched and padded target sentence matrices.\n- iterator[1][1] has the batched target size vectors.\n\n最后，bucketing 多个 batch 的大小差不多的源句子也是可以的。更多的代码实现详见文件[utils/iterator_utils.py](https://github.com/tensorflow/nmt/tree/master/nmt/utils/iterator_utils.py)。\n\n从 dataset 中读取数据需要三行的代码：创建 iterator，取其值，初始化。\n\n```python\nbatched_iterator = batched_dataset.make_initializable_iterator()\n\n((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next()\n\n# At initialization time.\nsession.run(batched_iterator.initializer, feed_dict={...})\n```\n\n一旦 iterator 被初始化，那么 [session.run](http://session.run) 每一次调用 source 和 target ，都会从dataset中自动提取下一个 minibatch 的数据。\n\n### 5.3 让 NMT 模型更完美的其他技巧\n\n#### 双向RNN(Bidirectional RNN)\n\n一般来讲，encoder 的双向 RNNs 可以让模型表现更好（训练速度会下降，因为有更多的层需要计算）。这里，我们给出了构建一个单层双向层的 encoder 的简单代码：\n\n```python\n# Construct forward and backward cells\nforward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\nbackward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n\nbi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n    forward_cell, backward_cell, encoder_emb_inp,\n    sequence_length=source_sequence_length, time_major=True)\nencoder_outputs = tf.concat(bi_outputs, -1)\n```\n\n*encoder_outputs* 和 *encoder_state* 也可以使用 Encoder 小节的方法获取到。需要注意的是，如果要创建多层双向层，你需要修改一下 encoder_state，见 [model.py](https://github.com/tensorflow/nmt/tree/master/nmt/model.py) 的*_build_bidirectional_rnn()*方法。\n\n#### 集束搜索(Beam Search)\n\nGreedy decoding 可以给我们非常合理的翻译结果，但是 beam search decoding 可以让翻译结果更好。Beam search 的思想是，考虑我们可以选择的所有翻译结果的排名最靠前的几个候选的集合，我们探索其所有的可能翻译结果（大家也可以参考[知乎的beam search讨论](https://www.zhihu.com/question/54356960/answer/138990060)）。Beam 的这个 size 我们称为 *beam width*，一个较小的 beam width 比如说 10，就已经足够大了。我们推荐读者阅读 [Neubig, (2017)](https://arxiv.org/abs/1703.01619) 的 7.2.3 小节。这是 beam search 的一个例子：\n\n```python\n# Replicate encoder infos beam_width times\ndecoder_initial_state = tf.contrib.seq2seq.tile_batch(\n    encoder_state, multiplier=hparams.beam_width)\n\n# Define a beam-search decoder\ndecoder = tf.contrib.seq2seq.BeamSearchDecoder(\n        cell=decoder_cell,\n        embedding=embedding_decoder,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        initial_state=decoder_initial_state,\n        beam_width=beam_width,\n        output_layer=projection_layer,\n        length_penalty_weight=0.0)\n\n# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n```\n\n在 Decoder 小节，*dynamic_decode()* API 也被使用过。解码结束，我们就可以使用下面的代码得到翻译结果：\n\n```python\ntranslations = outputs.predicted_ids\n# Make sure translations shape is [batch_size, beam_width, time]\nif self.time_major:\n   translations = tf.transpose(translations, perm=[1, 2, 0])\n```\n\n更多细节，可查看 [model.py](https://github.com/tensorflow/nmt/tree/master/nmt/model.py), *_build_decoder()* 函数。\n\n#### 超参数(Hyperparameters)\n\n有一些超参数也可以供我们调节。这里，根据我们的实验，我们列举了几个超参数【你可以表示不认同，保留自己的看法】。\n\n- **optimizer**：对于“不太常见”的网络结构，Adam 可能可以给出一个较合理的结果，如果你用 SGD 进行训练，那么 SGD 往往可以取得更好的结果。\n- **Attention**：Bahdanau 类型的 attention，encoder 需要双向结构才能表现很好；同时 Luong 类型的 attention 需要其他的一些设置才能表现很好。在本教程中，我们推荐使用被改进的这两个类型的 attention：**scaled_luong** 和 **normed_bahdanau**。\n\n**多 GPU 训练 | Multi-GPU training**\n\n训练一个 NMT 模型可能需要几天的时间，我们可以把不同的 RNN layers 放在不同的 GPUs 进行训练可以加快训练速度。这里是使用多 GPUs 创建 RNN layers 的例子：\n\n```python\ncells = []\nfor i in range(num_layers):\n  cells.append(tf.contrib.rnn.DeviceWrapper(\n      tf.contrib.rnn.LSTMCell(num_units),\n      \"/gpu:%d\" % (num_layers % num_gpus)))\ncell = tf.contrib.rnn.MultiRNNCell(cells)\n```\n\n另外，我们还需要 tf.gradients 的 colocate_gradients_with_ops 参数来同步梯度的计算。\n\n你会发现，尽管我们使用了多个 GPUs，但是 attention-based NMT 模型的训练速度提升不大。问题的关键在于，在标准的 attention 模型中，在每个时间步，我们都需要用最后一层的输出去“查询”attention，这就意味着，每一个解码的时间步都需要等前面的时间步完全完成。因此，我们不能简单的通过在多 GPUs 上部署 RNN layers 来同步解码过程。\n\n[GNMT attention architecture](https://arxiv.org/pdf/1609.08144.pdf) 可以通过使用第一层的输出来查询 attention 的方法来同步 decoder 的计算。这样，解码器的每一步就可以在前一步的第一层和 attention 计算完成之后就可以进行解码了。我们的 API 实现了这个结构 [GNMTAttentionMultiCell](https://github.com/tensorflow/nmt/tree/master/nmt/gnmt_model.py)，其是*tf.contrib.rnn.MultiRNNCell*的子类。这里是使用 *GNMTAttentionMultiCell* 创建一个 decoder 的例子：\n\n```python\ncells = []\nfor i in range(num_layers):\n  cells.append(tf.contrib.rnn.DeviceWrapper(\n      tf.contrib.rnn.LSTMCell(num_units),\n      \"/gpu:%d\" % (num_layers % num_gpus)))\nattention_cell = cells.pop(0)\nattention_cell = tf.contrib.seq2seq.AttentionWrapper(\n    attention_cell,\n    attention_mechanism,\n    attention_layer_size=None,  # don't add an additional dense layer.\n    output_attention=False,)\ncell = GNMTAttentionMultiCell(attention_cell, cells)\n```\n\n### 结果与预测质量\n\nIWSLT 英语-越南语\n\n训练：133k 的样本，dev=tst2012，test=tst2013\n\nSystems | tst2012 (dev) | test2013 (test)\n--- | :---: | :---:\nNMT (greedy) | 23.2 | 25.5\nNMT (beam=10) | 23.8 | **26.1**\n[(Luong & Manning, 2015)](https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf) | - | 23.3\n\n*训练速度：在英伟达 K40m 上是 0.37s 的时间步、15.3k 的 wps，在 Titan X 上是 0.17 s 的时间步，32.2k 的 wps。*\n\nWMT 德语-英语\n\n训练：4.5M 的样本量，dev=newstest2013，test=newtest2015\n\nSystems | newstest2013 (dev) | newstest2015\n--- | :---: | :---:\nNMT (greedy) | 27.1 | 27.6\nNMT (beam=10) | 28.0 | 28.9\nNMT + GNMT attention (beam=10) | 29.0 | **29.9**\n[WMT SOTA](http://matrix.statmt.org/) | - | 29.3\n\n*训练速度：在英伟达 K40m 上是 2.1s 的时间步，3.4k 的 wps，在英伟达 Titan X 上是 0.7s 的时间步，8.7k 的 wps。*\n\n为了查看 GNMT 注意的加速度，我们只在 K40m 上做了基准测试：\n\nSystems | 1 gpu | 4 gpus | 8 gpus\n--- | :---: | :---: | :---:\nNMT (4 layers) | 2.2s, 3.4K | 1.9s, 3.9K | -\nNMT (8 layers) | 3.5s, 2.0K | - | 2.9s, 2.4K\nNMT + GNMT attention (4 layers) | 2.6s, 2.8K | 1.7s, 4.3K | -\nNMT + GNMT attention (8 layers) | 4.2s, 1.7K | - | 1.9s, 3.8K\n\n*WMT 英语-德语 全对比*\n\n第二行是我们 GNMT 注意模型：模型 1（4 层），模型 2（8 层）。\n\nSystems | newstest2014 | newstest2015\n--- | :---: | :---:\n*Ours* &mdash; NMT + GNMT attention (4 layers) | 23.7 | 26.5\n*Ours* &mdash; NMT + GNMT attention (8 layers) | 24.4 | **27.6**\n[WMT SOTA](http://matrix.statmt.org/) | 20.6 | 24.9\nOpenNMT [(Klein et al., 2017)](https://arxiv.org/abs/1701.02810) | 19.3 | -\ntf-seq2seq [(Britz et al., 2017)](https://arxiv.org/abs/1703.03906) | 22.2 | 25.2\nGNMT [(Wu et al., 2016)](https://research.google.com/pubs/pub45610.html) | **24.6** | -\n\n**其他资源**\n\n若想深入了解神经机器翻译和序列-序列模型，我们非常推荐以下资源：\n\n- Neural Machine Translation and Sequence-to-sequence Models: A Tutorial：https://arxiv.org/abs/1703.01619\n- Neural Machine Translation - Tutorial ACL 2016：https://sites.google.com/site/acl16nmt/\n- Thang Luong's Thesis on Neural Machine Translation：https://github.com/lmthang/thesis\n\n用于构建 seq2seq 模型的工具很多：\n\n- Stanford NMT https://nlp.stanford.edu/projects/nmt/ [Matlab] \n- tf-seq2seq https://github.com/google/seq2seq [TensorFlow] \n- Nemantus https://github.com/rsennrich/nematus [Theano] \n- OpenNMT http://opennmt.net/ [Torch]\n\n## 6.seq2seq构建的聊天机器人应用\n我们来使用seq2seq框架完成一个聊天机器人构建的任务，我给大家准备了一些对话语料，我们使用这份数据来构建聊天机器人的AI应用。在此之前，我们先了解一下原有的翻译系统需要准备的语料格式，我们把中文数据处理成格式一致的形态。\n\n我们先拉取一份样例数据。\n\n\n```python\n%cd nmt\n!bash nmt/scripts/download_iwslt15.sh /tmp/nmt_data\n```\n\n    /content/nmt\n    Download training dataset train.en and train.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100 12.9M  100 12.9M    0     0  4415k      0  0:00:03  0:00:03 --:--:-- 4415k\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100 17.2M  100 17.2M    0     0  5386k      0  0:00:03  0:00:03 --:--:-- 5386k\n    Download dev dataset tst2012.en and tst2012.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  136k  100  136k    0     0   188k      0 --:--:-- --:--:-- --:--:--  188k\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  183k  100  183k    0     0   253k      0 --:--:-- --:--:-- --:--:--  253k\n    Download test dataset tst2013.en and tst2013.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  129k  100  129k    0     0   196k      0 --:--:-- --:--:-- --:--:--  196k\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  179k  100  179k    0     0   246k      0 --:--:-- --:--:-- --:--:--  246k\n    Download vocab file vocab.en and vocab.vi.\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100  136k  100  136k    0     0   207k      0 --:--:-- --:--:-- --:--:--  208k\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100 46767  100 46767    0     0   102k      0 --:--:-- --:--:-- --:--:--  102k\n    \n\n**查看一下包含的文件**\n\n\n```python\n!ls /tmp/nmt_data\n```\n\n    train.en  tst2012.en  tst2013.en  vocab.en\n    train.vi  tst2012.vi  tst2013.vi  vocab.vi\n    \n\n**看一下源语言与目标语言的格式，以及对应的数据量**\n\n可以看到都是做过tokenization之后的数据。\n\n\n```python\n!head -10 /tmp/nmt_data/train.en\n```\n\n    Rachel Pike : The science behind a climate headline\n    In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n    I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n    Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n    They are both two branches of the same field of atmospheric science .\n    Recently the headlines looked like this when the Intergovernmental Panel on Climate Change , or IPCC , put out their report on the state of understanding of the atmospheric system .\n    That report was written by 620 scientists from 40 countries .\n    They wrote almost a thousand pages on the topic .\n    And all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n    It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n    \n\n\n```python\n!wc -l /tmp/nmt_data/train.en\n```\n\n    133317 /tmp/nmt_data/train.en\n    \n\n**还需要准备好vocabulary词表**\n\n\n```python\n!head -10 /tmp/nmt_data/vocab.en\n```\n\n    <unk>\n    <s>\n    </s>\n    Rachel\n    :\n    The\n    science\n    behind\n    a\n    climate\n    \n\n\n```python\n!wc -l /tmp/nmt_data/vocab.en\n```\n\n    17191 /tmp/nmt_data/vocab.en\n    \n\n\n```python\n!wc -l /tmp/nmt_data/vocab.vi\n```\n\n    7709 /tmp/nmt_data/vocab.vi\n    \n\n## 聊天机器人语料\n这里列举一些从网络中找到的用于训练中文（英文）聊天机器人的对话语料。\n\n### 公开语料\n搜集到的一些数据集如下，点击链接可以进入原始地址\n\n1. [dgk_shooter_min.conv.zip](https://github.com/rustch3n/dgk_lost_conv)\n<br>中文电影对白语料，噪音比较大，许多对白问答关系没有对应好\n\n2. [The NUS SMS Corpus](https://github.com/kite1988/nus-sms-corpus)\n<br>包含中文和英文短信息语料，据说是世界最大公开的短消息语料\n\n3. [ChatterBot中文基本聊天语料](https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data)\n<br>ChatterBot聊天引擎提供的一点基本中文聊天语料，量很少，但质量比较高\n\n4. [Datasets for Natural Language Processing](https://github.com/karthikncode/nlp-datasets)\n<br>这是他人收集的自然语言处理相关数据集，主要包含Question Answering，Dialogue Systems， Goal-Oriented Dialogue Systems三部分，都是英文文本。可以使用机器翻译为中文，供中文对话使用\n\n5. [小黄鸡](https://github.com/rustch3n/dgk_lost_conv/tree/master/results)\n<br>据传这就是小黄鸡的语料：xiaohuangji50w_fenciA.conv.zip （已分词） 和 xiaohuangji50w_nofenci.conv.zip （未分词）\n\n6. [白鹭时代中文问答语料](https://github.com/Samurais/egret-wenda-corpus)\n<br>由白鹭时代官方论坛问答板块10,000+ 问题中，选择被标注了“最佳答案”的纪录汇总而成。人工review raw data，给每一个问题，一个可以接受的答案。目前，语料库只包含2907个问答。([备份](./egret-wenda-corpus.zip))\n\n7. [Chat corpus repository](https://github.com/Marsan-Ma/chat_corpus)\n<br>chat corpus collection from various open sources\n<br>包括：开放字幕、英文电影字幕、中文歌词、英文推文\n\n8. [保险行业QA语料库](https://github.com/Samurais/insuranceqa-corpus-zh)\n<br>通过翻译 [insuranceQA](https://github.com/shuzi/insuranceQA)产生的数据集。train_data含有问题12,889条，数据 141779条，正例：负例 = 1:10； test_data含有问题2,000条，数据 22000条，正例：负例 = 1:10；valid_data含有问题2,000条，数据 22000条，正例：负例 = 1:10\n\n**我们下载小黄鸡语料，并对它做一个处理，使得它符合seq2seq模型的输入格式**\n\n\n```python\n!wget https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n!unzip xiaohuangji50w_nofenci.conv.zip\n```\n\n    --2019-01-27 07:52:17--  https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n    Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n    Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip [following]\n    --2019-01-27 07:52:17--  https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip\n    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 10232473 (9.8M) [application/zip]\n    Saving to: ‘xiaohuangji50w_nofenci.conv.zip’\n    \n    xiaohuangji50w_nofe 100%[===================>]   9.76M  19.1MB/s    in 0.5s    \n    \n    2019-01-27 07:52:18 (19.1 MB/s) - ‘xiaohuangji50w_nofenci.conv.zip’ saved [10232473/10232473]\n    \n    Archive:  xiaohuangji50w_nofenci.conv.zip\n      inflating: xiaohuangji50w_nofenci.conv  \n    \n\n\n```python\n!perl -pi.bak -e 's/(E\\s)/\\1Q /g' xiaohuangji50w_nofenci.conv\n!perl -pi.bak -e 's/(Q M)/Q/g' xiaohuangji50w_nofenci.conv\n!perl -pi.bak -e 's/(M )/A /g' xiaohuangji50w_nofenci.conv\n!head -30 xiaohuangji50w_nofenci.conv\n```\n\n    E\n    Q 呵呵\n    A 是王若猫的。\n    E\n    Q 不是\n    A 那是什么？\n    E\n    Q 怎么了\n    A 我很难过，安慰我~\n    E\n    Q 开心点哈,一切都会好起来\n    A 嗯 会的\n    E\n    Q 我还喜欢她,怎么办\n    A 我帮你告诉她？发短信还是打电话？\n    E\n    Q 短信\n    A 嗯嗯。我也相信\n    E\n    Q 你知道谁么\n    A 肯定不是我，是阮德培\n    E\n    Q 许兵是谁\n    A 吴院四班小帅哥\n    E\n    Q 这么假\n    A 三鹿奶粉也假，不一样的卖啊\n    E\n    Q 许兵是傻逼\n    A 被你发现了。\n    \n\n\n```python\ntext = open(\"xiaohuangji50w_nofenci.conv\").read().split(\"E\\n\")\n```\n\n\n```python\ntext[1]\n```\n\n\n\n\n    'Q 呵呵\\nA 是王若猫的。\\n'\n\n\n\n\n```python\nimport jieba\n\ndef split_conv(in_f, out_q, out_a):\n  out_question = open(out_q, 'w')\n  out_answer = open(out_a, 'w')\n  text = open(in_f).read().split(\"E\\n\")\n  for pair in text:\n    # 句子长度太短的问题对话，跳过\n    if len(pair)<=4:\n      continue\n    # 切分问题和回答\n    contents = pair.split(\"\\n\")\n    out_question.write(\" \".join(jieba.lcut(contents[0].strip(\"Q \")))+\"\\n\")\n    out_answer.write(\" \".join(jieba.lcut(contents[1].strip(\"A \")))+\"\\n\")\n  out_question.close()\n  out_answer.close()\n```\n\n\n```python\nin_f = \"xiaohuangji50w_nofenci.conv\"\nout_q = 'question.file'\nout_a = 'answer.file'\nsplit_conv(in_f, out_q, out_a)\n```\n\n\n```python\n!head -10 question.file\n```\n\n    呵呵\n    不是\n    怎么 了\n    开心 点哈 , 一切 都 会 好 起来\n    我 还 喜欢 她 , 怎么办\n    短信\n    你 知道 谁 么\n    许兵 是 谁\n    这么 假\n    许兵 是 傻 逼\n    \n\n\n```python\n!head -10 answer.file\n```\n\n    是 王若 猫 的 。\n    那 是 什么 ？\n    我 很 难过 ， 安慰 我 ~\n    嗯   会 的\n    我 帮 你 告诉 她 ？ 发短信 还是 打电话 ？\n    嗯 嗯 。 我 也 相信\n    肯定 不是 我 ， 是 阮德培\n    吴院 四班 小帅哥\n    三鹿 奶粉 也 假 ， 不 一样 的 卖 啊\n    被 你 发现 了 。\n    \n\n\n```python\n!wc -l question.file\n```\n\n    454131 question.file\n    \n\n\n```python\n!wc -l answer.file\n```\n\n    454131 answer.file\n    \n\n\n```python\nimport re\ndef get_vocab(in_f, out_f):\n    vocab_dic = {}\n    for line in open(in_f, encoding='utf-8'):\n        words = line.strip().split(\" \")\n        for word in words:\n            # 保留汉字内容\n            if not re.match(r\"[\\u4e00-\\u9fa5]+\", word):\n                continue\n            try:\n                vocab_dic[word] += 1\n            except:\n                vocab_dic[word] = 1\n    out = open(out_f, 'w', encoding='utf-8')\n    out.write(\"<unk>\\n<s>\\n</s>\\n\")\n    vocab = sorted(vocab_dic.items(),key = lambda x:x[1],reverse = True)\n    for word in [x[0] for x in vocab[:80000]]:\n        out.write(word)\n        out.write(\"\\n\")\n    out.close()\n```\n\n#### 切分训练，验证，测试集\n\n\n```python\n!mkdir data\n!head -300000 question.file > data/train.input\n!head -300000 answer.file > data/train.output\n!head -380000 question.file | tail -80000 > data/val.input\n!head -380000 answer.file | tail -80000 > data/val.output\n!tail -75000 question.file > data/test.input\n!tail -75000 answer.file > data/test.output\n```\n\n**构建词表**\n\n\n```python\nin_file = \"question.file\"\nout_file = \"./data/vocab.input\"\nget_vocab(in_file, out_file)\n```\n\n\n```python\nin_file = \"answer.file\"\nout_file = \"./data/vocab.output\"\nget_vocab(in_file, out_file)\n```\n\n**新建文件夹**\n\n\n```python\n!mkdir /tmp/nmt_attention_model\n```\n\n**训练摘要生成模型**\n\n\n```python\n!python3 -m nmt.nmt \\\n    --attention=scaled_luong \\\n    --src=input --tgt=output \\\n    --vocab_prefix=./data/vocab  \\\n    --train_prefix=./data/train \\\n    --dev_prefix=./data/val  \\\n    --test_prefix=./data/test \\\n    --out_dir=/tmp/nmt_attention_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=1 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\n    # Job id 0\n    # Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 9021483762835584285), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 656894764299535912)]\n    # Loading hparams from /tmp/nmt_attention_model/hparams\n    # Vocab file ./data/vocab.input exists\n    # Vocab file ./data/vocab.output exists\n      saving hparams to /tmp/nmt_attention_model/hparams\n      saving hparams to /tmp/nmt_attention_model/best_bleu/hparams\n      attention=scaled_luong\n      attention_architecture=standard\n      avg_ckpts=False\n      batch_size=128\n      beam_width=0\n      best_bleu=0\n      best_bleu_dir=/tmp/nmt_attention_model/best_bleu\n      check_special_token=True\n      colocate_gradients_with_ops=True\n      decay_scheme=\n      dev_prefix=./data/val\n      dropout=0.2\n      embed_prefix=None\n      encoder_type=uni\n      eos=</s>\n      epoch_step=0\n      forget_bias=1.0\n      infer_batch_size=32\n      infer_mode=greedy\n      init_op=uniform\n      init_weight=0.1\n      language_model=False\n      learning_rate=1.0\n      length_penalty_weight=0.0\n      log_device_placement=False\n      max_gradient_norm=5.0\n      max_train=0\n      metrics=['bleu']\n      num_buckets=5\n      num_dec_emb_partitions=0\n      num_decoder_layers=2\n      num_decoder_residual_layers=0\n      num_embeddings_partitions=0\n      num_enc_emb_partitions=0\n      num_encoder_layers=2\n      num_encoder_residual_layers=0\n      num_gpus=1\n      num_inter_threads=0\n      num_intra_threads=0\n      num_keep_ckpts=5\n      num_sampled_softmax=0\n      num_train_steps=12000\n      num_translations_per_input=1\n      num_units=128\n      optimizer=sgd\n      out_dir=/tmp/nmt_attention_model\n      output_attention=True\n      override_loaded_hparams=False\n      pass_hidden_state=True\n      random_seed=None\n      residual=False\n      sampling_temperature=0.0\n      share_vocab=False\n      sos=<s>\n      src=input\n      src_embed_file=\n      src_max_len=50\n      src_max_len_infer=None\n      src_vocab_file=./data/vocab.input\n      src_vocab_size=56491\n      steps_per_external_eval=None\n      steps_per_stats=100\n      subword_option=\n      test_prefix=./data/test\n      tgt=output\n      tgt_embed_file=\n      tgt_max_len=50\n      tgt_max_len_infer=None\n      tgt_vocab_file=./data/vocab.output\n      tgt_vocab_size=50041\n      time_major=True\n      train_prefix=./data/train\n      unit_type=lstm\n      use_char_encode=False\n      vocab_prefix=./data/vocab\n      warmup_scheme=t2t\n      warmup_steps=0\n    WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Use `tf.data.experimental.group_by_window(...)`.\n    # Creating train graph ...\n    # Build a basic encoder\n      num_layers = 2, num_residual_layers=0\n      cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n      DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n      cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n      cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n      cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n      learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n      decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1\n    # Trainable variables\n    Format: <name>, <shape>, <(soft) device placement>\n      embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n      embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n      dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n    # Creating eval graph ...\n    # Build a basic encoder\n      num_layers = 2, num_residual_layers=0\n      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n    # Trainable variables\n    Format: <name>, <shape>, <(soft) device placement>\n      embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n      embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n      dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n    # Creating infer graph ...\n    # Build a basic encoder\n      num_layers = 2, num_residual_layers=0\n      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n      decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000\n    # Trainable variables\n    Format: <name>, <shape>, <(soft) device placement>\n      embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n      embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n      dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n      dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), \n    # log_file=/tmp/nmt_attention_model/log_1548576134\n      created train model with fresh parameters, time 0.34s\n      created infer model with fresh parameters, time 0.26s\n      # 8630\n        src: 老子 不 搞\n        ref: 你 有 啥 不 高兴 的 事 ， 说 出来 让 大家 开心 一下\n        nmt: 熄灯 幹 卡则 幹 陈虹 陈虹\n      created eval model with fresh parameters, time 0.29s\n    2019-01-27 08:03:58.745213: W tensorflow/core/framework/allocator.cc:122] Allocation of 4919230464 exceeds 10% of system memory.\n    tcmalloc: large alloc 4919230464 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    2019-01-27 08:05:12.886265: W tensorflow/core/framework/allocator.cc:122] Allocation of 999218688 exceeds 10% of system memory.\n    2019-01-27 08:06:03.966427: W tensorflow/core/framework/allocator.cc:122] Allocation of 3996874752 exceeds 10% of system memory.\n    tcmalloc: large alloc 3996876800 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    2019-01-27 08:07:30.739635: W tensorflow/core/framework/allocator.cc:122] Allocation of 1255428608 exceeds 10% of system memory.\n    2019-01-27 08:07:43.000768: W tensorflow/core/framework/allocator.cc:122] Allocation of 3791906816 exceeds 10% of system memory.\n    tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 3971260416 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 4201848832 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 3996876800 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n      eval dev: perplexity 50033.89, time 906s, Sun Jan 27 08:17:23 2019.\n    tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n    tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n      eval test: perplexity 50037.16, time 815s, Sun Jan 27 08:30:59 2019.\n    2019-01-27 08:30:59.371143: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 08:30:59.371143: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 08:30:59.371386: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      created infer model with fresh parameters, time 0.22s\n    # Start step 0, lr 1, Sun Jan 27 08:30:59 2019\n    # Init train iterator, skipping 0 elements\n      step 100 lr 1 step-time 2.60s wps 0.56K ppl 17827.47 gN 21.16 bleu 0.00, Sun Jan 27 08:35:19 2019\n      step 200 lr 1 step-time 2.21s wps 0.65K ppl 976.06 gN 5.97 bleu 0.00, Sun Jan 27 08:39:00 2019\n      step 300 lr 1 step-time 2.17s wps 0.67K ppl 552.16 gN 3.62 bleu 0.00, Sun Jan 27 08:42:38 2019\n      step 400 lr 1 step-time 2.38s wps 0.66K ppl 590.66 gN 4.01 bleu 0.00, Sun Jan 27 08:46:36 2019\n      step 500 lr 1 step-time 2.30s wps 0.66K ppl 433.11 gN 3.06 bleu 0.00, Sun Jan 27 08:50:26 2019\n      step 600 lr 1 step-time 2.30s wps 0.66K ppl 347.75 gN 2.77 bleu 0.00, Sun Jan 27 08:54:15 2019\n      step 700 lr 1 step-time 2.32s wps 0.66K ppl 314.29 gN 2.56 bleu 0.00, Sun Jan 27 08:58:07 2019\n      step 800 lr 1 step-time 2.21s wps 0.66K ppl 246.35 gN 2.13 bleu 0.00, Sun Jan 27 09:01:48 2019\n      step 900 lr 1 step-time 2.21s wps 0.65K ppl 224.77 gN 2.48 bleu 0.00, Sun Jan 27 09:05:29 2019\n      step 1000 lr 1 step-time 2.33s wps 0.66K ppl 223.08 gN 2.10 bleu 0.00, Sun Jan 27 09:09:22 2019\n    # Save eval, global step 1000\n    2019-01-27 09:09:23.307011: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 09:09:23.307011: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 09:09:23.307277: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.11s\n      # 41510\n        src: 说个 傅里叶 变换\n        ref: =   =\n        nmt: <unk> <unk> <unk> <unk> <unk> <unk>\n    2019-01-27 09:09:23.459672: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 09:09:23.459672: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 09:09:23.460115: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.12s\n    tcmalloc: large alloc 4919230464 bytes == 0x5241a000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n      eval dev: perplexity 328.47, time 897s, Sun Jan 27 09:24:21 2019.\n      eval test: perplexity 436.29, time 812s, Sun Jan 27 09:37:54 2019.\n      step 1100 lr 1 step-time 2.27s wps 0.65K ppl 208.46 gN 2.07 bleu 0.00, Sun Jan 27 09:41:41 2019\n      step 1200 lr 1 step-time 2.26s wps 0.65K ppl 187.18 gN 2.13 bleu 0.00, Sun Jan 27 09:45:27 2019\n      step 1300 lr 1 step-time 2.36s wps 0.66K ppl 208.60 gN 2.36 bleu 0.00, Sun Jan 27 09:49:23 2019\n      step 1400 lr 1 step-time 2.31s wps 0.65K ppl 179.46 gN 2.10 bleu 0.00, Sun Jan 27 09:53:14 2019\n      step 1500 lr 1 step-time 2.24s wps 0.65K ppl 162.42 gN 2.01 bleu 0.00, Sun Jan 27 09:56:58 2019\n      step 1600 lr 1 step-time 2.45s wps 0.66K ppl 203.35 gN 2.22 bleu 0.00, Sun Jan 27 10:01:03 2019\n      step 1700 lr 1 step-time 2.32s wps 0.65K ppl 157.10 gN 1.97 bleu 0.00, Sun Jan 27 10:04:55 2019\n      step 1800 lr 1 step-time 2.31s wps 0.66K ppl 165.35 gN 2.15 bleu 0.00, Sun Jan 27 10:08:46 2019\n      step 1900 lr 1 step-time 2.31s wps 0.66K ppl 151.73 gN 2.09 bleu 0.00, Sun Jan 27 10:12:37 2019\n      step 2000 lr 1 step-time 2.26s wps 0.66K ppl 133.99 gN 1.79 bleu 0.00, Sun Jan 27 10:16:23 2019\n    # Save eval, global step 2000\n    2019-01-27 10:16:23.894014: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 10:16:23.894014: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 10:16:23.894265: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.10s\n      # 51767\n        src: 讲个 黄色笑话 呗\n        ref: 二\n        nmt: <unk> <unk> <unk>\n    2019-01-27 10:16:24.020441: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 10:16:24.020441: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n    2019-01-27 10:16:24.020711: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n      loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.11s\n      eval dev: perplexity 133.70, time 901s, Sun Jan 27 10:31:25 2019.\n    \n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/3Text_Representation/Chapter1_Basic_Text_Representation/Representation_of_words_sentences/","content":"\n# 第3门：文本表示\n## 第1章：文本词与句的表示\n\n\n\n## 1.文本表示概述\n\n文本表示，简单的说就是不将文本视为字符串，而视为在**数学上处理起来更为方便的向量**。而怎么把字符串变为向量，就是文本表示的核心问题。\n\n### 1.1 为什么要进行文本表示\n1. 根本原因是计算机不方便直接对文本字符串进行处理，因此需要进行数值化或者向量化。\n2. 便于机器学习。不仅传统的机器学习算法需要这个过程，深度学习也需要这个过程。\n3. 良好的文本表示形式可以极大的提升算法效果。\n\n### 1.2 文本表示分类（基于粒度）\n* 文本表示\n* 句子表示（短文本）\n* 词表示\n\n### 1.3 文本表示分类（基于表示方法）\n* 离散表示\n * one-hot表示\n * multi-hot表示\n* 分布式表示\n * 基于矩阵\n   * 基于降维的方法\n   * 基于聚类的方法\n * 基于神经网络\n   * CBOW\n   * Skip-gram\n   * NNLM\n   * C&W\n\n## 2. 文本离散表示：词袋模型与TF-IDF\n\n\n### 2.1 最简单的文本表示：词袋子模型（bag of words）\n\n词袋子模型是一种非常经典的文本表示。顾名思义，它就是将字符串视为一个 **“装满字符（词）的袋子”** ，袋子里的 **词语是随便摆放的**。而两个词袋子的相似程度就以它们重合的词及其相关分布进行判断。\n\n![图片](http://ww1.sinaimg.cn/large/b57cc2efly1fxcjker3m9j205k07kq3u.jpg)\n\n举个例子，对于句子：\n>“我们这些傻傻的路痴走啊走，好不容易找到了饭店的西门”。\n\n我们先进行**分词**，将所有出现的词储存为一个词表。然后依据 **“词语是否出现在词表中”** 可以将这句话变为这样的向量：\n\n> [1,0,1,1,1,0,0,1,…]\n\n> 词表：[我们，你们，走，西门，的，吃饭，旅游，找到了,...]\n\n其中向量的**每个维度唯一对应着词表中的一个词**。可见这个向量的大部分位置是0值，这种情况叫作**“稀疏”**。为了减少存储空间，我们也可以只储存非零值的位置。\n\n在实际应用中，这种方法非常常用。\n\n#### 2.1.1 词袋子模型的优点\n\n1. 简单，方便，快速\n2. 在语料充足的前提下，对于简单的自然语言处理任务效果不错。如文本分类。\n\n#### 2.1.2 词袋子模型的缺点\n1. 其准确率往往比较低。凡是出现在文本中的词一视同仁，不能体现不同词在一句话中的不同的重要性。\n2. **无法关注词语之间的顺序关系，这是词袋子模型最大的缺点**。如“武松打老虎”跟“老虎打武松”在词袋子模型中是认为一样的。\n\n### 2.2 对词袋子模型的改进：TF-IDF\n\n#### 2.2.1 不仅考虑词语是否出现，还考虑其出现的次数或者频率（TF）\n> [1,0,2,1,2,0,0,1,…]\n\n> 词表：[我们，你们，走，西门，的，吃饭，旅游，找到了,...]\n\n“的”这个次占了词频的很大的比重，而它对确定文本信息几乎没什么用。所以我们应该忽略掉这些词，取消掉它们的影响。一种方法是维护一个停用词表。但这种方式太粗暴。\n\n改进方式：一个词预测主题的能力越强（与主题的关联程度），权重越大，反之，权重越小。在网页中看到“原子能”这个词，或多或少能够了解网页的主题，而看到“应用”一词，则对主题基本上还是一无所知。因此，“原子能”的权重应该比应用大。\n容易发现，如果一个关键词只在很少的网页出现，通过它就容易锁定搜索目标，它的权重也就应该比较大。反正，如果一个词在大量的网页中出现，看到它仍然不清楚要找什么内容，因此它的权重应该小。（比如你在搜索“python gensim”，“python”这个关键词会在很多的网页中出现，内容可能是python入门介绍，python官网，python应用，而“gensim”却只会在相对比较少的网页中出现，一般所以gensim的官网，gensim的安装教程，gensim的学习笔记等，而后者是我们更倾向于看到的内容）。\n\n#### 2.2.2 统计逆文档频率——**IDF**\n\n不仅考虑这个词在当下文本的出现的概率，还考虑出现该词语的文档占总文档出现的频率（DF）。其基本假设是**如果一个词语在不同的文档中反复出现，那么它对于识别该文本并不重要**。如高频词“我们”、“那么”之类。\n\n严格来说，逆文档频率的公式为-log(出现该词语的文档占总文档出现的频率)\n\n如关键字“python”在10万个网页中出现，而“gensim”只在1000个网页中出现，那么“gensim”的权重就会比“python”多，这样搜索出来的结果就与你想要的结果越贴近。比如，假定中文网页数是=10亿，停止词的在所有的网页中都出现，即D=10亿，那么它的IDF = log(10亿 / 10亿) = log(1) =0。假如专用词“原子能”在两百万个网页中出现，即Dw=200万，则它的权重IDF=log(500) =2.7。又假定通用词“应用”，出现在五亿个网页中，它的权重IDF = log(2)则只有 0.3。\n\n![图片](http://ww1.sinaimg.cn/large/b57cc2efly1fxcjkg43yxj218g0pktow.jpg)\n\nTF-IDF的概念被公认为信息检索中最重要的发明。在搜索，文献分类，与其他相关领域有广泛的应用。\n\n\n\n## 3.文本分布式表示：word2vec\n\n* **参考笔记**：[斯坦福cs224d Lecture 1](https://blog.csdn.net/longxinchen_ml/article/details/51567960)\n\n### 3.1 词向量的one-hot表示\n我们拿英文举例。\n\n英语中大约有1300万个词组（token，自定义字符串，译作词组），不过他们全部是独立的吗？并不是哦，比如有一些词组，“Feline猫科动物”和“Cat猫”，“Hotel宾馆“和”Motel汽车旅馆”，其实有一定的关联或者相似性在。因此，我们希望用词向量编码词组，使它代表在词组的N维空间中的一个点（而点与点之间有距离的远近等关系，可以体现深层一点的信息）。每一个词向量的维度都可能会表征一些意义（物理含义），这些意义我们用“声明speech”来定义。例如，语义维度可以用来表明时态（过去与现在与未来），计数（单数与复数），和性别（男性与女性）。\n\n说起来，词向量的编码方式其实挺有讲究的。咱们从最简单的看起，最简单的编码方式叫做one-hot vector：假设我们的词库总共有n个词，那我们开一个1*n的高维向量，而每个词都会在某个索引index下取到1，其余位置全部都取值为0.词向量在这种类型的编码中如下图所示：\n$$ w^{aardcark}=\n\\begin{bmatrix}\n     1  \\\\\n     0  \\\\\n     0 \\\\\n     \\vdots \\\\\n     0\n\\end{bmatrix} ,\nw^{a}=\n\\begin{bmatrix}\n     0  \\\\\n     1  \\\\\n     0 \\\\\n     \\vdots \\\\\n     0\n\\end{bmatrix}\nw^{at}=\n\\begin{bmatrix}\n     0  \\\\\n     0  \\\\\n     1 \\\\\n     \\vdots \\\\\n     0\n\\end{bmatrix}\n\\cdots \\\\\nw^{zebra}=\n\\begin{bmatrix}\n     0  \\\\\n     0  \\\\\n     0 \\\\\n     \\vdots \\\\\n     1\n\\end{bmatrix}\n$$\n这种词向量编码方式简单粗暴，我们将每一个词作为一个完全独立的个体来表达。遗憾的是，这种方式下，我们的词向量没办法给我们任何形式的词组相似性权衡。例如:\n$$(w^{hotel})^Tw^{motel}=(w^{hotel})^Tw^{cat}=0$$\n（注：这里$W^{-1}$是$W$的逆矩阵，它们有关系：$W^{-1}*W=1$，注意到hotel和motel是近义词）\n\n究其根本你会发现，是你开了一个极高维度的空间，然后每个词语都会占据一个维度，因此没有办法在空间中关联起来。因此我们可能可以把词向量的维度降低一些，在这样一个子空间中，可能原本没有关联的词就关联起来了。\n\n### 3.2 基于SVD降维的表示方法\n这是一种构造词嵌入（即词向量）的方法，我们首先会遍历所有的文本数据集，然后统计词出现的次数，接着用一个矩阵$X$来表示所有的次数情况，紧接着对X进行奇异值分解得到一个$USV^T$的分解。然后用$U$的行（rows）作为所有词表中词的词向量。对于矩阵$X$，我们有几种选择，咱们一起来比较一下。\n\n#### 3.2.1 词-文档矩阵\n最初的想法是，我们猜测相互关联的词组同时出现在相同的文件中的概率很高。例如，“银行”、“债券”、“股票”、“钱”等都可能出现在一起。但是，“银行”、“章鱼”、“香蕉”和“曲棍球”可能不会一直一起出现。基于这个想法，我们建立一个词组文档矩阵$X$，具体是这么做的：遍历海量的文件，每次词组i出现在文件j中时，将$X_{ij}$的值加1。不过大家可想而知，这会是个很大的矩阵$R^{|V| ×M}$，而且矩阵大小还和文档个数M有关系。所以咱们最好想办法处理和优化一下。\n\n#### 3.2.2 基于窗口的共现矩阵X\n我们还是用一样的逻辑，不过换一种统计方式，把矩阵$X$记录的词频变成一个相关性矩阵。我们先规定一个固定大小的窗口，然后统计每个词出现在窗口中次数，这个计数是针对整个语料集做的。可能说得有点含糊，咱们一起来看个例子，假定我们有如下的3个句子，同时我们的窗口大小设定为1（把原始的句子分拆成一个一个的词）：\n1. I enjoy flying.\n2. I like NLP.\n3. I like deep learning.\n由此产生的计数矩阵如下：\n\n![](./img/Co-occurrence_matrix.png)\n\n然后我们对X做奇异值分解，观察观察奇异值（矩阵的对角元素），并根据我们期待保留的百分比来进行截断（只保留前k个维度）：\n\n![](./img/Energy_ratio.png)\n\n\n然后我们把子矩阵$U_{1:|V|,1:k}$视作我们的词嵌入矩阵。也就是说，对于词表中的每一个词，我们都用一个k维的向量来表达了。\n\n对X采用奇异值分解\n\n\n![](https://pic4.zhimg.com/80/a01cce7a35161a838df99a655c00a823_hd.png)\n\n\n通过选择前K个奇异向量来进行降维：\n\n\n![](https://pic4.zhimg.com/80/44da818d314a1be5b33f8c9c9f9dab33_hd.png)\n\n\n这两种方法都能产生词向量，它们能够充分地编码语义和句法的信息，但同时也带来了其他的问题：\n\n* 矩阵的维度会经常变化（新的词语经常会增加，语料库的大小也会随时变化）。\n* 矩阵是非常稀疏的，因为大多数词并不同时出现。\n* 矩阵的维度通常非常高（$≈10^6×10^6$）\n* 训练需要$O(n^2)$的复杂度（比如SVD）\n* 需要专门对矩阵X进行特殊处理，以应对词组频率的极度不平衡的状况\n\n当然，有一些办法可以缓解一下上述提到的问题：\n\n* 忽视诸如“he”、“the” 、“has”等功能词。\n* 应用“倾斜窗口”（ramp window），即:根据文件中词组之间的距离给它们的共现次数增加相应的权重。\n* 使用皮尔森的相关性（Pearson correlation），将0记为负数，而不是它原来的数值。\n\n不过缓解终归只是缓解，咱们需要更合理地解决这些问题，这也就是我们马上要提到的基于神经网络的方法。\n\n### 3.3 基于神经网络的表示方法\n现在我们退后一步，来尝试一种新的方法。在这里我们并不计算和存储全局信息，因为这会包含太多大型数据集和数十亿句子。我们尝试创建一个模型，它能够一步步迭代地进行学习，并最终得出每个单词基于其上下文的条件概率。\n\n```\n词语的上下文：\n一个词语的上下文是它周围C个词以内的词。如果C=2，句子\"The quick brown fox jumped over the lazy dog\"中单词\"fox\"的上下文为 {\"quick\", \"brown\", \"jumped\", \"over\"}.\n```\n\n我们想建立一个概率模型，它包含已知和未知参数。每增加一个训练样本，它就能从模型的输入、输出和期望输出（标签），多学到一点点未知参数的信息。\n\n在每次迭代过程中，这个模型都能够评估其误差，并按照一定的更新规则，惩罚那些导致误差的参数。这种想法可以追溯到1986年（Learning representations by back-propagating errors. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams (1988)），我们称之为误差“反向传播”法。\n\n#### 3.3.1 语言模型（1-gram,2-gram等等）\n首先，我们需要建立一个能给“分词序列”分配概率的模型。我们从一个例子开始：\n\n`\"The cat jumped over the puddle.\"（猫 跳 过 水坑）`\n\n一个好的语言模型会给这句话以很高的概率，因为这是一个在语法和语义上完全有效的句子。同样地，这句\"stock boil fish is toy\"（股票 煮 鱼 是 玩具）就应该有一个非常低的概率 ，因为它是没有任何意义的。在数学上，我们可以令任意给定的n个有序的分词序列的概率为：\n $$P(w_1,w_2,w_3...w_n)$$\n我们可以采用一元语言模型。它假定词语的出现是完全独立的，于是可以将整个概率拆开相乘：\n $$P(w_1,w_2,w_3...w_n)=\\prod_{i=1}^NP(w_i)$$\n看到这里，肯定很多同学就要喷了，这不对，词和词之间没有关联吗？确实，我们知道一句话中每一个词语都跟它前面的词语有很强的依赖关系，忽略这一点的话，一些完全无意义的句子，可能会有很高的概率。咱们稍微改一改，让一个词语的概率依赖于它前面一个词语。我们将这种模型称作bigram（2-gram，二元语言模型），表示为：\n $$P(w_1,w_2,w_3...w_n)=\\prod_{i=2}^NP(w_i|w_{i-1})$$\n看起来还是有点简单？恩，也对，我们只考虑一个词语依赖于其相邻的一个词语的关系，而不是考虑其依赖整个句子的情况。别着急，接下来将会看到，这种方法能让我们有非常显著的进步。考虑到前面 “词-词”矩阵的情况，我们至少可以算出两个词语共同出现的概率。但是，旧话重提，这仍然要求储存和计算一个非常的大数据集里面的全部信息。\n现在我们理解了“分词序列”的概率（其实就是N-gram语言模型啦），让我们观察一些能够学习到这些概率的例子。\n\n#### 3.3.2 连续词袋模型（CBOW）\n有种模型是以{\"The\", \"cat\", ’over\", \"the’, \"puddle\"}为上下文，能够预测或产生它们中心的词语\"jumped\"，叫做连续词袋模型。\n\n上面是最粗粒度的描述，咱们来深入一点点，看点细节。\n\n首先，我们要建立模型的一些已知参数。它们就是将句子表示为一些one-hot向量，作为模型的输入，咱们记为x(c)吧。模型的输出记为y(c)吧。因为连续词袋模型只有一个输出，所以其实我们只需记录它为y。在我们上面举的例子中，y就是我们已经知道的（有标签的）中心词（如本例中的\"jumped\"）。\n\n好了，已知参数有了，现在我们一起来定义模型中的未知参数。我们建立两矩阵，$V\\in R^{n*|V|}$和$U\\in R^{|V|*n}$ 。其中的n是可以任意指定的，它用来定义我们“嵌入空间”（embedding space）的维度。V是输入词矩阵。当词语$w_i$（译注：$w_i$是只有第i维是1其他维是0的one-hot向量）作为模型的一个输入的时候，V的第i列就是它的n维“嵌入向量”（embedded vector）。我们将V的这一列表示为$v_i$。类似的，U是输出矩阵。当$w_j$作为模型输出的时候，U的第j行就是它的n维“嵌入向量”。我们将U的这一行表示为$u_j$。要注意我们实际上对于每个词语$w_i$学习了两个向量。（作为输入词的向量$v_i$，和作为输出词的向量$u_j$）。\n\n连续词袋模型（CBOW）中的各个记号：\n\n* $w_i$:单词表V中的第i个单词\n* $v\\in R^{n*|V|}$：输入词矩阵\n* $v_i$：V的第i列，单词$w_i$的输入向量\n* $u\\in R^{|V|*n}$：输出词矩阵\n* $u_i$：U的第i行，单词$w_i$的输出向量\n\n那这个模型是如何运作的呢？我们把整个过程拆分成以下几步：\n\n1.\t对于m个词长度的输入上下文，我们产生它们的one-hot向量（$x^{(c-m)},\\cdots,x^{(c-1)},x^{(c+1)},\\cdots,x^{(c+m)}$）。\n2.\t我们得到上下文的嵌入词向量（$v_{c-m+1}=Vx^{(c-m+1)},\\cdots, $v_{c+m}=Vx^{(c+m)} ）\n3.\t将这些向量取平均$\\hat v={v_{c-m}+v_{c-m+1}+\\cdots+v_{c+m}\\over2m}$\n4.\t产生一个得分向量 $z=U\\hat v$\n5.\t将得分向量转换成概率分布形式$\\hat y=softmax(z)$\n6.\t我们希望我们产生的概率分布 ,与真实概率分布$\\hat y$相匹配。而$y$刚好也就是我们期望的真实词语的one-hot向量。\n\n用一幅图来表示就是下面这个样子：\n\n![](http://i.stack.imgur.com/fYxO9.png)\n\n\n通过上面说的种种步骤，我们知道有了矩阵U、V整个过程是如何运作的，那我们怎样找到U和V呢？——我们需要有一个目标函数。通常来说，当我们试图从已知概率学习一个新的概率时，最常见的是从信息论的角度寻找方法来评估两个概率分布的差距。其中广受好评又广泛应用的一个评估差异/损失的函数是交叉熵：\n\n$$H(\\hat y,y)=-\\sum_{j=1}^{|V|}y_jlog(\\hat y_j)$$\n\n结合我们当下的例子，y只是一个one-hot向量，于是上面的损失函数就可以简化为：\n\n$$H(\\hat y,y)=-y_ilog(\\hat y_i)$$\n\n我们用c表示y这个one-hot向量取值为1的那个维度的下标。所以在我们预测为准确值的情况下$\\hat y_c =1$。于是损失为 −1 log(1) = 0。所以对于一个理想的预测值，因为预测得到的概率分布和真实概率分布完全一样，因此损失为0。现在让我们看一个相反的情况，也就是我们的预测结果非常不理想，此时$\\hat y_c =0.01$。计算得到的损失为−1 log(0.01) ≈ 4.605，损失非常大，原本这才是标准结果，可是你给了一个非常低的概率，因此会拿到一个非常大的loss 。可见交叉熵为我们提供了一个很好的衡量两个概率分布的差异的方法。于是我们最终的优化函数为：\n\n![](./img/cbow-loss.png)\n\n我们用梯度下降法去更新每一个相关的词向量$u_c$和$v_j$ 。\n\n#### 3.3.3 Skip-Gram 模型\n很上面提到的模型对应的另一种思路，是以中心的词语\"jumped\"为输入，能够预测或产生它周围的词语\"The\", \"cat\", ’over\", \"the”, \"puddle\"等。这里我们叫\"jumped\"为上下文。我们把它叫做Skip-Gram 模型。\n这个模型的建立与连续词袋模型（CBOM）非常相似，但本质上是交换了输入和输出的位置。我们令输入的one-hot向量（中心词）为x（因为它只有一个），输出向量为y(j)。U和V的定义与连续词袋模型一样。\n\nSkip-Gram 模型中的各个记号：\n\n* $w_i$:单词表V中的第i个单词\n* $v\\in R^{n*|V|}$：输入词矩阵\n* $v_i$：V的第i列，单词$w_i$的输入向量\n* $u\\in R^{|V|*n}$：输出词矩阵\n* $u_i$：U的第i行，单词$w_i$的输出向量\n\n对应到上面部分，我们可以把Skip-Gram 模型的运作方式拆分成以下几步：\n\n1.\t生成one-hot输入向量x。\n2.\t得到上下文的嵌入词向量$v_c=Vx$。\n3.\t因为这里不需要取平均值的操作，所以直接是$\\hat v=v_c$。\n4.\t通过$u=Uv_c$产生2m个得分向量$u_{c-m},\\cdots,u_{c-1},u_{c+1},\\cdots,u_{(c+m)}$。\n5.\t将得分向量转换成概率分布形式$y=softmax(u)$。\n6.\t我们希望我们产生的概率分布与真实概率分布$y^{c-m},\\cdots,y^{c-1},,y^{c+1}\\cdots,y^{c+m}$ 相匹配，也就是我们真实输出结果的one-hot向量。\n\n用一幅图来表示这个过程如下：\n\n![](http://i.stack.imgur.com/igSuE.png)\n\n像连续词袋模型一样，我们需要为模型设定一个目标/损失函数。不过不同的地方是我们这里需要引入朴素贝叶斯假设来将联合概率拆分成独立概率相乘。如果你之前不了解它，可以先跳过。这是一个非常强的条件独立性假设。也就是说只要给出了中心词，所有的输出词是完全独立的。\n\n![](./img/sg-loss.png)\n\n我们可以用随机梯度下降法去更新未知参数的梯度。\n\n#### 3.3.4 负例采样（Negative Sampling）\n我们再次观察一下目标函数，注意到对整个单词表|V|求和的计算量是非常巨大的，任何一个对目标函数的更新和求值操作都会有O(|V|)的时间复杂度。我们需要一个思路去简化一下，我们想办法去求它的近似。\n对于每一步训练，我们不去循环整个单词表，而只是抽象一些负面例子就够了！我们可以从一个噪声分布$(P_n(w))$中抽样，其概率分布与单词表中的频率相匹配。为了将描述问题的公式与负例采样相结合，我们只需要更新我们的：\n\n* 目标函数\n* 梯度\n* 更新规则\n\nMikolov ET AL.在他的《Distributed Representations of Words and Phrases and their Compositionality》中提出了负例采样。虽然负例采样是基于Skip-Gram 模型，它实际上是对一个不同的目标函数进行最优化。考虑一个“词-上下文”对（w,c），令P(D = 1|w, c)为(w, c)来自于语料库的概率。相应的，P(D = 0|w, c) 则是不来自于语料库的概率。我们首先对P(D = 1|w, c)用sigmoid函数建模：\n$$p(D=1|w,c,\\theta)= {1\\over{1+e^{(-v_c^Tv_w)}}}$$\n现在我们需要建立一个新的目标函数。如果(w, c)真是来自于语料库，目标函数能够最大化P(D = 1|w, c)。反之亦然。我们对这两个概率采用一个简单的最大似然法。（这里令θ为模型的参数，在我们的例子中，就是对应的U和V。）\n\n![](./img/sgns-loss.png)\n\n\n注意这里的$\\widetilde D$表示“错误的”或者“负面的”语料库，像句子\"stock boil fish is toy\"就是从这样的语料库来的。不自然的句子应该有比较低的发生概率，我们可以从词库中随机采样来产生这样的“负面的”语料库。我们的新目标函数就变成了：\n\n$$log\\sigma(u_(c-m+j)^T.v_c)+\\sum_{k=1}^Klog\\sigma(-\\widetilde u_k^T.v_c)$$\n\n在这里$\\{\\widetilde u_k|k=1,\\cdots,K\\}$是从(Pn(w))中抽样取到的。需要多说一句的是，虽然关于怎么样最好地近似有许多讨论和研究，但是工作效果最好的似乎是指数为3/4的一元语言模型。至于为什么是3/4，下面有几个例子来帮助大家感性地理解一下：\n\n$$is:0.9^{3/4}=0.92\\\\\nconstitution:0.09^{3/4}=0.16 \\\\\nbombastic:0.01^{3/4}=0.032$$\n\n你看，经过3/4这样一个指数处理，\"Bombastic\"(少见)被采样的概率是之前的3倍，而“is”这个词(多见)被采样的概率只是稍微增长了一点点。\n\n### 3.4 词向量的作用与获取\n\n很多高阶的深度学习自然语言处理任务，都可以用词向量作为基础。我们课程后面的很多任务，可以用预训练好的word2vec初始化。可以从[开源链接](https://github.com/Embedding/Chinese-Word-Vectors)获取。\n\n![](./img/Pre-training_word_vector.png)\n\n\n\n### 3.5 词向量为什么有用\n\n* 基于词与其他词的某种共现关系\n * sgns 与 PMI矩阵 的等价性证明, 降维版本不等价。《Neural-Word-Embeddings-as-Implicit-Matrix-Factorization》\n     * 神经网络与SVD的求解方法只是降维方式的不同\n     * 神经网络更像MF，而MF与SVD的降维的约束条件不同，神经网络的目标函数与MF的目标函数也不同\n * GloVe与MF关系更近\n     * 目标函数更像\n     * CBOW没有类似的降维矩阵对应\n* 基于语言模型\n * 词向量与语言模型本来是两个独立的NLP问题领域，因为深度学习联系在了一起。CBOW。\n * 基于词向量构建成句子向量进而进而完成语言模型的任务\n* 基于其他监督学习任务\n * 词向量并不只是语言模型可以得到，基于有监督学习也可以得到：C&W\n * 基于词向量构建成句子向量进而进而完成文本分类或文本相似度判断的任务\n\n## 4.python中文文本向量化表示\n\n* [参考代码](https://github.com/AimeeLee77/keyword_extraction)\n* [python实现tfidf](https://github.com/Jasonnor/tf-idf-python)\n\n\n```python\n#!/usr/bin/python\n# coding=utf-8\n# 采用TF-IDF方法提取文本关键词\n# http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting\nimport sys,codecs\nimport pandas as pd\nimport numpy as np\nimport jieba.posseg\nimport jieba.analyse\nfrom sklearn import feature_extraction\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\"\"\"\n       TF-IDF权重：\n           1、CountVectorizer 构建词频矩阵\n           2、TfidfTransformer 构建tfidf权值计算\n           3、文本的关键字\n           4、对应的tfidf矩阵\n\"\"\"\n# 数据预处理操作：分词，词性筛选\ndef dataPrepos(text):\n    l = []\n    pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd']  # 定义选取的词性\n    seg = jieba.posseg.cut(text)  # 分词\n    for i in seg:\n        if i.word and i.flag in pos:  # 词性筛选\n            l.append(i.word)\n    return l\n\n# tf-idf获取文本top10关键词\ndef getKeywords_tfidf(data,topK):\n    idList, titleList, abstractList = data['id'], data['title'], data['abstract']\n    corpus = [] # 将所有文档输出到一个list中，一行就是一个文档\n    for index in range(len(idList)):\n        text = '%s。%s' % (titleList[index], abstractList[index]) # 拼接标题和摘要\n        text = dataPrepos(text) # 文本预处理\n        text = \" \".join(text) # 连接成字符串，空格分隔\n        corpus.append(text)\n\n    # 1、构建词频矩阵，将文本中的词语转换成词频矩阵\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频\n    # 2、统计每个词的tf-idf权值\n    transformer = TfidfTransformer()\n    tfidf = transformer.fit_transform(X)\n    # 3、获取词袋模型中的关键词\n    word = vectorizer.get_feature_names()\n    # 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重\n    weight = tfidf.toarray()\n    # 5、打印词语权重\n    ids, titles, keys = [], [], []\n    for i in range(len(weight)):\n        print(u\"-------这里输出第\", i+1 , u\"篇文本的词语tf-idf------\")\n        ids.append(idList[i])\n        titles.append(titleList[i])\n        df_word,df_weight = [],[] # 当前文章的所有词汇列表、词汇对应权重列表\n        for j in range(len(word)):\n            print (word[j],weight[i][j])\n    \n    return corpus\n\n\ndef main():\n    # 读取数据集\n    dataFile = '/data/NLP/sample_data.csv'\n    data = pd.read_csv(dataFile)\n    # tf-idf关键词抽取\n    global corpusA\n    corpusA = getKeywords_tfidf(data,10)\n    \n\ncorpusA = []\nmain()\n```\n\n    Building prefix dict from the default dictionary ...\n    Dumping model to file cache /tmp/jieba.cache\n    Loading model cost 1.783 seconds.\n    Prefix dict has been built succesfully.\n    \n\n    -------这里输出第 1 篇文本的词语tf-idf------\n    一定 0.054830755756848815\n    一段时间 0.0\n    主体 0.0\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.0\n    交叉 0.0\n    产品 0.0\n    产生 0.09322230345997895\n    介绍 0.0\n    仍然 0.054830755756848815\n    仪表板 0.0\n    传感器 0.054830755756848815\n    估计 0.0\n    位置 0.0407792498624949\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.054830755756848815\n    使该 0.0\n    信号 0.09322230345997895\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.0\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.054830755756848815\n    利用 0.0\n    制动 0.10966151151369763\n    制造 0.0\n    刹车 0.054830755756848815\n    力矩 0.16449226727054644\n    功能 0.054830755756848815\n    包围 0.0\n    包括 0.0\n    单元 0.21932302302739526\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.10966151151369763\n    基准 0.0\n    增加 0.0\n    处于 0.054830755756848815\n    处在 0.0\n    大于 0.054830755756848815\n    大巴车 0.10966151151369763\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.054830755756848815\n    容量 0.0\n    导引 0.0\n    小于 0.10966151151369763\n    峰值 0.0\n    布置 0.0\n    座椅 0.0\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.10966151151369763\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.0\n    恢复 0.054830755756848815\n    悬挂 0.0\n    成使 0.0\n    手刹 0.054830755756848815\n    手动 0.0\n    持续 0.054830755756848815\n    接到 0.0\n    控制 0.3262780621099263\n    控制系统 0.0\n    描述 0.0\n    提供 0.0\n    摊开 0.0\n    操作 0.0\n    支承 0.0\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.16449226727054644\n    斜倚 0.0\n    方向 0.0\n    方式 0.0\n    方法 0.0\n    方面 0.0\n    旋转 0.0\n    无需 0.054830755756848815\n    时间 0.054830755756848815\n    更改 0.054830755756848815\n    最低 0.0\n    最靠近 0.0\n    本发明 0.026727743968140975\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.0\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.10966151151369763\n    检测 0.10966151151369763\n    椅背 0.0\n    模式 0.0\n    横向 0.0\n    步骤 0.0\n    气囊 0.0\n    永磁 0.3289845345410929\n    没有 0.0\n    油门 0.10966151151369763\n    泡沫 0.0\n    涉及 0.0\n    添加 0.054830755756848815\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.0\n    用于 0.0\n    电动 0.10966151151369763\n    电机 0.3838152902979417\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.054830755756848815\n    目的 0.054830755756848815\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.0\n    着横 0.0\n    硬件 0.054830755756848815\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.21932302302739526\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.026727743968140975\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.046611151729989475\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.13983345518996843\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.054830755756848815\n    计算 0.0\n    设备 0.054830755756848815\n    设定值 0.10966151151369763\n    设置 0.0407792498624949\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.10966151151369763\n    越过 0.0\n    踏板 0.21932302302739526\n    踩下 0.10966151151369763\n    车厢 0.0\n    车身 0.0\n    车辆 0.06511929167127112\n    车门 0.0\n    转速 0.16449226727054644\n    轴线 0.0\n    辅助 0.054830755756848815\n    输出 0.10966151151369763\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.10966151151369763\n    远离 0.0\n    退出 0.054830755756848815\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.054830755756848815\n    间隔 0.0\n    间隙 0.0\n    防止 0.0\n    防溜 0.3289845345410929\n    降低 0.0\n    限制 0.054830755756848815\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.0815584997249898\n    驱动器 0.0\n    驻车 0.054830755756848815\n    -------这里输出第 2 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.0\n    乘客 0.0\n    乘车 0.1407995509845537\n    事件 0.1407995509845537\n    二者 0.0\n    互相 0.0\n    交叉 0.1407995509845537\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.0\n    作为 0.1407995509845537\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.1407995509845537\n    免受 0.1407995509845537\n    关闭 0.0\n    具有 0.0686340047223278\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.1407995509845537\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.0\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.1407995509845537\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.1407995509845537\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.1407995509845537\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.1407995509845537\n    布置 0.0\n    座椅 0.0\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.11969248178082373\n    摊开 0.0\n    操作 0.0\n    支承 0.0\n    支撑 0.11969248178082373\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.11969248178082373\n    方法 0.0\n    方面 0.0\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.0\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.2815991019691074\n    条件 0.0\n    构件 0.0\n    构造 0.0\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.0\n    步骤 0.0\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.1407995509845537\n    涉及 0.0\n    添加 0.0\n    溃缩 0.5631982039382148\n    满足 0.0\n    特别 0.1407995509845537\n    状态 0.0\n    独立 0.0\n    用于 0.09310071944108866\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.0\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.1407995509845537\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.2745360188893112\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.0\n    行驶 0.0\n    表面 0.0\n    衬板 0.1407995509845537\n    装置 0.0\n    装饰 0.11969248178082373\n    覆盖 0.1407995509845537\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.1407995509845537\n    评估 0.0\n    负荷 0.1407995509845537\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.08360970864971078\n    车门 0.2815991019691074\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.11969248178082373\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.0\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 3 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.0\n    乘客 0.06978374171377333\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.0\n    交叉 0.0\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.2791349668550933\n    传感器 0.0\n    估计 0.0\n    位置 0.0\n    作为 0.0\n    使得 0.06978374171377333\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.0\n    偏压 0.2791349668550933\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.03401671116728386\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.08287808124921273\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.06978374171377333\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.06978374171377333\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.20935122514131996\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.0\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.13956748342754666\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.0\n    摊开 0.0\n    操作 0.0\n    支承 0.0\n    支撑 0.4152578912849576\n    支架 0.5582699337101866\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.06978374171377333\n    方式 0.0\n    方法 0.0\n    方面 0.0\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.03401671116728386\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.0\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.29661277948925546\n    步骤 0.0\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.0\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.0\n    用于 0.04614302043872387\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.06978374171377333\n    相交 0.0\n    相关联 0.0\n    相对 0.0\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.2791349668550933\n    策略 0.0\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.10205013350185158\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.0\n    行驶 0.0\n    表面 0.06978374171377333\n    衬板 0.0\n    装置 0.0\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.06978374171377333\n    车身 0.13956748342754666\n    车辆 0.08287808124921273\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.13956748342754666\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.0\n    配置 0.06978374171377333\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.0\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.13956748342754666\n    首先 0.0\n    驱动 0.0\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 4 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.0\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.06773943981051334\n    互相 0.0\n    交叉 0.0\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.10075963766862049\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.03302019785810716\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.06773943981051334\n    包括 0.08045018307299812\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.06773943981051334\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.4741760786735933\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.3455082750762128\n    延伸 0.057584712512702134\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.11516942502540427\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.0\n    摊开 0.0\n    操作 0.0\n    支承 0.1727541375381064\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.0\n    方法 0.0\n    方面 0.0\n    旋转 0.057584712512702134\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.0\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.5419155184841067\n    构造 0.0\n    枢转 0.0\n    枢转地 0.13547887962102667\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.0\n    步骤 0.0\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.0\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.0\n    用于 0.0\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.0\n    着横 0.06773943981051334\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.06773943981051334\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.0990605935743215\n    继续 0.0\n    耦接 0.0\n    联接 0.27095775924205334\n    膨胀 0.0\n    自动 0.0\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.0\n    装饰 0.057584712512702134\n    覆盖 0.0\n    角度 0.13547887962102667\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.04022509153649906\n    车门 0.0\n    转速 0.0\n    轴线 0.06773943981051334\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.06773943981051334\n    闭环控制 0.0\n    间隔 0.06773943981051334\n    间隙 0.0\n    防止 0.0\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.3386971990525667\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.0\n    驱动器 0.13547887962102667\n    驻车 0.0\n    -------这里输出第 5 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.06340310784873436\n    主体 0.0\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.0\n    交叉 0.0\n    产品 0.0\n    产生 0.10779686835599028\n    介绍 0.06340310784873436\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.06340310784873436\n    位置 0.0\n    作为 0.0\n    使得 0.0\n    使用 0.06340310784873436\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.06340310784873436\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.03090641392725522\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.06340310784873436\n    分部 0.25361243139493744\n    分隔 0.0\n    切换 0.0\n    利用 0.12680621569746872\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.03765008721725557\n    单元 0.0\n    卸载 0.0\n    参数 0.12680621569746872\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.06340310784873436\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.12680621569746872\n    实现 0.0\n    容量 0.25361243139493744\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.0\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.25361243139493744\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.16169530253398542\n    控制系统 0.12680621569746872\n    描述 0.0\n    提供 0.0\n    摊开 0.0\n    操作 0.04715476088799479\n    支承 0.0\n    支撑 0.0\n    支架 0.0\n    放电 0.06340310784873436\n    数值 0.12680621569746872\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.0\n    方法 0.12577188451756513\n    方面 0.06340310784873436\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.06340310784873436\n    最靠近 0.0\n    本发明 0.03090641392725522\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.10779686835599028\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.0\n    步骤 0.0\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.05389843417799514\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.12680621569746872\n    用于 0.20961980752927517\n    电动 0.0\n    电机 0.0\n    电池 0.5072248627898749\n    电池组 0.06340310784873436\n    电能 0.06340310784873436\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.12680621569746872\n    相对 0.14146428266398436\n    着横 0.0\n    硬件 0.0\n    确定 0.12680621569746872\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.4243928479919531\n    纵向 0.0\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.0\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.0\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.0\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.1902093235462031\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.0\n    评估 0.12680621569746872\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.0\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.05389843417799514\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.0\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 6 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.17677438626880906\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.08838719313440453\n    交叉 0.0\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.0\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.10397375775779942\n    信号 0.0\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.05068294132365404\n    内燃机 0.0\n    内管 0.20794751551559884\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.20794751551559884\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.0\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.0\n    延伸 0.08838719313440453\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.10397375775779942\n    总和 0.0\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.0\n    摊开 0.0\n    操作 0.0\n    支承 0.0\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.0\n    方法 0.0\n    方面 0.0\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.05068294132365404\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.0\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.17677438626880906\n    步骤 0.0\n    气囊 0.17677438626880906\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.0\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.0\n    用于 0.0\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.10397375775779942\n    相关联 0.0\n    相对 0.0\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.6187103519408317\n    结合 0.35354877253761813\n    结合部 0.31192127327339825\n    结构 0.2534147066182702\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.20794751551559884\n    自动 0.0\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.17677438626880906\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.07732834954072673\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.10397375775779942\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.0\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.07732834954072673\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.0\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.0\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 7 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.28055482201972454\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.07013870550493113\n    交叉 0.0\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.0\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.0\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.16501451210303722\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.0\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.16501451210303722\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.0\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.08250725605151861\n    形成 0.0\n    总和 0.0\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.0\n    摊开 0.08250725605151861\n    操作 0.0\n    支承 0.0\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.07013870550493113\n    方法 0.0545561746738041\n    方面 0.0\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.04021890241743363\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.0\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.0\n    步骤 0.21041611651479342\n    气囊 0.14027741100986227\n    永磁 0.0\n    没有 0.08250725605151861\n    油门 0.0\n    泡沫 0.0\n    涉及 0.0\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.07013870550493113\n    独立 0.0\n    用于 0.0\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.0\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.6312483495443802\n    结合 0.49097093853451795\n    结合部 0.0\n    结构 0.20109451208716816\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.0\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.0\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.0\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.08250725605151861\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.16501451210303722\n    部分 0.12272615846895224\n    部向 0.0\n    配置 0.0\n    重叠 0.16501451210303722\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.0\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.08250725605151861\n    面板 0.0\n    首先 0.0\n    驱动 0.0\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 8 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.0\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.0\n    交叉 0.0\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.22847664899512088\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.0\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.0\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.03648482401259563\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.0\n    变型 0.0\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.06144079343998167\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.12288158687996334\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.05223028765355313\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.10446057530710626\n    恢复 0.0\n    悬挂 0.24576317375992668\n    成使 0.0\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.0\n    摊开 0.0\n    操作 0.09139065959804835\n    支承 0.1566908629606594\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.12288158687996334\n    方向 0.0\n    方式 0.0\n    方法 0.0\n    方面 0.0\n    旋转 0.05223028765355313\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.0\n    机动 0.06144079343998167\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.0\n    枢转 0.12288158687996334\n    枢转地 0.0\n    枢轴 0.36864476063989005\n    档位 0.0\n    检测 0.0\n    椅背 0.6758487278397984\n    模式 0.0\n    横向 0.0\n    步骤 0.0\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.0\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.0\n    用于 0.0\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.09139065959804835\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.12288158687996334\n    端部 0.0\n    策略 0.0\n    系统 0.04569532979902417\n    纵向 0.0\n    组件 0.24576317375992668\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.17969919694840006\n    继续 0.0\n    耦接 0.18432238031994502\n    联接 0.0\n    膨胀 0.0\n    自动 0.10446057530710626\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.0\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.04569532979902417\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.03648482401259563\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.06144079343998167\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.06144079343998167\n    防止 0.0\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.12288158687996334\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.04569532979902417\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 9 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.0\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.0\n    交叉 0.0\n    产品 0.0\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.0\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.07308042017969112\n    倾斜 0.0\n    偏压 0.0\n    停转 0.17193544981221315\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.25790317471831975\n    具有 0.08381147803263639\n    内燃机 0.5158063494366395\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.0\n    单元 0.0\n    卸载 0.0\n    参数 0.0\n    发生器 0.17193544981221315\n    变型 0.0\n    变速器 0.17193544981221315\n    启动 0.25790317471831975\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.0\n    座椅 0.0\n    延伸 0.0\n    开启 0.08596772490610657\n    开始 0.17193544981221315\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.0\n    手刹 0.0\n    手动 0.17193544981221315\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.08596772490610657\n    提供 0.0\n    摊开 0.0\n    操作 0.06393673196121238\n    支承 0.0\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.0\n    方法 0.28422168186992836\n    方面 0.0\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.0\n    本发明 0.041905739016318194\n    机动 0.0\n    机动车 0.3438708996244263\n    机动车辆 0.0\n    条件 0.08596772490610657\n    构件 0.0\n    构造 0.0\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.08596772490610657\n    横向 0.0\n    步骤 0.07308042017969112\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.0\n    添加 0.0\n    溃缩 0.0\n    满足 0.08596772490610657\n    特别 0.0\n    状态 0.14616084035938223\n    独立 0.0\n    用于 0.11368867274797136\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.08596772490610657\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.0\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.08596772490610657\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.0\n    纵向 0.0\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.0\n    继续 0.08596772490610657\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.0\n    行驶 0.25790317471831975\n    表面 0.0\n    衬板 0.0\n    装置 0.0\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.0\n    货箱 0.0\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.0\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.17193544981221315\n    运行 0.08596772490610657\n    运输工具 0.0\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.08596772490610657\n    邻接 0.0\n    邻近 0.0\n    部上 0.0\n    部分 0.0\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.08596772490610657\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.0\n    防溜 0.0\n    降低 0.0\n    限制 0.0\n    限定 0.0\n    随后 0.08596772490610657\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.08596772490610657\n    驱动 0.06393673196121238\n    驱动器 0.0\n    驻车 0.0\n    -------这里输出第 10 篇文本的词语tf-idf------\n    一定 0.0\n    一段时间 0.0\n    主体 0.0\n    乘客 0.0\n    乘车 0.0\n    事件 0.0\n    二者 0.0\n    互相 0.0\n    交叉 0.0\n    产品 0.04425175534721659\n    产生 0.0\n    介绍 0.0\n    仍然 0.0\n    仪表板 0.0\n    传感器 0.0\n    估计 0.0\n    位置 0.0\n    作为 0.0\n    使得 0.0\n    使用 0.0\n    使能 0.0\n    使该 0.0\n    信号 0.0\n    倾斜 0.5752728195138157\n    偏压 0.0\n    停转 0.0\n    储能 0.0\n    元件 0.0\n    免受 0.0\n    关闭 0.0\n    具有 0.10785457639013879\n    内燃机 0.0\n    内管 0.0\n    凹陷 0.0\n    分析 0.0\n    分部 0.0\n    分隔 0.0\n    切换 0.0\n    利用 0.0\n    制动 0.0\n    制造 0.0\n    刹车 0.0\n    力矩 0.0\n    功能 0.0\n    包围 0.0\n    包括 0.18394330394970265\n    单元 0.0\n    卸载 0.13275526604164978\n    参数 0.0\n    发生器 0.0\n    变型 0.08850351069443319\n    变速器 0.0\n    启动 0.0\n    吸收 0.0\n    命令 0.0\n    围绕 0.0\n    地向 0.0\n    坡道 0.0\n    基准 0.0\n    增加 0.0\n    处于 0.0\n    处在 0.0\n    大于 0.0\n    大巴车 0.0\n    头枕 0.0\n    安排 0.0\n    安装 0.0\n    定位 0.0\n    实施 0.0\n    实现 0.0\n    容量 0.0\n    导引 0.0\n    小于 0.0\n    峰值 0.0\n    布置 0.08850351069443319\n    座椅 0.0\n    延伸 0.0\n    开启 0.0\n    开始 0.0\n    开度 0.0\n    引到 0.0\n    形成 0.0\n    总和 0.0\n    总成 0.0\n    恢复 0.0\n    悬挂 0.0\n    成使 0.08850351069443319\n    手刹 0.0\n    手动 0.0\n    持续 0.0\n    接到 0.0\n    控制 0.0\n    控制系统 0.0\n    描述 0.0\n    提供 0.03761803488455194\n    摊开 0.0\n    操作 0.0\n    支承 0.0\n    支撑 0.0\n    支架 0.0\n    放电 0.0\n    数值 0.0\n    整车 0.0\n    斜倚 0.0\n    方向 0.0\n    方式 0.0\n    方法 0.02926053549566272\n    方面 0.0\n    旋转 0.0\n    无需 0.0\n    时间 0.0\n    更改 0.0\n    最低 0.0\n    最靠近 0.22125877673608296\n    本发明 0.021570915278027757\n    机动 0.0\n    机动车 0.0\n    机动车辆 0.0\n    条件 0.0\n    构件 0.0\n    构造 0.07523606976910388\n    枢转 0.0\n    枢转地 0.0\n    枢轴 0.0\n    档位 0.0\n    检测 0.0\n    椅背 0.0\n    模式 0.0\n    横向 0.0\n    步骤 0.0\n    气囊 0.0\n    永磁 0.0\n    没有 0.0\n    油门 0.0\n    泡沫 0.0\n    涉及 0.03761803488455194\n    添加 0.0\n    溃缩 0.0\n    满足 0.0\n    特别 0.0\n    状态 0.0\n    独立 0.0\n    用于 0.0\n    电动 0.0\n    电机 0.0\n    电池 0.0\n    电池组 0.0\n    电能 0.0\n    监测 0.0\n    目标 0.0\n    目的 0.0\n    直到 0.0\n    相交 0.0\n    相关联 0.0\n    相对 0.19746801187573307\n    着横 0.0\n    硬件 0.0\n    确定 0.0\n    碰撞 0.0\n    移动 0.0\n    空挡 0.0\n    竖直 0.0\n    端部 0.0\n    策略 0.0\n    系统 0.06582267062524436\n    纵向 0.35401404277773274\n    组件 0.0\n    织物 0.0\n    结合 0.0\n    结合部 0.0\n    结构 0.0\n    继续 0.0\n    耦接 0.0\n    联接 0.0\n    膨胀 0.0\n    自动 0.0\n    行驶 0.0\n    表面 0.0\n    衬板 0.0\n    装置 0.0\n    装饰 0.0\n    覆盖 0.0\n    角度 0.0\n    触发 0.0\n    计算 0.0\n    设备 0.0\n    设定值 0.0\n    设置 0.0\n    设计 0.0\n    评估 0.0\n    负荷 0.0\n    货物 0.08850351069443319\n    货箱 0.4425175534721659\n    起步 0.0\n    越过 0.0\n    踏板 0.0\n    踩下 0.0\n    车厢 0.0\n    车身 0.0\n    车辆 0.0\n    车门 0.0\n    转速 0.0\n    轴线 0.0\n    辅助 0.0\n    输出 0.0\n    过程 0.0\n    运行 0.0\n    运输工具 0.17700702138886637\n    进入 0.0\n    远离 0.0\n    退出 0.0\n    速度 0.0\n    邻接 0.0\n    邻近 0.04425175534721659\n    部上 0.0\n    部分 0.32911335312622175\n    部向 0.0\n    配置 0.0\n    重叠 0.0\n    重启 0.0\n    铰接 0.0\n    闭环控制 0.0\n    间隔 0.0\n    间隙 0.0\n    防止 0.0\n    防溜 0.0\n    降低 0.08850351069443319\n    限制 0.0\n    限定 0.0\n    随后 0.0\n    靠背 0.0\n    靠近 0.0\n    面板 0.0\n    首先 0.0\n    驱动 0.0\n    驱动器 0.0\n    驻车 0.0\n    \n\n## 5.基于gensim的中文文本词向量训练与相似度匹配\n\n\n\n```python\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\n\n```\n\n\n```python\ncorpusA = [i.split(\" \") for i in corpusA]\n```\n\n\n```python\ncorpusA\n```\n\n\n\n\n    [['永磁',\n      '电机',\n      '驱动',\n      '纯',\n      '电动',\n      '大巴车',\n      '坡道',\n      '起步',\n      '防溜',\n      '策略',\n      '本发明',\n      '永磁',\n      '电机',\n      '驱动',\n      '纯',\n      '电动',\n      '大巴车',\n      '坡道',\n      '起步',\n      '防溜',\n      '策略',\n      '即',\n      '策略',\n      '制动',\n      '踏板',\n      '已',\n      '踩下',\n      '永磁',\n      '电机',\n      '转速',\n      '小于',\n      '设定值',\n      '持续',\n      '一定',\n      '时间',\n      '整车',\n      '控制',\n      '单元',\n      '产生',\n      '刹车',\n      '触发',\n      '信号',\n      '油门',\n      '踏板',\n      '开度',\n      '小于',\n      '设定值',\n      '档位',\n      '装置',\n      '时',\n      '电机',\n      '控制',\n      '单元',\n      '产生',\n      '防溜',\n      '功能',\n      '使能',\n      '信号',\n      '自动',\n      '进入',\n      '防溜',\n      '控制',\n      '使',\n      '永磁',\n      '电机',\n      '进入',\n      '转速',\n      '闭环控制',\n      '目标',\n      '转速',\n      '整车',\n      '控制',\n      '单元',\n      '检测',\n      '到',\n      '制动',\n      '踏板',\n      '仍然',\n      '踩下',\n      '则',\n      '限制',\n      '永磁',\n      '电机',\n      '输出',\n      '力矩',\n      '恢复',\n      '永磁',\n      '电机',\n      '输出',\n      '力矩',\n      '整车',\n      '控制',\n      '单元',\n      '检测',\n      '到',\n      '油门',\n      '踏板',\n      '开度',\n      '大于',\n      '设置',\n      '值',\n      '档位',\n      '装置',\n      '手刹',\n      '装置',\n      '处于',\n      '驻车',\n      '位置',\n      '则',\n      '退出',\n      '防溜',\n      '控制',\n      '切换',\n      '到',\n      '力矩',\n      '控制',\n      '策略',\n      '无需',\n      '更改',\n      '车辆',\n      '结构',\n      '添加',\n      '辅助',\n      '传感器',\n      '硬件',\n      '设备',\n      '实现',\n      '车辆',\n      '防溜',\n      '目的'],\n     ['机动车辆',\n      '车门',\n      '靠',\n      '溃缩',\n      '结构',\n      '是',\n      '作为',\n      '支撑',\n      '提供',\n      '机动车辆',\n      '车门',\n      '衬板',\n      '靠',\n      '溃缩',\n      '结构',\n      '具有',\n      '交叉',\n      '形',\n      '方式',\n      '设计',\n      '凹陷',\n      '装饰',\n      '覆盖',\n      '泡沫',\n      '元件',\n      '安排',\n      '溃缩',\n      '结构',\n      '溃缩',\n      '结构',\n      '特别',\n      '用于',\n      '吸收',\n      '碰撞',\n      '事件',\n      '负荷',\n      '防止',\n      '车辆',\n      '乘车',\n      '免受',\n      '增加',\n      '力',\n      '峰值'],\n     ['仪表板',\n      '支撑',\n      '结构',\n      '本发明',\n      '支撑',\n      '结构',\n      '配置',\n      '用于',\n      '车辆',\n      '乘客',\n      '车厢',\n      '内',\n      '定位',\n      '仪表板',\n      '支撑',\n      '结构',\n      '包括',\n      '支撑',\n      '支架',\n      '端部',\n      '支架',\n      '支撑',\n      '支架',\n      '附',\n      '接到',\n      '车辆',\n      '车身',\n      '面板',\n      '支撑',\n      '支架',\n      '包括',\n      '横向',\n      '偏压',\n      '导引',\n      '端部',\n      '支架',\n      '附',\n      '接到',\n      '仪表板',\n      '端部',\n      '支架',\n      '具有',\n      '邻接',\n      '支撑',\n      '支架',\n      '横向',\n      '偏压',\n      '导引',\n      '使得',\n      '横向',\n      '偏压',\n      '导引',\n      '横向',\n      '方向',\n      '偏压',\n      '仪表板',\n      '直到',\n      '端部',\n      '支架',\n      '邻接',\n      '车身',\n      '面板',\n      '横向',\n      '基准',\n      '表面'],\n     ['铰接',\n      '头枕',\n      '总成',\n      '车辆',\n      '座椅',\n      '总成',\n      '包括',\n      '座椅',\n      '靠背',\n      '头枕',\n      '支承',\n      '结构',\n      '支承',\n      '结构',\n      '头枕',\n      '座椅',\n      '靠背',\n      '延伸',\n      '支承',\n      '结构',\n      '包括',\n      '构件',\n      '构件',\n      '包围',\n      '构件',\n      '装饰',\n      '构件',\n      '头枕',\n      '座椅',\n      '靠背',\n      '枢转地',\n      '联接',\n      '构件',\n      '具有',\n      '围绕',\n      '着横',\n      '轴线',\n      '头枕',\n      '枢转地',\n      '联接',\n      '构件',\n      '间隔',\n      '开',\n      '驱动器',\n      '构件',\n      '座椅',\n      '靠背',\n      '二者',\n      '联接',\n      '位置',\n      '位置',\n      '旋转',\n      '头枕',\n      '驱动器',\n      '联接',\n      '构件',\n      '座椅',\n      '靠背',\n      '角度',\n      '角度',\n      '移动',\n      '头枕'],\n     ['用于',\n      '评估',\n      '控制',\n      '电池',\n      '系统',\n      '系统',\n      '方法',\n      '本发明',\n      '涉及',\n      '用于',\n      '评估',\n      '控制',\n      '电池',\n      '系统',\n      '系统',\n      '方法',\n      '介绍',\n      '用于',\n      '估计',\n      '电池',\n      '系统',\n      '独立',\n      '电池',\n      '分部',\n      '相对',\n      '容量',\n      '系统',\n      '方法',\n      '实施',\n      '例',\n      '系统',\n      '可',\n      '包括',\n      '构造',\n      '成',\n      '分析',\n      '电',\n      '参数',\n      '计算',\n      '系统',\n      '产生',\n      '一段时间',\n      '参数',\n      '导',\n      '数值',\n      '计算',\n      '系统',\n      '还',\n      '可',\n      '导',\n      '数值',\n      '计算',\n      '独立',\n      '电池',\n      '分部',\n      '相关联',\n      '总和',\n      '值',\n      '电池',\n      '控制系统',\n      '可',\n      '利用',\n      '总和',\n      '值',\n      '产生',\n      '构造',\n      '成',\n      '利用',\n      '总和',\n      '值',\n      '控制',\n      '电池组',\n      '操作',\n      '方面',\n      '命令',\n      '实施',\n      '例',\n      '电池',\n      '分部',\n      '相关联',\n      '总和',\n      '值',\n      '可',\n      '用于',\n      '确定',\n      '用于',\n      '电能',\n      '相对',\n      '容量',\n      '相对',\n      '容量',\n      '确定',\n      '可',\n      '控制系统',\n      '使用',\n      '防止',\n      '具有',\n      '最低',\n      '储能',\n      '容量',\n      '电池',\n      '分部',\n      '过',\n      '放电'],\n     ['侧',\n      '气囊',\n      '装置',\n      '本发明',\n      '侧',\n      '气囊',\n      '装置',\n      '横向',\n      '分隔',\n      '结构',\n      '织物',\n      '部',\n      '形成',\n      '结构',\n      '织物',\n      '部',\n      '具有',\n      '部',\n      '结构',\n      '织物',\n      '部',\n      '主体',\n      '织物',\n      '部',\n      '结合',\n      '部',\n      '设置',\n      '结合部',\n      '使',\n      '结构',\n      '织物',\n      '部',\n      '互相',\n      '结合',\n      '内管',\n      '延伸',\n      '越过',\n      '膨胀',\n      '室',\n      '下',\n      '膨胀',\n      '室',\n      '横向',\n      '分隔',\n      '相交',\n      '结合部',\n      '使',\n      '内管',\n      '后',\n      '主体',\n      '织物',\n      '部',\n      '结合',\n      '结合部',\n      '使该',\n      '部分',\n      '前',\n      '结构',\n      '织物',\n      '部',\n      '结合'],\n     ['制造',\n      '气囊',\n      '方法',\n      '本发明',\n      '方式',\n      '制造',\n      '气囊',\n      '结合',\n      '步骤',\n      '使',\n      '结构',\n      '织物',\n      '安装',\n      '引到',\n      '更',\n      '靠近',\n      '主体',\n      '织物',\n      '摊开',\n      '状态',\n      '主体',\n      '织物',\n      '部',\n      '结合',\n      '结合',\n      '步骤',\n      '使',\n      '没有',\n      '重叠',\n      '到',\n      '结构',\n      '织物',\n      '部上',\n      '部分',\n      '主体',\n      '织物',\n      '部',\n      '结合',\n      '使',\n      '重叠',\n      '到',\n      '结构',\n      '织物',\n      '部上',\n      '部分',\n      '只',\n      '结构',\n      '织物',\n      '部',\n      '结合',\n      '结合',\n      '步骤',\n      '使',\n      '结构',\n      '织物',\n      '安装',\n      '时',\n      '远离',\n      '主体',\n      '织物',\n      '部',\n      '互相',\n      '结合'],\n     ['椅背',\n      '枢轴',\n      '系统',\n      '车辆',\n      '座椅',\n      '总成',\n      '包括',\n      '侧',\n      '支承',\n      '部',\n      '侧',\n      '支承',\n      '部',\n      '限定',\n      '椅背',\n      '结构',\n      '椅背',\n      '结构',\n      '竖直',\n      '斜倚',\n      '位置',\n      '可',\n      '操作',\n      '机动',\n      '化',\n      '驱动',\n      '总成',\n      '设置',\n      '侧',\n      '支承',\n      '部',\n      '可',\n      '操作',\n      '耦接',\n      '枢轴',\n      '枢轴',\n      '椅背',\n      '结构',\n      '可',\n      '旋转',\n      '耦接',\n      '椅背',\n      '悬挂',\n      '组件',\n      '耦接',\n      '枢轴',\n      '椅背',\n      '结构',\n      '处在',\n      '竖直',\n      '位置',\n      '时',\n      '椅背',\n      '悬挂',\n      '组件',\n      '相对',\n      '枢轴',\n      '自动',\n      '地向',\n      '位置',\n      '枢转',\n      '椅背',\n      '结构',\n      '处在',\n      '斜倚',\n      '位置',\n      '时',\n      '椅背',\n      '悬挂',\n      '组件',\n      '相对',\n      '枢轴',\n      '自动',\n      '部向',\n      '位置',\n      '枢转',\n      '间隙',\n      '限定',\n      '椅背',\n      '悬挂',\n      '组件',\n      '椅背',\n      '结构'],\n     ['用于',\n      '机动车',\n      '行驶',\n      '过程',\n      '关闭',\n      '启动',\n      '内燃机',\n      '方法',\n      '本发明',\n      '描述',\n      '用于',\n      '具有',\n      '手动',\n      '变速器',\n      '机动车',\n      '行驶',\n      '过程',\n      '关闭',\n      '开启',\n      '内燃机',\n      '方法',\n      '方法',\n      '具有',\n      '方法',\n      '步骤',\n      '首先',\n      '机动车',\n      '速度',\n      '满足',\n      '手动',\n      '变速器',\n      '空挡',\n      '条件',\n      '时',\n      '开始',\n      '方法',\n      '随后',\n      '关闭',\n      '机动车',\n      '内燃机',\n      '内燃机',\n      '停转',\n      '状态',\n      '无',\n      '驱动',\n      '运行',\n      '模式',\n      '继续',\n      '行驶',\n      '内燃机',\n      '停转',\n      '状态',\n      '监测',\n      '启动',\n      '发生器',\n      '启动',\n      '信号',\n      '发生器',\n      '操作',\n      '时',\n      '开始',\n      '重启',\n      '内燃机'],\n     ['倾斜',\n      '货箱',\n      '卸载',\n      '系统',\n      '本发明',\n      '涉及',\n      '倾斜',\n      '货箱',\n      '卸载',\n      '系统',\n      '变型',\n      '可',\n      '包括',\n      '产品',\n      '包括',\n      '运输工具',\n      '包括',\n      '具有',\n      '倾斜',\n      '部分',\n      '倾斜',\n      '部分',\n      '货箱',\n      '运输工具',\n      '具有',\n      '纵向',\n      '侧',\n      '相对',\n      '纵向',\n      '侧',\n      '倾斜',\n      '部分',\n      '构造',\n      '布置',\n      '成使',\n      '最靠近',\n      '纵向',\n      '侧',\n      '可',\n      '相对',\n      '最靠近',\n      '纵向',\n      '侧',\n      '相对',\n      '侧',\n      '降低',\n      '变型',\n      '可',\n      '包括',\n      '方法',\n      '包括',\n      '提供',\n      '包括',\n      '具有',\n      '倾斜',\n      '部分',\n      '倾斜',\n      '部分',\n      '货箱',\n      '运输工具',\n      '运输工具',\n      '具有',\n      '纵向',\n      '侧',\n      '相对',\n      '纵向',\n      '侧',\n      '倾斜',\n      '部分',\n      '构造',\n      '布置',\n      '成使',\n      '最靠近',\n      '纵向',\n      '侧',\n      '可',\n      '相对',\n      '最靠近',\n      '纵向',\n      '侧',\n      '相对',\n      '侧',\n      '降低',\n      '货箱',\n      '具有',\n      '倾斜',\n      '部分',\n      '最靠近',\n      '货箱',\n      '倾斜',\n      '部分',\n      '邻近',\n      '倾斜',\n      '部分',\n      '将',\n      '货物',\n      '货箱',\n      '到',\n      '货箱',\n      '将',\n      '货物',\n      '货箱',\n      '卸载',\n      '包括',\n      '使',\n      '货箱',\n      '倾斜',\n      '部分',\n      '倾斜']]\n\n\n\n\n```python\nmodel = Word2Vec(corpusA, size=100, window=5, min_count=1, workers=4, sg = 1, negative = 5, ns_exponent = 0.75)\n```\n\n\n```python\nmodel.most_similar('最靠近')\n```\n\n    /usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n      \"\"\"Entry point for launching an IPython kernel.\n    /usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n      if np.issubdtype(vec.dtype, np.int):\n    \n\n\n\n\n    [('椅背', 0.43153685331344604),\n     ('具有', 0.424590140581131),\n     ('手动', 0.4134489595890045),\n     ('驱动器', 0.4102057218551636),\n     ('方式', 0.39596235752105713),\n     ('操作', 0.3881428837776184),\n     ('自动', 0.3764950931072235),\n     ('方法', 0.3684544563293457),\n     ('确定', 0.3612304925918579),\n     ('转速', 0.3547886610031128)]\n\n\n\n\n```python\nmodel.save(\"data/w2v\")\n```\n\n## 6.Tensorflow训练中文词向量，并可视化\n\n\n\n```python\n# -*- coding: utf-8 -*-\n# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\n%matplotlib inline\nfrom __future__ import print_function\nimport collections\nimport math\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport zipfile\nfrom matplotlib import pylab\nfrom matplotlib import font_manager\nfrom six.moves import range\nfrom six.moves.urllib.request import urlretrieve\nfrom six.moves import cPickle as pickle\nfrom sklearn.manifold import TSNE\n\n```\n\n\n```python\nvocabulary_size = 3000\n\ndef build_dataset(words):\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count = unk_count + 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n  return data, count, dictionary, reverse_dictionary\n\ndef maybe_pickle(target_data, set_filename, force=False):\n  if os.path.exists(set_filename) and not force:\n    if os.path.getsize(set_filename) > 0:\n      # You may override by setting force=True.\n      print('%s already present - Skipping pickling.' % set_filename)\n      return set_filename\n  print('Pickling %s.' % set_filename)\n  try:\n    with open(set_filename, 'wb') as f:\n      pickle.dump(target_data, f, pickle.HIGHEST_PROTOCOL)\n  except Exception as e:\n    print('Unable to save data to', set_filename, ':', e)\n\n#with open(\"wiki_cn_chunk.txt\", 'r') as f:\ndef loadData(data_file=\"./datat/data.pickle\", count_file=\"./data/count.pickle\", dict_file=\"./data/dictionary.pickle\", rev_dict_file=\"./data/reverse_dictionary.pickle\", force=False):\n  if os.path.exists(data_file) and os.path.exists(count_file) and os.path.exists(dict_file) and os.path.exists(rev_dict_file) and not force:\n    try:\n      print(\"Pickle files found, try to load data from pickle files...\")\n      with open(data_file, 'rb') as f:\n        data = pickle.load(f)\n      with open(count_file, 'rb') as f:\n        count = pickle.load(f)\n      with open(dict_file, 'rb') as f:\n        dictionary = pickle.load(f)\n      with open(rev_dict_file, 'rb') as f:\n        reverse_dictionary = pickle.load(f)\n      print(\"Data loaded from pickle files successfully\")\n      print('Most common words (+UNK)', count[:5])\n      print('Least common words', count[-10:])\n      print('Sample data', data[:10])\n      return data, count, dictionary, reverse_dictionary\n    except Exception as e:\n      print('Unable to load data', ':', e)\n  else:\n    #lines = tf.compat.as_str(f.read().decode(\"utf-8\")).strip().split()\n    #lines = f.read().strip().decode(\"utf-8\", \"ignore\").split()\n    #print(lines[:10])\n    global corpusA\n    words = []\n    for line in corpusA:\n        words.extend(list(line))\n    print('Data size %d' % len(words))\n    print(words[:10])\n\n    print(\"Cooking data from words loaded...\")\n    data, count, dictionary, reverse_dictionary = build_dataset(words)\n    print('Most common words (+UNK)', count[:5])\n    print('Least common words', count[-10:])\n    print('Sample data', data[:10])\n    del words  # Hint to reduce memory.\n\n    print(\"Saving cooked data into pickle files...\")\n    maybe_pickle(dictionary, \"dictionary.pickle\")\n    maybe_pickle(reverse_dictionary, \"reverse_dictionary.pickle\")\n    maybe_pickle(count, \"count.pickle\")\n    maybe_pickle(data, \"data.pickle\")\n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = loadData()\n```\n\n    Data size 783\n    ['永磁', '电机', '驱动', '纯', '电动', '大巴车', '坡道', '起步', '防溜', '策略']\n    Cooking data from words loaded...\n    Most common words (+UNK) [['UNK', 0], ('结构', 27), ('部', 16), ('织物', 16), ('侧', 15)]\n    Least common words [('使能', 1), ('更改', 1), ('恢复', 1), ('二者', 1), ('监测', 1), ('设备', 1), ('相交', 1), ('定位', 1), ('间隙', 1), ('配置', 1)]\n    Sample data [36, 29, 51, 143, 142, 104, 88, 145, 33, 43]\n    Saving cooked data into pickle files...\n    Pickling dictionary.pickle.\n    Pickling reverse_dictionary.pickle.\n    Pickling count.pickle.\n    Pickling data.pickle.\n    \n\n\n```python\ndata_index = 0\n\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [ skip_window ]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\nprint('data:', [reverse_dictionary[di] for di in data[:8]])\n\nfor num_skips, skip_window in [(2, 1), (4, 2)]:\n    data_index = 0\n    batch, labels = generate_batch(batch_size=16, num_skips=num_skips, skip_window=skip_window)\n    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(16)])\n\n```\n\n    data: ['永磁', '电机', '驱动', '纯', '电动', '大巴车', '坡道', '起步']\n    \n    with num_skips = 2 and skip_window = 1:\n        batch: ['电机', '电机', '驱动', '驱动', '纯', '纯', '电动', '电动', '大巴车', '大巴车', '坡道', '坡道', '起步', '起步', '防溜', '防溜']\n        labels: ['驱动', '永磁', '电机', '纯', '电动', '驱动', '大巴车', '纯', '坡道', '电动', '大巴车', '起步', '防溜', '坡道', '策略', '起步']\n    \n    with num_skips = 4 and skip_window = 2:\n        batch: ['驱动', '驱动', '驱动', '驱动', '纯', '纯', '纯', '纯', '电动', '电动', '电动', '电动', '大巴车', '大巴车', '大巴车', '大巴车']\n        labels: ['电动', '纯', '永磁', '电机', '驱动', '电动', '电机', '大巴车', '纯', '大巴车', '坡道', '驱动', '起步', '纯', '电动', '坡道']\n    \n\n\n```python\nbatch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 2 # How many words to consider left and right.\nnum_skips = 4 # How many times to reuse an input to generate a label.\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. \nvalid_size = 16 # Random set of words to evaluate similarity on.\nvalid_window = 100 # Only pick dev samples in the head of the distribution.\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64 # Number of negative examples to sample.\n\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device('/cpu:0'):\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n  train_labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  \n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n  \n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, train_labels,  embed,\n                               num_sampled, vocabulary_size))\n\n  # Optimizer.\n  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n  # This is because the embeddings are defined as a variable quantity and the\n  # optimizer's `minimize` method will by default modify all variable quantities \n  # that contribute to the tensor it is passed.\n  # See docs on `tf.train.Optimizer.minimize()` for more details.\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n  \n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n\n```\n\n    WARNING:tensorflow:From <ipython-input-11-e91213785cb7>:46: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n    Instructions for updating:\n    keep_dims is deprecated, use keepdims instead\n    \n\n\n```python\nnum_steps = 6001\n#num_steps = 100001\n#num_steps = 5000001\n#max_steps = len(data) * num_skips - 100\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print('Initialized')\n  average_loss = 0\n  for step in range(num_steps):\n    batch_data, batch_labels = generate_batch(\n      batch_size, num_skips, skip_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n    average_loss += l\n    if step % 2000 == 0:\n      if step > 0:\n        average_loss = average_loss / 2000\n      # The average loss is an estimate of the loss over the last 2000 batches.\n      print('Average loss at step %d: %f' % (step, average_loss))\n      average_loss = 0\n  final_embeddings = normalized_embeddings.eval()\n```\n\n    WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n    Instructions for updating:\n    Use `tf.global_variables_initializer` instead.\n    Initialized\n    Average loss at step 0: 5.761660\n    Average loss at step 2000: 1.798778\n    Average loss at step 4000: 1.446995\n    Average loss at step 6000: 1.372941\n    \n\n## 7.中文词向量可视化\n\n\n\n```python\nnum_points = 200\n\ntsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\ntwo_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])\n```\n\n\n```python\nmyfont = font_manager.FontProperties(fname='/usr/share/fonts/zh/STKAITI.TTF')\n\ndef plot(embeddings, labels):\n  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n  pylab.figure(figsize=(15,15))  # in inches\n  for i, label in enumerate(labels):\n    x, y = embeddings[i,:]\n    pylab.scatter(x, y)\n    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n                   ha='right', va='bottom',fontproperties=myfont)\n  pylab.show()\n\nwords = [reverse_dictionary[i] for i in range(1, num_points+1)]\nplot(two_d_embeddings, words)\n\n```\n\n\n![png](Representation_of_words_%26_sentences_files/Representation_of_words_%26_sentences_20_0.png)\n\n\n\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"NLP系列","url":"/2019-03-21/nlp/9chatbot_v2/2.generative_chatbot/5.transformer_based_chatbot/","content":"\n# 基于Transformer的聊天机器人构建\n\n\n代码大家可以参考[Transformer-in-generating-dialogue](https://github.com/EternalFeather/Transformer-in-generating-dialogue)的实现，基于上述代码的小黄鸡对话语料构建聊天机器人版本可以参考[transformer-chatbot](https://github.com/dengqiqi123/transformer-chatbot.git)\n\n\n```python\n!git clone https://github.com/dengqiqi123/transformer-chatbot.git\n```\n\n    Cloning into 'transformer-chatbot'...\n    remote: Enumerating objects: 46, done.\u001b[K\n    remote: Counting objects: 100% (46/46), done.\u001b[K\n    remote: Compressing objects: 100% (46/46), done.\u001b[K\n    Unpacking objects:  21% (10/46)   \n\n\n\n```python\n!ls transformer-chatbot\n```\n\n    data  getData.py  main.py  model  model.py  modules.py\ttrain.py  utils.py\n\n\n\n\n```python\n%cd transformer-chatbot/\n!python main.py\n```\n\n\n```python\n# %load transformer-chatbot/utils.py\nimport codecs\nimport csv\nimport array\nimport numpy as np\nimport tensorflow as tf\nimport re\nimport math\nimport random\nimport jieba\nimport logging\nimport os\ndef create_model_and_embedding(session,Model_class,path,config,is_train):\n    model = Model_class(config,is_train)\n    ckpt = tf.train.get_checkpoint_state(path) \n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        model.saver.restore(session, ckpt.model_checkpoint_path)\n    else:\n        session.run(tf.global_variables_initializer())\n    return model \ndef save_model(sess, model, path,logger):\n    checkpoint_path = os.path.join(path, \"chatbot.ckpt\")\n    model.saver.save(sess, checkpoint_path)\n    logger.info(\"model saved\")\ndef load_sor_vocab():\n    vocab = [line.split()[0] for line in codecs.open('data/vocab.tsv', 'r', 'utf-8').read().splitlines()]\n    word2idx = {word: idx for idx, word in enumerate(vocab)}\n    idx2word = {idx: word for idx, word in enumerate(vocab)}\n    return word2idx, idx2word    \ndef load_mub_vocab():   \n    vocab = [line.split()[0] for line in codecs.open('data/vocab.answer.tsv', 'r', 'utf-8').read().splitlines()]\n    #word2idx = {word: idx for idx, word in enumerate(vocab)}\n    #idx2word = {idx: word for idx, word in enumerate(vocab)}\n    #return word2idx, idx2word    \ndef load_sentences(sor_path,mub_path):\n    de_sents = [line.strip().replace('\\r','') for line in codecs.open(sor_path, 'r', 'utf-8').read().split(\"\\n\")]\n    en_sents = [line.strip().replace('\\r','') for line in codecs.open(mub_path, 'r', 'utf-8').read().split(\"\\n\")]\n    de_sents = [' '.join([i for i in line.strip()])  for line in de_sents]\n    en_sents = [' '.join([i for i in line.strip()])  for line in en_sents]\n    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n    return X, Y \ndef create_data(source_sents, target_sents):\n    word2id,id2word = load_sor_vocab()\n    #mub2id,id2mud = load_mub_vocab()\n    x_list, y_list, Sources, Targets = [], [], [], []\n    for source_sent, target_sent in zip(source_sents, target_sents):\n        x = [word2id.get(word, 1) for word in (source_sent).split()] # 1: OOV, </S>: End of Text\n        y = [word2id.get(word, 1) for word in (target_sent+\" </S>\").split()] \n        if max(len(x), len(y)) <= 20:\n            x_list.append(np.array(x))\n            y_list.append(np.array(y))\n            Sources.append(source_sent)\n            Targets.append(target_sent)\n    return x_list, y_list, Sources, Targets \n#实例化日志类\ndef get_logger(log_file):\n    logger = logging.getLogger(log_file)\n    logger.setLevel(logging.DEBUG)\n    fh = logging.FileHandler(log_file)\n    fh.setLevel(logging.DEBUG)\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n    ch.setFormatter(formatter)\n    fh.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.addHandler(fh)\n    return logger\ndef input_from_line(line, char_to_id):\n    inputs = list()\n    #把空格替换为$\n    line = line.replace(\" \", \"\")    \n    #查字典，把输入字符中能查到字典的字符转换为ID值，查不到的字标记为<UNK>\n    ids = [char_to_id[char] if char in char_to_id else char_to_id[\"<UNK>\"] for char in line] \n    #+[char_to_id['</S>']]\n    inputs.append([ids])\n    inputs.append([line])\n    return inputs\nclass BatchManager(object):\n    def __init__(self, sor_data,mub_data,batch_size):\n        self.batch_data = self.sort_and_pad(sor_data,mub_data,batch_size)\n        self.len_data = len(self.batch_data)\n    def sort_and_pad(self,sor_data,mub_data, batch_size):\n        alldata = []\n        for ask,answer in zip(sor_data, mub_data):\n            sentence = []\n            sentence.append(ask)\n            sentence.append(answer)\n            alldata.append(sentence)\n        num_batch = int(math.ceil(len(alldata) /batch_size))\n        \n        #sorted_data = sorted(sor_data, key=lambda x: len(x[0]))\n        #sorted_data = sor_data\n               \n        random.shuffle(alldata)\n        batch_data = []\n        for i in range(num_batch):\n            batch_data.append(self.pad_data(alldata[i*int(batch_size) : (i+1)*int(batch_size)]))\n        return batch_data\n    @staticmethod\n    def pad_data(data):\n        ask,answer = [],[]\n        max_sor = max([len(sentence[0]) for sentence in data])\n        max_mub = max([len(sentence[1]) for sentence in data])\n        for line in data:\n            qpadding = [0] * (max_sor- len(line[0]))\n            ask.append(list(line[0])+qpadding)\n            apadding = [0] * (max_mub - len(line[1]))\n            answer.append(list(line[1])+apadding)            \n        return [ask,answer]\n    def iter_batch(self, shuffle=False):\n        if shuffle:\n            random.shuffle(self.batch_data)\n        for idx in range(self.len_data):\n            yield self.batch_data[idx]\n```\n\n\n```python\n# %load transformer-chatbot/getData.py\n#enconding=utf-8\nimport os,sys,csv\nimport numpy as np\nimport pandas as pd\nimport codecs\nimport tensorflow as tf\nfrom modules import *\n\ndef full_to_half(s):\n    \"\"\"\n    将全角字符转换为半角字符 \n    \"\"\"\n    n = []\n    for char in s:\n        num = ord(char)\n        if num == 0x3000:\n            num = 32\n        elif 0xFF01 <= num <= 0xFF5E:\n            num -= 0xfee0\n        char = chr(num)\n        n.append(char)\n    return ''.join(n)\n\ndef replace_html(s):\n    s = s.replace('&quot;','\"')\n    s = s.replace('&amp;','&')\n    s = s.replace('&lt;','<')\n    s = s.replace('&gt;','>')\n    s = s.replace('&nbsp;',' ')\n    s = s.replace(\"&ldquo;\", \"\")\n    s = s.replace(\"&rdquo;\", \"\")\n    s = s.replace(\"&mdash;\",\"\")\n    s = s.replace(\"\\xa0\", \" \")\n    return(s)\ndef setdata(line):\n    line = line.replace('。','')\n    line = line.replace('？','')\n    line = line.replace('！','')\n    line = line.replace('，','')\n    line = line.replace('.','')\n    line = line.replace(',','')\n    line = line.replace('?','')\n    line = line.replace('!','')\n    line = line.replace('“','')\n    line = line.replace('”','')\n    return line\n'''\ny = tf.constant([[4,2,3,4,5,6,7,8,9]])\nenc = embedding(y, \n                         vocab_size=20, \n                                  num_units=8, \n                                  scale=True,\n                                  scope=\"enc_embed\")\n\nkey_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(enc), axis=-1)), -1)\nwith tf.Session() as sess:\n    initall = tf.global_variables_initializer()\n    sess.run(initall)\n    print(sess.run(key_masks))\n'''\nvocab = {line.split()[0]:int(line.split()[1]) for line in codecs.open('data/vocab.tsv', 'r', 'utf-8').read().splitlines()}\nfp = codecs.open('data/train.answer.tsv','r',encoding='utf-8-sig').read().split('\\n')\n#vocab = {}\nfor w in fp:\n    for i in w.strip():\n        if i in vocab.keys():\n            vocab[i] += 1\n        else:\n            vocab[i] = 1\n\nwith open('data/vocab.tsv','w',encoding='utf-8') as fa:\n    for k,v in vocab.items():\n        strs = k+' '+str(v)\n        fa.write(strs+'\\n')\nfa.close()\n'''\nfp = codecs.open('data/xiaohuangji50w_nofenci.conv','r',encoding='utf-8')\ni = 1\nasks = []\nanswers = []\nsentence = []\nfor k,w in enumerate(fp):\n    w = w.strip()\n    if k > 0:\n        if \"M\" not in w and w != 'E':\n            continue        \n        if i%3 == 0:\n            sentence[1] = sentence[1].replace(' ','')\n            sentence[2] = sentence[2].replace(' ','')\n            if sentence[1][1:] != '' and sentence[2][1:] != '':\n                asks.append(sentence[1][1:])\n                answers.append(sentence[2][1:])\n            sentence = []\n            i = 1\n            sentence.append(w)\n        else:\n            i += 1\n            sentence.append(w)\n    else:\n        sentence.append(w)\nasks = list(filter(None,asks))\nanswers = list(filter(None,answers))\n'''\nfp = codecs.open('data/123.txt','r',encoding='utf-8-sig')\ni = 1\nasks = []\nanswers = []\nfor k,w in enumerate(fp):\n    w = w.strip()\n    w = full_to_half(w)\n    w = replace_html(w)    \n    w = setdata(w)\n    if k%2 == 0:\n        asks.append(w)\n    else:\n        answers.append(w)\nwith open('data/train.ask.tsv','w',encoding='utf-8') as fa:\n    for w in asks:\n        fa.write(w+'\\n')\nwith open('data/train.answer.tsv','w',encoding='utf-8') as fs:\n    for w in answers:\n        fs.write(w+'\\n')\nfa.close()\nfs.close()\nprint('ok')\n```\n\n\n```python\n# %load transformer-chatbot/model.py\nimport numpy as np\nimport tensorflow as tf\nfrom utils import load_sor_vocab,load_mub_vocab\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom modules import *\n\nclass Model(object):\n    def __init__(self, config,is_train=True):\n        self.is_train =  is_train\n        self.config = config\n        self.lr = config[\"learning_rate\"]\n        self.maxlen = config['sequence_length']\n        self.dropout_rate = config['dropout_rate']\n        self.hidden_units = config['hidden_units']\n        self.num_blocks = config['num_blocks']\n        self.num_heads = config['num_heads']        \n        \n        self.global_step = tf.Variable(0,trainable=False)  \n        #定义编码输入input\n        self.sor_inputs = tf.placeholder(dtype=tf.int32,shape=[None,None],name='sorinput')\n        #定义编码输入output\n        self.out_inputs = tf.placeholder(dtype=tf.int32,shape=[None,None],name='outinput')\n        self.decode_input = tf.concat((tf.ones_like(self.out_inputs[:, :1])*2, self.out_inputs[:, :-1]), -1)\n        word2id,id2word = load_sor_vocab()\n        # Encoder\n        with tf.variable_scope(\"encoder\"):\n            self.enc = embedding(self.sor_inputs, len(word2id), self.hidden_units,scale=True,scope=\"enc_embed\")\n            key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.enc), axis=-1)), -1)\n            # Positional Encoding\n            if False:\n                self.enc += positional_encoding(self.sor_inputs,num_units=self.hidden_units,zero_pad=False,scale=False,scope=\"enc_pe\")\n            else:\n                self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.sor_inputs)[1]), 0), [tf.shape(self.sor_inputs)[0], 1]),vocab_size=self.maxlen, \n                                      num_units=self.hidden_units,zero_pad=False,scale=False,scope=\"enc_pe\")\n\n            self.enc *= key_masks\n            # Dropout\n            self.enc = tf.layers.dropout(self.enc,rate=self.dropout_rate,training=tf.convert_to_tensor(self.is_train))   \n            # Blocks\n            for i in range(self.num_blocks):\n                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                    # Multihead Attention\n                    self.enc = multihead_attention(queries=self.enc,keys=self.enc,num_units=self.hidden_units,num_heads=self.num_heads,dropout_rate=self.dropout_rate,is_training=self.is_train,\n                                                                causality=False)\n                    # Feed Forward\n                    self.enc = feedforward(self.enc, num_units=[4*self.hidden_units, self.hidden_units])  \n        #Decode\n        with tf.variable_scope(\"decoder\"):\n            # Embedding\n            self.dec = embedding(self.decode_input,vocab_size=len(word2id),num_units=self.hidden_units,scale=True,scope=\"dec_embed\") \n            key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.dec), axis=-1)), -1)\n            # Positional Encoding\n            if False:\n                self.dec += positional_encoding(self.decode_input,vocab_size=self.maxlen,num_units=self.hidden_units,zero_pad=False,scale=False,scope=\"dec_pe\")\n            else:\n                self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decode_input)[1]), 0), [tf.shape(self.decode_input)[0], 1]),vocab_size=self.maxlen,num_units=self.hidden_units, \n                                              zero_pad=False, \n                                              scale=False,\n                                              scope=\"dec_pe\")\n            self.dec *= key_masks \n            # Dropout\n            self.dec = tf.layers.dropout(self.dec,rate=self.dropout_rate,training=tf.convert_to_tensor(self.is_train)) \n            # Blocks\n            for i in range(self.num_blocks):\n                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                    # Multihead Attention ( self-attention)\n                    self.dec = multihead_attention(queries=self.dec,keys=self.dec,num_units=self.hidden_units,num_heads=self.num_heads, dropout_rate=self.dropout_rate,is_training=self.is_train,\n                                                                causality=True, \n                                                                scope=\"self_attention\")\n                    # Multihead Attention ( vanilla attention)\n                    self.dec = multihead_attention(queries=self.dec,keys=self.enc,num_units=self.hidden_units,num_heads=self.num_heads,dropout_rate=self.dropout_rate,is_training=self.is_train, \n                                                                causality=False,\n                                                                scope=\"vanilla_attention\")\n                    # Feed Forward\n                    self.dec = feedforward(self.dec, num_units=[4*self.hidden_units, self.hidden_units]) \n        # Final linear projection\n        self.logits = tf.layers.dense(self.dec, len(word2id))\n        self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n        self.istarget = tf.to_float(tf.not_equal(self.out_inputs, 0))\n        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.out_inputs))*self.istarget)/ (tf.reduce_sum(self.istarget))\n        #tf.summary.scalar('acc', self.acc)   \n        # Loss\n        self.y_smoothed = label_smoothing(tf.one_hot(self.out_inputs, depth=len(word2id)))\n        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n        self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n       \n        # 定义优化器\n        with tf.variable_scope('optimizer'):\n            self.optimizer = tf.train.AdamOptimizer(self.lr)#, beta1=0.9, beta2=0.98, epsilon=1e-8\n            grads_vars = self.optimizer.compute_gradients(self.loss)\n            capped_grads_vars = [[tf.clip_by_value(g,-5,5),v] for g,v in grads_vars]        \n            self.train_op = self.optimizer.apply_gradients(capped_grads_vars,self.global_step)\n        self.saver = tf.train.Saver(tf.global_variables(),max_to_keep=1)\n    def create_feed_dict(self,is_train,batch):\n        if is_train:\n            ask,answer = batch\n            feed_dict = {\n                self.sor_inputs: np.asarray(ask),\n                self.out_inputs: np.asarray(answer)\n            }\n        else:\n            ask,_ = batch\n            feed_dict = {\n            #self.sor_inputs: np.asarray(ask),\n            #self.out_inputs:np.zeros((1, len(ask[0])), np.int32)\n            }            \n        return feed_dict        \n    def run_step(self,sess,is_train,batch):\n        feed_dict = self.create_feed_dict(is_train,batch)\n        if is_train:\n            global_step,y_smoothed,loss,logits,preds,_ = sess.run([self.global_step,self.y_smoothed,self.mean_loss,self.logits,self.preds,self.train_op],feed_dict)                 \n            return global_step, loss\n        else:\n            ask,_ = batch\n            preds = np.ones((1,20), np.int32)\n            #preds[:,0] = 2\n            #preds[:,19] = 3\n            for i in range(20):\n                _preds = sess.run(self.preds, {self.sor_inputs: np.asarray(ask), self.out_inputs:preds})\n                preds[:,i] = _preds[:,i]                \n            #preds = sess.run([self.preds], feed_dict)\n            return preds\n    def evaluate_line(self, sess, inputs):\n        probs = self.run_step(sess, False, inputs)\n        return probs     \n```\n\n\n```python\n# %load transformer-chatbot/modules.py\n#/usr/bin/python2\n'''\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n'''\n\nfrom __future__ import print_function\nimport tensorflow as tf\n\ndef normalize(inputs, \n              epsilon = 1e-8,\n              scope=\"ln\",\n              reuse=None):\n    '''Applies layer normalization.\n    \n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`.\n      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n      \n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n    \n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta= tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n        outputs = gamma * normalized + beta\n        \n    return outputs\n\ndef embedding(inputs, \n              vocab_size, \n              num_units, \n              zero_pad=True, \n              scale=True,\n              scope=\"embedding\", \n              reuse=None):\n    '''Embeds a given tensor.\n\n    Args:\n      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n         to be looked up in `lookup table`.\n      vocab_size: An int. Vocabulary size.\n      num_units: An int. Number of embedding hidden units.\n      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n        should be constant zeros.\n      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A `Tensor` with one more rank than inputs's. The last dimensionality\n        should be `num_units`.\n        \n    For example,\n    \n    ```\n    import tensorflow as tf\n    \n    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n    outputs = embedding(inputs, 6, 2, zero_pad=True)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        print sess.run(outputs)\n    >>\n    [[[ 0.          0.        ]\n      [ 0.09754146  0.67385566]\n      [ 0.37864095 -0.35689294]]\n\n     [[-1.01329422 -1.09939694]\n      [ 0.7521342   0.38203377]\n      [-0.04973143 -0.06210355]]]\n    ```\n    \n    ```\n    import tensorflow as tf\n    \n    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n    outputs = embedding(inputs, 6, 2, zero_pad=False)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        print sess.run(outputs)\n    >>\n    [[[-0.19172323 -0.39159766]\n      [-0.43212751 -0.66207761]\n      [ 1.03452027 -0.26704335]]\n\n     [[-0.11634696 -0.35983452]\n      [ 0.50208133  0.53509563]\n      [ 1.22204471 -0.96587461]]]    \n    ```    \n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        lookup_table = tf.get_variable('lookup_table',\n                                       dtype=tf.float32,\n                                       shape=[vocab_size, num_units],\n                                       initializer=tf.contrib.layers.xavier_initializer())\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n        \n        if scale:\n            outputs = outputs * (num_units ** 0.5) \n            \n    return outputs\n    \n\ndef positional_encoding(inputs,\n                        num_units,\n                        zero_pad=True,\n                        scale=True,\n                        scope=\"positional_encoding\",\n                        reuse=None):\n    '''Sinusoidal Positional_Encoding.\n\n    Args:\n      inputs: A 2d Tensor with shape of (N, T).\n      num_units: Output dimensionality\n      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n        A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units'\n    '''\n\n    N, T = inputs.get_shape().as_list()\n    with tf.variable_scope(scope, reuse=reuse):\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n\n        # First part of the PE function: sin and cos argument\n        position_enc = np.array([\n            [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)]\n            for pos in range(T)])\n\n        # Second part, apply the cosine to even columns and sin to odds.\n        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n\n        # Convert to a tensor\n        lookup_table = tf.convert_to_tensor(position_enc)\n\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n\n        if scale:\n            outputs = outputs * num_units**0.5\n\n        return outputs\n\n\n\ndef multihead_attention(queries, \n                        keys, \n                        num_units=None, \n                        num_heads=8, \n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False,\n                        scope=\"multihead_attention\", \n                        reuse=None):\n    '''Applies multihead attention.\n    \n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A scalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked. \n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns\n      A 3d tensor with shape of (N, T_q, C)  \n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        # Set the fall back option for num_units\n        if num_units is None:\n            num_units = queries.get_shape().as_list()[-1]\n        \n        # Linear projections\n        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n        \n        # Split and concat\n        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n\n        # Multiplication\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n        \n        # Scale\n        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n        \n        # Key Masking\n        key_masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1)) # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n        \n        paddings = tf.ones_like(outputs)*(-2**32+1)\n        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n  \n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n   \n            paddings = tf.ones_like(masks)*(-2**32+1)\n            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n  \n        # Activation\n        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n         \n        # Query Masking\n        query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n        outputs *= query_masks # broadcasting. (N, T_q, C)\n          \n        # Dropouts\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n               \n        # Weighted sum\n        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n        \n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n              \n        # Residual connection\n        outputs += queries\n              \n        # Normalize\n        outputs = normalize(outputs) # (N, T_q, C)\n \n    return outputs\n\ndef feedforward(inputs, \n                num_units=[2048, 512],\n                scope=\"multihead_attention\", \n                reuse=None):\n    '''Point-wise feed forward net.\n    \n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: A list of two integers.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n        \n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    '''\n    with tf.variable_scope(scope, reuse=reuse):\n        # Inner layer\n        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\"activation\": tf.nn.relu, \"use_bias\": True}\n        outputs = tf.layers.conv1d(**params)\n        \n        # Readout layer\n        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\"activation\": None, \"use_bias\": True}\n        outputs = tf.layers.conv1d(**params)\n        \n        # Residual connection\n        outputs += inputs\n        \n        # Normalize\n        outputs = normalize(outputs)\n    \n    return outputs\n\ndef label_smoothing(inputs, epsilon=0.1):\n    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n    \n    Args:\n      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n      epsilon: Smoothing rate.\n    \n    For example,\n    \n    ```\n    import tensorflow as tf\n    inputs = tf.convert_to_tensor([[[0, 0, 1], \n       [0, 1, 0],\n       [1, 0, 0]],\n\n      [[1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0]]], tf.float32)\n       \n    outputs = label_smoothing(inputs)\n    \n    with tf.Session() as sess:\n        print(sess.run([outputs]))\n    \n    >>\n    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n        [ 0.03333334,  0.93333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334]],\n\n       [[ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.93333334,  0.03333334,  0.03333334],\n        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n    ```    \n    '''\n    K = inputs.get_shape().as_list()[-1] # number of channels\n    return ((1-epsilon) * inputs) + (epsilon / K)\n```\n\n\n```python\n# %load transformer-chatbot/train.py\n#/usr/bin/python2\n'''\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n'''\nfrom __future__ import print_function\nimport tensorflow as tf\n\nfrom hyperparams import Hyperparams as hp\nfrom data_load import get_batch_data, load_de_vocab, load_en_vocab\nfrom modules import *\nimport os, codecs\nfrom tqdm import tqdm\n\nclass Graph():\n    def __init__(self, is_training=True):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            if is_training:\n                self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n            else: # inference\n                self.x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n                self.y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n\n            # define decoder inputs\n            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n\n            # Load vocabulary    \n            de2idx, idx2de = load_de_vocab()\n            en2idx, idx2en = load_en_vocab()\n            \n            # Encoder\n            with tf.variable_scope(\"encoder\"):\n                ## Embedding\n                self.enc = embedding(self.x, \n                                      vocab_size=len(de2idx), \n                                      num_units=hp.hidden_units, \n                                      scale=True,\n                                      scope=\"enc_embed\")\n\n                key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.enc), axis=-1)), -1)\n\n                ## Positional Encoding\n                if hp.sinusoid:\n                    self.enc += positional_encoding(self.x,\n                                      num_units=hp.hidden_units, \n                                      zero_pad=False, \n                                      scale=False,\n                                      scope=\"enc_pe\")\n                else:\n                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n                                      vocab_size=hp.maxlen, \n                                      num_units=hp.hidden_units, \n                                      zero_pad=False, \n                                      scale=False,\n                                      scope=\"enc_pe\")\n\n                self.enc *= key_masks\n                 \n                ## Dropout\n                self.enc = tf.layers.dropout(self.enc, \n                                            rate=hp.dropout_rate, \n                                            training=tf.convert_to_tensor(is_training))\n                \n                ## Blocks\n                for i in range(hp.num_blocks):\n                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                        ### Multihead Attention\n                        self.enc = multihead_attention(queries=self.enc, \n                                                        keys=self.enc, \n                                                        num_units=hp.hidden_units, \n                                                        num_heads=hp.num_heads, \n                                                        dropout_rate=hp.dropout_rate,\n                                                        is_training=is_training,\n                                                        causality=False)\n                        \n                        ### Feed Forward\n                        self.enc = feedforward(self.enc, num_units=[4*hp.hidden_units, hp.hidden_units])\n            \n            # Decoder\n            with tf.variable_scope(\"decoder\"):\n                ## Embedding\n                self.dec = embedding(self.decoder_inputs, \n                                      vocab_size=len(en2idx), \n                                      num_units=hp.hidden_units,\n                                      scale=True, \n                                      scope=\"dec_embed\")\n\n                key_masks = tf.expand_dims(tf.sign(tf.reduce_sum(tf.abs(self.dec), axis=-1)), -1)\n\n                ## Positional Encoding\n                if hp.sinusoid:\n                    self.dec += positional_encoding(self.decoder_inputs,\n                                      vocab_size=hp.maxlen, \n                                      num_units=hp.hidden_units, \n                                      zero_pad=False, \n                                      scale=False,\n                                      scope=\"dec_pe\")\n                else:\n                    self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n                                      vocab_size=hp.maxlen, \n                                      num_units=hp.hidden_units, \n                                      zero_pad=False, \n                                      scale=False,\n                                      scope=\"dec_pe\")\n                self.dec *= key_masks\n                \n                ## Dropout\n                self.dec = tf.layers.dropout(self.dec, \n                                            rate=hp.dropout_rate, \n                                            training=tf.convert_to_tensor(is_training))\n                \n                ## Blocks\n                for i in range(hp.num_blocks):\n                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                        ## Multihead Attention ( self-attention)\n                        self.dec = multihead_attention(queries=self.dec, \n                                                        keys=self.dec, \n                                                        num_units=hp.hidden_units, \n                                                        num_heads=hp.num_heads, \n                                                        dropout_rate=hp.dropout_rate,\n                                                        is_training=is_training,\n                                                        causality=True, \n                                                        scope=\"self_attention\")\n                        \n                        ## Multihead Attention ( vanilla attention)\n                        self.dec = multihead_attention(queries=self.dec, \n                                                        keys=self.enc, \n                                                        num_units=hp.hidden_units, \n                                                        num_heads=hp.num_heads,\n                                                        dropout_rate=hp.dropout_rate,\n                                                        is_training=is_training, \n                                                        causality=False,\n                                                        scope=\"vanilla_attention\")\n                        \n                        ## Feed Forward\n                        self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])\n                \n            # Final linear projection\n            self.logits = tf.layers.dense(self.dec, len(en2idx))\n            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n            self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n            tf.summary.scalar('acc', self.acc)\n                \n            if is_training:  \n                # Loss\n                self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\n                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n                self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n               \n                # Training Scheme\n                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n                self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n                # Summary \n                tf.summary.scalar('mean_loss', self.mean_loss)\n                self.merged = tf.summary.merge_all()\n\nif __name__ == '__main__':                \n    # Load vocabulary    \n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n    \n    # Construct graph\n    g = Graph(\"train\"); print(\"Graph loaded\")\n    \n    # Start session\n    sv = tf.train.Supervisor(graph=g.graph,logdir=hp.logdir,save_model_secs=0)\n    with sv.managed_session() as sess:\n        for epoch in range(1, hp.num_epochs+1): \n            if sv.should_stop(): break\n            for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n                sess.run(g.train_op)\n                \n            gs = sess.run(g.global_step)   \n            sv.saver.save(sess, hp.logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n    \n    print(\"Done\")\n```\n\n\n```python\n# %load transformer-chatbot/main.py\nfrom __future__ import print_function\nimport tensorflow as tf\nimport os, codecs,sys\nimport numpy as np\nimport pandas as pd\nfrom utils import load_sentences,BatchManager,create_model_and_embedding,get_logger,save_model,input_from_line,load_sor_vocab,load_mub_vocab\nfrom model import Model\nfrom flask import Flask, jsonify, request\nfrom collections import OrderedDict\n\nflags = tf.app.flags\nflags.DEFINE_integer(\"block\",6,\"layer_size\")\nflags.DEFINE_integer(\"sequence_length\",20,\"word vector dim\")\nflags.DEFINE_integer(\"steps_check\", 10, \"steps per checkpoint\")\nflags.DEFINE_integer(\"num_of_epoch\", 100000, \"epoch number\")\nflags.DEFINE_integer(\"batch_size\",64 ,\"word vector dim\")\nflags.DEFINE_integer('hidden_units',128,'   ')\nflags.DEFINE_integer('num_blocks',6,'   ')\nflags.DEFINE_integer('num_heads',8,'   ')\nflags.DEFINE_float(\"dropout_rate\", 0.0, \"Learning rate\")\n\nflags.DEFINE_string(\"model_path\",\"model/\",\"vocab file path\")\nflags.DEFINE_string(\"train_sor_path\",\"data/train.ask.tsv\",\"train file path\")\nflags.DEFINE_string(\"train_mub_path\",\"data/train.answer.tsv\",\"train file path\")\nflags.DEFINE_string(\"logger_path\",\"logger/train.log\",\"vocab file path\")\nflags.DEFINE_float(\"learning_rate\", 0.00001, \"Learning rate\")\nflags.DEFINE_string(\"optimizer\",    \"adam\",     \"Optimizer for training\")\nflags.DEFINE_boolean('flag',True,' ')\nFLAGS = tf.app.flags.FLAGS\napp = Flask(__name__)\ndef config_model():\n    config = OrderedDict()\n    config[\"optimizer\"] = FLAGS.optimizer\n    config[\"layer_size\"] = FLAGS.block\n    config[\"sequence_length\"] = FLAGS.sequence_length\n    config[\"batch_size\"] = FLAGS.batch_size\n    config[\"hidden_units\"] = FLAGS.hidden_units\n    config[\"num_blocks\"] = FLAGS.num_blocks\n    config[\"num_heads\"] = FLAGS.num_heads\n    config[\"dropout_rate\"] = FLAGS.dropout_rate    \n    \n    config[\"train_sor_path\"] = FLAGS.train_sor_path\n    config[\"train_mub_path\"] = FLAGS.train_mub_path\n    config[\"model_path\"] = FLAGS.model_path\n    config[\"logger_path\"] = FLAGS.logger_path\n    config[\"learning_rate\"] = FLAGS.learning_rate\n    config['flag'] = FLAGS.flag\n    return config\ndef train():\n    #加载训练数据并生成可训练数据\n    train_sor_data,train_mub_data = load_sentences(FLAGS.train_sor_path,FLAGS.train_mub_path)\n    #将训练数据处理成N批次数据\n    train_manager = BatchManager(train_sor_data,train_mub_data, FLAGS.batch_size)\n    #设置gpu参数\n    tf_config = tf.ConfigProto()\n    tf_config.gpu_options.allow_growth = True\n    #加载FLAGS参数\n    config = config_model()\n    logger = get_logger(config[\"logger_path\"])\n    #计算批次数\n    word2id,id2word = load_sor_vocab() \n    steps_per_epoch = train_manager.len_data\n    with tf.Session(config=tf_config) as sess:\n        model = create_model_and_embedding(sess, Model, FLAGS.model_path, config,True)\n        logger.info(\"start training\")\n        loss = []  \n        with tf.device('/gpu:0'):\n            for i in range(FLAGS.num_of_epoch):\n                for batch in train_manager.iter_batch(shuffle=True):\n                    step,batch_loss = model.run_step(sess,True,batch)\n                    loss.append(batch_loss)\n                    if step%FLAGS.steps_check == 0:\n                        iteration = step // steps_per_epoch + 1\n                        logger.info(\"iteration:{} step:{}/{},chatbot loss:{:>9.6f}\".format(iteration, step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n                        loss = []\n                if i%10 == 0:\n                    save_model(sess, model, FLAGS.model_path,logger) \ndef predict():\n    word2id,id2word = load_sor_vocab()   \n    tf_config = tf.ConfigProto()\n    tf_config.gpu_options.allow_growth = True\n    config = config_model() \n    logger = get_logger(config[\"logger_path\"])  \n    graph = tf.Graph()\n    sess = tf.Session(graph=graph,config=tf_config)\n    with graph.as_default():\n        sess.run(tf.global_variables_initializer())\n        model = create_model_and_embedding(sess, Model, FLAGS.model_path, config,False)\n        sys.stdout.write('请输入测试句子：')\n        sys.stdout.flush()\n        sentences = sys.stdin.readline()\n        while True:\n            sentences = sentences.replace('\\n','')        \n            rs = model.evaluate_line(sess,input_from_line(sentences,word2id))\n            res = ''.join([id2word[w] for w in rs[0]]).split('</S>')[0].strip()\n            print(res)\n            print('请输入测试句子：',end='')\n            sys.stdout.flush()\n            sentences = sys.stdin.readline()            \n        print('ok')\ndef main(_):\n    predict()\nif __name__ == '__main__':\n    tf.app.run(main)\n```\n\n\n","tags":["nlp","自然语言处理"],"categories":["NLP"]},{"title":"webrtc使用RTCDataChannel交换数据","url":"/2019-03-11/webrtc_tutorial_5/","content":"\n### demo内容\n```\n<!DOCTYPE html>\n<html>\n<head>\n    <title>demo2</title>\n</head>\n<style>\n</style>\n<body>\n<textarea id=\"dataChannelSend\" disabled\n    placeholder=\"Press Start, enter some text, then press Send.\"></textarea>\n<textarea id=\"dataChannelReceive\" disabled></textarea>\n\n<div id=\"buttons\">\n  <button id=\"startButton\">Start</button>\n  <button id=\"sendButton\">Send</button>\n  <button id=\"closeButton\">Stop</button>\n</div>\n</body>\n\n<script src=\"https://webrtc.github.io/adapter/adapter-latest.js\"></script>\n<script type=\"text/javascript\">\n    'use strict';\n    \n    let localConnection;\n    let remoteConnection;\n    let sendChannel;\n    let receiveChannel;\n    const dataChannelSend = document.querySelector('textarea#dataChannelSend');\n    const dataChannelReceive = document.querySelector('textarea#dataChannelReceive');\n    const startButton = document.querySelector('button#startButton');\n    const sendButton = document.querySelector('button#sendButton');\n    const closeButton = document.querySelector('button#closeButton');\n\n    startButton.onclick = createConnection;\n    sendButton.onclick = sendData;\n    closeButton.onclick = closeDataChannels;\n\n    function enableStartButton() {\n      startButton.disabled = false;\n    }\n\n    function disableSendButton() {\n      sendButton.disabled = true;\n    }\n\n    function createConnection() {\n      dataChannelSend.placeholder = '';\n      const servers = null;\n      window.localConnection = localConnection = new RTCPeerConnection(servers);\n      console.log('Created local peer connection object localConnection');\n\n      sendChannel = localConnection.createDataChannel('sendDataChannel');\n      console.log('Created send data channel');\n\n      localConnection.onicecandidate = e => {\n        onIceCandidate(localConnection, e);\n      };\n      sendChannel.onopen = onSendChannelStateChange;\n      sendChannel.onclose = onSendChannelStateChange;\n\n      window.remoteConnection = remoteConnection = new RTCPeerConnection(servers);\n      console.log('Created remote peer connection object remoteConnection');\n\n      remoteConnection.onicecandidate = e => {\n        onIceCandidate(remoteConnection, e);\n      };\n      remoteConnection.ondatachannel = receiveChannelCallback;\n\n      localConnection.createOffer().then(\n        gotDescription1,\n        onCreateSessionDescriptionError\n      );\n      startButton.disabled = true;\n      closeButton.disabled = false;\n    }\n\n    function onCreateSessionDescriptionError(error) {\n      console.log('Failed to create session description: ' + error.toString());\n    }\n\n    function sendData() {\n      const data = dataChannelSend.value;\n      sendChannel.send(data);\n      console.log('Sent Data: ' + data);\n    }\n\n    function closeDataChannels() {\n      console.log('Closing data channels');\n      sendChannel.close();\n      console.log('Closed data channel with label: ' + sendChannel.label);\n      receiveChannel.close();\n      console.log('Closed data channel with label: ' + receiveChannel.label);\n      localConnection.close();\n      remoteConnection.close();\n      localConnection = null;\n      remoteConnection = null;\n      console.log('Closed peer connections');\n      startButton.disabled = false;\n      sendButton.disabled = true;\n      closeButton.disabled = true;\n      dataChannelSend.value = '';\n      dataChannelReceive.value = '';\n      dataChannelSend.disabled = true;\n      disableSendButton();\n      enableStartButton();\n    }\n\n    function gotDescription1(desc) {\n      localConnection.setLocalDescription(desc);\n      console.log(`Offer from localConnection\\n${desc.sdp}`);\n      remoteConnection.setRemoteDescription(desc);\n      remoteConnection.createAnswer().then(\n        gotDescription2,\n        onCreateSessionDescriptionError\n      );\n    }\n\n    function gotDescription2(desc) {\n      remoteConnection.setLocalDescription(desc);\n      console.log(`Answer from remoteConnection\\n${desc.sdp}`);\n      localConnection.setRemoteDescription(desc);\n    }\n\n    function getOtherPc(pc) {\n      return (pc === localConnection) ? remoteConnection : localConnection;\n    }\n\n    function getName(pc) {\n      return (pc === localConnection) ? 'localPeerConnection' : 'remotePeerConnection';\n    }\n\n    function onIceCandidate(pc, event) {\n      getOtherPc(pc)\n        .addIceCandidate(event.candidate)\n        .then(\n          () => onAddIceCandidateSuccess(pc),\n          err => onAddIceCandidateError(pc, err)\n        );\n      console.log(`${getName(pc)} ICE candidate: ${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n\n    function onAddIceCandidateSuccess() {\n      console.log('AddIceCandidate success.');\n    }\n\n    function onAddIceCandidateError(error) {\n      console.log(`Failed to add Ice Candidate: ${error.toString()}`);\n    }\n\n    function receiveChannelCallback(event) {\n      console.log('Receive Channel Callback');\n      receiveChannel = event.channel;\n      receiveChannel.onmessage = onReceiveMessageCallback;\n      receiveChannel.onopen = onReceiveChannelStateChange;\n      receiveChannel.onclose = onReceiveChannelStateChange;\n    }\n\n    function onReceiveMessageCallback(event) {\n      console.log('Received Message');\n      dataChannelReceive.value = event.data;\n    }\n\n    function onSendChannelStateChange() {\n      const readyState = sendChannel.readyState;\n      console.log('Send channel state is: ' + readyState);\n      if (readyState === 'open') {\n        dataChannelSend.disabled = false;\n        dataChannelSend.focus();\n        sendButton.disabled = false;\n        closeButton.disabled = false;\n      } else {\n        dataChannelSend.disabled = true;\n        sendButton.disabled = true;\n        closeButton.disabled = true;\n      }\n    }\n\n    function onReceiveChannelStateChange() {\n      const readyState = receiveChannel.readyState;\n      console.log(`Receive channel state is: ${readyState}`);\n    }\n</script>\n\n</html>\n```\n\n### 工作原理\n\n此代码使用RTCPeerConnection和RTCDataChannel来启用文本消息的交换。\n\n此步骤中的大部分代码与RTCPeerConnection示例相同。\n\nRTCDataChannel的语法有意类似于WebSocket，带有send（）方法和消息事件。\n\n注意dataConstraint的使用。可以配置数据通道以实现不同类型的数据共享 - 例如，优先考虑可靠的交付而不是性能。您可以在[Mozilla Developer Network](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createDataChannel)上找到有关选项的更多信息。\n\n\n### 加分项\n1. WebRTC数据通道使用的协议使用[SCTP](https://bloggeek.me/sctp-data-channel/)，默认使用可靠和有序的数据传输。在 RTCDataChannel 需要提供可靠的数据传输时或更重要的信息时使用它， 即使这可能意味着丢失一部分数据。[查阅为什么使用sctp传输?](https://bloggeek.me/sctp-data-channel/)\n2. 使用CSS改进页面布局，并将“占位符”属性添加到“dataChannelReceive”textarea。\n3. 在移动设备上测试页面。\n\n\n\n\n\n\n","tags":["Tutorial","webrtc"],"categories":["WebRTC"]},{"title":"webrtc使用RTCPeerConnection进行流传输","url":"/2019-03-09/webrtc_tutorial_4/","content":"\n## 视频流与RTCPeerConnection\n\n### 学习提纲\n- 各浏览器质检webrtc的差异,[adapter.js](https://github.com/webrtc/adapter)\n- 使用RTCPeerConnection API 传输视频流.\n- 流的控制与捕捉\n\n\n### RTCPeerConnection简介\n\nRTCPeerConnection是一个是得webrtc之间视频与音频数据交换的api协议.本案例是在同一个页面上使用RTCPeerConnection建立一个P2P连接.没有太大实用性只是为了方便理解.\n\n\n### 在页面添加视频控制按钮\n\n一个视频元素将显示来自getUserMedia()的流,另一个将显示通过RTCPeerconnection流式传输的相同视频. (在实际应用程序中,一个视频元素将显示本地流,另一个视频元素将显示远程流.)\n```\n<video id=\"localVideo\" autoplay playsinline></video>\n<video id=\"remoteVideo\" autoplay playsinline></video>\n\n<div>\n  <button id=\"startButton\">Start</button>\n  <button id=\"callButton\">Call</button>\n  <button id=\"hangupButton\">Hang Up</button>\n</div>\n```\n\n### 去除浏览器间的差异\n\n在页面js最前边增加 [adapter.js](https://github.com/webrtc/adapter)\n```\n<script src=\"https://webrtc.github.io/adapter/adapter-latest.js\"></script>\n```\n\n### 新增RTCPeerConnection代码\n```\n<!DOCTYPE html>\n<html>\n<head>\n    <title>demo2</title>\n</head>\n<style>\n</style>\n<body>\n\n    <video id=\"localVideo\" autoplay playsinline></video>\n    <video id=\"remoteVideo\" autoplay playsinline></video>\n\n    <div>\n        <button id=\"startButton\">Start</button>\n        <button id=\"callButton\">Call</button>\n        <button id=\"hangupButton\">Hang Up</button>\n    </div>\n\n    <div class=\"box\">\n        <span>SDP Semantics:</span>\n        <select id=\"sdpSemantics\">\n            <option selected value=\"\">Default</option>\n            <option value=\"unified-plan\">Unified Plan</option>\n            <option value=\"plan-b\">Plan B</option>\n        </select>\n    </div>\n</body>\n\n<script src=\"https://webrtc.github.io/adapter/adapter-latest.js\"></script>\n<script type=\"text/javascript\">\n    'use strict';\n\n    const startButton = document.getElementById('startButton');\n    const callButton = document.getElementById('callButton');\n    const hangupButton = document.getElementById('hangupButton');\n    callButton.disabled = true;\n    hangupButton.disabled = true;\n    startButton.addEventListener('click', start);\n    callButton.addEventListener('click', call);\n    hangupButton.addEventListener('click', hangup);\n\n    let startTime;\n    const localVideo = document.getElementById('localVideo');\n    const remoteVideo = document.getElementById('remoteVideo');\n\n    localVideo.addEventListener('loadedmetadata', function() {\n      console.log(`Local video videoWidth: ${this.videoWidth}px,  videoHeight: ${this.videoHeight}px`);\n    });\n\n    remoteVideo.addEventListener('loadedmetadata', function() {\n      console.log(`Remote video videoWidth: ${this.videoWidth}px,  videoHeight: ${this.videoHeight}px`);\n    });\n\n    remoteVideo.addEventListener('resize', () => {\n      console.log(`Remote video size changed to ${remoteVideo.videoWidth}x${remoteVideo.videoHeight}`);\n      // We'll use the first onsize callback as an indication that video has started\n      // playing out.\n      if (startTime) {\n        const elapsedTime = window.performance.now() - startTime;\n        console.log('Setup time: ' + elapsedTime.toFixed(3) + 'ms');\n        startTime = null;\n      }\n    });\n\n    let localStream;\n    let pc1;\n    let pc2;\n    const offerOptions = {\n      offerToReceiveAudio: 1,\n      offerToReceiveVideo: 1\n    };\n\n    function getName(pc) {\n      return (pc === pc1) ? 'pc1' : 'pc2';\n    }\n\n    function getOtherPc(pc) {\n      return (pc === pc1) ? pc2 : pc1;\n    }\n\n    async function start() {\n      console.log('Requesting local stream');\n      startButton.disabled = true;\n      try {\n        const stream = await navigator.mediaDevices.getUserMedia({audio: true, video: true});\n        console.log('Received local stream');\n        localVideo.srcObject = stream;\n        localStream = stream;\n        callButton.disabled = false;\n      } catch (e) {\n        alert(`getUserMedia() error: ${e.name}`);\n      }\n    }\n\n    function getSelectedSdpSemantics() {\n      const sdpSemanticsSelect = document.querySelector('#sdpSemantics');\n      const option = sdpSemanticsSelect.options[sdpSemanticsSelect.selectedIndex];\n      return option.value === '' ? {} : {sdpSemantics: option.value};\n    }\n\n    async function call() {\n      callButton.disabled = true;\n      hangupButton.disabled = false;\n      console.log('Starting call');\n      startTime = window.performance.now();\n      const videoTracks = localStream.getVideoTracks();\n      const audioTracks = localStream.getAudioTracks();\n      if (videoTracks.length > 0) {\n        console.log(`Using video device: ${videoTracks[0].label}`);\n      }\n      if (audioTracks.length > 0) {\n        console.log(`Using audio device: ${audioTracks[0].label}`);\n      }\n      const configuration = getSelectedSdpSemantics();\n      console.log('RTCPeerConnection configuration:', configuration);\n      pc1 = new RTCPeerConnection(configuration);\n      console.log('Created local peer connection object pc1');\n      pc1.addEventListener('icecandidate', e => onIceCandidate(pc1, e));\n      pc2 = new RTCPeerConnection(configuration);\n      console.log('Created remote peer connection object pc2');\n      pc2.addEventListener('icecandidate', e => onIceCandidate(pc2, e));\n      pc1.addEventListener('iceconnectionstatechange', e => onIceStateChange(pc1, e));\n      pc2.addEventListener('iceconnectionstatechange', e => onIceStateChange(pc2, e));\n      pc2.addEventListener('track', gotRemoteStream);\n\n      localStream.getTracks().forEach(track => pc1.addTrack(track, localStream));\n      console.log('Added local stream to pc1');\n\n      try {\n        console.log('pc1 createOffer start');\n        const offer = await pc1.createOffer(offerOptions);\n        await onCreateOfferSuccess(offer);\n      } catch (e) {\n        onCreateSessionDescriptionError(e);\n      }\n    }\n\n    function onCreateSessionDescriptionError(error) {\n      console.log(`Failed to create session description: ${error.toString()}`);\n    }\n\n    async function onCreateOfferSuccess(desc) {\n      console.log(`Offer from pc1\\n${desc.sdp}`);\n      console.log('pc1 setLocalDescription start');\n      try {\n        await pc1.setLocalDescription(desc);\n        onSetLocalSuccess(pc1);\n      } catch (e) {\n        onSetSessionDescriptionError();\n      }\n\n      console.log('pc2 setRemoteDescription start');\n      try {\n        await pc2.setRemoteDescription(desc);\n        onSetRemoteSuccess(pc2);\n      } catch (e) {\n        onSetSessionDescriptionError();\n      }\n\n      console.log('pc2 createAnswer start');\n      // Since the 'remote' side has no media stream we need\n      // to pass in the right constraints in order for it to\n      // accept the incoming offer of audio and video.\n      try {\n        const answer = await pc2.createAnswer();\n        await onCreateAnswerSuccess(answer);\n      } catch (e) {\n        onCreateSessionDescriptionError(e);\n      }\n    }\n\n    function onSetLocalSuccess(pc) {\n      console.log(`${getName(pc)} setLocalDescription complete`);\n    }\n\n    function onSetRemoteSuccess(pc) {\n      console.log(`${getName(pc)} setRemoteDescription complete`);\n    }\n\n    function onSetSessionDescriptionError(error) {\n      console.log(`Failed to set session description: ${error.toString()}`);\n    }\n\n    function gotRemoteStream(e) {\n      if (remoteVideo.srcObject !== e.streams[0]) {\n        remoteVideo.srcObject = e.streams[0];\n        console.log('pc2 received remote stream');\n      }\n    }\n\n    async function onCreateAnswerSuccess(desc) {\n      console.log(`Answer from pc2:\\n${desc.sdp}`);\n      console.log('pc2 setLocalDescription start');\n      try {\n        await pc2.setLocalDescription(desc);\n        onSetLocalSuccess(pc2);\n      } catch (e) {\n        onSetSessionDescriptionError(e);\n      }\n      console.log('pc1 setRemoteDescription start');\n      try {\n        await pc1.setRemoteDescription(desc);\n        onSetRemoteSuccess(pc1);\n      } catch (e) {\n        onSetSessionDescriptionError(e);\n      }\n    }\n\n    async function onIceCandidate(pc, event) {\n      try {\n        await (getOtherPc(pc).addIceCandidate(event.candidate));\n        onAddIceCandidateSuccess(pc);\n      } catch (e) {\n        onAddIceCandidateError(pc, e);\n      }\n      console.log(`${getName(pc)} ICE candidate:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n\n    function onAddIceCandidateSuccess(pc) {\n      console.log(`${getName(pc)} addIceCandidate success`);\n    }\n\n    function onAddIceCandidateError(pc, error) {\n      console.log(`${getName(pc)} failed to add ICE Candidate: ${error.toString()}`);\n    }\n\n    function onIceStateChange(pc, event) {\n      if (pc) {\n        console.log(`${getName(pc)} ICE state: ${pc.iceConnectionState}`);\n        console.log('ICE state change event: ', event);\n      }\n    }\n\n    function hangup() {\n      console.log('Ending call');\n      pc1.close();\n      pc2.close();\n      pc1 = null;\n      pc2 = null;\n      hangupButton.disabled = true;\n      callButton.disabled = false;\n    }\n</script>\n\n</html>\n```\n\n### 工作原理\n\nWebRTC使用RTCPeerConnection API在不同的rtc客户端建立连接从来进行流式视频传输,这就是点对点通讯.\n\n在webrtc点对点通讯的的三个步骤中设置对应的通话:\n- 在每个会话端添加一个RTCPeerConnection对象,同时用getUserMedia()获取本地视频流\n- 获取并共享网络信息:潜在的连接端点称为ICE候选者\n- 获取并共享本地和远程描述:SDP格式的本地媒体元数据.\n\n想象一下,Alice和Bob想要使用RTCPeerConnection来设置视频聊天.\n首先,Alice和Bob交换网络信息.\"查找候选者\"一词是指使用ICE框架查找网络接口和端口的过程.\n1. Alice 使用 onicecandidate(addEventListener('icecandidate')) 处理程序创建一个  RTCPeerConnection 对象.这对应于main.js中的以下代码:\n```\nlet localPeerConnection;\n//作为此过程的一部分,WebRTC  API 使用STUN服务器获取计算机的IP地址,并使用TURN服务器作为中继服务器,以防对等通信失败\nlocalPeerConnection = new RTCPeerConnection(servers); // servers特指stun或者turn服务 \nlocalPeerConnection.addEventListener('icecandidate', handleConnection);\nlocalPeerConnection.addEventListener('iceconnectionstatechange',handleConnectionChange);\n```\n2.Alice调用getUserMedia()并添加参数传递本地流:\n```\nnavigator.mediaDevices.getUserMedia(mediaStreamConstraints). then(gotLocalMediaStream). catch(handleLocalMediaStreamError);\n  \nfunction gotLocalMediaStream(mediaStream) {\n    localVideo.srcObject = mediaStream;\n    localStream = mediaStream;\n    trace('Received local stream.');\n    callButton.disabled = false;  // Enable call button.\n}\n\nlocalPeerConnection.addStream(localStream);\ntrace('Added local stream to localPeerConnection.');\n```\n3.当网络候选可用时,将调用步骤1中的onicecandidate处理程序.\n\n4.lice将序列化的候选数据发送给Bob.在实际应用程序中,此过程(称为信令)通过消息传递服务进行 - 您将在后续步骤中学习如何执行此操作. 当然,在此步骤中,两个RTCPeerConnection对象位于同一页面上,可以直接通信,无需外部消息传递.\n\n5.当Bob从Alice获取候选消息时,他调用addIceCandidate(),将候选者添加到远程对等描述中:\n```\nfunction handleConnection(event) {\n  const peerConnection = event.target;\n  const iceCandidate = event.candidate;\n\n  if (iceCandidate) {\n    const newIceCandidate = new RTCIceCandidate(iceCandidate);\n    const otherPeer = getOtherPeer(peerConnection);\n\n    otherPeer.addIceCandidate(newIceCandidate)\n      .then(() => {\n        handleConnectionSuccess(peerConnection);\n      }).catch((error) => {\n        handleConnectionFailure(peerConnection, error);\n      });\n\n    trace(`${getPeerName(peerConnection)} ICE candidate:\\n` +\n          `${event.candidate.candidate}.`);\n  }\n}\n```\n\nWebRTC点对点还需要找出交换本地和远程的音频与视频媒体信息,例如分辨率和编解码器功能.通过使用称为SDP的会话描述协议格式交换元数据块(称为offer和answer)来进行交换媒体配置信息的信令:\n\n1.Alice运行RTCPeerConnection createOffer()方法.返回的promise提供了一个RTCSessionDescription:Alice的本地会话描述:\n```\ntrace('localPeerConnection createOffer start.');\nlocalPeerConnection.createOffer(offerOptions)\n  .then(createdOffer).catch(setSessionDescriptionError);\n```\n2.如果成功,Alice使用setLocalDescription()设置本地描述,然后通过其信令通道将此会话描述发送给Bob.\n\n3.Bob使用setRemoteDescription()将Alice发送给他的描述设置为远程描述.\n\n4.ob运行RTCPeerConnection createAnswer()方法,向其传递从Alice获得的远程描述,因此可以生成与她兼容的本地会话.createAnswer()承诺传递RTCSessionDescription:Bob将其设置为本地描述并将其发送给Alice.\n\n5.当Alice获取Bob的会话描述时,她使用setRemoteDescription()将其设置为远程描述.\n```\n// Logs offer creation and sets peer connection session descriptions.\nfunction createdOffer(description) {\n  trace(`Offer from localPeerConnection:\\n${description.sdp}`);\n\n  trace('localPeerConnection setLocalDescription start.');\n  localPeerConnection.setLocalDescription(description)\n    .then(() => {\n      setLocalDescriptionSuccess(localPeerConnection);\n    }).catch(setSessionDescriptionError);\n\n  trace('remotePeerConnection setRemoteDescription start.');\n  remotePeerConnection.setRemoteDescription(description)\n    .then(() => {\n      setRemoteDescriptionSuccess(remotePeerConnection);\n    }).catch(setSessionDescriptionError);\n\n  trace('remotePeerConnection createAnswer start.');\n  remotePeerConnection.createAnswer()\n    .then(createdAnswer)\n    .catch(setSessionDescriptionError);\n}\n\n// Logs answer to offer creation and sets peer connection session descriptions.\nfunction createdAnswer(description) {\n  trace(`Answer from remotePeerConnection:\\n${description.sdp}.`);\n\n  trace('remotePeerConnection setLocalDescription start.');\n  remotePeerConnection.setLocalDescription(description)\n    .then(() => {\n      setLocalDescriptionSuccess(remotePeerConnection);\n    }).catch(setSessionDescriptionError);\n\n  trace('localPeerConnection setRemoteDescription start.');\n  localPeerConnection.setRemoteDescription(description)\n    .then(() => {\n      setRemoteDescriptionSuccess(localPeerConnection);\n    }).catch(setSessionDescriptionError);\n}\n```\n6.ping!\n\n### 关键点\n1.看看chrome://webrtc-internals.这提供了WebRTC统计信息和调试数据.(Chrome网址的完整列表位于chrome:// about.)\n\n2.使用CSS设置页面样式:\n- 将视频并排放置.\n- 使按钮具有相同的宽度,文本更大.\n- 确保布局适用于移动设备.\n\n3.在Chrome DevTools控制台中,查看localStream,localPeerConnection和remotePeerConnection.\n\n4.在控制台中,查看localPeerConnectionpc1.localDescription. SDP格式是什么样的?\n\n### 提示\n- 这一步有很多值得学习的地方!要查找更详细地解释RTCPeerConnection的其他资源,请查看[webrtc.org/start](https://webrtc.org/start).此页面包含JavaScript框架的建议 - 如果您想使用WebRTC,但又不想争论API.\n- 从adapter.js GitHub repo中了解有关adapter.js shim的更多信息.\n- 想看看世界上最好的视频聊天应用程序是什么样的? 看看AppRTC,这是WebRTC项目的WebRTC调用规范应用程序:app,code.呼叫建立时间小于500毫秒.\n\n\n\n","tags":["Tutorial","webrtc"],"categories":["WebRTC"]},{"title":"webrtc从摄像头获取视频流","url":"/2019-03-08/webrtc_tutorial_3/","content":"\n## 从摄像头获取视频流\n\n### 学习提纲\n\n- 从摄像头获取视频流 \n- 操纵流的回播\n- 使用css和svg来操作视频\n\n### 简短的HTML\n\n在你的项目文件夹下的index.html文件中添加video和script元素\n```\n<!DOCTYPE html>\n<html>\n<head>\n    <title>demo1</title>\n</head>\n<body>\n\n<video autoplay playsinline></video>\n\n</body>\n<script type=\"text/javascript\">\n    'use strict'; //避免常见的编码陷阱。\n\n    var constrains= {\n        video: true, // 视频流\n        audio: true // 音频流\n    };\n\n    // 页面中video放置的位置\n    var localVideo  = document.querySelector('video')\n    \n    // 视频的本地流\n    var localStream;\n    //通过将MediaStream添加到页面中视频元素节点来处理成功\n    function gotLocalMediaStream(mediaStream){\n        //成功后回调\n        localStream = mediaStream;\n        localVideo.srcObject = mediaStream;\n    }\n\n    // 调起摄像头失败后回调\n    function handleError(error) {\n        console.error(\"getUserMedia error\", error)\n    }\n\n    // 调起摄像头 初始化媒体流。\n    navigator.mediaDevices.getUserMedia(constrains).then(gotLocalMediaStream).catch(handleError);\n\n</script>\n</html>\n\n```\n\n### 运行原理\n\n在getUserMedia（）调用之后，浏览器请求用户访问其摄像头的权限（如果这是第一次请求当前源的摄像头访问）。如果成功，则返回MediaStream，媒体元素可以通过srcObject属性使用它：\n```\nnavigator.mediaDevices.getUserMedia(mediaStreamConstraints).then(gotLocalMediaStream).catch(handleLocalMediaStreamError);\n\nfunction gotLocalMediaStream(mediaStream) {\n  localVideo.srcObject = mediaStream;\n}\n\n```\n你可以使用约束来满足其他要求，例如视频分辨率：\n```\nconst hdConstraints = {\n  video: {\n    width: {\n      min: 1280\n    },\n    height: {\n      min: 720\n    }\n  }\n}\n```\n[MediaTrackConstraints](https://w3c.github.io/mediacapture-main/getusermedia.html#media-track-constraints)规范列出了所有可能的约束类型，但并非所有浏览器都支持所有选项。如果当前选定的摄像机不支持所请求的分辨率，则getUserMedia（）将被OverconstrainedError拒绝，并且不会提示用户授予访问其摄像机的权限。\n\n> 您可以在[此处](https://simpl.info/getusermedia/constraints/)查看演示如何使用约束来请求不同分辨率的演示，以及使用约束在[此处](https://simpl.info/getusermedia/sources/)选择摄像机和麦克风的演示。\n\n### 关键点\n\n- 传递给getUserMedia() 的localStream对象在全局范围内，因此您可以从浏览器控制台进行检查：打开控制台，键入stream并按Return键。\n- 使用localStream.getVideoTracks() 会返回什么呢？ //此流中视频轨道的MediaStreamTrack对象序列。\n- 试试 localStream.getVideoTracks()[0].stop(). // 关闭摄像头\n- 改变传入参数{audio: true, video: true}查看变化\n- 视频元素的大小是多少？如何从JavaScript中获取视频的分辨率大小，而不是显示大小？使用Chrome开发工具进行检查。 // 使用localVideo.videoWidth \n- 使用css过滤器改变视频元素\n```\nvideo {\n  filter: blur(4px) invert(1) opacity(0.5);\n}\n```\n- 使用svg过滤器改变视频元素样式\n```\nvideo {\n   filter: hue-rotate(180deg) saturate(200%);\n }\n```\n\n### 提示\n\n- 不要忘记视频元素上的autoplay属性。没有它，你只会看到一个帧！\n- getUserMedia()约束有很多选项。在 webrtc.github.io/samples/src/content/peerconnection/constraints 上查看演示。正如您将看到的，该网站上有许多有趣的WebRTC样本。\n\n### 分辨率练习\n\n确保您的视频元素不会溢出其容器。我们添加了宽度和最大宽度来设置视频的首选大小和最大大小。浏览器将自动计算高度：\n```\nvideo {\n  max-width: 100%;\n  width: 320px;\n}\n```\n","tags":["Tutorial","webrtc"],"categories":["WebRTC"]},{"title":"webrtc简介","url":"/2019-03-06/webrtc_tutorial_2/","content":"\n## 概括\n\n构建应用程序以获取视频并使用网络摄像头拍摄快照，并通过WebRTC进行点对点共享。在此过程中，你将学习如何使用WebRTC 核心 API 并使用 Node.js 设置消息传递服务器。\n\n\n### 能学到什么\n\n- [x] 从摄像头获取视频\n- [x] 通过RTCPeerConnection获取视频流\n- [x] 通过RTCDataChannel传输流数据\n- [x] 建立信令服务交换消息\n- [x] 和信令服务建立peer连接\n- [x] 拍照并通过数据通道分享\n\n### 环境背景\n\n- chrome 47 及以上\n- [适用于Chrome的Web Server](https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb)，或使用自己选择的Web服务器。\n- 案例代码\n- 文本编辑器[notpad++/subline]\n- 基本的前端知识[html/css/js]\n\n\n---\n\n\n## 运行demo案例\n\n### 下载\n\n从git上获取\n```\ngit clone https://github.com/googlecodelabs/webrtc-web\n```\n或者直接下载[zip包](https://github.com/googlecodelabs/webrtc-web/archive/master.zip)\n\n\n### 安装并验证Web服务器\n\n虽然可以自由使用自己的Web服务器，但此codelab可以与Chrome Web服务器配合使用。如果尚未安装该应用，则可以从Chrome网上应用店安装该应用。\n> https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb?hl=en\n\n安装Web Server for Chrome应用程序后，单击书签栏，新标签页或应用启动器中的Chrome应用程序快捷方式：","tags":["Tutorial","webrtc"],"categories":["WebRTC"]},{"title":"webrtc简介","url":"/2019-03-03/webrtc_tutorial_1/","content":"\n## 简介\n\nwebrtc是一个再web端和移动app端实时交流与数据传输的开源项目。webrtc有一些js接口 - 点击下面的案例链接即可看demo。\n\n- [getUserMedia()](https://webrtc.github.io/samples/src/content/getusermedia/gum/): 获取本地音频和视频.\n- [MediaRecorder](https://webrtc.github.io/samples/src/content/getusermedia/record/): 录制本地音频和视频.\n- [RTCPeerConnection](https://webrtc.github.io/samples/src/content/peerconnection/pc1/): 用户间建立点对点通讯传输传输音频和视频.\n- [RTCDataChannel](https://webrtc.github.io/samples/src/content/datachannel/basic/): 用户间数据传输.\n\n\n### 在哪使用webrtc\n\n在Firefox，Opera和桌面和Android上的Chrome中。WebRTC也可用于iOS和Android上的原生应用程序。\n\n### 信令服务\nWebRTC使用RTCPeerConnection在浏览器之间传递流数据，但也需要一种协调通信和发送控制消息的机制，这一过程称为信令.WebRTC未指定信令方法和协议。在这个代码框中，您将使用Socket.IO进行消息传递，但有许多[替代方案](https://github.com/muaz-khan/WebRTC-Experiment/blob/master/Signaling.md)。\n\n\n### 什么是STUN和TURN\n\nwebrtc设计的初衷是为了p2p服务，因为用户间可以建立点对点通讯线路.然而，WebRTC旨在应对真实的网络：客户端应用程序需要遍历NAT网关和防火墙，并且在直接连接失败的情况下，对等网络需要回退。作为此过程的一部分，WebRTC API 使用 STUN 服务器获取计算机的IP地址,并使用TURN服务器作为中继服务器，以防p2p通信失败。 （WebRTC在[实际网络](https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/)中有更详细的解释。）\n\n\n### webrtc安全机制\n\n所有WebRTC组件都必须进行加密，并且其JavaScript API 只能用于安全来源（HTTPS或localhost）。 WebRTC标准没有定义信令机制，因此您需要确保使用安全协议。\n\n> 更多关于webrtc的讯息和学习资料可查阅[webrtc.org/start.](https://webrtc.org/start/)","tags":["Tutorial","webrtc"],"categories":["WebRTC"]},{"title":"解决简书等图床防盗链的问题","url":"/2018-06-24/解决简书等图床防盗链的问题/","content":"\n## 尝试清除来源引用\n在静态网页的 头部 代码中【即 head 标记】添加如下配置项：\n\n<meta name=\"referrer\" content=\"no-referrer\" />\n它的作用就是阻止浏览器发送 Referer 信息，对整个页面的所有链接生效【当然也有针对单个链接设置的方法：<a rel=”noreferrer” href=”your-website-url” />，这里不采用】，这样一来微博图床就不知道请求的引用来源了，可以达到和直接在浏览器中访问一样的效果。 但是要注意，不是每种浏览器都支持这种语法的，此设置对有的浏览器来说无效。\n\n那么在 Hexo 框架中怎么增加呢，显然不会有相关配置项，只能更改源代码，而且使用了 Next 主题，应该要更改主题的源代码，以保证 Hexo 在渲染静态页面为每个页面都增加这个配置。查阅文档，了解了渲染模板所在位置，打开 themes/next/layout/_partials/head.swig 文件，在里面添加 meta 标记就行。\n\n\n修改完成后查看页面的源代码，已经有这个属性了，并且所有的图片都可以正常访问了，完美。","tags":["外链"],"categories":["运维"]},{"title":"RPC机制以及原理","url":"/2018-06-15/reference/RPC机制及原理/","content":"\n>[RPC详解](<https://yq.aliyun.com/articles/611352>)\n>\n>[远程过程调用(RPC)详解](<https://waylau.com/remote-procedure-calls/>)\n>\n>[RPC原理详解](https://www.cnblogs.com/metoy/p/4321311.html)\n>\n>[RPC入门总结（一）RPC定义和原理](<https://blog.csdn.net/KingCat666/article/details/78577079>)\n>\n>[RPC框架设计和调用详解](<https://blog.csdn.net/zjx86320/article/details/51019050>)\n\nTODO\n\n","tags":["RPC"],"categories":["RPC"]},{"title":"计算机相关技术资料整理","url":"/2018-05-13/reference/practical-programming-books/README/","content":"\n> [文章来源](<https://github.com/EZLippi/practical-programming-books>)\n\n这里收录比较实用的计算机相关技术书籍，可以在短期之内入门的简单实用教程、一些技术网站以及一些写的比较好的博文，欢迎Fork，你也可以通过Pull Request参与编辑。\n\n\n\n* [程序员必读书籍](http://www.ezlippi.com/blog/2014/07/qualified-programmer-should-read-what-books.html)\n\n<!-- more -->\n\n## 目录\n\n\n* [语言相关类](#语言相关类)\n  * [Android](#android)\n  * [AWK](#awk)\n  * [SED](#SED)\n  * [C/C++](#cc)\n  * [CSS/HTML](#css)\n  * [Dart](#dart)\n  * [Erlang](#erlang)\n  * [Fortran](#fortran)\n  * [Go](#go)\n  * [Groovy](#groovy)\n  * [Haskell](#haskell)\n  * [iOS](#ios)\n  * [Java](#java)\n  * [JavaScript](#javascript)\n  * [LaTeX](#latex)\n  * [LISP](#lisp)\n  * [Lua](#lua)\n  * [Perl](#perl)\n  * [PHP](#php)\n  * [Prolog](#prolog)\n  * [Python](#python)\n  * [R](#r)\n  * [Ruby](#ruby)\n  * [Scala](#scala)\n  * [Scheme](#scheme)\n  * [Shell](#shell)\n  * [Swift](#swift)\n  * [WebAssembly](#webassembly)\n\n* [语言无关类](#语言无关类)\n  * [操作系统](#操作系统)\n  * [版本控制](#版本控制)\n  * [分布式系统](#分布式系统)\n  * [编辑器](#编辑器)\n  * [NoSQL](#nosql)\n  * [MySQL](#mysql)\n  * [PostgreSQL](#postgresql)\n  * [项目相关](#项目相关)\n  * [设计模式](#设计模式)\n  * [Web](#web)\n  * [大数据](#大数据)\n  * [编程艺术](#编程艺术)\n  * [函数式编程](#函数式编程)\n  * [运维监控](#运维监控)\n  * [WEB服务器](#web服务器)\n\t\n## 语言无关类\n\n### 操作系统\n\n* [开源世界旅行手册](http://i.linuxtoy.org/docs/guide/index.html)\n* [鸟哥的Linux私房菜](http://vbird.dic.ksu.edu.tw/)\n* [Linux 系统高级编程](http://sourceforge.net/apps/trac/elpi/wiki/ALP)\n* [Zephyr OS 中文文档](https://github.com/tidyjiang8/zephyr-doc)(v1.6.0)\n* [The Linux Command Line](http://billie66.github.io/TLCL/index.html) (中英文版)\n* [Linux 设备驱动](http://oss.org.cn/kernel-book/ldd3/index.html) (第三版)\n* [深入分析Linux内核源码](http://www.kerneltravel.net/kernel-book/%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90Linux%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81.html)\n* [UNIX TOOLBOX](http://cb.vu/unixtoolbox_zh_CN.xhtml)\n* [Docker中文指南](https://github.com/widuu/chinese_docker)\n* [Docker —— 从入门到实践](https://github.com/yeasy/docker_practice)\n* [Docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n* [Docker Cheat Sheet](https://github.com/wsargent/docker-cheat-sheet/tree/master/zh-cn#docker-cheat-sheet)\n* [FreeRADIUS新手入门](http://freeradius.akagi201.org)\n* [Mac 开发配置手册](https://aaaaaashu.gitbooks.io/mac-dev-setup/content/)\n* [FreeBSD 使用手册](https://www.freebsd.org/doc/zh_CN/books/handbook/index.html)\n* [Linux 命令行(中文版)](http://billie66.github.io/TLCL/book/)\n* [Linux 构建指南](http://works.jinbuguo.com/lfs/lfs62/index.html)\n* [Linux工具快速教程](https://github.com/me115/linuxtools_rst)\n* [Linux Documentation (中文版)](https://www.gitbook.com/book/tinylab/linux-doc/details)\n* [嵌入式 Linux 知识库 (eLinux.org 中文版)](https://www.gitbook.com/book/tinylab/elinux/details)\n* [理解Linux进程](https://github.com/tobegit3hub/understand_linux_process)\n* [Linux From Scratch systemd 中文翻译](https://github.com/ryanzz/LFS-systemd-zh_CN)\n* [55分钟学会正则表达式](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/30-minutes-to-learn-regex.md)\n* [每个Linux用户都应该知道的命令行技巧](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/use-linux.md)\n* [每个程序员都应该了解的内存知识](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/memory.md) \n* [每个程序员都应该了解的CPU缓存知识](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/cpu-cache.md) \n* [每个程序员都应该了解的虚拟内存知识](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/virtual-memory.md) \n* [shell脚本教程](http://billie66.gitbooks.io/tlcl-cn/content/)\n* [查找命令行的网站](http://www.commandlinefu.com/commands/matching/ls/bHM=/sort-by-votes)\n* [正则表达式在线测试](http://www.rubular.com/)\n* [科学上网](http://www.ezlippi.com/blog/2017/02/ss-proxy-guide.html)\n\n---------------------------------\n\n### 分布式系统\n* [走向分布式](http://dcaoyuan.github.io/papers/pdfs/Scalability.pdf)\n\n----------------------------------\n\n### 函数式编程\n\n* [傻瓜函数编程](https://github.com/justinyhuang/Functional-Programming-For-The-Rest-of-Us-Cn)\n\n----------------------------------\n\n### web服务器\n\n* [Nginx开发从入门到精通](http://tengine.taobao.org/book/index.html) (淘宝团队出品)\n* [Nginx教程从入门到精通](http://www.ttlsa.com/nginx/nginx-stu-pdf/)(PDF版本，运维生存时间出品)\n* [OpenResty最佳实践](https://www.gitbook.com/book/moonbingbing/openresty-best-practices/details)\n* [Apache 中文手册](http://works.jinbuguo.com/apache/menu22/index.html)\n* [Elasticsearch权威指南](http://looly.gitbooks.io/elasticsearch-the-definitive-guide-cn/)\n* [25 台服务器是怎样支撑 StackOverflow 的？](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/how-stackoverflow-works.md)\n* [图片服务架构演进（孔凡勇）](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/picture-server.md)\n* [最佳日志实践（王健）](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/logging.md)\n\n--------------------------------\n\n### 版本控制\n\n* [Git教程](http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000) \n* [git简易指南](http://rogerdudler.github.io/git-guide/index.zh.html)\n* [猴子都能懂的GIT入门](http://backlogtool.com/git-guide/cn/)\n* [Git 参考手册](http://gitref.justjavac.com)\n* [Pro Git](http://git-scm.com/book/zh/v2)\n* [Pro Git 中文版](https://www.gitbook.com/book/0532/progit/details) (整理在gitbook上)\n* [Git Magic](http://www-cs-students.stanford.edu/~blynn/gitmagic/intl/zh_cn/)\n* [GotGitHub](http://www.worldhello.net/gotgithub/index.html)\n* [Git权威指南](http://www.worldhello.net/gotgit/)\n* [Git Community Book 中文版](http://gitbook.liuhui998.com/index.html)\n* [Mercurial 使用教程](https://www.mercurial-scm.org/wiki/ChineseTutorial)\n* [HgInit (中文版)](http://bucunzai.net/hginit/)\n* [沉浸式学 Git](http://igit.linuxtoy.org/)\n* [Git-Cheat-Sheet](https://github.com/flyhigher139/Git-Cheat-Sheet) （感谢 @flyhigher139 翻译了中文版）\n* [GitHub秘籍](http://snowdream86.gitbooks.io/github-cheat-sheet/content/zh/index.html)\n* [Github帮助文档](https://github.com/waylau/github-help)\n* [git-flow 备忘清单](http://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html)\n* [svn 手册](http://svnbook.red-bean.com/nightly/zh/index.html)\n* [GitHub漫游指南](https://github.com/phodal/github-roam)\n\n### 编辑器\n\n* [vimplus](<https://github.com/chxuan/vimplus>)\n* [exvim--vim 改良成IDE项目](http://exvim.github.io/docs-zh/intro/)\n* [笨方法学Vimscript 中译本](http://learnvimscriptthehardway.onefloweroneworld.com/)\n* [Vim中文文档](https://github.com/vimcn/vimcdoc)\n* [所需即所获：像 IDE 一样使用 vim](https://github.com/yangyangwithgnu/use_vim_as_ide)\n* [在线MarkDown编辑](https://stackedit.io/#)\n* [简明VIM练级攻略](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/vim.md)\n\n-------------------------------\n\n### MySQL\n\n* [MySQL中文手册](http://www.php100.com/manual/MySQL/)\n* [十步完全理解SQL](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/sql.md)\n* [MySQL索引背后的数据结构及算法原理](http://www.cnblogs.com/leoo2sk/archive/2011/07/10/mysql-index.html)\n* [21分钟MySQL入门教程](http://www.cnblogs.com/mr-wid/archive/2013/05/09/3068229.html)\n\n-----------------------------------\n\n### NoSQL\n\n* [NoSQL数据库笔谈](http://old.sebug.net/paper/databases/nosql/Nosql.html)\n* [Redis 设计与实现](http://redisbook.com/)\n* [Redis 命令参考](http://redisdoc.com/)\n* [带有详细注释的 Redis 3.0 代码](https://github.com/huangz1990/redis-3.0-annotated)\n* [带有详细注释的 Redis 2.6 代码](https://github.com/huangz1990/annotated_redis_source)\n* [The Little MongoDB Book](https://github.com/justinyhuang/the-little-mongodb-book-cn/blob/master/mongodb.md)\n* [The Little Redis Book](https://github.com/JasonLai256/the-little-redis-book/blob/master/cn/redis.md)\n* [Neo4j 简体中文手册 v1.8](http://docs.neo4j.org.cn/)\n* [Neo4j .rb 中文資源](http://neo4j.tw/)\n* [Disque 使用教程](http://disquebook.com)\n* [Apache Spark 设计与实现](https://github.com/JerryLead/SparkInternals/tree/master/markdown)\n* [8种Nosql数据库系统对比](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/nosql.md)\n\n---------------------------\n\n### PostgreSQL\n\n* [PostgreSQL 8.2.3 中文文档](http://works.jinbuguo.com/postgresql/menu823/index.html)\n* [PostgreSQL 9.3.1 中文文档](http://www.postgres.cn/docs/9.3/index.html)\n* [PostgreSQL 9.5.3 中文文档](http://www.postgres.cn/docs/9.5/index.html)\n\n--------------------------\n\n### 运维监控\n\n* [ELKstack 中文指南](http://kibana.logstash.es)\n* [Mastering Elasticsearch(中文版)](http://udn.yyuap.com/doc/mastering-elasticsearch/)\n* [ElasticSearch 权威指南](https://www.gitbook.com/book/fuxiaopang/learnelasticsearch/details)\n* [Elasticsearch 权威指南（中文版）](http://es.xiaoleilu.com)\n* [Logstash 最佳实践](https://github.com/chenryn/logstash-best-practice-cn)\n* [Puppet 2.7 Cookbook 中文版](http://bbs.konotes.org/workdoc/puppet-27/)\n\n--------------------------\n\n### 项目相关\n\n* [Gradle实战](http://lippiouyang.gitbooks.io/gradle-in-action-cn/content/)\n* [持续集成（第二版）](http://article.yeeyan.org/view/2251/94882) (译言网)\n* [让开发自动化系列专栏](http://www.ibm.com/developerworks/cn/java/j-ap/)\n* [追求代码质量](http://www.ibm.com/developerworks/cn/java/j-cq/)\n* [selenium 中文文档](https://github.com/fool2fish/selenium-doc)\n* [Joel谈软件](http://local.joelonsoftware.com/wiki/Chinese_\\(Simplified\\))\n* [約耳談軟體(Joel on Software)](http://local.joelonsoftware.com/wiki/%E9%A6%96%E9%A0%81)\n* [Gradle 2 用户指南](https://github.com/waylau/Gradle-2-User-Guide)\n* [编码规范](https://github.com/ecomfe/spec)\n* [开源软件架构](http://www.ituring.com.cn/book/1143)\n* [GNU make 指南](http://docs.huihoo.com/gnu/linux/gmake.html)\n* [GNU make 中文手册](http://www.yayu.org/book/gnu_make/)\n* [The Twelve-Factor App](http://12factor.net/zh_cn/)\n\n----------------------------\n\n### 设计模式\n\n* [图说设计模式](https://github.com/me115/design_patterns)\n* [史上最全设计模式导学目录](http://blog.csdn.net/lovelion/article/details/17517213)\n* [design pattern 包教不包会](https://github.com/AlfredTheBest/Design-Pattern)\n* [设计模式 Java 版](https://quanke.gitbooks.io/design-pattern-java/content/)\n\n------------------------------\n\n### Web\n\n* [网络传输基础](http://coolshell.info/blog/2015/04/web-transmission-basis.html)\n* [关于浏览器和网络的 20 项须知](http://www.20thingsilearned.com/zh-CN/home)\n* [前端知识体系](http://knowledge.ecomfe.com/)\n* [浏览器开发工具的秘密](http://jinlong.github.io/2013/08/29/devtoolsecrets/)\n* [Chrome 开发者工具中文手册](https://github.com/CN-Chrome-DevTools/CN-Chrome-DevTools)\n* [Chrome扩展开发文档](http://open.chrome.360.cn/extension_dev/overview.html)\n* [Grunt中文文档](http://www.gruntjs.org/)\n* [Yeoman中文文档](http://yeomanjs.org/)\n* [移动Web前端知识库](https://github.com/AlloyTeam/Mars)\n* [正则表达式30分钟入门教程](http://deerchao.net/tutorials/regex/regex.htm)\n* [前端开发体系建设日记](https://github.com/fouber/blog/issues/2)\n* [移动前端开发收藏夹](https://github.com/hoosin/mobile-web-favorites)\n* [JSON风格指南](https://github.com/darcyliu/google-styleguide/blob/master/JSONStyleGuide.md)\n* [HTTP 接口设计指北](https://github.com/bolasblack/http-api-guide)\n* [前端资源分享（一）](https://github.com/hacke2/hacke2.github.io/issues/1)\n* [前端资源分享（二）](https://github.com/hacke2/hacke2.github.io/issues/3)\n* [前端代码规范 及 最佳实践](http://coderlmn.github.io/code-standards/)\n* [前端开发者手册](https://www.gitbook.com/book/dwqs/frontenddevhandbook/details)\n* [前端工程师手册](https://www.gitbook.com/book/leohxj/front-end-database/details)\n* [w3school教程整理](https://github.com/wizardforcel/w3school)\n* [Wireshark用户手册](http://man.lupaworld.com/content/network/wireshark/index.html)\n* [一站式学习Wireshark](https://community.emc.com/thread/194901)\n* [HTTP 下午茶](http://happypeter.github.io/tealeaf-http/)\n* [HTTP/2.0 中文翻译](http://yuedu.baidu.com/ebook/478d1a62376baf1ffc4fad99?pn=1)\n* [RFC 7540 - HTTP/2 中文翻译版](https://github.com/abbshr/rfc7540-translation-zh_cn)\n* [http2讲解](https://www.gitbook.com/book/ye11ow/http2-explained/details)\n* [3 Web Designs in 3 Weeks](https://www.gitbook.com/book/juntao/3-web-designs-in-3-weeks/details)\n* [站点可靠性工程](https://github.com/hellorocky/Site-Reliability-Engineering)\n\n-----------------------------------\n\n### 大数据\n\n* [大数据/数据挖掘/推荐系统/机器学习相关资源](https://github.com/Flowerowl/Big-Data-Resources)\n* [面向程序员的数据挖掘指南](https://github.com/jizhang/guidetodatamining)\n* [大型集群上的快速和通用数据处理架构](https://code.csdn.net/CODE_Translation/spark_matei_phd)\n* [数据挖掘中经典的算法实现和详细的注释](https://github.com/linyiqun/DataMiningAlgorithm)\n* [Spark 编程指南简体中文版](https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/)\n\n------------------------------\n\n## 编程艺术\n\n* [程序员编程艺术](https://github.com/julycoding/The-Art-Of-Programming-by-July)\n* [每个程序员都应该了解的内存知识(译)](http://www.oschina.net/translate/what-every-programmer-should-know-about-memory-part1?print)【第一部分】\n* [取悦的工序：如何理解游戏](http://read.douban.com/ebook/4972883/) (豆瓣阅读，免费书籍)\n* [编程技巧总汇](http://xiaobeicn.gitbooks.io/programming-skills-summary/)\n\n--------------------------\n\n## 语言相关类\n\n### AWK\n\n* [awk程序设计语言](http://awk.readthedocs.org/en/latest/)\n* [awk教程](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/awk.md)\n\n--------------------------\n\n### SED\n\n* [sed教程](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/sed.md)\n* [SED简明教程](http://www.ezlippi.com/blog/2017/02/sed-introduction.html)\n\n------------------------\n\n### Java\n\n* [Apache Shiro 用户指南](https://github.com/waylau/apache-shiro-1.2.x-reference)\n* [Jersey 2.x 用户指南](https://github.com/waylau/Jersey-2.x-User-Guide)\n* [Spring Framework 4.x参考文档](https://github.com/waylau/spring-framework-4-reference)\n* [Spring Boot参考指南](https://github.com/qibaoguang/Spring-Boot-Reference-Guide) (翻译中)\n* [MyBatis中文文档](http://mybatis.github.io/mybatis-3/zh/index.html)\n* [用jersey构建REST服务](https://github.com/waylau/RestDemo)\n* [Activiti 5.x 用户指南](https://github.com/waylau/activiti-5.x-user-guide)\n* [Google Java编程风格指南](http://www.hawstein.com/posts/google-java-style.html)\n* [Netty 4.x 用户指南](https://github.com/waylau/netty-4-user-guide)\n* [Netty 实战(精髓)](https://github.com/waylau/essential-netty-in-action)\n* [REST 实战](https://github.com/waylau/rest-in-action)\n* [Java 编码规范](https://github.com/waylau/java-code-conventions)\n* [Apache MINA 2 用户指南](https://github.com/waylau/apache-mina-2.x-user-guide)\n* [JVM必备指南](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/jvm.md)\n* [Java入门教程](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/java-string.md)\n* [javarevisited博客](http://javarevisited.blogspot.com/)\n* [journaldev教程](http://www.journaldev.com/)\n\n-------------------------\n\n### Android\n\n* [开发工具下载](http://www.androiddevtools.cn/)\n* [CodePath Android教程](http://guides.codepath.com/android/Home)\n* [Android Design(中文版)](http://www.apkbus.com/design/index.html)\n* Google Material Design 正體中文版 ([译本一](http://wcc723.gitbooks.io/google_design_translate/content/style-icons.html) [译本二](https://github.com/1sters/material_design_zh))\n* [Google Android官方培训课程中文版](http://hukai.me/android-training-course-in-chinese/index.html)\n* [Android学习之路](http://stormzhang.github.io/android/2014/07/07/learn-android-from-rookie/)\n* [Android构建工具](http://tools.android.com/tech-docs/new-build-system)\n* [Android开发技术前线(android-tech-frontier)](https://github.com/bboyfeiyu/android-tech-frontier)\n* [Android内存优化(上)](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/android-memory-prof1.md)\n* [Android内存优化(中)](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/android-memory-prof2.md)\n* [Android内存优化(全)](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/android-memory-prof3.md)\n* [查找代码的一个网站](http://www.codota.com/)\n* [Android开源库汇总](https://android-arsenal.com/free)\n* [查找示例代码的网站](http://www.codota.com/)\n* [Android SDK使用教程](http://tutsplus.com/)\n* [Android最佳实践](https://github.com/futurice/android-best-practices)\n* [Android Material icons](http://google.github.io/material-design-icons/#icons-for-android)\n\n------------------------\n\n### C/C++\n\n* [C/C++ 中文参考手册](http://zh.cppreference.com/) (欢迎大家参与在线翻译和校对)\n* [C 语言编程透视](https://www.gitbook.com/book/tinylab/cbook/details)\n* [C++ 并发编程指南](https://github.com/forhappy/Cplusplus-Concurrency-In-Practice)\n* [Linux C编程一站式学习](http://akaedu.github.io/book/) (宋劲杉, 北京亚嵌教育研究中心)\n* [CGDB中文手册](https://github.com/leeyiw/cgdb-manual-in-chinese)\n* [100个gdb小技巧](https://github.com/hellogcc/100-gdb-tips/blob/master/src/index.md)\n* [100个gcc小技巧](https://github.com/hellogcc/100-gcc-tips/blob/master/src/index.md)\n* [学习gdb调试技巧](https://github.com/hellogcc/100-gdb-tips)\n* [ZMQ 指南](https://github.com/anjuke/zguide-cn)\n* [How to Think Like a Computer Scientist](http://www.ituring.com.cn/book/1203) (中英文版)\n* [跟我一起写Makefile(PDF)](http://scc.qibebt.cas.cn/docs/linux/base/%B8%FA%CE%D2%D2%BB%C6%F0%D0%B4Makefile-%B3%C2%F0%A9.pdf)\n* [GNU make中文手册](http://www.yayu.org/book/gnu_make/)\n* [GNU make 指南](http://docs.huihoo.com/gnu/linux/gmake.html)\n* [Google C++ 风格指南](http://zh-google-styleguide.readthedocs.org/en/latest/google-cpp-styleguide/contents/)\n* [C/C++ Primer](https://github.com/andycai/cprimer) (by @andycai)\n* [简单易懂的C魔法](http://www.nowamagic.net/librarys/books/contents/c)\n* [Cmake 实践](http://sewm.pku.edu.cn/src/paradise/reference/CMake%20Practice.pdf) (PDF版)\n* [C++ FAQ LITE(中文版)](http://www.sunistudio.com/cppfaq/)\n* [C++ Primer 5th Answers](https://github.com/Mooophy/Cpp-Primer)\n* [C++ 并发编程(基于C++11)](https://www.gitbook.com/book/chenxiaowei/cpp_concurrency_in_action/details)\n* [QT 教程](http://www.kuqin.com/qtdocument/tutorial.html)\n* [C进阶指南（1)](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/c1.md)\n* [libuv中文教程](https://github.com/luohaha/Chinese-uvbook)\n* [Boost 库中文教程](http://zh.highscore.de/cpp/boost/)\n* [笨办法学C](https://github.com/wizardforcel/lcthw-zh)\n* [C进阶指南（2)](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/c2.md)\n* [C进阶指南（3)](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/c3.md)\n* [C语言全局变量那些事儿](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/c-globle-variable.md)\n* [如何实现一个malloc](https://github.com/LippiOuYang/practical-computer-skills/blob/master/src/malloc.md)\n* [在线编程和调试的网站](http://ideone.com/)\n\n------------------------------------\n\n### CSS\n\n* [学习CSS布局](http://zh.learnlayout.com/)\n* [通用 CSS 笔记、建议与指导](https://github.com/chadluo/CSS-Guidelines/blob/master/README.md)\n* [CSS参考手册](http://css.doyoe.com/)\n* [Emmet 文档](http://yanxyz.github.io/emmet-docs/)\n* [前端代码规范](http://alloyteam.github.io/code-guide/) (腾讯alloyteam团队)\n* [HTML和CSS编码规范](http://codeguide.bootcss.com/)\n* [Sass Guidelines 中文](http://sass-guidelin.es/zh/)\n* [CSS3 Tutorial 《CSS3 教程》](https://github.com/waylau/css3-tutorial)\n* [MDN HTML 中文文档](https://developer.mozilla.org/zh-CN/docs/Web/HTML)\n* [MDN CSS 中文文档](https://developer.mozilla.org/zh-CN/docs/Web/CSS)\n\n----------------------------\n\n### Go\n\n* [Go编程基础](https://github.com/Unknwon/go-fundamental-programming)\n* [Go入门指南](https://github.com/Unknwon/the-way-to-go_ZH_CN)\n* [学习Go语言](http://mikespook.com/learning-go/) ([PDF](http://xxiyy.qiniudn.com/%E5%AD%A6%E4%B9%A0%20Go%20%E8%AF%AD%E8%A8%80\\(Golang\\).pdf?download))\n* [Go Web 编程](https://github.com/astaxie/build-web-application-with-golang) (此书已经出版，希望开发者们去购买，支持作者的创作)\n* [Go实战开发](https://github.com/astaxie/Go-in-Action) (当我收录此项目时，作者已经写完第三章，如果读完前面章节觉得有帮助，可以给作者[捐赠](https://me.alipay.com/astaxie)，以鼓励作者的继续创作)\n* [Network programming with Go 中文翻译版本](https://github.com/astaxie/NPWG_zh)\n* [Effective Go](http://www.hellogcc.org/effective_go.html)\n* [Go 语言标准库](https://github.com/polaris1119/The-Golang-Standard-Library-by-Example)\n* [Golang标准库文档](http://godoc.ml/)\n* [Revel 框架手册](http://gorevel.cn/docs/manual/index.html)\n* [Java程序员的Golang入门指南](http://blog.csdn.net/dc_726/article/details/46565241)\n* [Go命令教程](https://github.com/hyper-carrot/go_command_tutorial)\n* [Go语言博客实践](https://github.com/achun/Go-Blog-In-Action)\n* [Go 官方文档翻译](https://github.com/golang-china/golangdoc.translations)\n* [深入解析Go](https://github.com/tiancaiamao/go-internals)\n* [Go语言圣经(中文版)](https://bitbucket.org/golang-china/gopl-zh/wiki/Home) ([GitBook](https://www.gitbook.com/book/wizardforcel/gopl-zh/details))\n* [Go语言高级编程](https://github.com/chai2010/advanced-go-programming-book)\n\n-------------------------------\n\n### Groovy\n\n* [实战 Groovy 系列](http://www.ibm.com/developerworks/cn/java/j-pg/)\n\n-----------------\n\n### Haskell\n\n* [Real World Haskell 中文版](http://rwh.readthedocs.org/en/latest/)\n* [Haskell趣学指南](http://fleurer-lee.com/lyah/)\n* [Learn You a Haskell for Great Good!](http://learnyouahaskell.com/chapters)(质量不错的一个网书)\n\n--------------------\n\n### iOS\n\n* [iOS开发60分钟入门](https://github.com/qinjx/30min_guides/blob/master/ios.md)\n* [iOS7人机界面指南](http://isux.tencent.com/ios-human-interface-guidelines-ui-design-basics-ios7.html)\n* [Google Objective-C Style Guide 中文版](http://zh-google-styleguide.readthedocs.org/en/latest/google-objc-styleguide/)\n* [iPhone 6 屏幕揭秘](http://wileam.com/iphone-6-screen-cn/)\n* [Apple Watch开发初探](http://nilsun.github.io/apple-watch/)\n* [马上着手开发 iOS 应用程序](https://developer.apple.com/library/ios/referencelibrary/GettingStarted/RoadMapiOSCh/index.html)\n* [网易斯坦福大学公开课：iOS 7应用开发字幕文件](https://github.com/jkyin/Subtitle)\n\n-----------------------\n\n\n### JavaScript\n\n* [Google JavaScript 代码风格指南](http://bq69.com/blog/articles/script/868/google-javascript-style-guide.html)\n* [Google JSON 风格指南](https://github.com/darcyliu/google-styleguide/blob/master/JSONStyleGuide.md)\n* [Airbnb JavaScript 规范](https://github.com/adamlu/javascript-style-guide)\n* [JavaScript 标准参考教程（alpha）](http://javascript.ruanyifeng.com/)\n* [Javascript编程指南](http://pij.robinqu.me/) ([源码](https://github.com/RobinQu/Programing-In-Javascript))\n* [javascript 的 12 个怪癖](https://github.com/justjavac/12-javascript-quirks)\n* [JavaScript 秘密花园](http://bonsaiden.github.io/JavaScript-Garden/zh/)\n* [JavaScript核心概念及实践](http://icodeit.org/jsccp/) (PDF) (此书已由人民邮电出版社出版发行，但作者依然免费提供PDF版本，希望开发者们去购买，支持作者)\n* [《JavaScript 模式》](https://github.com/jayli/javascript-patterns) “JavaScript patterns”中译本\n* [命名函数表达式探秘](http://justjavac.com/named-function-expressions-demystified.html)  (注:原文由[为之漫笔](http://www.cn-cuckoo.com)翻译，原始地址无法打开，所以此处地址为我博客上的备份)\n* [学用 JavaScript 设计模式](http://www.oschina.net/translate/learning-javascript-design-patterns) (开源中国)\n* [深入理解JavaScript系列](http://www.cnblogs.com/TomXu/archive/2011/12/15/2288411.html)\n* [ECMAScript 6 入门](http://es6.ruanyifeng.com/) (作者：阮一峰)\n* [JavaScript Promise迷你书](http://liubin.github.io/promises-book/)\n* [You-Dont-Know-JS](https://github.com/getify/You-Dont-Know-JS) (深入JavaScript语言核心机制的系列图书)\n* [JavaScript 教程](http://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000) 廖雪峰\n* [MDN JavaScript 中文文档](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript)\n\n* jQuery\n    * [jQuery 解构](http://www.cn-cuckoo.com/deconstructed/jquery.html)\n    * [简单易懂的JQuery魔法](http://www.nowamagic.net/librarys/books/contents/jquery)\n    * [How to write jQuery plugin](http://i5ting.github.io/How-to-write-jQuery-plugin/build/jquery.plugin.html)\n* Node.js\n    * [Node入门](http://www.nodebeginner.org/index-zh-cn.html)\n    * [七天学会NodeJS](http://nqdeng.github.io/7-days-nodejs/)\n    * [Nodejs Wiki Book](https://github.com/nodejs-tw/nodejs-wiki-book) (繁体中文)\n    * [express.js 中文文档](http://expressjs.jser.us/)\n    * [koa 中文文档](https://github.com/guo-yu/koa-guide)\n    * [一起学koa](http://base-n.github.io/koa-generator-examples/)\n    * [使用 Express + MongoDB 搭建多人博客](https://github.com/nswbmw/N-blog)\n    * [Express框架](http://javascript.ruanyifeng.com/nodejs/express.html)\n    * [Node.js 包教不包会](https://github.com/alsotang/node-lessons)\n    * [Learn You The Node.js For Much Win! (中文版)](https://www.npmjs.org/package/learnyounode-zh-cn)\n    * [Node debug 三法三例](http://i5ting.github.io/node-debug-tutorial/)\n    * [nodejs中文文档](https://www.gitbook.com/book/0532/nodejs/details)\n    * [orm2 中文文档](https://github.com/wizardforcel/orm2-doc-zh-cn)\n* underscore.js\n    * [Underscore.js中文文档](http://learningcn.com/underscore/)\n* backbone.js\n    * [backbone.js中文文档](http://www.css88.com/doc/backbone/)\n    * [backbone.js入门教程](http://www.the5fire.com/backbone-js-tutorials-pdf-download.html) (PDF)\n    * [Backbone.js入门教程第二版](https://github.com/the5fire/backbonejs-learning-note)\n    * [Developing Backbone.js Applications(中文版)](http://feliving.github.io/developing-backbone-applications)\n* AngularJS\n    * [AngularJS最佳实践和风格指南](https://github.com/mgechev/angularjs-style-guide/blob/master/README-zh-cn.md)\n    * [AngularJS中译本](https://github.com/peiransun/angularjs-cn)\n    * [AngularJS入门教程](https://github.com/zensh/AngularjsTutorial_cn)\n    * [构建自己的AngularJS](https://github.com/xufei/Make-Your-Own-AngularJS/blob/master/01.md)\n    * [在Windows环境下用Yeoman构建AngularJS项目](http://www.waylau.com/build-angularjs-app-with-yeoman-in-windows/)\n* Zepto.js\n    * [Zepto.js 中文文档](http://mweb.baidu.com/zeptoapi/)\n* Sea.js\n    * [Hello Sea.js](http://island205.github.io/HelloSea.js/)\n* React.js\n    * [React.js 中文文档](http://reactjs.cn/)\n    * [React webpack-cookbook](https://github.com/fakefish/react-webpack-cookbook)\n    * [React 入门教程](http://fraserxu.me/intro-to-react/)\n    * [React Native 中文文档(含最新Android内容)](http://wiki.jikexueyuan.com/project/react-native/)\n    * [Learn React & Webpack by building the Hacker News front page](https://github.com/theJian/build-a-hn-front-page)\n* impress.js\n    * [impress.js的中文教程](https://github.com/kokdemo/impress.js-tutorial-in-Chinese)\n* CoffeeScript\n    * [CoffeeScript Cookbook](http://island205.github.io/coffeescript-cookbook.github.com/)\n    * [The Little Book on CoffeeScript中文版](http://island205.github.io/tlboc/)\n    * [CoffeeScript 编码风格指南](https://github.com/geekplux/coffeescript-style-guide)\n* TypeScipt\n    * [TypeScript Handbook](https://zhongsp.gitbooks.io/typescript-handbook/content/)\n* ExtJS\n    * [Ext4.1.0 中文文档](http://extjs-doc-cn.github.io/ext4api/)\n* Meteor\n    * [Discover Meteor](http://zh.discovermeteor.com/)\n    * [Meteor 中文文档](http://docs.meteorhub.org/#/basic/)\n    * [Angular-Meteor 中文教程](http://angular.meteorhub.org/)\n* [Chrome扩展及应用开发](http://www.ituring.com.cn/minibook/950)\n\n-----------------\n\n### LaTeX\n\n* [一份其实很短的 LaTeX 入门文档](http://liam0205.me/2014/09/08/latex-introduction/)\n* [一份不太简短的 LATEX 2ε 介绍](http://www.mohu.org/info/lshort-cn.pdf) （PDF版）\n\n-----------------\n\n### LISP\n* Common Lisp\n    * [ANSI Common Lisp 中文翻譯版](http://acl.readthedocs.org/en/latest/)\n    * [On Lisp 中文翻译版本](http://www.ituring.com.cn/minibook/862)\n* Scheme\n    * [Yet Another Scheme Tutorial Scheme入门教程](http://deathking.github.io/yast-cn/)\n    * [Scheme语言简明教程](http://songjinghe.github.io/TYS-zh-translation/)\n    * Racket\n        * [Racket book](https://github.com/tyrchen/racket-book)\n\n---------------------\n\n### Lua\n\n* [Lua编程入门](https://github.com/andycai/luaprimer)\n* [Lua 5.1 参考手册 中文翻译](http://www.codingnow.com/2000/download/lua_manual.html)\n* [Lua 5.3 参考手册 中文翻译](http://cloudwu.github.io/lua53doc/)\n* [Lua源码欣赏](http://www.codingnow.com/temp/readinglua.pdf)\n\n-----------------\n\n### Perl\n\n* [Modern Perl 中文版](https://github.com/horus/modern_perl_book)\n* [Perl 程序员应该知道的事](http://perl.linuxtoy.org/)\n\n-----------------\n\n### PHP\n\n* [PHP 官方手册](http://php.net/manual/zh/)\n* [PHP调试技术手册](http://www.laruence.com/2010/06/21/1608.html)(PDF)\n* [XDebug 2中文手册(译)](http://www.blogkun.com/project.html) (CHM)\n* [PHP之道](http://wulijun.github.io/php-the-right-way/)\n* [PHP 最佳实践](https://github.com/justjavac/PHP-Best-Practices-zh_CN)\n* [PHP 开发者实践](http://ryancao.gitbooks.io/php-developer-prepares/content/)\n* [深入理解PHP内核](https://github.com/reeze/tipi)\n* [PHP扩展开发及内核应用](http://www.walu.cc/phpbook/)\n* [CodeIgniter 用户指南](http://codeigniter.org.cn/user_guide/index.html)\n* [Laravel4 中文文档](http://www.golaravel.com/docs/)\n* [Laravel 入门](https://github.com/huanghua581/laravel-getting-started)\n* [Symfony2中文文档](http://symfony-docs-chs.readthedocs.org/en/latest/) (未译完)\n* [Phalcon中文文档](http://phalcon.5iunix.net/)（翻译进行中）\n* [YiiBook几本Yii框架的在线教程](http://yiibook.com//doc)\n* [深入理解 Yii 2.0](http://www.digpage.com/)\n* [Yii 框架中文文檔](http://www.yiichina.com/)\n* [简单易懂的PHP魔法](http://www.nowamagic.net/librarys/books/contents/php)\n* [swoole文档及入门教程](https://github.com/LinkedDestiny/swoole-doc)\n* [Composer 中文网](http://www.phpcomposer.com)\n* [Slim 中文文档](http://ww1.minimee.org/php/slim)\n* [Lumen 中文文档](http://lumen.laravel-china.org/)\n* [PHPUnit 中文文档](https://phpunit.de/manual/current/zh_cn/installation.html)\n\n--------------------\n\n### Prolog\n\n* [笨办法学Prolog](http://fengdidi.github.io/blog/2011/11/15/qian-yan/)\n\n-----------------\n\n### Python\n\n* [廖雪峰 Python 2.7 中文教程](http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000)\n* [廖雪峰 Python 3 中文教程](http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000)\n* [简明Python教程](http://www.kuqin.com/abyteofpython_cn/)\n* [零基础学 Python 第一版](http://www.kancloud.cn/kancloud/python-basic)\n* [零基础学 Python 第二版](http://www.kancloud.cn/kancloud/starter-learning-python)\n* [可爱的 Python](http://lovelypython.readthedocs.org/en/latest/)\n* [Python 2.7 官方教程中文版](http://www.pythondoc.com/pythontutorial27/index.html)\n* [Python 3.3 官方教程中文版](http://www.pythondoc.com/pythontutorial3/index.html)\n* [Python Cookbook 中文版](http://www.kancloud.cn/thinkphp/python-cookbook)\n* [Python3 Cookbook 中文版](https://github.com/yidao620c/python3-cookbook)\n* [深入 Python](http://www.kuqin.com/docs/diveintopythonzh-cn-5.4b/html/toc/)\n* [深入 Python 3](http://old.sebug.net/paper/books/dive-into-python3/)\n* [PEP8 Python代码风格规范](https://code.google.com/p/zhong-wiki/wiki/PEP8)\n* [Google Python 风格指南 中文版](http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/)\n* [Python入门教程](http://liam0205.me/2013/11/02/Python-tutorial-zh_cn/) ([PDF](http://liam0205.me/attachment/Python/The_Python_Tutorial_zh-cn.pdf))\n* [Python的神奇方法指南](http://article.yeeyan.org/view/311527/287706)\n* [笨办法学 Python](http://sebug.net/paper/books/LearnPythonTheHardWay/) （[PDF](http://liam0205.me/attachment/Python/PyHardWay/Learn_Python_The_Hard_Way_zh-cn.pdf)版下载）\n* [The Django Book 中文版](http://djangobook.py3k.cn/2.0/)\n* [web.py 0.3 新手指南](http://webpy.org/tutorial3.zh-cn)\n* [Web.py Cookbook 简体中文版](http://webpy.org/cookbook/index.zh-cn)\n* [Dive Into Python 中文版](http://woodpecker.org.cn/diveintopython/)\n* [Bottle 文档中文版](https://associates.amazon.cn/gp/associates/network/main.html) (需翻墙)\n* [Flask 文档中文版](http://docs.jinkan.org/docs/flask/)\n* [Jinja2 文档中文版](http://docs.jinkan.org/docs/jinja2/)\n* [Werkzeug 文档中文版](http://werkzeug-docs-cn.readthedocs.org/zh_CN/latest/)\n* [Flask之旅](http://spacewander.github.io/explore-flask-zh)\n* [Introduction to Tornado 中文翻译](http://demo.pythoner.com/itt2zh/index.html)\n* [Python自然语言处理中文版](http://pan.baidu.com/s/1qW4pvnY) （感谢陈涛同学的翻译，也谢谢 [@shwley](https://github.com/shwley) 联系了作者）\n* [Python 绘图库 matplotlib 官方指南中文翻译](http://liam0205.me/2014/09/11/matplotlib-tutorial-zh-cn/)\n* [Scrapy 0.25 文档](http://scrapy-chs.readthedocs.org/zh_CN/latest/)\n* [ThinkPython](https://github.com/carfly/thinkpython-cn)\n* [Python快速教程](http://www.cnblogs.com/vamei/archive/2012/09/13/2682778.html)\n* [Python 正则表达式操作指南](http://wiki.ubuntu.org.cn/Python正则表达式操作指南)\n* [python初级教程：入门详解](http://www.crifan.com/files/doc/docbook/python_beginner_tutorial/release/html/python_beginner_tutorial.html)\n* [Python Cookbook 第3版 中文版](http://python3-cookbook.readthedocs.org/zh_CN/latest/)\n* [Twisted 与异步编程入门](http://likebeta.gitbooks.io/twisted-intro-cn/)\n* [TextGrocery 中文 API](http://textgrocery.readthedocs.org/zh/latest/index.html) ( 基于svm算法的一个短文本分类 Python 库 )\n* [Requests: HTTP for Humans](http://requests-docs-cn.readthedocs.org/zh_CN/latest/)\n* [Pillow 中文文档](http://pillow-cn.readthedocs.org/en/latest/#)\n* [PyMOTW 中文版](http://pymotwcn.readthedocs.org/en/latest/index.html)\n* [Python 官方文档中文版](http://data.digitser.net/zh-CN/python_index.html)\n* [Fabric 中文文档](http://fabric-chs.readthedocs.org)\n* [Beautiful Soup 4.2.0 中文文档](http://beautifulsoup.readthedocs.org/zh_CN/latest/)\n* [用Python做科学计算](http://old.sebug.net/paper/books/scipydoc)\n* [Sphinx 中文文档](http://www.pythondoc.com/sphinx/index.html)\n* [精通 Python 设计模式](https://github.com/cundi/Mastering.Python.Design.Patterns)\n* [python 安全编程教程](https://github.com/smartFlash/pySecurity)\n* [程序设计思想与方法](https://www.gitbook.com/book/wizardforcel/sjtu-cs902-courseware/details)\n* [知乎周刊·编程小白学Python](https://read.douban.com/ebook/16691849/)\n* [Scipy 讲义](https://github.com/cloga/scipy-lecture-notes_cn)\n* [Python 学习笔记 基础篇](http://www.kuqin.com/docs/pythonbasic.html)\n* [Python 学习笔记 模块篇](http://www.kuqin.com/docs/pythonmodule.html)\n* [Python 标准库 中文版](http://old.sebug.net/paper/books/python/%E3%80%8APython%E6%A0%87%E5%87%86%E5%BA%93%E3%80%8B%E4%B8%AD%E6%96%87%E7%89%88.pdf)\n* [Python进阶](https://www.gitbook.com/book/eastlakeside/interpy-zh/details)\n* [Python 核心编程 第二版](https://wizardforcel.gitbooks.io/core-python-2e/content/) CPyUG译\n* [Python最佳实践指南](http://pythonguidecn.readthedocs.io/zh/latest/)\n* [Python 精要教程](https://www.gitbook.com/book/wizardforcel/python-essential-tutorial/details)\n* [Python 量化交易教程](https://www.gitbook.com/book/wizardforcel/python-quant-uqer/details)\n* Django\n    * [Django 1.5 文档中文版](http://django-chinese-docs.readthedocs.org/en/latest/) 正在翻译中\n    * [Diango 1.7 文档中文版](http://django-1-7-doc.coding.io/)  正在翻译中，目前只翻译了目录\n    * [Django 1.8.2 文档中文版](http://python.usyiyi.cn/django/index.html)\n     正在翻译中\n    * [Django 最佳实践](https://github.com/yangyubo/zh-django-best-practices)\n    * [Django搭建简易博客教程](https://www.gitbook.com/book/andrew-liu/django-blog/details)\n    * [The Django Book 中文版](http://djangobook.py3k.cn/2.0/)\n    * [Django 设计模式与最佳实践](https://github.com/cundi/Django-Design-Patterns-and-Best-Practices)\n    * [Django 网站开发 Cookbook](https://github.com/cundi/Web.Development.with.Django.Cookbook)\n    * [Django Girls 學習指南](https://www.gitbook.com/book/djangogirlstaipei/django-girls-taipei-tutorial/details)\n* Flask\n    * [Flask 文档中文版](http://docs.jinkan.org/docs/flask/)\n    * [Jinja2 文档中文版](http://docs.jinkan.org/docs/jinja2/)\n    * [Werkzeug 文档中文版](http://werkzeug-docs-cn.readthedocs.org/zh_CN/latest/)\n    * [Flask之旅](http://spacewander.github.io/explore-flask-zh/)\n    * [Flask 扩展文档汇总](https://www.gitbook.com/book/wizardforcel/flask-extension-docs/details)\n    * [Flask 大型教程](http://www.pythondoc.com/flask-mega-tutorial/index.html)\n    * [SQLAlchemy 中文文档](https://github.com/sixu05202004/sqlalchemy-docs-cn)\n* web.py\n    * [web.py 0.3 新手指南](http://webpy.org/tutorial3.zh-cn)\n    * [Web.py Cookbook 简体中文版](http://webpy.org/cookbook/index.zh-cn)\n* Tornado\n    * [Introduction to Tornado 中文翻译](http://demo.pythoner.com/itt2zh/index.html)\n    * [Tornado源码解析](http://www.nowamagic.net/academy/detail/13321002)\n    * [Tornado 4.3 文档中文版](https://tornado-zh.readthedocs.org/zh/latest/)\n\n### R\n\n* [R语言忍者秘笈](https://github.com/yihui/r-ninja)\n\n-----------------\n\n### Ruby\n\n* [Ruby 风格指南](https://github.com/JuanitoFatas/ruby-style-guide/blob/master/README-zhCN.md)\n* [Rails 风格指南](https://github.com/JuanitoFatas/rails-style-guide/blob/master/README-zhCN.md)\n* [笨方法學 Ruby](http://lrthw.github.io/)\n* [Ruby on Rails 指南](http://guides.ruby-china.org/)\n* [Ruby on Rails 實戰聖經](http://ihower.tw/rails4/index.html)\n* [Ruby on Rails Tutorial 原书第 3 版](http://railstutorial-china.org/) (本书网页版免费提供，电子版以 PDF、EPub 和 Mobi 格式提供购买，仅售 9.9 美元)\n* [Rails 实践](http://rails-practice.com/content/index.html)\n* [Rails 5 开发进阶(Beta)](https://www.gitbook.com/book/kelby/rails-beginner-s-guide/details)\n* [Rails 102](https://www.gitbook.com/book/rocodev/rails-102/details)\n* [编写Ruby的C拓展](https://wusuopu.gitbooks.io/write-ruby-extension-with-c/content/)\n* [Ruby 源码解读](https://ruby-china.org/topics/22386)\n* [Ruby中的元编程](http://deathking.github.io/metaprogramming-in-ruby/)\n\n-----------------\n\n### Scala\n\n* [Scala课堂](http://twitter.github.io/scala_school/zh_cn/index.html) (Twitter的Scala中文教程)\n* [Effective Scala](http://twitter.github.io/effectivescala/index-cn.html)(Twitter的Scala最佳实践的中文翻译)\n* [Scala指南](http://zh.scala-tour.com/)\n* [Scala-for-the-impatient-2nd](https://www.amazon.com/Scala-Impatient-2nd-Cay-Horstmann/dp/0134540565)(自行购买或pdf)\n* [Scala|写点什么](http://hongjiang.info/scala/)(国人的一个很好的关于Scala的博客)\n\n-----------------\n\n### Scheme\n* [Yet Another Scheme Tutorial Scheme入门教程](http://deathking.github.io/yast-cn/)\n* [Scheme语言简明教程](http://songjinghe.github.io/TYS-zh-translation/)\n\n-----------------\n\n### Shell\n\n* [Shell脚本编程30分钟入门](https://github.com/qinjx/30min_guides/blob/master/shell.md)\n* [Bash脚本15分钟进阶教程](http://blog.sae.sina.com.cn/archives/3606)\n* [Linux工具快速教程](https://github.com/me115/linuxtools_rst)\n* [shell十三问](https://github.com/wzb56/13_questions_of_shell)\n\n-----------------\n\n### Swift\n\n* [The Swift Programming Language 中文版](http://numbbbbb.github.io/the-swift-programming-language-in-chinese/)\n* [Swift 语言指南](http://dev.swiftguide.cn)\n* [Stanford 公开课，Developing iOS 8 Apps with Swift 字幕翻译文件](https://github.com/x140yu/Developing_iOS_8_Apps_With_Swift)\n\n-----------------\n\n### WebAssembly\n\n* [C/C++面向WebAssembly编程](https://github.com/3dgen/cppwasm-book)\n\n-----------------\n\n## 计算机原理\n\n- [视频地址](https://www.bilibili.com/video/av21376839/)\n\n- [Github地址](https://github.com/1c7/crash-course-computer-science-chinese)\n\n","tags":["计算机相关技术资料整理"],"categories":["计算机相关技术资料整理"]},{"title":"决策树算法","url":"/2018-05-06/decisionTree/","content":"\n#### 使用决策树预测隐形眼镜类型\n\n```\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport operator\nfrom math import log\nimport decisionTreePlot as dtPlot\nfrom collections import Counter\n\n# 计算香农熵\ndef calcShannonEnt(dataset):\n\tnumEnteries = len(dataset)\n\tlabelCounts = {}\n\t# 统计分类中标签出现的次数\n\tfor featVec in dataset:\n\t\t# 默认每一行中最后一个特征为标签\n\t\tcurrentLable = featVec[-1]\n\t\tif currentLable not in labelCounts.key():\n\t\t\tlabelCounts[currentLable] = 0\n\t\tlabelCounts[currentLable] +=1\n\n\tshannonEnt = 0.0\n\tfor key in labelCounts:\n\t\t# 计算每种标签出现的频率 作为 概论\n\t\tprob = float(labelCounts[key])/numEnteries\n\t\t# 根据香农熵的公式 - 负 概论*log以2为底概率值的对数\n\t\tshannonEnt -= prob * log(prob,2)\n\n\t## 方式二\n\t# # 统计标签出现的次数\n\t# label_count = Counter(data[-1] for data in dataSet)\n\t# # 计算概率\n\t# probs = [p[1] / len(dataSet) for p in label_count.items()]\n\t# # 计算香农熵\n\t# shannonEnt = sum([-p * log(p, 2) for p in probs])\n\t# # \n\n\treturn shannonEnt\n\ndef majorityCnt(classList):\n\t# 选择出现次数最多的一个结果\n\tclassCount = {}\n\tfor vote in classList:\n\t\tif vote not in classCount.keys():\n\t\t\tclassCount[vote] = 0\n\t\tclassCount[vote] +=1\n\t# 倒叙排列classCount得到一个字典集合，然后取出第一个就是结果（yes/no），即出现次数最多的结果\n\tsortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n\t# print('sortedClassCount:', sortedClassCount)\n\treturn sortedClassCount[0][0]\n\n# 选择切分数据集的最佳特征\ndef chooseBestFeatureToSplit(dataSet):\n\t# 求第一行有多少列的 Feature, 最后一列是label列嘛\n\tnumFeatures = len(dataSet[0]) - 1\n\t# label的信息熵\n\tbaseEntropy = calcShannonEnt(dataSet)\n\t# 最优的信息增益值, 和最优的Featurn编号\n\tbestInfoGain, bestFeature = 0.0, -1\n\t# iterate over all the features\n\tfor i in range(numFeatures):\n\t\t# create a list of all the examples of this feature\n\t\t# 获取每一个实例的第i+1个feature，组成list集合\n\t\tfeatList = [example[i] for example in dataSet]\n\t\t# get a set of unique values\n\t\t# 获取剔重后的集合，使用set对list数据进行去重\n\t\tuniqueVals = set(featList)\n\t\t# 创建一个临时的信息熵\n\t\tnewEntropy = 0.0\n\t\t# 遍历某一列的value集合，计算该列的信息熵 \n\t\t# 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。\n\t\tfor value in uniqueVals:\n\t\t\tsubDataSet = splitDataSet(dataSet, i, value)\n\t\t\tprob = len(subDataSet)/float(len(dataSet))\n\t\t\tnewEntropy += prob * calcShannonEnt(subDataSet)\n\t\t# gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值\n\t\t# 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。\n\t\tinfoGain = baseEntropy - newEntropy\n\t\tprint('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)\n\t\tif (infoGain > bestInfoGain):\n\t\t\tbestInfoGain = infoGain\n\t\t\tbestFeature = i\n\treturn bestFeature\n\n# 创建决策树\ndef createTree(dataset, lables):\n\tclassList = [example[-1] for example in dataset]\n\t# 如果数据集只有一种则直接返回\n\t# \n\tif classList.count(classList[0]) == len(classList):\n\t\treturn classList[0]\n\t# ?使用完了所有特征 依旧还有未包含的数据集\n\tif len(dataset[0]) == 1:\n\t\treturn majorityCnt(classList)\n\n\t# 选择最优的列，得到最优列对应的label含义\n\tbestFeat = chooseBestFeatureToSplit(dataSet)\n\t# 获取label的名称\n\tbestFeatLabel = labels[bestFeat]\n\t# 初始化myTree\n\tmyTree = {bestFeatLabel: {\\}\\} # hexo的bug 双大括号 会异常 需转义\n\t# 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改\n\t# 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list\n\tdel(labels[bestFeat])\n\t# 取出最优列，然后它的branch做分类\n\tfeatValues = [example[bestFeat] for example in dataSet]\n\tuniqueVals = set(featValues)\n\tfor value in uniqueVals:\n\t\t# 求出剩余的标签label\n\t\tsubLabels = labels[:]\n\t\t# 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()\n\t\tmyTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n\t\t# print('myTree', value, myTree)\n\treturn myTree\n\n\ndef ContactLensesTest():\n\t# 预测隐形眼镜的测试代码，并将结果画出来\n\t# fr = open('https://raw.githubusercontent.com/pbharrin/machinelearninginaction/master/Ch03/lenses.txt')\n\tfr = open('../../../../data/3.DecisionTree/lenses.txt')\n\t# 解析数据，获得 features 数据\n\tlenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n\t# 得到数据的对应的 Labels\n\t# age（年龄）、prescript（症状）、astigmatic（是否散光）、tearRate（眼泪数量）\n\tlensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n\t# 使用上面的创建决策树的代码，构造预测隐形眼镜的决策树\n\tlensesTree = createTree(lenses, lensesLabels)\n\tprint(lensesTree)\n\t# 画图可视化展现\n\tdtPlot.createPlot(lensesTree)\n\n\nif __name__ == \"__main__\":\n\tContactLensesTest()\n\n```","tags":["机器学习","decisionTree"],"categories":["MachineLearning"]},{"title":"开源测试工具 Network Emulator for Windows Toolkit 使用方法","url":"/2018-05-04/windows限速测试/","content":"\n\n1.  下载地址： [https://pan.baidu.com/s/1dFurINZ](https://pan.baidu.com/s/1dFurINZ)\n2.  使用步骤：\n    1.  建立一个虚拟信道\n    2.  建议一个虚拟过滤器；\n    3.  建立一个虚拟链路； [![image](https://upload-images.jianshu.io/upload_images/18390058-2a8d64e5dcab92fd.png!large)](https://testerhome.com/uploads/photo/2016/e2975659b11501acc9e34d88147fed9e.png!large) \n    4.  选择一种网络模式；\n        1.  GPRS：移动2.5G网络；\n        2.  ADSL：宽带；\n        3.  CDMA2000：移动3G网络；\n        4.  CDMA：3G；\n        5.  Dialup：远程连接方式，即wifi； [![image](https://upload-images.jianshu.io/upload_images/18390058-6511ed6eb2c66a60.png!large)](https://testerhome.com/uploads/photo/2016/575dca39a87d9f0e71f530408898992c.png!large) \n        6.  编辑过滤器：\n        7.  doc环境下ping需要监控的域名 [![image](https://upload-images.jianshu.io/upload_images/18390058-14d4e7f3d1e5b4ab.png!large)](https://testerhome.com/uploads/photo/2016/2463a146295812e6e826519933359993.png!large) \n        8.  编辑过滤器： [![image](https://upload-images.jianshu.io/upload_images/18390058-06c4064820097aab.png!large)](https://testerhome.com/uploads/photo/2016/1b6cc37aaf049a4002f4030c03c481c0.png!large) \n        9.  编辑链路： [![image](https://upload-images.jianshu.io/upload_images/18390058-762befa1ebe0a720.png!large)](https://testerhome.com/uploads/photo/2016/b138fb2551eedd0a9279b9e108fde7d2.png!large)  [![image](https://upload-images.jianshu.io/upload_images/18390058-65bdf07a89375a3a.png!large)](https://testerhome.com/uploads/photo/2016/e1bf9e2d4c94caa748bdde9bb50b7d6b.png!large) \n3.  一般需要修改的数据：\n    1.  Loss：丢包\n    2.  Error：错误包\n    3.  Latency：延时\n    4.  BW&Quene：带宽及队列\n    5.  Reorder：重新排列（模拟是否按顺序排列）\n    6.  Disconnect：断网 （模拟周期性的断开or链接网络）\n\n总结：这个工具测试网络引起的数据传输问题非常给力，charles设置的丢包和错误包都没有效果，不知道后面的版本会不会改进，但是这个工具能准确监测丢包、错误包引起的数据传输问题。\n","tags":["限速","测试工具"],"categories":["Network"]},{"title":"浏览器支持情况","url":"/2018-05-04/webrtc_business_3/","content":"\n\n#### 浏览器支持\n关于webrtc人们关注的主要问题是浏览器的支持情况,据最新的报告表示,下面这些浏览器分别有不同的支持程度。\n\nwebrtc应用的几个重要方面：\n1. 最主要的支持者谷歌和火狐\n2. 微软ie旧版本不支持,到2018年11月,微软宣布采用Chromium(google内核)作为其心浏览器Edge的发展方向\n3. 苹果 Safar从2017年开始支持webrtc,但ios系统的非safari浏览器暂不提供兼容\n\n\n##### 处理不支持的浏览器\n你无法控制浏览器供应商何时添加对webrtc的支持,这里提供了一些替代方案:\n1. 只试用chrome和/或firefox浏览器.\n2. 使用全部有webrtc支持的浏览器(Chrome, Firefox, Edge and Safari)\n3. 使用桌面应用程序.\n4. 不适用webrtc。\n\n\n\n##### 处理移动设备\n在移动设备上有两种调用方式:apps 和 web.\n应用程式是设备上最常用的交互方式,但是有时候通过浏览器访问网络更有意义，这包括一次性互动，在网站上单击拨号并通过SMS消息中的链接开始交互.\n\n在ios的safari中最近才添加webrtc支持.","tags":["Business","Browser"],"categories":["WebRTC"]},{"title":"knn算法预测约会对象是否符合条件","url":"/2018-05-03/knn/","content":"\n#### 使用knn算法进行分类预测\n\n```\nfrom numpy import *\nimport operator # 运算符操作模块\nimport os\nfrom collections import Counter # 快速计数工具\n\n# inx 自定义用于分类的输入向量 - 标准向量 即计算数据集到标准向量的\"距离\"\n# dataset 训练数据集 lables 标签\n# 最近邻的特征数量\ndef classify(inx, dataset, lables, k):\n\tdataSetSize = dataset.shape[0]\n\t# tile(A,(2,3)) 表示第一个维度重复三次，第二个维度重复两次 tile(A,(2,3)) => array([[1, 2, 1, 2, 1, 2],[1, 2, 1, 2, 1, 2]])\n\tdiffMat = tile(inx, (dataSetSize, 1)) - dataset\n\t# 计算训练集的数据与标准向量的距离 - 欧氏距离 => [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n\tsqDiffMat = diffMat ** 2\n\t# 计算平方和 再开方 => (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n\tsqDistances = sqDiffMat.sum(axis = 1)\n\tdistances = sqDistances ** 0.5\n\t# 按照距离从小到大排序 并获取其对应索引放入数组中 => y=array([3,0,2,1,4,5]) x[3]距离最小 x[5]最大\n\tsorteDistIndicies = distances.argsort()\n\n\t# 从距离排序中选取最小距离的k个向量\n\tclassCount = {}\n\tfor i in range(k):\n\t\tvoteIlable = lables[sorteDistIndicies[i]]\n\t\t# 字典的get方法\n\t\t# 如：list.get(k,d) 其中 get相当于一条if...else...语句,参数k在字典中，字典将返回list[k];如果参数k不在字典中则返回参数d,如果K在字典中则返回k对应的value值\n\t\t# l = {5:2,3:4}\n\t\t# print l.get(3,0)返回的值是4；\n\t\t# Print l.get（1,0）返回值是0；\n\n\t\tclassCount[voteIlable] = classCount.get(voteIlable,0) + 1\n\n\t# 排序选取 出现次数最多的标签\n\tsortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n\treturn sortedClassCount[0][0]\n\n# 数据归一化 0,1矩阵\ndef autoNorm(dataset):\n\t# 计算数据集中的最小最大特征值\n\tminVals = dataset.min(0)\n\tmaxVals = dataset.max(0)\n\trangs = maxVals - minVals\n\t# 转为同维度的零矩阵\n\tnormDataset = zeros(shape(dataset))\n\tm = dataset.shape[0]\n\t# 生成与最小值之差的矩阵\n\tnormDataset = dataset - tile(minVals,(m,1))\n\t# 将最小值除以极差组成的矩阵\n\tnormDataset = normDataset / tile(rangs, (m,1))\n\treturn normDataset, rangs, minVals\n\ndef file2matrix(filename):\n\t\"\"\"\n\t导入训练数据\n\t:param filename: 数据文件路径\n\t:return: 数据矩阵returnMat和对应的类别classLabelVector\n\t\"\"\"\n\tfr = open(filename, 'r')\n\t# 获得文件中的数据行的行数\n\tnumberOfLines = len(fr.readlines())\n\t# 生成对应的空矩阵\n\t# 例如：zeros(2，3)就是生成一个 2*3 的矩阵，各个位置上全是 0 \n\treturnMat = zeros((numberOfLines, 3))  # prepare matrix to return\n\tclassLabelVector = []  # prepare labels return\n\tfr = open(filename, 'r')\n\tindex = 0\n\tfor line in fr.readlines():\n\t\t# str.strip([chars]) --返回移除字符串头尾指定的字符生成的新字符串\n\t\tline = line.strip()\n\t\t# 以 '\\t' 切割字符串\n\t\tlistFromLine = line.split('\\t')\n\t\t# 每列的属性数据，即 features\n\t\treturnMat[index] = listFromLine[0 : 3]\n\t\t# 每列的类别数据，就是 label 标签数据\n\t\tclassLabelVector.append(int(listFromLine[-1]))\n\t\tindex += 1\n\t# 返回数据矩阵returnMat和对应的类别classLabelVector\n\treturn returnMat, classLabelVector\n\n\n# 约会网站用户数据进行分类 预测 数据在kaggle\ndef datingClassTest():\n\t# 设置测试数据的的一个比例（训练数据集比例=1-hoRatio）\n\thoRatio = 0.1  # 测试范围,一部分测试一部分作为样本\n\t# datingDataMat, datingLabels = file2matrix(\"https://raw.githubusercontent.com/pbharrin/machinelearninginaction/master/Ch02/datingTestSet2.txt\")  # load data setfrom file\n\tdatingDataMat, datingLabels = file2matrix(\"E:\\\\python\\\\MachineLearning\\\\data\\\\2.KNN/datingTestSet2.txt\")  # load data setfrom file\n\t# 归一化数据\n\tnormMat, ranges, minVals = autoNorm(datingDataMat)\n\t# m 表示数据的行数，即矩阵的第一维\n\tm = normMat.shape[0]\n\t# 设置测试的样本数量， numTestVecs:m表示训练样本的数量\n\tnumTestVecs = int(m * hoRatio)\n\tprint('numTestVecs=', numTestVecs)\n\terrorCount = 0\n\tfor i in range(numTestVecs):\n\t\t# 对数据测试\n\t\tclassifierResult = classify(normMat[i], normMat[numTestVecs : m], datingLabels[numTestVecs : m], 3)\n\t\tprint(\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, datingLabels[i]))\n\t\terrorCount += classifierResult != datingLabels[i]\n\tprint(\"the total error rate is: %f\" % (errorCount / numTestVecs))\n\tprint(errorCount)\n\n\nif __name__ == '__main__':\n\tdatingClassTest()\n\n```","tags":["机器学习","knn"],"categories":["MachineLearning"]},{"title":"webrtc的便利性","url":"/2018-04-28/webrtc_business_2/","content":"\n#### webrtc的便利性\n在Webrtc的使用情况中,其目标用户主要有开发者和服务商，他们都是需要实时通信能力的用户,对他们而言，采用webrtc主要取决于两个方面:\n1. 减少他们的花费 - 降低准入门槛\n2. 减少使用他们技术的终端用户的使用障碍\n\n\n##### 新服务商进入的壁垒\n作为一个宽泛协议下的开源项目，webrtc为开发者提供了一个巨大的开端，其意义在于它使得开发和测试的成本都降低了很多。\n准入壁垒的降低给市场带了3种不同的变化：\n1. 更多的开发者和服务商开始使用这种实时通信技术\n2. 之前没有过的商业案例现在也开始开展业务，并取得了可喜的ROI(投资回报率)\n3. 正在尝试创建新的商业模式\n\n\n##### 减少终端用户的不便利性\n作为浏览器内置的技术，webrtc能够使得web更加有互动性和功能更强大，数字通讯和支持服务可以直接在网页中\"拨号\"通讯，这减少了用户使用障碍从而增加了使用该服务的机会。\n服务商们有很多种方式来利用这些能力:\n- 在网站中通讯来添加和处理上下文请求\n- 使得用户利用网站通讯而无需拨打电话\n- 在同一个应用中使用社交互通的功能，而不是需要通过其他服务来通信，如skype\n- 在没有任何额外的安装或设置的情况下，可以使联系中心和cafe店进行通讯\n- 利用服务本身音频和视频功能将用户在应用中联系在一起\n- 为在线访谈等流程创建进行临时录制，例如视频墙等\n这种阻碍的减少有很可能增加webrtc的使用价值。\n\n在减少用户障碍和准入门槛的同时也带来了另一方面：\nwebrtc将通讯从服务转换为了功能\n\n目前为止，企业和商业不得不考虑区分他们提供的服务和通讯本身给客户提供的通信渠道的区别，迫使他们自己的通信服务。\n\n随着webtc的介绍的变化，我们能看到通信作为一个巨大的服务和功能使得我们能够将实时沟通作为们创建和提供的服务的一部分。\n\n\n\n##### 创新者在webrtc中的困境\nWebRTC就像当前的VoIP，但同时又非常不同：\n- 技术开发人员类型不一样\n- 更容易开发和更容易终端用户使用\n- 它依赖于一些后端的技术\n可见webrtc与现如今的技术并没有什么太大的区别,这使得开发者们有着盲目的自信。\n\n\n2017年，在每一个从linkedln上提及webrtc的开发人员,他们中有25人有用过SIP或者node.js，有92个人谈论到SIP或HTML5。\n到2019年基于SIP的webrtc与voip已经减少，相比于node或者html5已经减少到1：73和1：20。","tags":["Business","Tutorial"],"categories":["WebRTC"]},{"title":"paddle练习","url":"/2018-04-26/paddlepaddle/","content":"\n# paddle\n\n> 组网类api\n- variables \n- layers 模型层\n- block 模型块\n\n\n- 反向传播\ndeeplearn 架构\n- nn\n- alex net \n- vgg\n- resnet ","tags":["pracitce","paddle"],"categories":["deeplearn"]},{"title":"关于WebRtc","url":"/2018-04-26/webrtc_business_1/","content":"\n#### 什么是webrtc\nwebrtc是实时通讯的标准方式之一，它包含了两种技术：VoIP语音通话和web开发.\n\n图一：webrtc技术处于VoIP与web之间\n\nVoIP是基于ip的语音通话,通过网络连接来传输媒体流(通常是音频和视屏流).VoIP语音通讯一直处于自己生态系统的孤岛上,直到webrtc推出后,通过蓬勃发展的互联网我们可以通过浏览器相互连接.\n\nwebrtc使用VoIP来连接我们的浏览器,网站和移动端app。webrtc通过浏览器实现了简约的JavaScript的api接口,是HTML5规范的一部分。这意味着每个web开发人员都可以在他们的网站或者web应用上使用实时通讯的功能。\n\nWebRtc有两个方面的优势:\n- 免费\n- 由VoIP发展而来\n\n它们共同将通讯服务的准入门槛降低，但又不使用旧的案例技术来实现.\n\n\n##### webrtc是什么\nWebrtc这个术语有一定的误导性,它意味着两种不同的东西:\n1. webrtc协议规范\n2. webrtc开源项目\n这两种概念在大多数时间是可以互换的，最重要的是你能区别两者的区别\n\n###### 1. webrtc协议规范\nwebrtc是W3C和IETF的标准规范.\n规范处理：\n- 数据传输协议- 通过网络传输,两个webrtc应用能够进行交互\n- 浏览器本身的api接口 - 封装Webrtc暴露出javascript接口\n\nwebrtc1.0标准化已经在最后的审批阶段了。这已经持续了1年之久,很多的第三方软件都忽略webrtc的不便之处转而开始采用其产品.当然,在不停地浏览器间的webrtc的实现方式有差异,但是这种差异随着浏览器的发展将会慢慢弥合。\n\n###### 2.webrtc开源项目\n\nwebrtc同样也是一个开源项目，当google第一次公布webrtc时,他只是一个在它自己网站上的开源项目。\n\ngoogle明确表示他们将webrtc标准化并将其嵌入到浏览器中,最后，他们开源了代码并将置以最宽泛的开源协议。\n\n作为一个开源项目，webrtc是一个非常强大的项目使得任何人都可以在遵循webrtc协议规范的情形下来使用它。\n\n尽管有其他尝试提供完整的webrtc开源栈，但还是以google开源的项目作为最主要的选择。\n\n##### 免费\n###### webrtc在每个方面都是免费的\n1. 处于最宽泛的BSD开源协议的的开源项目包.在过去类似的技术是需要购买或者更严格的license协议,这对许多公司来来说都是不适用的\n2. 它提供免费的编解码器并且不需要版权付费和专利许可证.编解码器通常都是需要付费使用,专利使用费和基于使用数量的版权费.每个季度的许可费和使用量费用的重新计算都需要花费一笔不菲的资金.\n\n允许开发者重新规划webrtc和其包含的组件:\n- 将其移植带有流媒体服务的操作系统或者环境中是可行的\n- 在设备数量多和性能低的机器上部署使用\n- 剥离webrtc的模块单独使用,特别是webrtc的回声消除和编解码实现。\n\n\n##### 小结\n最重要的是记住webrtc是免费的，它有很多商业的产品,服务和供需市场，这意味着可以减少风险，开发者的花费和从市场获得服务的时间。","tags":["Business","Tutorial"],"categories":["WebRTC"]},{"title":"机器学习面试指南","url":"/2018-04-24/机器学习面试指南/","content":"\nQ1: What's the trade-off between bias and variance?\n\n问题1: 什么是偏差（bias）、方差（variable）之间的均衡？\n\nBias 是由于你使用的学习算法过度简单地拟合结果或者错误地拟合结果导致的错误。它反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，即算法本身的拟合能力。Bias 可能会导致模型欠拟合，使其难以具有较高的预测准确性，也很难将你的知识从训练集推广到测试集。\n\nVariance 是由于你使用的学习算法过于复杂而产生的错误。它反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。反应预测的波动情况。Variance 过高会导致算法对训练数据的高纬度变化过于敏感，这样会导致模型过度拟合数据。从而你的模型会从训练集里带来太多噪音，这会对测试数据有一定的好处。\n\nBias-Variance 的分解，本质上是通过在基础数据集中添加偏差、方差和一点由噪声引起的不可约误差，来分解算法上的学习误差。从本质上讲，如果你使模型更复杂并添加更多变量，你将会失去一些 Bias 但获得一些 Variance，这就是我们所说的权衡（tradeoff）。这也是为什么我们在建模的过程中，不希望这个模型同时拥有高的偏差和方差。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211174737.jpg)\n\nQ2: What is the difference between supervised and unsupervised machine learning?\n\n问题2：监督学习和非监督学习有什么不同？\n\n监督学习需要train有label的数据。例如，为了进行classification（一项受监督的学习任务），您需要首先标记将用于培训模型的数据，以便将数据分类到标记的组中。相反的，无监督学习不需要明确标记数据。\n\nQ3: How is KNN different from k-means clustering?\n\n问题3: KNN和 k-means 聚类由什么不同？\n\nK-Nearest Neighbors是一种监督分类算法，而 k-means聚类是一种无监督的聚类算法。 虽然这些机制起初可能看起来相似，但这实际上意味着为了使K-Nearest Neighbors工作，你需要标记数据，以便将未标记的点分类（因此是最近邻居部分）。 K均值聚类仅需要一组未标记的点和阈值：算法将采用未标记的点并逐渐学习如何通过计算不同点之间的距离的平均值将它们聚类成组。\n\n这里的关键区别在于，KNN需要标记点，因此是有监督的学习，而k-means不是，因此是无监督学习。\n\nQ4: Explain how a ROC curve works.\n\n问题4：解释一下ROC曲线的原理\n\nROC曲线是真阳率与各种阈值下的假阳率之间的对比度的图形表示。 它通常用作代表模型灵敏度（真阳性）与跌落之间的平衡或它将触发误报（假阳性）的概率。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211174905.jpg)\n\nQ5: Define precision and recall.\n\n问题5：定义精度和召回率\n\n召回（率）也称为真阳性率：您的模型声称的阳性数量与整个数据中的实际阳性数量相比。 精确度也称为阳性预测值，它衡量的是您的模型声称与实际声称的阳性数量相比的准确阳性数量。 在您预测在10个苹果的情况下有10个苹果和5个橙子的情况下，可以更容易地想到回忆和精确度。 你有完美的召回（实际上有10个苹果，你预测会有10个），但66.7％的精度，因为在你预测的15个事件中，只有10个（苹果）是正确的。\n\nQ6: What is Bayes' Theorem? How is it useful in a machine learning context?\n\n问题6：什么是贝叶斯定理？它在机器学习环境中如何有用?\n\n贝叶斯定理描述了当你不能准确知悉一个事物的本质时，你可以依靠与事物特定本质相关的事件出现的多少去判断其本质属性的概率。 它给出了已知先验知识下事件的后验概率。\n\n在数学上，它表示为条件样本的真阳性率除以总体的假阳性率和条件的真阳性率之和。假设你在流感测试后有60%的机会真的感染了流感，但是在感染了流感的人中，50%的测试都是错误的，总人口只有5%的机会感染了流感。在做了阳性测试后，你真的有60%的机会患上流感吗？\n\n贝叶斯定理说不，它说你有一个（0.6*0.05）（条件样本的真阳性率）/（0.6*0.05）（条件样本的真阳性率）+（0.5*0.95）（人群的假阳性率）= 5.94%的机会感染流感。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211175006-1024x657.jpg)\n\n贝叶斯理论是机器学习一个分支的幕后操纵大佬，所以在你考虑要准备一个机器学习的面试的时候一定不能忽略这个知识点。\n\nQ7: Why is \"Naive\" Bayes naive?\n\n问题7：为什么我们要称\"朴素\"贝叶斯？\n\n尽管 Naive Bayes 具有实际应用，特别是在文本挖掘中，但它被认为是\"天真的\"，因为它假设在实际数据中几乎不可能看到：条件概率被计算为组件个体概率的纯乘积。 这意味着特征的绝对独立性 -- 这种情况在现实生活中可能永远不会遇到。\n\n正如 Quora 上一些评论者所说的那样，Naive Bayes 分类器发现你喜欢泡菜和冰淇淋之后，可能会天真地推荐你一个泡菜冰淇淋。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211175110.jpg)\n\nQ8: Explain the difference between L1 and L2 regularization.\n\n问题8：L1、L2正则之间有什么不同？\n\nL2正则，对应的是加入2范数，使得对权重进行衰减，从而达到惩罚损失函数的目的，防止模型过拟合。保留显著减小损失函数方向上的权重，而对于那些对函数值影响不大的权重使其衰减接近于0。相当于加入一个gaussian prior。\n\nL1正则 对应得失加入1范数，同样可以防止过拟合。它会产生更稀疏的解，即会使得部分权重变为0，达到特征选择的效果。相当于加入了一个laplacean prior。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211175158.jpg)\n\nQ9: What's your favorite algorithm, and can you explain it to me in less than a minute?\n\n问题9：你最喜欢的算法是什么？把它解释一下。\n\n这种类型的问题测试了你对如何用平衡来传达复杂和技术上的细微差别的理解，以及快速和有效地总结的能力。确保你有选择，确保你能简单有效地解释不同的算法，使一个五岁的孩子能够掌握基础知识！\n\nQ10: What's the difference between Type I and Type II error?\n\n问题10：第一类误差和第二类误差有什么区别？\n\n第一类误差指的是假正率，第二类指的是假负率。简单来说，第一类误差意味着假设为真的情况下，作出了拒绝原假设的一种错误推断。第二类误差意味着假设为假的情况下，做出了接受原假设的一种错误判断。\n\n举个例子：第一类误差，你误判一个男的他怀孕了。第二类误差，你误判了一位其实已经怀孕的女子没怀孕。\n\nQ11: What's a Fourier transform?\n\n问题11：什么是傅立叶变换？\n\n傅立叶变换是将一般函数分解成对称函数叠加的一般方法。或者，正如这篇更直观的教程所说，在一杯冰沙中，我们就是这样找到配方的。傅立叶变换找到一组循环速度、振幅和相位，以匹配任何时间信号。傅立叶变换将信号从时间域转换为频率域-这是从音频信号或其他时间序列（如传感器数据）中提取特征的一种非常常见的方法。\n\nQ12: What's the difference between probability and likelihood?\n\n问题12：概率和似然有什么区别？\n\n概率和似然都是指可能性，但在统计学中，概率和似然有截然不同的用法。概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。例如，对于\"一枚正反对称的硬币上抛十次\"这种事件，我们可以问硬币落地时十次都是正面向上的\"概率\"是多少；而对于\"一枚硬币上抛十次，我们则可以问，这枚硬币正反面对称的\"似然\"程度是多少。\n\n概率(密度)表达给定θ下样本随机向量X=x的可能性，而似然表达了给定样本X=x下参数θ1(相对于另外的参数θ2)为真实值的可能性。我们总是对随机变量的取值谈概率，而在非贝叶斯统计的角度下，参数是一个实数而非随机变量，所以我们一般不谈一个参数的概率，而说似然。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190211175334.png)\n\nQ13: What is deep learning, and how does it contrast with other machine learning algorithms?\n\n问题13：什么是深度学习，它与机器学习算法之间有什么联系？\n\n深度学习是与神经网络有关的机器学习的一个子集：如何使用反向传播和神经科学中的某些原理来更精确地建模大量未标记或半结构化数据。从这个意义上说，深度学习是一种无监督的学习算法，它通过使用神经网络来学习数据的表示。\n\nQ14: What's the difference between a generative and discriminative model?\n\n问题14：生成模型与判别模型有什么区别？\n\n生成模型将学习数据类别，而判别模型将简单地学习不同类别数据之间的区别。 判别模型通常优于分类任务的生成模型。\n\nQ15- What cross-validation technique would you use on a time series dataset?\n\n问题15：交叉检验如何用在时间序列数据上？\n\n与标准的k-folds 交叉检验不同，数据不是随机分布的，而是具有时序性的。如果模式出现在后期，模型仍然需要选择先前时间的数据，尽管前期对模式无影响。我们可以如下这么做：\n\nfold1：training[1], test[2]\n\nfold2：training[1 2], test[3]\n\nfold3：training[1 2 3], test[4]\n\nfold4：training[1 2 3 4], test[5]\n\nfold5：training[1 2 3 4 5], test[6]\n\nQ16- How is a decision tree pruned?\n\n问题16：如何对决策树进行剪枝？\n\n剪枝是在决策树中，为了降低模型的复杂度，提高决策树模型的预测精度，去除预测能力较弱的分支后所发生的现象。修剪可以自下而上和自上而下进行，方法包括减少错误修剪和成本复杂度修剪。\n\n减少错误修剪可能是最简单的版本：替换每个节点。如果不降低预测精度，则保持修剪。虽然很简单，但这种启发式方法实际上非常接近于一种可以最大限度地优化准确性的方法。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211175732.jpg)\n\nQ17: Which is more important to you? Model accuracy, or model performance?\n\n问题17：模型的精度和模型的性能哪个对你更重要？\n\n这个问题测试您对机器学习模型性能细微差别的理解！机器学习面试问题往往着眼于细节。有些模型具有更高的准确度，而在预测能力方面表现较差 --- 这有什么意义？\n\n好吧，这一切都与模型的准确性仅仅是模型性能的一个子集有关，在这一点上，有时是一个误导。例如，如果你想在一个拥有数百万样本的海量数据集中检测欺诈行为，那么一个更准确的模型很可能会预测，如果只有极少数的案例是欺诈行为，那么根本就不会有欺诈行为。然而，对于预测模型来说，这是无用的------一个旨在发现声称根本没有欺诈的欺诈的模型！这样的问题可以帮助您证明您理解模型的准确性并不是模型性能的全部。\n\nQ18: What's the F1 score? How would you use it?\n\n问题18：什么是F1数，怎么使用它？\n\nF1分数是衡量模型性能的指标。它是模型精度和召回的加权平均值，结果趋向于1是最好的，结果趋向于0是最差的。你可以在分类测试中使用它，而真正的否定并不重要。\n\nQ19: How would you handle an imbalanced dataset?\n\n问题19：如何处理一个不平衡的数据集？\n\n例如，当您有一个分类测试，并且90%的数据都在一个类中时，就会产生一个不平衡的数据集。这就导致了问题：如果您对其他类别的数据没有预测能力，那么90%的精度然而可能会出现偏差！下面是一些克服困难的策略：\n\n1-收集更多数据，甚至数据集中的不平衡。\n\n2-对数据集重新取样以纠正不平衡。\n\n3-在你的数据集中尝试一个不同的算法。\n\n这里重要的是，您对不平衡数据集可能造成的损害以及如何平衡具有敏锐的感知。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190211175908.jpg)\n\nQ20: When should you use classification over regression?\n\n问题20：什么时候你应该使用分类而不是回归？\n\n分类产生离散值并将数据集转换为严格的类别，而回归则提供连续的结果，使您能够更好地区分各个点之间的差异。如果您希望结果反映数据集中数据点对某些明确类别的归属性（例如：如果您希望知道某个名称是男性还是女性，而不仅仅是它们与男性和女性名称之间的关联性），则可以使用分类而不是回归。\n\nQ21: Name an example where ensemble techniques might be useful.\n\n问题21：举个例子，说明使用集成学习会很有用。\n\n集成学习通过组合一些基学习算法来优化得到更好的预测性能，通常可以防止模型的过拟合使模型更具有鲁棒性。\n\n你可以列举一些集成学习的例子，如bagging、boosting、stacking等，并且了解他们是如何增加模型预测能力的。\n\nQ22: How do you ensure you're not overfitting with a model?\n\n问题22：你如何确保你的模型没有过拟合？\n\n过度拟合的训练数据以及数据携带的噪音，对于测试数据会带来不确定的推测。有如下三种方法避免过拟合：\n\n1\\. 保持模型尽可能地简单：通过考量较少的变量和参数来减少方差，达到数据中消除部分噪音的效果。\n\n2\\. 使用交叉检验的手段如：k-folds cross-validation。\n\n3\\. 使用正则化的技术如：LASSO方法来惩罚模型中可能导致过拟合的参数。\n\nQ23: What evaluation approaches would you work to gauge the effectiveness of a machine learning model?\n\n问题23：如何评估你的机器学习模型的有效性？\n\n首先你需要将数据分成训练集和测试集，或者使用给交叉验证方法分割。然后你需要选择度量模型表现的metrics，如F1数、准确率、混淆矩阵等。更重要的是，根据实际情况你需要理解模型度量的轻微差别，以便于选择正确的度量标准。\n\nQ24: How would you evaluate a logistic regression model?\n\n问题24：如何评估一个LR model？\n\n上述问题的一部分。你必须演示对逻辑回归的典型目标（分类、预测等）的理解，并提供一些示例和用例。\n\nQ25: What's the \"kernel trick\" and how is it useful?\n\n问题25：什么是核技巧，有什么用处？\n\n核技巧使用核函数，确保在高维空间不需要明确计算点的坐标，而是计算数据的特征空间中的内积。这使其具有一个很有用的属性：更容易的计算高维空间中点的坐标。许多算法都可以表示称这样的内积形式，使用核技巧可以保证低维数据在高维空间中运用算法进行计算。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190214105151.png)\n\nQ26: How do you handle missing or corrupted data in a dataset?\n\n问题26：如何处理数据集中丢失或损坏的数据？\n\n您可以在数据集中找到丢失/损坏的数据，然后删除这些行或列，或者决定用另一个值替换它们。\n\n在pandas中，有两种非常有用的方法：isNull（）和dropna（），这两种方法将帮助您查找缺少或损坏数据的数据列，并删除这些值。如果要用占位符值（例如0）填充无效值，可以使用fillna（）方法。\n\nQ27: Do you have experience with Spark or big data tools for machine learning?\n\n问题27：你是否有使用Spark或大数据工具进行机器学习的经验？\n\n您需要熟悉不同公司的大数据含义以及他们想要的不同工具。Spark是目前最受欢迎的大数据工具，能够快速处理海量数据集。老实说，如果你没有所需工具的经验，同时看看工作描述，看看什么工具需要：你会想投资去熟悉它们。\n\nQ28: Pick an algorithm. Write the psuedo-code for a parallel implementation.\n\n问题28：选择一个算法。为并行实现编写psuedo代码。\n\n这类问题展示了您并行思考的能力，以及如何在处理大数据的编程实现中处理并发性。请看一下伪代码框架（如peril-L）和可视化工具（如Web序列图），以帮助您展示编写反映并行性的代码的能力。\n\nQ29: What are some differences between a linked list and an array?\n\n问题29：链表和数组之间有什么区别？\n\n数组是有序的对象集合。 链表是一系列带有指针的对象，指示如何按顺序处理它们。 与链表不同，数组假定每个元素具有相同的大小。 链表可以更容易地有机增长：必须预先定义或重新定义阵列以进行有机增长。 改组链接列表涉及改变哪些点指向哪里 -- 同时，改组数组更复杂并占用更多内存。\n\nQ30: Describe a hash table.\n\n问题30：描述哈希表。\n\n哈希表是一种产生关联数组的数据结构。 通过使用散列函数将键映射到某些值。 它们通常用于数据库索引等任务。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190214105331.png)\n\nQ31: Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?\n\n问题31：你使用哪些数据可视化库？ 你对最佳数据可视化工具有何看法？\n\n这里重要的是定义您对如何在工具方面正确可视化数据和个人偏好的看法。 流行的工具包括R的ggplot，Python的seaborn和matplotlib，以及Plot.ly和Tableau等工具。\n\n这些机器学习面试问题涉及如何将您的一般机器学习知识应用于特定公司的要求。 您将被要求创建案例研究，并通过您的机器学习技能扩展您所申请的公司和行业的知识。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190214105536.png)\n\nQ32: How would you implement a recommendation system for our company's users?\n\n问题32：您如何为我们公司的用户实施推荐系统？\n\n许多这种类型的机器学习面试问题将涉及机器学习模型的实施以解决公司的问题。 您必须深入研究公司及其行业，尤其是公司的收入驱动因素，以及公司在其所在行业中所采用的用户类型。\n\nQ33: How can we use your machine learning skills to generate revenue?\n\n问题33：我们如何利用您的机器学习技能来创造收入？\n\n这是一个棘手的问题。理想的答案将证明您对推动业务发展的因素以及您的技能如何关联的了解。例如，如果你正在面试音乐流初创公司Spotify，你可以说，你在开发更好的推荐模式方面的技能将增加用户保留率，从长远来看这将增加收入。\n\n上面链接的Startup Metrics Slideshare将帮助您准确了解在考虑支出和成长时，哪些绩效指标对初创技术公司是重要的。\n\nQ34: What do you think of our current data process?\n\n问题34：你认为我们当前的数据处理过程如何？\n\n这类问题要求你认真倾听，并以富有建设性和洞察力的方式传达反馈。 你的面试官正在试图判断您是否是他们团队中的重要成员，以及你是否根据公司或行业特定条件，掌握了为什么某些事情按照公司数据流程的方式设置的细微差别。 他们试图看看你是否可以成为有见地同行。 随行而动。\n\n这一系列的机器学习面试问题试图衡量你对机器学习的热情和兴趣。正确的答案将作为你承诺终身学习机器学习的证明。\n\n![](https://www.dataapplab.com/wp-content/uploads/2019/02/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190214105639.png)\n\nQ35: What are the last machine learning papers you've read?\n\n问题35：你读过的最后一篇机器学习论文是什么？\n\n如果你想表现出对机器学习职位的兴趣，就必须掌握最新的机器学习科学文献。这篇深入学习的后代（从Hinton到Bengio再到LeCun）对自然的深入学习的概述可以是一篇很好的参考论文，也可以是一篇深入学习中正在发生的事情的概述，以及你可能想引用的那种论文。\n\nQ36: Do you have research experience in machine learning?\n\n问题36：你在机器学习方面有研究经验吗？\n\n与最后一点相关的是，大多数为机器学习职位招聘的组织都会寻找你在该领域的正式经验。由该领域的先行者共同撰写或监督的研究论文，可以使你在被雇佣和不被雇佣之间产生差异。确保你已经准备好了一份关于你的研究经验和论文的总结，如果你不准备的话，还要对你的背景和缺乏正式研究经验做出解释。\n\nQ37: What are your favorite use cases of machine learning models?\n\n问题37：你最喜欢的机器学习模型的用例是什么？\n\n这里我们拿 Quora 上面的一个帖子为例，帖子在这里：<https://bit.ly/2MGYyQY>\n\n上面的 Quora 帖子里包含一些示例，例如决策树，它根据智商分数将人们分类为不同的智力层次。确保你心里有几个例子，并描述与你产生共鸣的地方。重要的是你要对机器学习的实现方式表现出兴趣。\n\nQ38：How would you approach the \"Netflix Prize\" competition?\n\n问题38：你想以什么方式赢得\"Netflix奖\"比赛？\n\nNetflix奖是一项著名的竞赛，Netflix提供了 $1,000,000的奖金，以获得更好的协同过滤算法（collaborative filtering algorithm）。关于这个比赛的最后赢家， BellKor；他们让这个算法效率提升百分之十，并且给出了多种解法。多了解这些行业相关的 Case 并且和你面试官侃侃而谈能够体现你对于机器学习这个领域的关注\n\nQ39:  Where do you usually source datasets?\n\n问题39：您通常在哪里寻找数据集？\n\n像这样的机器学习面试问题试图让你了解机器学习兴趣的核心。 真正热衷于机器学习的人将会独自完成侧面项目，并且很清楚那些伟大的数据集是什么。 如果您遗失任何内容，请查看 Quandl 获取的经济和财务数据，以及 Kaggle 的数据集集合，以获取其他优秀列表。\n\nQ40: How do you think Google is training data for self-driving cars?\n\n问题40：你认为谷歌是如何为自动驾驶汽车提供培训数据的？\n\n像这样的机器学习面试问题确实测试了你对不同机器学习方法的知识，如果你不知道答案，你的创造力。谷歌目前正在使用 recaptcha 来获取店面和交通标志上的标签数据。他们还建立在由Sebastian Thrun在谷歌（Googlex）收集的培训数据的基础上 --- 其中一些数据是由他在沙漠沙丘上驾驶马车的研究生获得的！\n\nQ41: How would you simulate the approach AlphaGo took to beat Lee Sedol at Go?\n\n问题41：你将如何模拟阿尔法戈在围棋中击败李世乭的方法？\n\n在五个系列赛中，阿尔法戈击败了围棋中最优秀的人类选手李思多，这是机器学习和深度学习史上一个真正具有开创性的事件。上面的 Nature 论文描述了这是如何通过\"蒙特卡洛树搜索（Monte Carlo Tree Search）和深神经网络（Deep Neural Networks）来实现的，这些神经网络经过有监督的学习、人类专家游戏和加强自玩游戏的学习。\"\n\n\n原文链接：<https://www.springboard.com/blog/machine-learning-interview-questions/>","tags":["面试"],"categories":["machineLearning"]},{"title":"VIM 精简常用快捷键","url":"/2018-03-24/reference/Vim/vim编辑常用快捷键/","content":"\n## Vim 编辑常用快捷键\n\n| 快捷键       | 说明                                 |\n| ------------ | ------------------------------------ |\n| `,`          | Leader Key                           |\n| **函数跳转**      | |\n| `<leader>n`  | 打开/关闭代码资源管理器              |\n| `<leader>t`  | 打开/关闭函数列表                    |\n| `<leader>a`  | .h .cpp 文件切换                     |\n| `<leader>u`  | 转到函数声明                         |\n| `<leader>U`  | 转到函数实现                         |\n| `g]`         | 声明/定义跳转 |\n| `<leader>o`  | 打开include文件                      |\n| `<leader>y`  | 拷贝函数声明                         |\n| `<leader>p`  | 生成函数实现                         |\n| `<c-p>`      | 切换到上一个buffer                   |\n| `<c-n>`      | 切换到下一个buffer                   |\n| `:e <filename>` | 新建buffer打开文件 |\n| `Ctrl + ]`     | 找到光标所在位置的标签定义的地方      |\n| `Ctrl + t`     | 回到跳转之前的标签处             |\n| `Ctrl + o`     |  退回原来的地方  |\n|  **辅助操作**   |      |\n| `gcc`        | 注释代码                             |\n| `gcap`       | 注释段落                             |\n| `za`   | 打开或关闭当前折叠 |\n| `zM`   | 关闭所有折叠       |\n| `zR`   | 打开所有折叠       |\n| `<c-w>h`          | 跳到左边的窗口         |\n| `<c-w>j`          | 跳到下边的窗口         |\n| `<c-w>k`          | 跳到上边的窗口         |\n| `<c-w>l`          | 跳到右边的窗口         |\n| `<c-w>c`          | 关闭当前窗口           |\n| `<c-w>o`          | 关闭其他窗口           |\n| `:only`           | 关闭其他窗口           |\n\n\n\n| 快捷键 | 说明               |\n| ------ | ------------------ |\n| `<leader>f`  | 搜索~目录下的文件                    |\n| `<leader>F`  | 搜索当前目录下的文本                 |\n| `<leader>g`  | 显示git仓库提交记录                  |\n| `<leader>G`  | 显示当前文件提交记录                 |\n| `<leader>gg` | 显示当前文件在某个commit下的完整内容 |\n| `<leader>d`  | 删除当前buffer                       |\n| `<leader>D`  | 删除当前buffer外的所有buffer         |\n| `Ya`         | 复制行文本到字母a                    |\n| `Da`         | 剪切行文本到字母a                    |\n| `Ca`         | 改写行文本到字母a                    |\n| `rr`         | 替换文本                             |\n| `<leader>r`  | 全局替换，目前只支持单个文件         |\n| `vif`        | 选中函数内容                         |\n| `dif`        | 删除函数内容                         |\n| `cif`        | 改写函数内容                         |\n| `vaf`        | 选中函数内容（包括函数名 花括号）    |\n| `daf`        | 删除函数内容（包括函数名 花括号）    |\n| `caf`        | 改写函数内容（包括函数名 花括号）    |\n| `:sp <filename>`  | 横向切分窗口并打开文件 |\n| `:vsp <filename>` | 竖向切分窗口并打开文件 |\n| `gg=G`         | 缩进整个文件                 |\n| `=a{`          | 缩进光标所在代码块            |\n| `=i{`          | 缩进光标所在代码块，不缩进\"{\"     |\n| `<<`           | 减少缩进                    |\n| `>>`           | 增加缩进                    |\n| `==`           | 自动缩进                    |\n| `ctrl+f`    | 下翻一屏    |\n| `ctrl+b`    | 上翻一屏    |\n| `ctrl+d`    | 下翻半屏    |\n| `ctrl+u`    | 上翻半屏    |\n| `s`            | 替换字符（删除光标处字符，并进入插入模式，前可接数量）   |\n| `S`            | 替换行（删除当前行，并进入插入模式，前可接数量）         |\n| `cc`           | 改写当前行（删除当前行并进入插入模式），同 S             |\n| `cw`           | 改写光标开始处的当前单词                         |\n| `ciw`          | 改写光标所处的单词                              |\n| `caw`          | 改写光标所处的单词，并且包括前后空格（如果有的话）       |\n| `ct,`          | 改写到逗号                                      |\n| `c0`           | 改写到行首                                     |\n| `c^`           | 改写到行首（第一个非零字符）                        |\n| `c$`           | 改写到行末                                     |\n| `C`            | 改写到行末（同 c$）                       |\n| `ci\"`          | 改写双引号中的内容                             |\n| `ci'`          | 改写单引号中的内容                             |\n| `ci)`          | 改写小括号中的内容                             |\n| `ci]`          | 改写中括号中内容                               |\n| `ci}`          | 改写大括号中内容                               |\n| `cit`          | 改写 xml tag 中的内容                         |\n| `cis`          | 改写当前句子                                  |\n| `ciB`          | 改写'{}'中的内容                              |\n| `c2w`          | 改写下两个单词                                |\n| `ct(`          | 改写到小括号前                                |\n| `x`            | 删除当前字符，前面可以接数字，3x代表删除三个字符         |\n| `X`            | 向前删除字符                                  |\n| `dd`           | 删除当前行                                   |\n| `d0`           | 删除到行首                                   |\n| `d^`           | 删除到行首（第一个非零字符）                     |\n| `d$`           | 删除到行末                                    |\n| `D`            | 删除到行末（同 d$）                            |\n| `dw`           | 删除当前单词                                  |\n| `dt,`          | 删除到逗号                                    |\n| `diw`          | 删除光标所处的单词                             |\n| `daw`          | 删除光标所处的单词，并包含前后空格（如果有的话）     |\n| `di\"`          | 删除双引号中的内容                              |\n| `di'`          | 删除单引号中的内容                             |\n| `di)`          | 删除小括号中的内容                             |\n| `di]`          | 删除中括号中内容                               |\n| `di}`          | 删除大括号中内容                               |\n| `diB`          | 删除'{}'中的内容                              |\n| `dit`          | 删除 xml tag 中的内容                         |\n| `dis`          | 删除当前句子                                  |\n| `d2w`          | 删除下两个单词                                |\n| `dt(`          | 删除到小括号前                                |\n| `dgg`          | 删除到文件头部                                |\n| `dG`           | 删除到文件尾部                                |\n| `d}`           | 删除下一段                                   |\n| `d{`           | 删除上一段                                   |\n| `u`            | 撤销                                        |\n| `U`            | 撤销整行操作                                 |\n| `CTRL-R`       | 撤销上一次 u 命令                             |\n| `J`            | 连接若干行                                   |\n| `gJ`           | 连接若干行，删除空白字符                       |\n| `.`            | 重复上一次操作                                |\n| `~`            | 交换大小写                                   |\n| `g~iw`         | 替换当前单词的大小写                           |\n| `gUiw`         | 将单词转成大写                                |\n| `guiw`         | 将当前单词转成小写                             |\n| `guu`          | 全行转为小写                                  |\n| `gUU`          | 全行转为大写                                  |\n| `gg=G`         | 缩进整个文件                                  |\n| `=a{`          | 缩进光标所在代码块                             |\n| `=i{`          | 缩进光标所在代码块，不缩进\"{\"                   |\n| `<<`           | 减少缩进                                     |\n| `>>`           | 增加缩进                                     |\n| `==`           | 自动缩进                                     |\n| `CTRL-A`       | 增加数字                                     |\n| `CTRL-X`       | 减少数字                                     |\n| `p`            | 粘贴到光标后                                  |\n| `P`            | 粘贴到光标前                                  |\n| `v`            | 开始标记                                     |\n| `y`            | 复制标记内容                                  |\n| `V`            | 开始按行标记                                  |\n| `CTRL-V`       | 开始列标记                                    |\n| `y$`           | 复制当前位置到本行结束的内容                     |\n| `yy`           | 复制当前行                                    |\n| `Y`            | 复制当前行，同 yy                              |\n| `yt,`          | 复制到逗号                                    |\n| `yiw`          | 复制当前单词                                  |\n| `3yy`          | 复制光标下三行内容                             |\n| `v0`           | 选中当前位置到行首                             |\n| `v$`           | 选中当前位置到行末                             |\n| `vt,`          | 选中到逗号                                    |\n| `viw`          | 选中当前单词                                  |\n| `vi)`          | 选中小括号内的东西                             |\n| `vi]`          | 选中中括号内的东西                             |\n| `viB`          | 选中'{}'中的内容                              |\n| `vis`          | 选中句子中的东西                              |\n| `gv`           | 重新选择上一次选中的文字                    |\n| `:set paste`   | 允许粘贴模式（避免粘贴时自动缩进影响格式）          |\n| `:set nopaste` | 禁止粘贴模式                                    |\n| `\"?yy`         | 复制当前行到寄存器 ? ，问号代表 0-9 的寄存器名称         |\n| `\"?p`          | 将寄存器 ? 的内容粘贴到光标后                            |\n| `\"?P`          | 将寄存器 ? 的内容粘贴到光标前                            |\n| `:registers`   | 显示所有寄存器内容                             |\n| `:[range]y`    | 复制范围，比如 :20,30y 是复制20到30行，:10y 是复制第十行 |\n| `:[range]d`    | 删除范围，比如 :20,30d 是删除20到30行，:10d 是删除第十行 |\n| `ddp`          | 交换两行内容：先删除当前行复制到寄存器，并粘贴           |\n| `/pattern`           | 从光标处向文件尾搜索 pattern                     |\n| `?pattern`           | 从光标处向文件头搜索 pattern                     |\n| `n`                  | 向同一方向执行上一次搜索                         |\n| `N`                  | 向相反方向执行上一次搜索                         |\n| `*`                  | 向前搜索光标下的单词                             |\n| `#`                  | 向后搜索光标下的单词                             |\n| `:s/p1/p2/g`         | 替换当前行的p1为p2                               |\n| `:%s/p1/p2/g`        | 替换当前文件中的p1为p2                           |\n| `:%s/<p1>/p2/g`      | 替换当前文件中的p1单词为p2                       |\n| `:%s/p1/p2/gc`       | 替换当前文件中的p1为p2，并且每处询问你是否替换   |\n| `:10,20s/p1/p2/g`    | 将第10到20行中所有p1替换为p2                     |\n| `:%s/1\\\\2\\/3/123/g`  | 将“1\\2/3” 替换为 “123”（特殊字符使用反斜杠标注） |\n| `:%s/\\r//g`          | 删除 DOS 换行符 ^M                           |\n| `:g/^\\s*$/d`         | 删除空行                                   |\n| `:g/test/d`          | 删除所有包含 test 的行                           |\n| `:v/test/d`          | 删除所有不包含 test 的行                         |\n| `:%s/^/test/`        | 在行首加入特定字符(也可以用宏录制来添加)         |\n| `:%s/$/test/`        | 在行尾加入特定字符(也可以用宏录制来添加)         |\n| `:sort`              | 排序                                        |\n| `:g/^\\(.\\+\\)$\\n\\1/d` | 去除重复行(先排序)                               |\n| `:%s/^.\\{10\\}//`     | 删除每行前10个字符                               |\n| `:%s/.\\{10\\}$//`     | 删除每行尾10个字符                               |\n\n\n\n","tags":["vim"],"categories":["Vim"]},{"title":"VIM 编程相关的常用快捷键","url":"/2018-03-23/reference/Vim/vimProgramHotKey/","content":"\n> [vimplus github](<https://github.com/chxuan/vimplus>)\n\n## 插件相关\n\n| 快捷键       | 说明                                 |\n| ------------ | ------------------------------------ |\n| `,`          | Leader Key                           |\n| `<leader>n`  | 打开/关闭代码资源管理器              |\n| `<leader>t`  | 打开/关闭函数列表                    |\n| `<leader>a`  | .h .cpp 文件切换                     |\n| `<leader>u`  | 转到函数声明                         |\n| `<leader>U`  | 转到函数实现                         |\n| `<leader>o`  | 打开include文件                      |\n| `<leader>y`  | 拷贝函数声明                         |\n| `<leader>p`  | 生成函数实现                         |\n| `<leader>w`  | 单词跳转                             |\n| `<leader>f`  | 搜索~目录下的文件                    |\n| `<leader>F`  | 搜索当前目录下的文本                 |\n| `<leader>g`  | 显示git仓库提交记录                  |\n| `<leader>G`  | 显示当前文件提交记录                 |\n| `<leader>gg` | 显示当前文件在某个commit下的完整内容 |\n| `<leader>ff` | 语法错误自动修复(FixIt)              |\n| `<c-p>`      | 切换到上一个buffer                   |\n| `<c-n>`      | 切换到下一个buffer                   |\n| `<leader>d`  | 删除当前buffer                       |\n| `<leader>D`  | 删除当前buffer外的所有buffer         |\n| `Ya`         | 复制行文本到字母a                    |\n| `Da`         | 剪切行文本到字母a                    |\n| `Ca`         | 改写行文本到字母a                    |\n| `rr`         | 替换文本                             |\n| `<leader>r`  | 全局替换，目前只支持单个文件         |\n| `gcc`        | 注释代码                             |\n| `gcap`       | 注释段落                             |\n| `vif`        | 选中函数内容                         |\n| `dif`        | 删除函数内容                         |\n| `cif`        | 改写函数内容                         |\n| `vaf`        | 选中函数内容（包括函数名 花括号）    |\n| `daf`        | 删除函数内容（包括函数名 花括号）    |\n| `caf`        | 改写函数内容（包括函数名 花括号）    |\n| `fa`         | 查找字母a，然后再按f键查找下一个     |\n\n\n## 折叠代码\n\n| 快捷键 | 说明               |\n| ------ | ------------------ |\n| `za`   | 打开或关闭当前折叠 |\n| `zM`   | 关闭所有折叠       |\n| `zR`   | 打开所有折叠       |\n\n## 声明/定义跳转\n\n| 快捷键 | 说明          |\n| ------ | ------------- |\n| `g]`   | 声明/定义跳转 |\n\n\n\n## 缓存操作\n\n| 快捷键          | 说明               |\n| --------------- | ------------------ |\n| `:e <filename>` | 新建buffer打开文件 |\n| `:bp`           | 切换到上一个buffer |\n| `:bn`           | 切换到下一个buffer |\n| `:bd`           | 删除当前buffer     |\n\n\n## 窗口操作\n\n| 快捷键            | 说明                   |\n| ----------------- | ---------------------- |\n| `:sp <filename>`  | 横向切分窗口并打开文件 |\n| `:vsp <filename>` | 竖向切分窗口并打开文件 |\n| `<c-w>h`          | 跳到左边的窗口         |\n| `<c-w>j`          | 跳到下边的窗口         |\n| `<c-w>k`          | 跳到上边的窗口         |\n| `<c-w>l`          | 跳到右边的窗口         |\n| `<c-w>c`          | 关闭当前窗口           |\n| `<c-w>o`          | 关闭其他窗口           |\n| `:only`           | 关闭其他窗口           |\n\n## 光标移动\n\n| 快捷键  | 说明                                     |\n| ------- | ---------------------------------------- |\n| `h`     | 上下左右移动                             |\n| `j`     | 上下左右移动                             |\n| `k`     | 上下左右移动                             |\n| `l`     | 上下左右移动                             |\n| `0`     | 光标移动到行首                           |\n| `^`     | 跳到从行首开始第一个非空白字符           |\n| `$`     | 光标移动到行尾                           |\n| `<c-o>` | 跳到上一个位置                           |\n| `<c-i>` | 跳到下一个位置                           |\n| `<c-b>` | 上一页                                   |\n| `<c-f>` | 下一页                                   |\n| `<c-u>` | 上移半屏                                 |\n| `<c-d>` | 下移半屏                                 |\n| `H`     | 调到屏幕顶上                             |\n| `M`     | 调到屏幕中间                             |\n| `L`     | 调到屏幕下方                             |\n| `:n`    | 跳到第n行                                |\n| `w`     | 跳到下一个单词开头(标点或空格分隔的单词) |\n| `W`     | 跳到下一个单词开头(空格分隔的单词)       |\n| `e`     | 跳到下一个单词尾部(标点或空格分隔的单词) |\n| `E`     | 跳到下一个单词尾部(空格分隔的单词)       |\n| `b`     | 上一个单词头(标点或空格分隔的单词)       |\n| `B`     | 上一个单词头(空格分隔的单词)             |\n| `ge`    | 上一个单词尾                             |\n| `%`     | 在配对符间移动, 可用于()、{}、[]         |\n| `gg`    | 到文件首                                 |\n| `G`     | 到文件尾                                 |\n| `fx`    | 跳转到下一个为x的字符                    |\n| `Fx`    | 跳转到上一个为x的字符                    |\n| `tx`    | 跳转到下一个为x的字符前                  |\n| `Tx`    | 跳转到上一个为x的字符前                  |\n| `;`     | 跳到下一个搜索的结果                     |\n| `[[`    | 跳转到函数开头                           |\n| `]]`    | 跳转到函数结尾                           |\n\n\n## 文本编辑\n\n| 快捷键         | 说明                                                     |\n| -------------- | -------------------------------------------------------- |\n| `r`            | 替换当前字符                                             |\n| `R`            | 进入替换模式，直至 ESC 离开                              |\n| `s`            | 替换字符（删除光标处字符，并进入插入模式，前可接数量）   |\n| `S`            | 替换行（删除当前行，并进入插入模式，前可接数量）         |\n| `cc`           | 改写当前行（删除当前行并进入插入模式），同 S             |\n| `cw`           | 改写光标开始处的当前单词                                 |\n| `ciw`          | 改写光标所处的单词                                       |\n| `caw`          | 改写光标所处的单词，并且包括前后空格（如果有的话）       |\n| `ct,`          | 改写到逗号                                               |\n| `c0`           | 改写到行首                                               |\n| `c^`           | 改写到行首（第一个非零字符）                             |\n| `c$`           | 改写到行末                                               |\n| `C`            | 改写到行末（同 c$）                                      |\n| `ci\"`          | 改写双引号中的内容                                       |\n| `ci'`          | 改写单引号中的内容                                       |\n| `ci)`          | 改写小括号中的内容                                       |\n| `ci]`          | 改写中括号中内容                                         |\n| `ci}`          | 改写大括号中内容                                         |\n| `cit`          | 改写 xml tag 中的内容                                    |\n| `cis`          | 改写当前句子                                             |\n| `ciB`          | 改写'{}'中的内容                                         |\n| `c2w`          | 改写下两个单词                                           |\n| `ct(`          | 改写到小括号前                                           |\n| `x`            | 删除当前字符，前面可以接数字，3x代表删除三个字符         |\n| `X`            | 向前删除字符                                             |\n| `dd`           | 删除当前行                                               |\n| `d0`           | 删除到行首                                               |\n| `d^`           | 删除到行首（第一个非零字符）                             |\n| `d$`           | 删除到行末                                               |\n| `D`            | 删除到行末（同 d$）                                      |\n| `dw`           | 删除当前单词                                             |\n| `dt,`          | 删除到逗号                                               |\n| `diw`          | 删除光标所处的单词                                       |\n| `daw`          | 删除光标所处的单词，并包含前后空格（如果有的话）         |\n| `di\"`          | 删除双引号中的内容                                       |\n| `di'`          | 删除单引号中的内容                                       |\n| `di)`          | 删除小括号中的内容                                       |\n| `di]`          | 删除中括号中内容                                         |\n| `di}`          | 删除大括号中内容                                         |\n| `diB`          | 删除'{}'中的内容                                         |\n| `dit`          | 删除 xml tag 中的内容                                    |\n| `dis`          | 删除当前句子                                             |\n| `d2w`          | 删除下两个单词                                           |\n| `dt(`          | 删除到小括号前                                           |\n| `dgg`          | 删除到文件头部                                           |\n| `dG`           | 删除到文件尾部                                           |\n| `d}`           | 删除下一段                                               |\n| `d{`           | 删除上一段                                               |\n| `u`            | 撤销                                                     |\n| `U`            | 撤销整行操作                                             |\n| `CTRL-R`       | 撤销上一次 u 命令                                        |\n| `J`            | 连接若干行                                               |\n| `gJ`           | 连接若干行，删除空白字符                                 |\n| `.`            | 重复上一次操作                                           |\n| `~`            | 交换大小写                                               |\n| `g~iw`         | 替换当前单词的大小写                                     |\n| `gUiw`         | 将单词转成大写                                           |\n| `guiw`         | 将当前单词转成小写                                       |\n| `guu`          | 全行转为小写                                             |\n| `gUU`          | 全行转为大写                                             |\n| `gg=G`         | 缩进整个文件                                             |\n| `=a{`          | 缩进光标所在代码块                                       |\n| `=i{`          | 缩进光标所在代码块，不缩进\"{\"                            |\n| `<<`           | 减少缩进                                                 |\n| `>>`           | 增加缩进                                                 |\n| `==`           | 自动缩进                                                 |\n| `CTRL-A`       | 增加数字                                                 |\n| `CTRL-X`       | 减少数字                                                 |\n| `p`            | 粘贴到光标后                                             |\n| `P`            | 粘贴到光标前                                             |\n| `v`            | 开始标记                                                 |\n| `y`            | 复制标记内容                                             |\n| `V`            | 开始按行标记                                             |\n| `CTRL-V`       | 开始列标记                                               |\n| `y$`           | 复制当前位置到本行结束的内容                             |\n| `yy`           | 复制当前行                                               |\n| `Y`            | 复制当前行，同 yy                                        |\n| `yt,`          | 复制到逗号                                               |\n| `yiw`          | 复制当前单词                                             |\n| `3yy`          | 复制光标下三行内容                                       |\n| `v0`           | 选中当前位置到行首                                       |\n| `v$`           | 选中当前位置到行末                                       |\n| `vt,`          | 选中到逗号                                               |\n| `viw`          | 选中当前单词                                             |\n| `vi)`          | 选中小括号内的东西                                       |\n| `vi]`          | 选中中括号内的东西                                       |\n| `viB`          | 选中'{}'中的内容                                         |\n| `vis`          | 选中句子中的东西                                         |\n| `gv`           | 重新选择上一次选中的文字                                 |\n| `:set paste`   | 允许粘贴模式（避免粘贴时自动缩进影响格式）               |\n| `:set nopaste` | 禁止粘贴模式                                             |\n| `\"?yy`         | 复制当前行到寄存器 ? ，问号代表 0-9 的寄存器名称         |\n| `\"?p`          | 将寄存器 ? 的内容粘贴到光标后                            |\n| `\"?P`          | 将寄存器 ? 的内容粘贴到光标前                            |\n| `:registers`   | 显示所有寄存器内容                                       |\n| `:[range]y`    | 复制范围，比如 :20,30y 是复制20到30行，:10y 是复制第十行 |\n| `:[range]d`    | 删除范围，比如 :20,30d 是删除20到30行，:10d 是删除第十行 |\n| `ddp`          | 交换两行内容：先删除当前行复制到寄存器，并粘贴           |\n\n\n## 文件操作\n\n| 快捷键               | 说明                                   |\n| -------------------- | -------------------------------------- |\n| `:e <filename>`      | 打开文件并编辑                         |\n| `:saveas <filename>` | 另存为文件                             |\n| `:close`             | 关闭文件                               |\n| `:wa`                | 保存所有文件                           |\n| `:new`               | 打开一个新的窗口编辑新文件             |\n| `:enew`              | 在当前窗口创建新文件                   |\n| `:vnew`              | 在左右切分的新窗口中编辑新文件         |\n| `:tabnew`            | 在新的标签页中编辑新文件               |\n\n\n## 使用外部程序\n\n| 快捷键           | 说明                            |\n| ---------------- | ------------------------------- |\n| `!`              | 告诉vim正在执行一个过滤操作     |\n| `!5Gsort<Enter>` | 使用外部sort命令对1-5行文本排序 |\n| `!!`             | 对当前行执行过滤命令            |\n| `!!date<Enter>`  | 用\"date\"的输出代替当前行        |\n\n## 宏录制\n\n| 快捷键      | 说明                        |\n| ----------- | --------------------------- |\n| `qa`        | 开始录制名字为a的宏         |\n| `q`         | 结束录制宏                  |\n| `@a`        | 播放名字为a的宏             |\n| `100@a`     | 播放名字为a的宏100次        |\n| `:normal@a` | 播放名字为a的宏直到自动结束 |\n\n\n## 实用命令\n\n| 快捷键               | 说明                                             |\n| -------------------- | ------------------------------------------------ |\n| `/pattern`           | 从光标处向文件尾搜索 pattern                     |\n| `?pattern`           | 从光标处向文件头搜索 pattern                     |\n| `n`                  | 向同一方向执行上一次搜索                         |\n| `N`                  | 向相反方向执行上一次搜索                         |\n| `*`                  | 向前搜索光标下的单词                             |\n| `#`                  | 向后搜索光标下的单词                             |\n| `:s/p1/p2/g`         | 替换当前行的p1为p2                               |\n| `:%s/p1/p2/g`        | 替换当前文件中的p1为p2                           |\n| `:%s/<p1>/p2/g`      | 替换当前文件中的p1单词为p2                       |\n| `:%s/p1/p2/gc`       | 替换当前文件中的p1为p2，并且每处询问你是否替换   |\n| `:10,20s/p1/p2/g`    | 将第10到20行中所有p1替换为p2                     |\n| `:%s/1\\\\2\\/3/123/g`  | 将“1\\2/3” 替换为 “123”（特殊字符使用反斜杠标注） |\n| `:%s/\\r//g`          | 删除 DOS 换行符 ^M                               |\n| `:g/^\\s*$/d`         | 删除空行                                         |\n| `:g/test/d`          | 删除所有包含 test 的行                           |\n| `:v/test/d`          | 删除所有不包含 test 的行                         |\n| `:%s/^/test/`        | 在行首加入特定字符(也可以用宏录制来添加)         |\n| `:%s/$/test/`        | 在行尾加入特定字符(也可以用宏录制来添加)         |\n| `:sort`              | 排序                                             |\n| `:g/^\\(.\\+\\)$\\n\\1/d` | 去除重复行(先排序)                               |\n| `:%s/^.\\{10\\}//`     | 删除每行前10个字符                               |\n| `:%s/.\\{10\\}$//`     | 删除每行尾10个字符                               |\n\n\n## 帮助\n\n| 快捷键                 | 说明                         |\n| ---------------------- | ---------------------------- |\n| `h tutor`              | 入门文档                     |\n| `h quickref`           | 快速帮助                     |\n| `h index`              | 查询Vim所有键盘命令定义      |\n| `h summary`            | 帮助你更好的使用内置帮助系统 |\n| `h pattern.txt`        | 正则表达式帮助               |\n| `h eval`               | 脚本编写帮助                 |\n| `h function-list`      | 查看VimScript的函数列表      |\n| `h windows.txt`        | 窗口使用帮助                 |\n| `h tabpage.txt`        | 标签页使用帮助               |\n| `h tips`               | 查看Vim内置的常用技巧文档    |\n| `h quote`              | 寄存器                       |\n| `h autocommand-events` | 所有可能事件                 |\n| `h write-plugin`       | 编写插件                     |\n\n\n## 其他\n\n| 快捷键                | 说明                              |\n| --------------------- | --------------------------------- |\n| `vim -u NONE -N`      | 开启vim时不加载vimrc文件          |\n| `vimdiff file1 file2` | 显示文件差异                      |\n| `<leader>e`           | 快速编辑vimrc文件                 |\n| `<leader>s`           | 重新加载vimrc文件                 |\n| `<leader>h`           | 打开vimplus帮助文档               |\n| `<leader>H`           | 打开当前光标所在单词的vim帮助文档 |\n| `<leader><leader>i`   | 安装插件                          |\n| `<leader><leader>u`   | 更新插件                          |\n| `<leader><leader>c`   | 删除插件                          |\n\n","tags":["vim"],"categories":["Vim"]},{"title":"Vim 快捷键","url":"/2018-03-20/reference/Vim/vimHotKey/","content":"\n> [vimplus github](<https://github.com/chxuan/vimplus>)\n>\n> [Vim使用笔记](https://www.cnblogs.com/jiqingwu/archive/2012/06/14/vim_notes.html)\n>\n> [利用ctags+cscope+taglist+nerdree+srcexpl+trinity 将 VIM 变成 source insight](<https://www.robinjin.com/tech/?p=605>)\n\n## 1. 文档操作\n\n- `:e` -- 重新加载当前文档。\n- `:e!` -- 重新加载当前文档，并丢弃已做的改动。\n- `:e file` -- 关闭当前编辑的文件，并开启新的文件。 如果对当前文件的修改未保存，vi 会警告。\n- `:e! file` -- 放弃对当前文件的修改，编辑新的文件。\n- `:e# 或 ctrl+^` -- 回到刚才编辑的文件，很实用。\n- `gf` -- 打开以光标所在字符串为文件名的文件。\n- `:saveas newfilename` -- 另存为\n\n<!-- more -->\n\n## 2. 光标的移动\n\n- `gj` : 移动到一段内的下一行；\n- `gk` : 移动到一段内的上一行；\n- `w` : 前移一个单词，光标停在下一个单词开头；\n- `b` : 后移一个单词，光标停在上一个单词开头；\n- `(` : 前移1句。\n- `)` : 后移1句。\n- `{` : 前移1段。\n- `}` : 后移1段。\n- `fc` : 把光标移到同一行的下一个 c 字符处\n- `Fc` : 把光标移到同一行的上一个 c 字符处\n- `tc` : 把光标移到同一行的下一个 c 字符前\n- `Tc` : 把光标移到同一行的上一个 c 字符后\n- `;` : 配合 `f & t` 使用，重复一次\n- `,` : 配合 `f & t` 使用，反向重复一次\n\n上面的操作都可以配合 n 使用，比如在正常模式(下面会讲到)下输入3h， 则光标向左移动 3 个字符。\n\n- `0` : 移动到行首。\n- `g0` : 移到光标所在屏幕行行首。\n- `^` : 移动到本行第一个非空白字符。\n- `g^ `: 同 `^` ，但是移动到当前屏幕行第一个非空字符处。\n- `$` : 移动到行尾。\n- `g$` : 移动光标所在屏幕行行尾。\n- `n|` : 把光标移到递 n 列上。\n- `nG` : 到文件第 n 行。\n- `:n<cr>` : 移动到第 n 行。\n- `:$<cr>` : 移动到最后一行。\n- `H` : 把光标移到屏幕最顶端一行。\n- `M` : 把光标移到屏幕中间一行。\n- `L` : 把光标移到屏幕最底端一行。\n- `gg` : 到文件头部。\n- `G` : 到文件尾部。\n\n### 2.1 翻屏\n\n- `ctrl+f` : 下翻一屏。\n- `ctrl+b` : 上翻一屏。\n- `ctrl+d` : 下翻半屏。\n- `ctrl+u` : 上翻半屏。\n- `ctrl+e` : 向下滚动一行。\n- `ctrl+y` : 向上滚动一行。\n- `n%` : 到文件 `n%` 的位置。\n- `zz` : 将当前行移动到屏幕中央。\n- `zt` : 将当前行移动到屏幕顶端。\n- `zb` : 将当前行移动到屏幕底端。\n\n### 2.2 标记\n\n使用标记可以快速移动。到达标记后，可以用 `Ctrl+o` 返回原来的位置。 `Ctrl+o` 和 `Ctrl+i` 很像浏览器上的 *后退* 和 *前进* 。\n\n- `m{a-z}` : 标记光标所在位置，局部标记，只用于当前文件。\n- `m{A-Z}` : 标记光标所在位置，全局标记。标记之后，退出Vim， 重新启动，标记仍然有效。\n- ``{a-z}` : 移动到标记位置。\n- `'{a-z}` : 移动到标记行的行首。\n- ``{0-9}` ：回到上[2-10]次关闭vim时最后离开的位置。\n- \\`\\`: 移动到上次编辑的位置。'' 也可以，不过\\`\\`精确到列，而 '' 精确到行 。如果想跳转到更老的位置，可以按 C-o，跳转到更新的位置用 C-i。\n- `\" : 移动到上次离开的地方。\n- `. : 移动到最后改动的地方。\n- `:marks` -- 显示所有标记。\n- `:delmarks a b` -- 删除标记 a 和 b。\n- `:delmarks a-c` -- 删除标记 a、b 和 c。\n- `:delmarks a c-f` -- 删除标记 a、c、d、e、f。\n- `:delmarks!` -- 删除当前缓冲区的所有标记。\n- `:help mark-motions`  -- 查看更多关于 mark 的知识。\n\n## 3. 插入文本\n\n### 3.1 基本插入\n\n- `i` : 在光标前插入；一个小技巧：按 8，再按 `i`，进入插入模式，输入 `=`， 按 `esc` 进入命令模式，就会出现 8 个 `=` 。 这在插入分割线时非常有用，如`30i+<esc>` 就插入了 36 个 `+` 组成的分割线。\n- `:r filename` : 在当前位置插入另一个文件的内容。\n- `:r !date` :  在光标处插入当前日期与时间。同理，`:r !command` 可以将其它 shell 命令的输出插入当前文档。\n\n### 3.2 改写插入\n\n- `c[n]w` : 改写光标后 1(n) 个词。\n- `c[n]l` : 改写光标后 n 个字母。\n- `c[n]h` : 改写光标前 n 个字母。\n- `[n]cc` : 修改当前 [n] 行。\n- `[n]s` : 以输入的文本替代光标之后 1(n) 个字符，相当于 `c[n]l`。\n- `[n]S` : 删除指定数目的行，并以所输入文本代替之。\n\n注意，类似 `cnw,dnw,ynw` 的形式同样可以写为 `ncw,ndw,nyw`。\n\n## 4. 剪切复制和寄存器\n\n### 4.1 剪切和复制、粘贴\n\n- `[n]x` : 剪切光标右边 n 个字符，相当于 `d[n]l`。\n- `[n]X` : 剪切光标左边 n 个字符，相当于 `d[n]h`。\n- `y` : 复制在可视模式下选中的文本。\n- `yy or Y` : 复制整行文本。\n- `y[n]w` : 复制一 (n) 个词。\n- `y[n]l` : 复制光标右边 1(n) 个字符。\n- `y[n]h` : 复制光标左边 1(n) 个字符。\n- `y$` : 从光标当前位置复制到行尾。\n- `y0` : 从光标当前位置复制到行首。\n- `:m,ny<cr>` : 复制 m 行到 n 行的内容。\n- `y1G 或 ygg` : 复制光标以上的所有行。\n- `yG` : 复制光标以下的所有行。\n- `yaw 和 yas`：复制一个词和复制一个句子，即使光标不在词首和句首也没关系。\n- `d` : 删除（剪切）在可视模式下选中的文本。\n- `d$ or D` : 删除（剪切）当前位置到行尾的内容。\n- `d[n]w`: 删除（剪切）1(n)个单词\n- `d[n]l`: 删除（剪切）光标右边 1(n) 个字符。\n- `d[n]h`: 删除（剪切）光标左边 1(n) 个字符。\n- `d0`: 删除（剪切）当前位置到行首的内容\n- `[n] dd`: 删除（剪切）1(n) 行。\n- `:m,nd<cr>` : 剪切 m 行到 n 行的内容。\n- `d1G 或 dgg` : 剪切光标以上的所有行。\n- `dG` : 剪切光标以下的所有行。\n- `daw 和 das`：剪切一个词和剪切一个句子，即使光标不在词首和句首也没关系。\n- `d/f<cr>`：这是一个比较高级的组合命令，它将删除当前位置 到下一个 f 之间的内容。\n- `p`: 在光标之后粘贴。\n- `P` : 在光标之前粘贴。\n\n### 4.2 文本对象\n\n- `aw`：一个词\n- `as`：一句。\n- `ap`：一段。\n- `ab`：一块（包含在圆括号中的）。\n\n`y, d, c, v` 都可以跟文本对象。\n\n### 4.3 寄存器\n\n- `a-z`：都可以用作寄存器名。`\"ayy` 把当前行的内容放入 a 寄存器。\n- `A-Z`：用大写字母索引寄存器，可以在寄存器中追加内容。 如 `\"Ayy` 把当前行的内容追加到 a 寄存器中。\n- `:reg` : 显示所有寄存器的内容。\n- `\"\"`：不加寄存器索引时，默认使用的寄存器。\n- `\"*`：当前选择缓冲区，`\"*yy` 把当前行的内容放入当前选择缓冲区。\n- `\"+`：系统剪贴板。`\"+yy` 把当前行的内容放入系统剪贴板。\n\n## 5. 查找与替换\n\n### 5.1 查找\n\n- `/something` : 在后面的文本中查找 something。\n- `?something` : 在前面的文本中查找 something。\n- `/pattern/+number` : 将光标停在包含 pattern 的行后面第 number 行上。\n- `/pattern/-number` : 将光标停在包含 pattern 的行前面第 number 行上。\n- `n` : 向后查找下一个。\n- `N` : 向前查找下一个。\n\n可以用 grep 或 vimgrep 查找一个模式都在哪些地方出现过，其中 `:grep` 是调用外部的 grep 程序，而 `:vimgrep` 是 vim 自己的查找算法。\n\n用法为： `:vim[grep]/pattern/[g] [j] files`\n\n- `g` 的含义是如果一个模式在一行中多次出现，则这一行也在结果中多次出现。\n\n- `j` 的含义是 grep 结束后，结果停在第 j 项，默认是停在第一项。\n\nvimgrep 前面可以加数字限定搜索结果的上限，如 `:1vim/pattern/ %` 只查找那个模式在本文件中的第一个出现。\n\n其实 vimgrep 在读纯文本电子书时特别有用，可以生成导航的目录。\n\n比如电子书中每一节的标题形式为：`n. xxxx`。你就可以这样：`:vim/^d{1,}./ %` 然后用 `:cw` 或 `:copen` 查看结果，可以用 `C-w H` 把 quickfix 窗口移到左侧，就更像个目录了。\n\n### 5.2 替换\n\n- `:s/old/new` -- 用 new 替换当前行第一个 old。\n- `:s/old/new/g` -- 用 new 替换当前行所有的 old。\n- `:n1,n2s/old/new/g` -- 用 new 替换文件 n1 行到 n2 行所有的 old。\n- `:%s/old/new/g` -- 用 new 替换文件中所有的 old。\n- `:%s/^/xxx/g` -- 在每一行的行首插入 xxx，`^` 表示行首。\n- `:%s/$/xxx/g` -- 在每一行的行尾插入 xxx，`$` 表示行尾。\n- 所有替换命令末尾加上 c，每个替换都将需要用户确认。 如：`%s/old/new/gc`，加上i则忽略大小写(ignore)。\n\n还有一种比替换更灵活的方式，它是匹配到某个模式后执行某种命令，\n\n语法为 `:[range]g/pattern/command`\n\n例如 `: %g/^ xyz/normal dd`。\n\n表示对于以一个空格和 xyz 开头的行执行 normal 模式下的 dd 命令。\n\n关于 range 的规定为：\n\n- 如果不指定 range，则表示当前行。\n- `m,n` : 从 m 行到 n 行。\n- `0` : 最开始一行（可能是这样）。\n- `$` : 最后一行\n- `.` : 当前行\n- `%` : 所有行\n\n### 5.3 正则表达式\n\n高级的查找替换就要用到正则表达式。\n\n- `\\d` : 表示十进制数（我猜的）\n- `\\s` : 表示空格\n- `\\S` : 非空字符\n- `\\a` : 英文字母\n- `\\|` : 表示 或\n- `\\.` : 表示.\n- `{m,n}` : 表示 m 到 n 个字符。这要和 `\\s` 与 `\\a` 等连用，如 `\\a\\{m,n}` 表示 m 到 n 个英文字母。\n- `{m,}`: 表示 m 到无限多个字符。\n- `**`: 当前目录下的所有子目录。\n\n`:help pattern` 得到更多帮助。\n\n## 6. 编辑多个文件\n\n### 6.1 一次编辑多个文件\n\n我们可以一次打开多个文件，如\n\n```shell\n$ vi a.txt b.txt c.txt\n```\n\n- 使用 `:next(:n)` 编辑下一个文件。\n- `:2n` 编辑下 2 个文件。\n- 使用 `:previous或:N` 编辑上一个文件。\n- 使用 `:wnext`，保存当前文件，并编辑下一个文件。\n- 使用 `:wprevious`，保存当前文件，并编辑上一个文件。\n- 使用 `:args` 显示文件列表。\n- `:n filenames 或 :args filenames` 指定新的文件列表。\n- `vi -o filenames` 在水平分割的多个窗口中编辑多个文件。\n- `vi -O filenames` 在垂直分割的多个窗口中编辑多个文件。\n\n### 6.2 多标签编辑\n\n- `vim -p files` : 打开多个文件，每个文件占用一个标签页。\n- `:tabe, tabnew` -- 如果加文件名，就在新的标签中打开这个文件， 否则打开一个空缓冲区。\n- `^w gf` -- 在新的标签页里打开光标下路径指定的文件。\n- `:tabn` -- 切换到下一个标签。`Control + PageDown`，也可以。\n- `:tabp` -- 切换到上一个标签。`Control + PageUp`，也可以。\n- `[n] gt` -- 切换到下一个标签。如果前面加了 n ， 就切换到第 n 个标签。第一个标签的序号就是 1。\n- `:tab split` -- 将当前缓冲区的内容在新页签中打开。\n- `:tabc[lose]` -- 关闭当前的标签页。\n- `:tabo[nly]` -- 关闭其它的标签页。\n- `:tabs` -- 列出所有的标签页和它们包含的窗口。\n- `:tabm[ove] [N]` -- 移动标签页，移动到第N个标签页之后。 如 tabm 0 当前标签页，就会变成第一个标签页。\n\n### 6.3 缓冲区\n\n- `:buffers 或 :ls 或 :files` 显示缓冲区列表。\n- `ctrl+^`：在最近两个缓冲区间切换。\n- `:bn` -- 下一个缓冲区。\n- `:bp` -- 上一个缓冲区。\n- `:bl` -- 最后一个缓冲区。\n- `:b[n] 或 :[n]b` -- 切换到第 n 个缓冲区。\n- `:nbw(ipeout)` -- 彻底删除第 n 个缓冲区。\n- `:nbd(elete)` -- 删除第 n 个缓冲区，并未真正删除，还在 unlisted 列表中。\n- `:ba[ll]` -- 把所有的缓冲区在当前页中打开，每个缓冲区占一个窗口。\n\n## 7. 分屏编辑\n\n- `vim -o file1 file2` : 水平分割窗口，同时打开 file1 和 file2\n- `vim -O file1 file2` : 垂直分割窗口，同时打开 file1 和 file2\n\n### 7.1 水平分割\n\n- `:split(:sp)` -- 把当前窗水平分割成两个窗口。(`CTRL-W s` 或 `CTRL-W CTRL-S`) 注意如果在终端下，`CTRL-S` 可能会冻结终端，请按 `CTRL-Q` 继续。\n- `:split filename` -- 水平分割窗口，并在新窗口中显示另一个文件。\n- `:nsplit(:nsp)` -- 水平分割出一个 n 行高的窗口。\n- `:[N]new` -- 水平分割出一个N行高的窗口，并编辑一个新文件。 ( `CTRL-W n` 或  `CTRL-W CTRL-N`)\n- `ctrl+w f` --水平分割出一个窗口，并在新窗口打开名称为光标所在词的文件 。\n- `C-w C-^` -- 水平分割一个窗口，打开刚才编辑的文件。\n\n### 7.2 垂直分割\n\n- `:vsplit(:vsp)` -- 把当前窗口分割成水平分布的两个窗口。 (`CTRL-W v` 或 `CTRL CTRL-V`)\n- `:[N]vne[w]` -- 垂直分割出一个新窗口。\n- `:vertical 水平分割的命令`： 相应的垂直分割。\n\n### 7.3 关闭子窗口\n\n- `:qall` -- 关闭所有窗口，退出 vim。\n- `:wall` -- 保存所有修改过的窗口。\n- `:only` -- 只保留当前窗口，关闭其它窗口。(`CTRL-W o`)\n- `:close` -- 关闭当前窗口，`CTRL-W c`能实现同样的功能。 (象 `:q :x` 同样工作 )\n\n### 7.4 调整窗口大小\n\n- `ctrl+w +` --当前窗口增高一行。也可以用 n 增高 n 行。\n- `ctrl+w -` --当前窗口减小一行。也可以用 n 减小 n 行。\n- `ctrl+w _` --当前窗口扩展到尽可能的大。也可以用 n 设定行数。\n- `:resize n` -- 当前窗口 n 行高。\n- `ctrl+w =` -- 所有窗口同样高度。\n- `n ctrl+w _` -- 当前窗口的高度设定为 n 行。\n- `ctrl+w <` --当前窗口减少一列。也可以用 n 减少 n 列。\n- `ctrl+w >` --当前窗口增宽一列。也可以用 n 增宽 n 列。\n- `ctrl+w |` --当前窗口尽可能的宽。也可以用 n 设定列数。\n\n### 7.5 切换和移动窗口\n\n如果支持鼠标，切换和调整子窗口的大小就简单了。\n\n- `ctrl+w ctrl+w` : 切换到下一个窗口。或者是 `ctrl+w w`。\n- `ctrl+w p` : 切换到前一个窗口。\n- `ctrl+w h(l,j,k)` :切换到左（右，下，上）的窗口。\n- `ctrl+w t(b)` :切换到最上（下）面的窗口。\n- `ctrl+w H(L,K,J)` : 将当前窗口移动到最左（右、上、下）面。\n- `ctrl+w r`：旋转窗口的位置。\n- `ctrl+w T` : 将当前的窗口移动到新的标签页上。\n\n## 8. 快速编辑\n\n### 8.1 改变大小写\n\n- `~` : 反转光标所在字符的大小写。\n- 可视模式下的 U 或 u：把选中的文本变为大写或小写。\n- `gu(U)` 接范围（如`$`，或 `G`），可以把从光标当前位置到指定位置之间字母全部 转换成小写或大写。如`ggguG`，就是把开头到最后一行之间的字母全部变为小 写。再如 `gu5j`，把当前行和下面四行全部变成小写。\n\n### 8.2 替换（normal模式）\n\n- `r` : 替换光标处的字符，同样支持汉字。\n- `R` : 进入替换模式，按 `esc` 回到正常模式。\n\n### 8.3 撤消与重做（normal模式）\n\n- `[n] u` : 取消一(n)个改动。\n- `:undo 5` -- 撤销 5 个改变。\n- `:undolist` -- 你的撤销历史。\n- `ctrl + r` : 重做最后的改动。\n- `U` : 取消当前行中所有的改动。\n- `:earlier 4m` -- 回到 4 分钟前\n- `:later 55s` -- 前进 55 秒\n\n### 8.4 宏\n\n- `.` --重复上一个编辑动作\n- `qa`：开始录制宏 a（键盘操作记录）\n- `q`：停止录制\n- `@a`：播放宏 a\n\n## 9. 编辑特殊文件\n\n### 9.1 文件加解密\n\n- `vim -x file` : 开始编辑一个加密的文件。\n- `:X` -- 为当前文件设置密码。\n- `:set key=` -- 去除文件的密码。\n\n[这里是](http://www.cnblogs.com/jiqingwu/admin/vim-quick-edit.html) 滇狐总结的比较高级的 vi 技巧。\n\n### 9.2 文件的编码\n\n- `:e ++enc=utf8 filename`, 让 vim 用 utf-8 的编码打开这个文件。\n- `:w ++enc=gbk`，不管当前文件什么编码，把它转存成 gbk 编码。\n- `:set fenc 或 :set fileencoding`，查看当前文件的编码。\n- 在 vimrc 中添加 `set fileencoding=ucs-bom,utf-8,cp936`，vim 会根据要打开的文件选择合适的编码。 注意：编码之间不要留空格。 cp936 对应于 gbk 编码。 ucs-bom 对应于 windows 下的文件格式。\n\n让 vim 正确处理文件格式和文件编码，有赖于 [~/.vimrc的正确配置](http://www.cnblogs.com/jiqingwu/admin/vimrc.html)\n\n### 9.3 文件格式\n\n大致有三种文件格式：unix, dos, mac. 三种格式的区别主要在于回车键的编码：dos 下是回车加换行，unix 下只有 换行符，mac 下只有回车符。\n\n- `:e ++ff=dos filename`, 让 vim 用 dos 格式打开这个文件。\n- `:w ++ff=mac filename`, 以 mac 格式存储这个文件。\n- `:set ff`，显示当前文件的格式。\n- 在 vimrc 中添加 `set fileformats=unix,dos,mac`，让 vim 自动识别文件格式。\n\n## 10. 编程辅助\n\n### 10.1 一些按键\n\n- `gd` : 跳转到局部变量的定义处；\n- `gD` : 跳转到全局变量的定义处，从当前文件开头开始搜索；\n- `g;` : 上一个修改过的地方；\n- `g,` : 下一个修改过的地方；\n- `[[` : 跳转到上一个函数块开始，需要有单独一行的 {。\n- `]]` : 跳转到下一个函数块开始，需要有单独一行的 {。\n- `[]` : 跳转到上一个函数块结束，需要有单独一行的 }。\n- `][` : 跳转到下一个函数块结束，需要有单独一行的 }。\n- `[{` : 跳转到当前块开始处；\n- `]}` : 跳转到当前块结束处；\n- `[/` : 跳转到当前注释块开始处；\n- `]/` : 跳转到当前注释块结束处；\n- `%` : 不仅能移动到匹配的 `(),{} 或 []`上，而且能在 `#if，#else， #endif` 之间跳跃。\n\n下面的括号匹配对编程很实用的。\n\n- `ci', di', yi'`：修改、剪切或复制 `'` 之间的内容。\n- `ca', da', ya'`：修改、剪切或复制 `'` 之间的内容，包含 `'`。\n- `ci\", di\", yi\"`：修改、剪切或复制 `\"` 之间的内容。\n- `ca\", da\", ya\"`：修改、剪切或复制 `\"` 之间的内容，包含 `\"`。\n- `ci(, di(, yi(`：修改、剪切或复制 `()`之间的内容。\n- `ca(, da(, ya(`：修改、剪切或复制 `()` 之间的内容，包含 `()`。\n- `ci[, di[, yi[`：修改、剪切或复制 `[]` 之间的内容。\n- `ca[, da[, ya[`：修改、剪切或复制 `[]`之间的内容，包含 `[]`。\n- `ci{, di{, yi{`：修改、剪切或复制 `{}` 之间的内容。\n- `ca{, da{, ya{`：修改、剪切或复制 `{}` 之间的内容，包含 `{}`。\n- `ci<, di<, yi<`：修改、剪切或复制 `<>` 之间的内容。\n- `ca<, da<, ya<`：修改、剪切或复制 `<>` 之间的内容，包含`<>`。\n\n### 10.2 ctags\n\n| `Ctrl + ]`     | 找到光标所在位置的标签定义的地方                             |\n| :------------- | ------------------------------------------------------------ |\n| `Ctrl + t`     | 回到跳转之前的标签处                                         |\n| `Ctrl + o`     | 退回原来的地方                                               |\n| `[I`           | 查找全局标识符. Vim会列出它所找出的匹配行，<br />不仅在当前文件内查找，还会在所有的包含文件中查找 |\n| `[i`           | 从当前文件起始位置开始查找第一处包含光标所指关键字的位置     |\n| `]i`           | 类似上面的 `[i`，但这里是从光标当前位置开始往下搜索          |\n| `[{`           | 转到上一个位于第一列的”{“。（前提是 “{” 和 “}” 都在第一列。） |\n| `]}`           | 转到下一个位于第一列的”}”                                    |\n| `Ctrl+＼+ s`   | 会出现所有调用、定义该函数的地方，输入索引号，回车即可       |\n| `[ + ctrl + i` | 跳转到函数、变量和 `#define`   用 `ctrl+o` 返回              |\n| `[ + ctrl + d` | 跳转到 `#define` 处用 `ctrl+o` 返回                          |\n\n- `ctags -R` : 生成 tag 文件，`-R` 表示也为子目录中的文件生成 tags\n- `:set tags=path/tags` -- 告诉 ctags 使用哪个 tag 文件\n- `:tag xyz` -- 跳到 xyz 的定义处，或者将光标放在 xyz 上按 `C-]`，返回用 `C-t`\n- `:stag xyz` -- 用分割的窗口显示 xyz 的定义，或者 `C-w ]`， 如果用 `C-w n ]`，就会打开一个 n 行高的窗口\n- `:ptag xyz` -- 在预览窗口中打开 xyz 的定义，热键是 `C-w }`。\n- `:pclose` -- 关闭预览窗口。热键是 `C-w z`。\n- `:pedit abc.h` -- 在预览窗口中编辑 abc.h\n- `:psearch abc` -- 搜索当前文件和当前文件 include 的文件，显示包含 abc 的行。\n\n有时一个 tag 可能有多个匹配，如函数重载，一个函数名就会有多个匹配。 这种情况会先跳转到第一个匹配处。\n\n- `:[n]tnext` -- 下一 `[n]` 个匹配。\n- `:[n]tprev` -- 上一 `[n]`个匹配。\n- `:tfirst` -- 第一个匹配\n- `:tlast` -- 最后一个匹配\n- `:tselect tagname` -- 打开选择列表\n\ntab 键补齐\n\n- `:tag xyz<tab>` -- 补齐以 xyz 开头的 tag 名，继续按 tab 键，会显示其他的。\n- `:tag /xyz<tab>` -- 会用名字中含有 xyz 的 tag 名补全。\n\n**ctags 对 c++ 生成 tags** :\n\n```shell\nctags -R --c++-kinds=+p --fields=+iaS --extra=+q\n```\n\n每个参数解释如下：\n\n- `-R` : ctags 循环生成子目录的 tags\n\n- `--c++-kinds=+px` : ctags 记录 c++ 文件中的函数声明和各种外部和前向声明\n\n- `--fields=+iaS` : ctags 要求描述的信息\n  - 其中 `i` 表示如果有继承，则标识出父类；\n  - `a` 表示如果元素是类成员的话，要标明其调用权限（即是 public 还是 private）；\n  - `S` 表示如果是函数，则标识函数的 signature。\n\n- `--extra=+q` : 强制要求 ctags 做如下操作—如果某个语法元素是类的一个成员，ctags 默认会给其记录一行，可以要求 ctags 对同一个语法元斯屹记一行，这样可以保证在 VIM 中多个同名函数可以通过路径不同来区分。\n\n### 10.3 cscope\n\n查看阅读 c++ 代码\n\ncscope 缺省只解析 C 文件 (`.c` 和 `.h`)、lex 文件( `.l` )和 yacc 文件( `.y` )，虽然它也可以支持 C++ 以及 Java，但它在扫描目录时会跳过 C++ 及  Java 后缀的文件。如果希望 `cscope` 解析 C++ 或 Java 文件，需要把这些文件的名字和路径保存在一个名为 cscope.files 的文件。当 cscope 发现在当前目录中存在 cscope.files 时，就会为 cscope.files 中列出的所有文件生成索引数据库。\n\n下面的命令会查找当前目录及子目录中所有后缀名为 `\".h\", \".c\", \"cc\"` 和 `\".cpp\"` 的文件，并把查找结果重定向到文件 cscope.files 中。然后 cscope 根据 cscope.files 中的所有文件，生成符号索引文件。最后一条命令使用 ctags 命令，生成一个 tags 文件，在 vim 中执行 `\":help tags\"` 命令查询它的用法。它可以和 cscope 一起使用。\n\n```shell\n$ find . -name \"*.h\" -o -name \"*.c\" -o -name \"*.cc\" -o \"*.cpp\" > cscope.files\n$ cscope -bkq -i cscope.files\n$ ctags -R\n```\n\n- `cscope -Rbq` : 生成 cscope.out 文件\n- `:cs add /path/to/cscope.out /your/work/dir`\n- `:cs find c func` -- 查找 func 在哪些地方被调用\n  - s: 查找 C 语言符号，即查找函数名、宏、枚举值等出现的地方\n  - g: 查找函数、宏、枚举等定义的位置，类似 ctags 所提供的功能\n  - d: 查找本函数调用的函数\n  - c: 查找调用本函数的函数\n  - t: 查找指定的字符串\n  - e: 查找 egrep 模式，相当于 egrep 功能，但查找速度快多了\n  - f: 查找并打开文件，类似 vim 的 find 功能\n  - i: 查找包含本文件的文件\n- `:cw` -- 打开 quickfix 窗口查看结果\n\n### 10.4 gtags\n\nGtags 综合了 ctags 和 cscope 的功能。 使用 Gtags 之前，你需要安装 GNU Gtags。 然后在工程目录运行 gtags 。\n\n- `:Gtags funcname` 定位到 funcname 的定义处。\n- `:Gtags -r funcname` 查询 funcname被引用的地方。\n- `:Gtags -s symbol` 定位 symbol 出现的地方。\n- `:Gtags -g string` Goto string 出现的地方。 `:Gtags -gi string` 忽略大小写。\n- `:Gtags -f filename` 显示 filename 中的函数列表。 你可以用 `:Gtags -f %` 显示当前文件。\n- `:Gtags -P pattern` 显示路径中包含特定模式的文件。 如 `:Gtags -P .h$` 显示所有头文件， `:Gtags -P /vm/` 显示 vm 目录下的文件。\n\n### 10.5 编译\n\nvim 提供了 `:make` 来编译程序，默认调用的是 make， 如果你当前目录下有 makefile，简单地 `:make` 即可。\n\n如果你没有 make 程序，你可以通过配置 makeprg 选项来更改 make 调用的程序。 如果你只有一个 abc.java 文件，你可以这样设置：\n\n```\nset makeprg=javac\\ abc.java\n```\n\n然后 `:make` 即可。如果程序有错，可以通过 quickfix 窗口查看错误。 不过如果要正确定位错误，需要设置好errorformat，让 vim 识别错误信息。 如：\n\n```\n:setl efm=%A%f:%l:\\ %m,%-Z%p^,%-C%.%#\n```\n\n`%f` 表示文件名，`%l` 表示行号， `%m` 表示错误信息，其它的还不能理解。 请参考 `:help errorformat`。\n\n### 10.6 快速修改窗口\n\n其实是 quickfix 插件提供的功能， 对编译调试程序非常有用 \n\n- `:copen` -- 打开快速修改窗口。\n- `:cclose` -- 关闭快速修改窗口。\n\n快速修改窗口在 make 程序时非常有用，当 make 之后：\n\n- `:cl` -- 在快速修改窗口中列出错误。\n- `:cn` -- 定位到下一个错误。\n- `:cp` -- 定位到上一个错误。\n- `:cr` -- 定位到第一个错误。\n\n### 10.7 自动补全\n\n- `C-x C-s` -- 拼写建议。\n- `C-x C-v` -- 补全 vim 选项和命令。\n- `C-x C-l` -- 整行补全。\n- `C-x C-f` -- 自动补全文件路径。弹出菜单后，按 `C-f` 循环选择，当然也可以按 `C-n 和 C-p`。\n- `C-x C-p 和C-x C-n` -- 用文档中出现过的单词补全当前的词。 直接按 `C-p 和 C-n`也可以。\n- `C-x C-o` -- 编程时可以补全关键字和函数名啊。\n- `C-x C-i` -- 根据头文件内关键字补全。\n- `C-x C-d` -- 补全宏定义。\n- `C-x C-n` -- 按缓冲区中出现过的关键字补全。 直接按 `C-n 或 C-p` 即可。\n\n当弹出补全菜单后：\n\n- `C-p` 向前切换成员；\n- `C-n` 向后切换成员；\n- `C-e` 退出下拉菜单，并退回到原来录入的文字；\n- `C-y` 退出下拉菜单，并接受当前选项。\n\n### 10.8 多行缩进缩出\n\n- 正常模式下，按两下 `>;` 光标所在行会缩进。\n- 如果先按了 n，再按两下 `>;`，光标以下的 n 行会缩进。\n- 对应的，按两下 `<;`，光标所在行会缩出。\n- 如果在编辑代码文件，可以用 `=` 进行调整。\n- 在可视模式下，选择要调整的代码块，按 `=`，代码会按书写规则缩排好。\n- 或者 `n =`，调整 n 行代码的缩排。\n\n### 10.9 折叠\n\n- `zf` -- 创建折叠的命令，可以在一个可视区域上使用该命令；\n- `zd` -- 删除当前行的折叠；\n- `zD` -- 删除当前行的折叠；\n- `zfap` -- 折叠光标所在的段；\n- `zo` -- 打开折叠的文本；\n- `zc` -- 收起折叠；\n- `za` -- 打开/关闭当前折叠；\n- `zr` -- 打开嵌套的折行；\n- `zm` -- 收起嵌套的折行；\n- `zR (zO)` -- 打开所有折行；\n- `zM (zC)` -- 收起所有折行；\n- `zj` -- 跳到下一个折叠处；\n- `zk` -- 跳到上一个折叠处；\n- `zi -- enable/disable fold`;\n\n### 10.10 zshrc配置脚本\n\n```shell\n$ vi ~/.zshrc\nctags_fun()\n{\n    ctags -R\n    cscope -Rbqk\n    return 0\n}\nalias mctags=ctags_fun  # define mctags\n\nctags_cplus_fun()\n{\n    find . -name \"*.h\" -o -name \"*.c\" -o -name \"*.cc\" -o -name \"*.cpp\" > cscope.files\n    cscope -bkq -i cscope.files\n    ctags -R --c++-kinds=+p --fields=+iaS --extra=+q\n    return 0\n}\nalias cplusctags=ctags_cplus_fun  # define mctags\n```\n\n## 11. 其它\n\n### 11.1 工作目录\n\n- `:pwd` 显示vim的工作目录。\n- `:cd path` 改变 vim 的工作目录。\n- `:set autochdir`  可以让 vim 根据编辑的文件自动切换工作目录。\n\n### 11.2 一些快捷键（收集中）\n\n- `K` : 打开光标所在词的 manpage。\n- `*` : 向下搜索光标所在词。\n- `g*` : 同上，但部分符合即可。\n- `\\#` : 向上搜索光标所在词。\n- `g#` : 同上，但部分符合即可。\n- `g C-g` : 统计全文或统计部分的字数。\n\n### 11.3 在线帮助\n\n- `:h(elp) 或 F1` 打开总的帮助。\n- `:help user-manual` 打开用户手册。\n- 命令帮助的格式为`：`第一行指明怎么使用那个命令； 然后是缩进的一段解释这个命令的作用，然后是进一步的信息。\n- `:helptags somepath` 为 somepath 中的文档生成索引。\n- `:helpgrep` 可以搜索整个帮助文档，匹配的列表显示在 quickfix 窗口中。\n- `Ctrl+]` 跳转到 tag 主题，`Ctrl+t` 跳回。\n- `:ver` 显示版本信息。\n\n高亮所有搜索模式匹配\n\n- `shift + *` 向后搜索光标所在位置的单词\n\n- `shift + #` 向前搜索光标所在位置的单词\n\n-  n 和 N 可以继续向后或者向前搜索匹配的字符串\n- `:set hlsearch`  高亮所有匹配的字符串\n- `:nohlsearch` 临时关闭\n- `:set nohlsearch` 彻底关闭，只有重新 `:set hlsearch` 才可以高亮搜索\n\n- vim 高亮显示光标所在的单词，在单词的地方输入 `gd`\n\n语法高亮\n\n- syntax on\n\n- syntax off\n\nvim自动补全\n\n- `ctrl + n` 或者 `ctrl + p`\n\n复制 vim 文件中所有内容\n\n- `gg` 回到文件首\n\n- `shift + v` 进入 VISUAL LINE 模式\n\n- `shift + g`  全选所有内容\n\n- `ctrl + insert` 复制所选的内容\n\n## 12 ctags 安装\n\n> [Vim 配置 Catgs 用于前端开发](<https://yqc.im/vim-ctags-config-front-end.html>)\n>\n> [Ubuntu16.04安装配置和使用ctags](<https://www.cnblogs.com/zjutzz/p/9393397.html>)\n\n### 12.1 安装 universal-ctags\n\n`universal-ctags` 是一个现代化的`ctag`实现，本文只介绍使用Vim的安装方法\n\n```\nbrew install --HEAD universal-ctags/universal-ctags/universal-ctags\n```\n\n`universal-ctags` 是什么？A maintained ctags implementation,  https://ctags.io, 一个负责的 ctags 实现，在github上开源并且持续更新和维护。\n\n```shell\n$ sudo apt install autoconf\n$ cd /tmp\n$ git clone https://github.com/universal-ctags/ctags\n$ cd ctags\n$ ./autogen.sh\n$ ./configure --prefix=/opt/software/universal-ctags  # 我的安装路径。你按自己的情况调整。\n$ make -j8\n$ sudo make install\n```\n\n把 ctags 可执行文件更新到系统 PATH 上？No, 我选择创建链接的方式：\n\n```shell\n# 如果你装了 emacs-snapshot，那么现在的 ctags 命令实际上链接到了 /usr/bin/ctags-snapshot，要先删除链接文件：\n# sudo rm /usr/bin/ctags\n# 然后，把新编译安装的 universal-ctags 链接过来：\nsudo ln -s /opt/software/universal-ctags/bin/ctags /usr/bin/ctags\n```\n\n其他系统请参考项目主页：[ctags](https://github.com/universal-ctags/ctags)\n\n每次生成ctags文件都要手动run一次命令，这一点也不Vim，当然也有解决方法。\n\n### 12.2 安装vim-gutentags\n\n[vim-gutentags](https://github.com/ludovicchabant/vim-gutentags) 是一个用于自动生成 tag 文件的插件。使用 vim-plug 安装\n\n```shell\nPlug 'ludovicchabant/vim-gutentags'\n```\n\n### 12.3 配置\n\n如果只是介绍安装方法，那就必要写这篇文章了，安装完成之后还是要针对前端开发的特点手动调教一下。首先我们在任何目录打开文件，都会在目录下生成 ctags 文件，这样的话对项目代码有入侵性，并不推荐，建议把 tag 文件写在特定的目录里。可以做如下设置：\n\n```shell\nlet g:gutentags_cache_dir = '~/.cachetags'\n```\n\n这样生成的 tags 文件会统一放在 `~/.cachetags` 目录下。另外默认生成的文件名叫 tags，也可以根据个从喜好修改：\n\n```shell\nlet g:gutentags_ctags_tagfile = '.tags'\n```\n\n这样生成的文件是隐藏文件。另外一个很纠结的问题是有些文件我们并不想让他们生成 tags 文件，比如 `node_modules` 下文件，还有 `.git` 目录下的文件。这里有个取巧的方法是根据《[Vim配置使用FZF](https://yqc.im/vim-fzf-config.html)》中的方法，把 ctags 获取文件列表的命令改成 `ripgrep` 的搜索，这样就可以自动忽略 `.gitignore` 下的文件。如下：\n\n```shell\nlet g:gutentags_file_list_command = 'rg --files'\n```\n\n另外有的文件我们也不想让其生成 ctags 文件，比如 `*.md`、`*.svg` 文件，可以通过 `universal-ctags` 的全局配置来配置，这里要注意的是 `universal-ctags` 默认的全局配置文件已经不是 `~/.ctags` 和 `./.ctags`，而是在 `~/.ctags.d/*.ctags` 和 `./.ctags.d/*.ctags`。比如我的全局配置文件放在`~/.ctags.d/ignore.ctags`。简要配置如下 ：\n\n```shell\n--exclude=node_modules\n--exclude=gulp\n--exclude=.git\n--exclude=*.md\n--exclude=*.svg\n```\n\n### 12.4 结合FZF\n\n在文章《[Vim配置使用FZF](https://yqc.im/vim-fzf-config.html)》中的介绍，FZF 是支持 ctags 的，所以可以做个快捷键配置，如下：\n\n```\nmap <leader>t :Tags<CR>\n```\n\n这样就可以方便的使用 `ctrl-]` 和 `ctrl-o` 来进行 tag 跳转了。\n\n## 13 Gtags\n\n> [Vim 8 中 C/C++ 符号索引：GTags](<https://zhuanlan.zhihu.com/p/36279445>)\n>\n> [Ubuntu 安裝 GNU Global(gtags) 阅读Linux内核源码](<https://blog.csdn.net/gatieme/article/details/78819740>)\n>\n> \n\n## 14. VIM 插件\n\n> [vim 入坑指南（六）插件 UltiSnips](<https://vimzijun.net/2016/10/30/ultisnip/>)\n\n","tags":["vim"],"categories":["Vim"]},{"title":"网络传输基础","url":"/2018-03-16/reference/web-transmission-basis/","content":"\n示例：Web服务器向Http客户端传送数据的过程:\n\n在详细阐述网络传输过程之前，先来看一个最常见的例子，下图显示了一个网络服务器向客户端传送数据的完整过程：\n\n<!-- more -->\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90202/image002.jpg)\n\n\n1. 需要传送的数据是网络服务器的HTML页面。\n2. 应用协议HTTP报文首部添加到HTML数据之前。报文头信息包括：服务器所使用的HTTP版本（1.0/1.1），以及表明它包含发给网络客户端信息的状态编码（比如200表示Ok）。\n3. HTTP应用层协议将HTML格式的网页数据发送给传输层，传输层添加TCP首部信息。TCP传输层用于管理网络服务器和客户端之间的会话，TCP是面向连接的，通过拥赛控制、滑动窗口、超时重传等机制保证数据有效的传输给客户端。\n4. IP首部添加到TCP首部之前。IP层指定适当的源和目的IP地址。这些信息就构成了IP报文（IP Packet）。\n5. 以太网协议首部添加到IP报文的两端之后，就形成了数据链路帧(frame)。上述帧发送至通向网络客户端的路径上的最近一个路由器（默认网关）。每经过一个路由器会移除以太网信息，观察IP报文，判定最佳路径，将报文插入一个新的帧（下一个路由器的MAC地址），并发送至目标路径上下一个相邻路由器。每一个路由器在转发之前都移除并添加新的数据链路层信息。\n6. 数据通过互联网络传输，互联网络包含媒介和中间设备。\n7. 客户端接收到包含数据的数据链路帧，处理各层协议头，之后以与添加时相反的顺序移除协议头。首先处理并移除以太网信息，之后是IP协议信息，接下来TCP信息，最后是HTTP信息。\n8. 之后，将网页信息传递给客户端网页浏览器，浏览器把HTML网页渲染呈现给用户。\n\n### 数据封装:\n\n消息要在网络中传输，必须对它进行编码，以特定的格式进行封装，同时需要适当地封装以足够的控制和地址信息，以使它能够从发送方移动到接收方。\n\n**消息大小**\n\n理论上，视频或邮件信息是能够以大块非中断型流从网络源地址传送到目的地址，但这也意味着同一时刻同一网络其他设备就无法收发消息。这种大型数据流会造成显著延时。并且，如果传输过程中连接断开，整个数据流都会丢失需要全部重传。因此更好的方法是将数据流分割（segmentation）为较小的，便于管理的片段，能够带来两点好处：\n\n- 发送较小片段，网络上同时可有多个会话交错进行。这种在网络上将不同会话片段交错进行的过程称为多路传输（multiplexing）。\n- 分割可提高网络通讯的可靠性。各消息片段从源地址到目的地址无需经过相同路径，如果一条路径被堵塞或断开，其余消息可从替换路径到达目的地址。如果部分消息到不了目的地址，那只需重传丢失部分。\n\n通过对片段打上标签的方式来保证顺序以及在接收时重组。\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90202/image003.jpg)\n\n**协议数据单元（Protocol Data Unit, PDU）**\n\n应用层数据在传输过程中沿着协议栈向下传递，每一层协议都会向其中添加首部信息，TCP首部和IP首部都是20字节的长度。这就是封装的过程。\n数据片段在各层网络结构中采用的形式就称为协议数据单元（PDU）。封装过程中，下一层对从上一层收到的PDU进行封装。在处理的每一个阶段PDU都有不同的名字来反应它的功能。\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90203/image004.jpg)\n\nPDU按照TCP/IP协议的命名规范：\n\n- 数据（Data）：应用层PDU的常用术语\n- 分段（Segment）：传输层PDU\n- 数据报(Packet): 网络成PDU\n- 以太网帧（Frame）：链路层PDU\n- 比特（Bits）：在介质上物理传输数据所使用的PDU。\n\n**封装**\n\n封装是指在传输之前为数据添加额外的协议头信息的过程。在绝大多数数据通信过程中，源数据在传输前都会封装以数层协议。在网络上发送消息时，主机上的协议栈从上至下进行操作。\n\n以网络服务器为例，HTTP应用层协议发送HTML格式网页数据到传输层，应用层数据被分成TCP分段。各TCP分段被打上标签（主要是端口号，HTTP默认端口为80），称为首部（header），表明接收方哪一个进程应当接收此消息。同时也包含使得接收方能够按照原有的格式来重组数据的信息。\n\n传输层将网页HTML数据封装成分段并发送至网络层，执行IP层协议。整个TCP分段封装成IP报文，也就是再添上IP首部。IP首部包括源和目的IP地址，以及发送报文到目的地址所必须的信息，包括一些控制字段。\n\n之后，IP报文发送到链路层，封装以帧头和帧尾。每个帧头都包含源和目的物理地址。物理地址唯一指定了本地网络上的设备。帧尾包含差错校正信息。最后，由服务器网卡将比特编码传输给介质。  \n\n**解封装**\n\n接收主机以相反的方式（从下至上）进行操作称为解封装。解封装是接收设备移除一层或多层协议头的过程。数据在协议栈中向上移动直到终端应用层伴随着解封装。\n\n### 访问本地资源：\n\n访问本地网络资源需要两种类型的地址：网络层地址和数据链路层地址。网络层和数据链路层负责将数据从发送设备传输至接收设备。两层协议都有源和目的地址，但两种地址的目的不同。\n\n示例：客户端PC1与FTP在同一IP网络的通信\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90204/image005.jpg)\n\n**网络地址**\n\n网络层地址或IP地址包含两个部分：网络号和主机号。路由器使用网络前缀部分将报文转发给适当的网络。最后一个路由器使用主机部分将报文发送给目标设备。同一本地网络中，网络前缀部分是相同的，只有主机设备地址部分不同。\n\n源IP地址：发送设备，即客户端PC1的IP地址：192.168.1.110\n\n目的IP地址：接收设备，即FTP服务器：192.168.1.9\n\n**数据链路地址**\n\n数据链路地址（MAC）的目的是在**同一网络中**将数据链路帧从一个网络接口发送至另一个网络接口。以太网LAN和无线网LAN是两种不同物理介质的网络示例，分别有自己的数据链路协议。\n\n当IP报文的发送方和接收方位于同一网络，数据链路帧直接发送到接收设备（通过ARP来获取目的IP的MAC地址）。以太网上数据链路地址就是以太网MAC地址。MAC地址是物理植入网卡的48比特地址。\n源MAC地址：发送IP报文的PC1以太网卡MAC地址，AA-AA-AA-AA-AA-AA。\n\n目的MAC地址：当发送设备与接收设备位于同一网络，即为接收设备的数据链路地址。本例中，FTP MAC地址：CC-CC-CC-CC-CC-CC。\n\n源和目的MAC地址添加到以太网帧中。\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90205/image006.jpg)\n\n**MAC与IP地址**\n\n发送方必须知道接收方的物理和逻辑地址。发送方主机能够以多种方式学习到接收方的IP地址：比如浏览器缓存、getHostByName系统调用、域名系统（Domain Name System, DNS），或通过应用手动输入，如用户指定FTP地址。\n\n以太网MAC地址是怎么识别的呢？发送方主机使用地址解析协议（Address Resolution Protocol, ARP）以检测本地网络的所有MAC地址。如下图所示，发送主机在整个LAN发送ARP请求消息，这是一条广播消息。ARP请求包含目标设备的IP地址，LAN上的每一个设备都会检查该ARP请求，看看是否包含它自身的IP地址。只有符合该IP地址的设备才会发送ARP响应。ARP响应包含ARP请求中IP地址相对应的MAC地址。\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90206/670-270/image007.jpg)\n\n\n**访问远程资源：**\n\n**默认网关**\n\n当主机发送消息到远端网络，必须使用路由器，也称为默认网关。默认网关就是位于发送主机同一网络上的路由器的接口IP地址。有一点很重要：本地网络上的所有主机都能够配置自己的默认网关地址。如果该主机的TCP/IP设置中没有配置默认网关地址，或指定了错误的默认网关地址，则远端网络消息无法被送达。\n\n如下图所示，LAN上的主机PC 1使用IP地址为192.168.1.1的R1作为默认网关，如果PDU的目的地址位于另一个网络，则主机将PDU发送至路由器上的默认网关。\n\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90207/image008.jpg)\n\n**与远端网络设备通讯**\n\n下图显示了客户端主机PC 1与远端IP网络服务器进行通讯的网络层地址与数据链路层地址：\n\n![](https://community.emc.com/servlet/JiveServlet/downloadImage/2-831148-90208/image009.jpg)\n\n**网络地址**\n\n当报文的发送方与接收方位于不同网络，源和目的IP地址将会代表不同网络上的主机。\n\n源IP地址：发送设备即客户端主机PC 1的IP地址：192.168.1.110。\n\n目的IP地址：接收设备即网络服务器的IP地址：172.16.1.99。\n\n**数据链路地址**\n\n当报文的发送方与接收方位于不同网络，以太网数据链路帧无法直接被发送到目的主机。以太网帧必须先发送给路由器或默认网关。本例中，默认网关是R1，R1的接口IP地址与PC 1属于同一网络，因此PC 1能够直接达到路由器。\n\n源MAC地址：发送设备即PC 1的MAC地址，PC1的以太网接口MAC地址为：AA-AA-AA-AA-AA-AA。\n\n目的MAC地址：当报文的发送方与接收方位于不同网络，这一值为路由器或默认网关的以太网MAC地址。本例中，即R1的以太网接口MAC地址，即：11-11-11-11-11-11。\n\nIP报文封装成的以太网帧先被传输至R1，R1再转发给目的地址即网络服务器。R1可以转发给另一个路由器，如果目的服务器所在网路连接至R1，则直接发送给服务器。\n\n发送设备如何确定路由器的MAC地址？每一个设备通过自己的TCP/IP设置中的默认网关地址得知路由器的IP地址。之后，它通过ARP来得知默认网关的MAC地址，该MAC地址随后添加到帧中。\n\n\n","tags":["other"],"categories":["other"]},{"title":"计算机算法--图算法介绍","url":"/2018-03-08/reference/graph/","content":"\n## 图的定义：\n\n图（graph）由顶点（vertex）和边（edge）的集合组成，每一条边就是一个点对（v,w)。\n\n图的种类：地图，电路图，调度图，事物，网络，程序结构\n\n图的属性：有V个顶点的图最多有V*（V-1）/2条边\n\n<!-- more -->\n\n![](/images/imageGraph/graph1.jpg)\n\n![](/images/imageGraph/graph2.jpg)\n\n### 邻接矩阵：\n\n邻接矩阵是一个元素为bool值的V*V矩阵，若图中存在一条连接顶点V和W的边，折矩阵adj[v][w]=1,否则为0。占用的空间为V*V，当图是稠密时，邻接矩阵是比较合适的表达方法。\n\n![](/images/imageGraph/graph3.jpg)\n\n### 邻接表的表示\n\n对于非稠密的图，使用邻接矩阵有点浪费存储空间，可以使用邻接表，我们维护一个链表向量，给定一个顶点时，可以立即访问其链表,占用的空间为O(V+E)。\n\n![](/images/imageGraph/graph4.jpg)\n\n----------\n\n## 深度优先搜索\n\n### 深度优先搜索介绍\n\n图的深度优先搜索(Depth First Search)，和树的先序遍历比较类似。\n\n它的思想：假设初始状态是图中所有顶点均未被访问，则从某个顶点v出发，首先访问该顶点，然后依次从它的各个未被访问的邻接点出发深度优先搜索遍历图，直至图中所有和v有路径相通的顶点都被访问到。 若此时尚有其他顶点未被访问到，则另选一个未被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止。\n\n显然，深度优先搜索是一个递归的过程。\n\n### 深度优先搜索图解\n\n#### 无向图的深度优先搜索\n\n下面以\"无向图\"为例，来对深度优先搜索进行演示。\n\n![](/images/imageGraph/graph5.jpg)\n\n对上面的图G1进行深度优先遍历，从顶点A开始。\n\n![](/images/imageGraph/graph6.jpg)\n\n - 第1步：访问A。\n - 第2步：访问(A的邻接点)C。\n\n    在第1步访问A之后，接下来应该访问的是A的邻接点，即\"C,D,F\"中的一个。但在本文的实现中，顶点ABCDEFG是按照顺序存储，C在\"D和F\"的前面，因此，先访问C。 \n\n - 第3步：访问(C的邻接点)B。\n\n    在第2步访问C之后，接下来应该访问C的邻接点，即\"B和D\"中一个(A已经被访问过，就不算在内)。而由于B在D之前，先访问B。 \n\n - 第4步：访问(C的邻接点)D。\n\n    在第3步访问了C的邻接点B之后，B没有未被访问的邻接点；因此，返回到访问C的另一个邻接点D。 \n\n - 第5步：访问(A的邻接点)F。\n\n    前面已经访问了A，并且访问完了\"A的邻接点B的所有邻接点(包括递归的邻接点在内)\"；因此，此时返回到访问A的另一个邻接点F。 \n\n - 第6步：访问(F的邻接点)G。\n\n - 第7步：访问(G的邻接点)E。\n\n因此访问顺序是：A -> C -> B -> D -> F -> G -> E\n\n#### 有向图的深度优先搜索\n\n下面以\"有向图\"为例，来对深度优先搜索进行演示。\n\n![](/images/imageGraph/graph7.jpg)\n\n对上面的图G2进行深度优先遍历，从顶点A开始。\n\n![](/images/imageGraph/graph8.jpg)\n\n - 第1步：访问A。\n\n - 第2步：访问B。\n\n    在访问了A之后，接下来应该访问的是A的出边的另一个顶点，即顶点B。 \n\n - 第3步：访问C。\n\n    在访问了B之后，接下来应该访问的是B的出边的另一个顶点，即顶点C,E,F。在本文实现的图中，顶点ABCDEFG按照顺序存储，因此先访问C。 \n\n - 第4步：访问E。\n\n    接下来访问C的出边的另一个顶点，即顶点E。 \n\n - 第5步：访问D。\n\n    接下来访问E的出边的另一个顶点，即顶点B,D。顶点B已经被访问过，因此访问顶点D。 \n\n - 第6步：访问F。\n\n    接下应该回溯\"访问A的出边的另一个顶点F\"。 \n\n - 第7步：访问G。\n\n因此访问顺序是：A -> B -> C -> E -> D -> F -> G\n\n----------\n\n## 广度优先搜索\n\n### 广度优先搜索介绍\n\n广度优先搜索算法(Breadth First Search)，又称为\"宽度优先搜索\"或\"横向优先搜索\"，简称BFS。\n\n它的思想是：从图中某顶点v出发，在访问了v之后依次访问v的各个未曾访问过的邻接点，然后分别从这些邻接点出发依次访问它们的邻接点，并使得“先被访问的顶点的邻接点先于后被访问的顶点的邻接点被访问，直至图中所有已被访问的顶点的邻接点都被访问到。如果此时图中尚有顶点未被访问，则需要另选一个未曾被访问过的顶点作为新的起始点，重复上述过程，直至图中所有顶点都被访问到为止。\n\n换句话说，广度优先搜索遍历图的过程是以v为起点，由近至远，依次访问和v有路径相通且路径长度为1,2...的顶点。\n\n\n### 广度优先搜索图解\n\n#### 无向图的广度优先搜索\n\n下面以\"无向图\"为例，来对广度优先搜索进行演示。还是以上面的图G1为例进行说明。\n\n![](/images/imageGraph/graph9.jpg)\n\n - 第1步：访问A。\n - 第2步：依次访问C,D,F。\n\n    在访问了A之后，接下来访问A的邻接点。前面已经说过，在本文实现中，顶点ABCDEFG按照顺序存储的，C在\"D和F\"的前面，因此，先访问C。再访问完C之后，再依次访问D,F。 \n\n - 第3步：依次访问B,G。\n\n    在第2步访问完C,D,F之后，再依次访问它们的邻接点。首先访问C的邻接点B，再访问F的邻接点G。 \n\n - 第4步：访问E。\n\n    在第3步访问完B,G之后，再依次访问它们的邻接点。只有G有邻接点E，因此访问G的邻接点E。\n\n因此访问顺序是：A -> C -> D -> F -> B -> G -> E\n\n#### 有向图的广度优先搜索\n\n下面以\"有向图\"为例，来对广度优先搜索进行演示。还是以上面的图G2为例进行说明。\n\n![](/images/imageGraph/graph10.jpg)\n\n - 第1步：访问A。\n\n - 第2步：访问B。\n\n - 第3步：依次访问C,E,F。\n\n   在访问了B之后，接下来访问B的出边的另一个顶点，即C,E,F。前面已经说过，在本文实现中，顶点ABCDEFG按照顺序存储的，因此会先访问C，再依次访问E,F。 \n\n - 第4步：依次访问D,G。\n\n    在访问完C,E,F之后，再依次访问它们的出边的另一个顶点。还是按照C,E,F的顺序访问，C的已经全部访问过了，那么就只剩下E,F；先访问E的邻接点D，再访问F的邻接点G。\n\n因此访问顺序是：A -> B -> C -> E -> F -> D -> G\n\n## 搜索算法的源码\n\n<details><summary>1. 邻接矩阵表示的\"无向图</summary>\n\n\n```c++\n/**\n* C++: 邻接矩阵表示的\"无向图(Matrix Undirected Graph)\"\n*\n* @author LippiOuYang\n* @date 2013/04/19\n*/\n\n#include <iomanip>\n#include <iostream>\n#include <vector>\nusing namespace std;\n\n#define MAX 100\nclass MatrixUDG {\nprivate:\n    char mVexs[MAX];    // 顶点集合\n    int mVexNum;             // 顶点数\n    int mEdgNum;             // 边数\n    int mMatrix[MAX][MAX];   // 邻接矩阵\n\npublic:\n    // 创建图(自己输入数据)\n    MatrixUDG();\n    // 创建图(用已提供的矩阵)\n    MatrixUDG(char vexs[], int vlen, char edges[][2], int elen);\n    ~MatrixUDG();\n\n    // 深度优先搜索遍历图\n    void DFS();\n    // 广度优先搜索（类似于树的层次遍历）\n    void BFS();\n    // 打印矩阵队列图\n    void print();\n\nprivate:\n    // 读取一个输入字符\n    char readChar();\n    // 返回ch在mMatrix矩阵中的位置\n    int getPosition(char ch);\n    // 返回顶点v的第一个邻接顶点的索引，失败则返回-1\n    int firstVertex(int v);\n    // 返回顶点v相对于w的下一个邻接顶点的索引，失败则返回-1\n    int nextVertex(int v, int w);\n    // 深度优先搜索遍历图的递归实现\n    void DFS(int i, int *visited);\n};\n\n/* \n * 创建图(自己输入数据)\n */\nMatrixUDG::MatrixUDG()\n{\n    char c1, c2;\n    int i, p1, p2;\n\n    // 输入\"顶点数\"和\"边数\"\n    cout << \"input vertex number: \";\n    cin >> mVexNum;\n    cout << \"input edge number: \";\n    cin >> mEdgNum;\n    if ( mVexNum < 1 || mEdgNum < 1 || (mEdgNum > (mVexNum * (mVexNum-1))))\n    {\n        cout << \"input error: invalid parameters!\" << endl;\n        return ;\n    }\n\n    // 初始化\"顶点\"\n    for (i = 0; i < mVexNum; i++)\n    {\n        cout << \"vertex(\" << i << \"): \";\n        mVexs[i] = readChar();\n    }\n\n    // 初始化\"边\"\n    for (i = 0; i < mEdgNum; i++)\n    {\n        // 读取边的起始顶点和结束顶点\n        cout << \"edge(\" << i << \"): \";\n        c1 = readChar();\n        c2 = readChar();\n\n        p1 = getPosition(c1);\n        p2 = getPosition(c2);\n        if (p1==-1 || p2==-1)\n        {\n            cout << \"input error: invalid edge!\" << endl;\n            return ;\n        }\n\n        mMatrix[p1][p2] = 1;\n        mMatrix[p2][p1] = 1;\n    }\n}\n\n/*\n * 创建图(用已提供的矩阵)\n *\n * 参数说明：\n *     vexs  -- 顶点数组\n *     vlen  -- 顶点数组的长度\n *     edges -- 边数组\n *     elen  -- 边数组的长度\n */\nMatrixUDG::MatrixUDG(char vexs[], int vlen, char edges[][2], int elen)\n{\n    int i, p1, p2;\n\n    // 初始化\"顶点数\"和\"边数\"\n    mVexNum = vlen;\n    mEdgNum = elen;\n    // 初始化\"顶点\"\n    for (i = 0; i < mVexNum; i++)\n        mVexs[i] = vexs[i];\n\n    // 初始化\"边\"\n    for (i = 0; i < mEdgNum; i++)\n    {\n        // 读取边的起始顶点和结束顶点\n        p1 = getPosition(edges[i][0]);\n        p2 = getPosition(edges[i][1]);\n\n        mMatrix[p1][p2] = 1;\n        mMatrix[p2][p1] = 1;\n    }\n}\n\n/* \n * 析构函数\n */\nMatrixUDG::~MatrixUDG() \n{\n}\n\n/*\n * 返回ch在mMatrix矩阵中的位置\n */\nint MatrixUDG::getPosition(char ch)\n{\n    int i;\n    for(i=0; i<mVexNum; i++)\n        if(mVexs[i]==ch)\n            return i;\n    return -1;\n}\n\n/*\n * 读取一个输入字符\n */\nchar MatrixUDG::readChar()\n{\n    char ch;\n    do {\n        cin >> ch;\n    } while(!((ch>='a'&&ch<='z') || (ch>='A'&&ch<='Z')));\n\n    return ch;\n}\n\n/*\n* 返回顶点v的第一个邻接顶点的索引，失败则返回-1\n*/\nint MatrixUDG::firstVertex(int v)\n{\n    int i;\n    if (v<0 || v>(mVexNum-1))\n        return -1;\n\n    for (i = 0; i < mVexNum; i++)\n        if (mMatrix[v][i] == 1)\n            return i;\n    return -1;\n}\n\n/*\n* 返回顶点v相对于w的下一个邻接顶点的索引，失败则返回-1\n*/\nint MatrixUDG::nextVertex(int v, int w)\n{\n    int i;\n    if (v<0 || v>(mVexNum-1) || w<0 || w>(mVexNum-1))\n        return -1;\n\n    for (i = w + 1; i < mVexNum; i++)\n        if (mMatrix[v][i] == 1)\n            return i;\n\n    return -1;\n}\n\n/*\n * 深度优先搜索遍历图的递归实现\n */\nvoid MatrixUDG::DFS(int i, int *visited)\n{\n    int w;\n    visited[i] = 1;\n    cout << mVexs[i] << \" \";\n    // 遍历该顶点的所有邻接顶点。若是没有访问过，那么继续往下走\n    for (w = firstVertex(i); w >= 0; w = nextVertex(i, w)) {\n        if (!visited[w])\n            DFS(w, visited);\n    }\n}\n\n/*\n * 深度优先搜索遍历图\n */\nvoid MatrixUDG::DFS()\n{\n    int i;\n    int visited[MAX];       // 顶点访问标记\n\n    // 初始化所有顶点都没有被访问\n    for (i = 0; i < mVexNum; i++)\n        visited[i] = 0;\n\n    cout << \"DFS: \";\n    for (i = 0; i < mVexNum; i++) {\n        //printf(\"\\n== LOOP(%d)\\n\", i);\n        if (!visited[i])\n            DFS(i, visited);\n    }\n    cout << endl;\n}\n\n/*\n * 广度优先搜索（类似于树的层次遍历）\n */\nvoid MatrixUDG::BFS()\n{\n    int head = 0;\n    int rear = 0;\n    int queue[MAX];     // 辅组队列\n    int visited[MAX];   // 顶点访问标记\n    int i, j, k;\n\n    for (i = 0; i < mVexNum; i++)\n        visited[i] = 0;\n\n    cout << \"BFS: \";\n    for (i = 0; i < mVexNum; i++) {\n        if (!visited[i]) {\n            visited[i] = 1;\n            cout << mVexs[i] << \" \";\n            queue[rear++] = i;  // 入队列\n        }\n        while (head != rear)  {\n            j = queue[head++];  // 出队列\n            for (k = firstVertex(j); k >= 0; k = nextVertex(j, k)) { //k是为访问的邻接顶点\n                if (!visited[k]) {\n                    visited[k] = 1;\n                    cout << mVexs[k] << \" \";\n                    queue[rear++] = k;\n                }\n            }\n        }\n    }\n    cout << endl;\n}\n\n/*\n * 打印矩阵队列图\n */\nvoid MatrixUDG::print()\n{\n    int i,j;\n    cout << \"Martix Graph:\" << endl;\n    for (i = 0; i < mVexNum; i++) {\n        for (j = 0; j < mVexNum; j++)\n            cout << mMatrix[i][j] << \" \";\n        cout << endl;\n    }\n} \n\nint main()\n{\n    char vexs[] = {'A', 'B', 'C', 'D', 'E', 'F', 'G'};\n    char edges[][2] = {\n        {'A', 'C'}, \n        {'A', 'D'}, \n        {'A', 'F'}, \n        {'B', 'C'}, \n        {'C', 'D'}, \n        {'E', 'G'}, \n        {'F', 'G'}};\n    int vlen = sizeof(vexs)/sizeof(vexs[0]);\n    int elen = sizeof(edges)/sizeof(edges[0]);\n    MatrixUDG* pG;\n\n    // 自定义\"图\"(输入矩阵队列)\n    // pG = new MatrixUDG();\n    // 采用已有的\"图\"\n    pG = new MatrixUDG(vexs, vlen, edges, elen);\n    pG->print();   // 打印图\n    pG->DFS();     // 深度优先遍历\n    pG->BFS();     // 广度优先遍历\n    \n    return 0;\n}\n```\n\n </details>\n\n<details><summary>2. 邻接表表示的\"无向图</summary>\n\n```c++\n/**\n * C++: 邻接表表示的\"无向图(List Undirected Graph)\"\n *\n * @author LippiOuYang\n * @date 2013/04/19\n */\n\n#include <iomanip>\n#include <iostream>\n#include <vector>\nusing namespace std;\n\n#define MAX 100\n// 邻接表\nclass ListUDG\n{\nprivate: // 内部类\n    // 邻接表中表对应的链表的顶点\n    class ENode\n    {\n        public:\n        int ivex;           // 该边所指向的顶点的位置\n        ENode *nextEdge;    // 指向下一条弧的指针\n    };\n\n    // 邻接表中表的顶点\n    class VNode\n    {\n        public:\n        char data;          // 顶点信息\n        ENode *firstEdge;   // 指向第一条依附该顶点的弧\n    };\n\nprivate: // 私有成员\n    int mVexNum;             // 图的顶点的数目\n    int mEdgNum;             // 图的边的数目\n    VNode mVexs[MAX];\n\npublic:\n    // 创建邻接表对应的图(自己输入)\n    ListUDG();\n    // 创建邻接表对应的图(用已提供的数据)\n    ListUDG(char vexs[], int vlen, char edges[][2], int elen);\n    ~ListUDG();\n\n    // 深度优先搜索遍历图\n    void DFS();\n    // 广度优先搜索（类似于树的层次遍历）\n    void BFS();\n    // 打印邻接表图\n    void print();\n\nprivate:\n    // 读取一个输入字符\n    char readChar();\n    // 返回ch的位置\n    int getPosition(char ch);\n    // 深度优先搜索遍历图的递归实现\n    void DFS(int i, int *visited);\n    // 将node节点链接到list的最后\n    void linkLast(ENode *list, ENode *node);\n};\n\n/*\n * 创建邻接表对应的图(自己输入)\n */\nListUDG::ListUDG()\n{\n    char c1, c2;\n    int v, e;\n    int i, p1, p2;\n    ENode *node1, *node2;\n\n    // 输入\"顶点数\"和\"边数\"\n    cout << \"input vertex number: \";\n    cin >> mVexNum;\n    cout << \"input edge number: \";\n    cin >> mEdgNum;\n    if ( mVexNum < 1 || mEdgNum < 1 || (mEdgNum > (mVexNum * (mVexNum-1))))\n    {\n        cout << \"input error: invalid parameters!\" << endl;\n        return ;\n    }\n\n    // 初始化\"邻接表\"的顶点\n    for(i=0; i<mVexNum; i++)\n    {\n        cout << \"vertex(\" << i << \"): \";\n        mVexs[i].data = readChar();\n        mVexs[i].firstEdge = NULL;\n    }\n\n    // 初始化\"邻接表\"的边\n    for(i=0; i<mEdgNum; i++)\n    {\n        // 读取边的起始顶点和结束顶点\n        cout << \"edge(\" << i << \"): \";\n        c1 = readChar();\n        c2 = readChar();\n\n        p1 = getPosition(c1);\n        p2 = getPosition(c2);\n        // 初始化node1\n        node1 = new ENode();\n        node1->ivex = p2;\n        // 将node1链接到\"p1所在链表的末尾\"\n        if(mVexs[p1].firstEdge == NULL)\n            mVexs[p1].firstEdge = node1;\n        else\n            linkLast(mVexs[p1].firstEdge, node1);\n        // 初始化node2\n        node2 = new ENode();\n        node2->ivex = p1;\n        // 将node2链接到\"p2所在链表的末尾\"\n        if(mVexs[p2].firstEdge == NULL)\n            mVexs[p2].firstEdge = node2;\n        else\n            linkLast(mVexs[p2].firstEdge, node2);\n    }\n}\n\n/*\n * 创建邻接表对应的图(用已提供的数据)\n */\nListUDG::ListUDG(char vexs[], int vlen, char edges[][2], int elen)\n{\n    char c1, c2;\n    int i, p1, p2;\n    ENode *node1, *node2;\n\n    // 初始化\"顶点数\"和\"边数\"\n    mVexNum = vlen;\n    mEdgNum = elen;\n    // 初始化\"邻接表\"的顶点\n    for(i=0; i<mVexNum; i++)\n    {\n        mVexs[i].data = vexs[i];\n        mVexs[i].firstEdge = NULL;\n    }\n\n    // 初始化\"邻接表\"的边\n    for(i=0; i<mEdgNum; i++)\n    {\n        // 读取边的起始顶点和结束顶点\n        c1 = edges[i][0];\n        c2 = edges[i][1];\n\n        p1 = getPosition(c1);\n        p2 = getPosition(c2);\n        // 初始化node1\n        node1 = new ENode();\n        node1->ivex = p2;\n        // 将node1链接到\"p1所在链表的末尾\"\n        if(mVexs[p1].firstEdge == NULL)\n            mVexs[p1].firstEdge = node1;\n        else\n            linkLast(mVexs[p1].firstEdge, node1);\n        // 初始化node2\n        node2 = new ENode();\n        node2->ivex = p1;\n        // 将node2链接到\"p2所在链表的末尾\"\n        if(mVexs[p2].firstEdge == NULL)\n            mVexs[p2].firstEdge = node2;\n        else\n            linkLast(mVexs[p2].firstEdge, node2);\n    }\n}\n\n/* \n * 析构函数\n */\nListUDG::~ListUDG() \n{\n}\n\n/*\n * 将node节点链接到list的最后\n */\nvoid ListUDG::linkLast(ENode *list, ENode *node)\n{\n    ENode *p = list;\n\n    while(p->nextEdge)\n        p = p->nextEdge;\n    p->nextEdge = node;\n}\n\n/*\n * 返回ch的位置\n */\nint ListUDG::getPosition(char ch)\n{\n    int i;\n    for(i=0; i<mVexNum; i++)\n        if(mVexs[i].data==ch)\n            return i;\n    return -1;\n}\n\n/*\n * 读取一个输入字符\n */\nchar ListUDG::readChar()\n{\n    char ch;\n    do {\n        cin >> ch;\n    } while(!((ch>='a'&&ch<='z') || (ch>='A'&&ch<='Z')));\n\n    return ch;\n}\n\n/*\n * 深度优先搜索遍历图的递归实现\n */\nvoid ListUDG::DFS(int i, int *visited)\n{\n    ENode *node;\n\n    visited[i] = 1;\n    cout << mVexs[i].data << \" \";\n    node = mVexs[i].firstEdge;\n    while (node != NULL)\n    {\n        if (!visited[node->ivex])\n            DFS(node->ivex, visited);\n        node = node->nextEdge;\n    }\n}\n\n/*\n * 深度优先搜索遍历图\n */\nvoid ListUDG::DFS()\n{\n    int i;\n    int visited[MAX];       // 顶点访问标记\n\n    // 初始化所有顶点都没有被访问\n    for (i = 0; i < mVexNum; i++)\n        visited[i] = 0;\n\n    cout << \"DFS: \";\n    for (i = 0; i < mVexNum; i++)\n    {\n        if (!visited[i])\n            DFS(i, visited);\n    }\n    cout << endl;\n}\n   \n/*\n * 广度优先搜索（类似于树的层次遍历）\n */\nvoid ListUDG::BFS()\n{\n    int head = 0;\n    int rear = 0;\n    int queue[MAX];     // 辅组队列\n    int visited[MAX];   // 顶点访问标记\n    int i, j, k;\n    ENode *node;\n\n    for (i = 0; i < mVexNum; i++)\n        visited[i] = 0;\n\n    cout << \"BFS: \";\n    for (i = 0; i < mVexNum; i++)\n    {\n        if (!visited[i])\n        {\n            visited[i] = 1;\n            cout << mVexs[i].data << \" \";\n            queue[rear++] = i;  // 入队列\n        }\n        while (head != rear) \n        {\n            j = queue[head++];  // 出队列\n            node = mVexs[j].firstEdge;\n            while (node != NULL)\n            {\n                k = node->ivex;\n                if (!visited[k])\n                {\n                    visited[k] = 1;\n                    cout << mVexs[k].data << \" \";\n                    queue[rear++] = k;\n                }\n                node = node->nextEdge;\n            }\n        }\n    }\n    cout << endl;\n}\n\n/*\n * 打印邻接表图\n */\nvoid ListUDG::print()\n{\n    int i,j;\n    ENode *node;\n\n    cout << \"List Graph:\" << endl;\n    for (i = 0; i < mVexNum; i++)\n    {\n        cout << i << \"(\" << mVexs[i].data << \"): \";\n        node = mVexs[i].firstEdge;\n        while (node != NULL)\n        {\n            cout << node->ivex << \"(\" << mVexs[node->ivex].data << \") \";\n            node = node->nextEdge;\n        }\n        cout << endl;\n    }\n}\n\nint main()\n{\n    char vexs[] = {'A', 'B', 'C', 'D', 'E', 'F', 'G'};\n    char edges[][2] = {\n        {'A', 'C'}, \n        {'A', 'D'}, \n        {'A', 'F'}, \n        {'B', 'C'}, \n        {'C', 'D'}, \n        {'E', 'G'}, \n        {'F', 'G'}};\n    int vlen = sizeof(vexs)/sizeof(vexs[0]);\n    int elen = sizeof(edges)/sizeof(edges[0]);\n    ListUDG* pG;\n\n    // 自定义\"图\"(输入矩阵队列)\n    //pG = new ListUDG();\n    // 采用已有的\"图\"\n    pG = new ListUDG(vexs, vlen, edges, elen);\n\n    pG->print();   // 打印图\n    pG->DFS();     // 深度优先遍历\n    pG->BFS();     // 广度优先遍历\n\n    return 0;\n}\n```\n\n</details>\n\n----------\n\n## 迪杰斯特拉算法\n\n迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个节点到其他节点的最短路径。 \n它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。\n\n### 基本思想\n\n通过Dijkstra计算图G中的最短路径时，需要指定起点s(即从顶点s开始计算)。\n\n此外，引进两个集合S和U。S的作用是记录已求出最短路径的顶点(以及相应的最短路径长度)，而U则是记录还未求出最短路径的顶点(以及该顶点到起点s的距离)。\n\n初始时，S中只有起点s；U中是除s之外的顶点，并且U中顶点的路径是\"起点s到该顶点的路径\"。然后，从U中找出路径最短的顶点，并将其加入到S中；接着，更新U中的顶点和顶点对应的路径。 然后，再从U中找出路径最短的顶点，并将其加入到S中；接着，更新U中的顶点和顶点对应的路径。 ... 重复该操作，直到遍历完所有顶点。\n\n### 操作步骤\n\n\n - (1)\n   \n   初始时，S只包含起点s；U包含除s外的其他顶点，且U中顶点的距离为\"起点s到该顶点的距离\"[例如，U中顶点v的距离为(s,v)的长度，然后s和v不相邻，则v的距离为∞]。\n   \n - (2) 从U中选出\"距离最短的顶点k\"，并将顶点k加入到S中；同时，从U中移除顶点k。\n\n - (3)\n   \n   更新U中各个顶点到起点s的距离。之所以更新U中顶点的距离，是由于上一步中确定了k是求出最短路径的顶点，从而可以利用k来更新其它顶点的距离；例如，(s,v)的距离可能大于(s,k)+(k,v)的距离。\n   \n - (4) 重复步骤(2)和(3)，直到遍历完所有顶点。\n\n单纯的看上面的理论可能比较难以理解，下面通过实例来对该算法进行说明。\n\n5.3迪杰斯特拉算法图解\n\n![](/images/imageGraph/graph11.jpg)\n\n以上图G4为例，来对迪杰斯特拉进行算法演示(以第4个顶点D为起点)。\n\n![](/images/imageGraph/graph12.jpg)\n\n - 初始状态：S是已计算出最短路径的顶点集合，U是未计算除最短路径的顶点的集合！\n\n - 第1步：将顶点D加入到S中。\n\n    此时，S={D(0)}, U={A(∞),B(∞),C(3),E(4),F(∞),G(∞)}。     注:C(3)表示C到起点D的距离是3。\n\n - 第2步：将顶点C加入到S中。\n\n    上一步操作之后，U中顶点C到起点D的距离最短；因此，将C加入到S中，同时更新U中顶点的距离。以顶点F为例，之前F到D的距离为∞；但是将C加入到S之后，F到D的距离为9=(F,C)+(C,D)。 \n    此时，S={D(0),C(3)}, U={A(∞),B(23),E(4),F(9),G(∞)}。\n\n - 第3步：将顶点E加入到S中。\n\n    上一步操作之后，U中顶点E到起点D的距离最短；因此，将E加入到S中，同时更新U中顶点的距离。还是以顶点F为例，之前F到D的距离为9；但是将E加入到S之后，F到D的距离为6=(F,E)+(E,D)。 \n\n    此时，S={D(0),C(3),E(4)}, U={A(∞),B(23),F(6),G(12)}。\n\n - 第4步：将顶点F加入到S中。\n\n    此时，S={D(0),C(3),E(4),F(6)}, U={A(22),B(13),G(12)}。\n\n - 第5步：将顶点G加入到S中。\n\n    此时，S={D(0),C(3),E(4),F(6),G(12)}, U={A(22),B(13)}。\n\n - 第6步：将顶点B加入到S中。\n\n    此时，S={D(0),C(3),E(4),F(6),G(12),B(13)}, U={A(22)}。\n\n - 第7步：将顶点A加入到S中。\n\n    此时，S={D(0),C(3),E(4),F(6),G(12),B(13),A(22)}。\n\n此时，起点D到各个顶点的最短距离就计算出来了：A(22) B(13) C(3) D(0) E(4) F(6) G(12)。\n\n----------\n\n代码\n-----\n\n本文以\"邻接矩阵\"为例对迪杰斯特拉算法进行说明，\n\n#### 基本定义\n\n\n```c\n// 邻接矩阵\ntypedef struct _graph\n{\n    char vexs[MAX];       // 顶点集合\n    int vexnum;           // 顶点数\n    int edgnum;           // 边数\n    int matrix[MAX][MAX]; // 邻接矩阵\n}Graph, *PGraph;\n\n// 边的结构体\ntypedef struct _EdgeData\n{\n    char start; // 边的起点\n    char end;   // 边的终点\n    int weight; // 边的权重\n}EData;\n```\n\nGraph是邻接矩阵对应的结构体。 \n\nvexs用于保存顶点，vexnum是顶点数，edgnum是边数；matrix则是用于保存矩阵信息的二维数组。例如，matrix[i][j]=1，则表示\"顶点i(即vexs[i])\"和\"顶点j(即vexs[j])\"是邻接点；matrix[i][j]=0，则表示它们不是邻接点。 \n\nEData是邻接矩阵边对应的结构体。\n\n#### 迪杰斯特拉算法\n\n<details><summary>代码：</summary>\n\n\n```c++\n/*\n * Dijkstra最短路径。\n * 即，统计图(G)中\"顶点vs\"到其它各个顶点的最短路径。\n *\n * 参数说明：\n *     G -- 图\n *     vs -- 起始顶点(start vertex)。即计算\"顶点vs\"到其它顶点的最短路径。\n *     prev -- 前驱顶点数组。即，prev[i]的值是\"顶点vs\"到\"顶点i\"的最短路径所经历的全部顶点中，位于\"顶点i\"之前的那个顶点。\n *     dist -- 长度数组。即，dist[i]是\"顶点vs\"到\"顶点i\"的最短路径的长度。\n */\n\nvoid dijkstra(Graph G, int vs, int prev[], int dist[])\n{\n    int i,j,k;\n    int min;\n    int tmp;\n    int flag[MAX];      // flag[i]=1表示\"顶点vs\"到\"顶点i\"的最短路径已成功获取。\n\n    // 初始化\n    for (i = 0; i < G.vexnum; i++)\n    {\n        flag[i] = 0;              // 顶点i的最短路径还没获取到。\n        prev[i] = 0;              // 顶点i的前驱顶点为0。\n        dist[i] = G.matrix[vs][i];// 顶点i的最短路径为\"顶点vs\"到\"顶点i\"的权。\n    }\n\n    // 对\"顶点vs\"自身进行\n    // 初始化\n    flag[vs] = 1;\n    dist[vs] = 0;\n\n    // 遍历G.vexnum-1次；每次找出一个顶点的最短路径。\n    for (i = 1; i < G.vexnum; i++)\n    {\n        // 寻找当前最小的路径；\n        // 即，在未获取最短路径的顶点中，找到离vs最近的顶点(k)。\n        min = INF;\n        for (j = 0; j < G.vexnum; j++)\n        {\n            if (flag[j]==0 && dist[j]<min)\n            {\n                min = dist[j];\n                k = j;\n            }\n        }\n        // 标记\"顶点k\"为已经获取到最短路径\n        flag[k] = 1;\n\n        // 修正当前最短路径和前驱顶点\n        // 即，当已经\"顶点k的最短路径\"之后，更新\"未获取最短路径的顶点的最短路径和前驱顶点\"。\n        for (j = 0; j < G.vexnum; j++)\n        {\n            tmp = (G.matrix[k][j]==INF ? INF : (min + G.matrix[k][j])); // 防止溢出\n            if (flag[j] == 0 && (tmp  < dist[j]) )\n            {\n                dist[j] = tmp;\n                prev[j] = k;\n            }\n        }\n    }\n\n    // 打印dijkstra最短路径的结果\n    printf(\"dijkstra(%c): \\n\", G.vexs[vs]);\n    for (i = 0; i < G.vexnum; i++)\n        printf(\"  shortest(%c, %c)=%d\\n\", G.vexs[vs], G.vexs[i], dist[i]);\n}\n```\n\n</details>\n\n","tags":["algorithm"],"categories":["algorithm"]},{"title":"Git操作手册|命令速查表","url":"/2018-02-27/reference/Git/git-guide/","content":"\n这篇文章主要介绍Git分布式版本管理与集中式管理的一些差异，总结下Git常用命令作为日后的速查表，最后介绍Git进阶的一些案例。\n本文分为以下几个部分：\n1. Git与SVN差异\n2. Git常用命令\n3. Git进阶指南\n\n<!-- more -->\n\n## Git与SVN差异\n\nGit的第一个版本是Linux之父Linus Torvalds亲手操刀设计和实现的,Git 基于 DAG 结构 (Directed Acyclic Graph)，其运行起来相当的快,它已经是现在的主流。\n\nGit 和 SVN 思想最大的差别有四个：\n\n* 去中心化\n* 直接记录快照，而非差异\n* 不一样的分支概念\n* 三个文件状态\n\n**去中心化**\n\nGit是一个DVCS（分布式版本管理系统），在技术层面上并不存在一个像中心仓库这样的东西 ， 所有的数据都在本地，不存在谁是中心\n\n![](/images/images/git.gif)\n\n图中每个开发者拉取(pull)并推送(push)到origin。但除了这种集中式的推送拉取关系，每个开发者也可能会从其他的开发者处拉取代码的变更，从技术上讲，这意味着Alice定义了一个名为bob的Git的remote，它指向了Bob的软件仓库。反之亦然。\n\n**直接记录快照，而非差异**\n\nGit每一个版本都是直接记录快照，而非文件的差异。 下面两个对比图在网上是广为流传大家应该熟悉：\n\nSVN：\n\n![](/images/images/svn.png)\n\nGit:\n\n![](/images/images/gitgit.png)\n\nGit使用SHA-1算法计算数据的校验和，通过文件的内容或目录计算出SHA-1哈希值，作为指纹字符串，每个Version 都是一个快照。\n\n**不一样的分支概念**\n\nGit的分支本质是一个指向提交快照的指针，是从某个提交快照往回看的历史。当创建/切换分支的时候，只是变换了指针指向而已.而SVN创建一个分支， 是的的确确的复制了一份文件。\n\n**三个文件状态**\n\n在Git中文件有三种状态：\n\n* 已提交（committed）：该文件被安全地保存在了本地数据库\n* 已修改（modified）：修改了某个文件，但还没有保存\n* 已暂存（staged）：把已修改的文件放下下次保存的清单中\n\n## Git常用命令\n\n### 创建\n\n复制一个已创建的仓库:\n\n```shell\n$ git clone ssh://user@domain.com/repo.git\n```\n\n创建一个新的本地仓库:\n\n```shell\n$ git init\n```\n\n### 本地修改\n\n显示工作路径下已修改的文件：\n\n```shell\n$ git status\n```\n\n显示与上次提交版本文件的不同：\n\n```shell\n$ git diff\n```\n\n把当前所有修改添加到下次提交中：\n\n```shell\n$ git add\n```\n\n把对某个文件的修改添加到下次提交中：\n\n```shell\n$ git add -p <file>\n```\n\n提交本地的所有修改：\n\n```shell\n$ git commit -a\n```\n\n提交之前已标记的变化：\n\n```shell\n$ git commit\n```\n\n附加消息提交：\n\n```shell\n$ git commit -m 'message here'\n```\n\n提交，并将提交时间设置为之前的某个日期:\n\n```shell\n$ git commit --date=\"`date --date='n day ago'`\" -am \"Commit Message\"\n```\n\n### 修改上次提交\n\n请勿修改已发布的提交记录!\n\n```shell\n$ git commit --amend\n```\n\n把当前分支中未提交的修改移动到其他分支\n\n```shell\n$ git stash\n$ git checkout branch2\n$ git stash pop\n```\n\n### 搜索\n\n从当前目录的所有文件中查找文本内容：\n\n```shell\n$ git grep \"Hello\"\n```\n\n在某一版本中搜索文本：\n\n```shell\n$ git grep \"Hello\" v2.5\n```\n\n### 提交历史\n\n从最新提交开始，显示所有的提交记录（显示hash， 作者信息，提交的标题和时间）：\n\n```shell\n$ git log\n```\n\n显示所有提交（仅显示提交的hash和message）：\n\n```shell\n$ git log --oneline\n```\n\n显示某个用户的所有提交：\n\n```shell\n$ git log --author=\"username\"\n```\n\n显示某个文件的所有修改：\n\n```shell\n$ git log -p <file>\n```\n\n谁，在什么时间，修改了文件的什么内容：\n\n```shell\n$ git blame <file>\n```\n\n### 分支与标签\n\n列出所有的分支：\n\n```shell\n$ git branch\n```\n\n切换分支：\n\n```shell\n$ git checkout <branch>\n```\n\n创建并切换到新分支:\n\n```shell\n$ git checkout -b <branch>\n```\n\n基于当前分支创建新分支：\n\n```shell\n$ git branch <new-branch>\n```\n\n基于远程分支创建新的可追溯的分支：\n\n```shell\n$ git branch --track <new-branch> <remote-branch>\n```\n\n删除本地分支:\n\n```shell\n$ git branch -d <branch>\n```\n\n给当前版本打标签：\n\n```shell\n$ git tag <tag-name>\n```\n\n### 更新与发布\n\n列出当前配置的远程端：\n\n```shell\n$ git remote -v\n```\n\n显示远程端的信息：\n\n```shell\n$ git remote show <remote>\n```\n\n添加新的远程端：\n\n```shell\n$ git remote add <remote> <url>\n```\n\n下载远程端版本，但不合并到HEAD中：\n\n```shell\n$ git fetch <remote>\n```\n\n下载远程端版本，并自动与HEAD版本合并：\n\n```shell\n$ git remote pull <remote> <url>\n```\n\n将远程端版本合并到本地版本中：\n\n```shell\n$ git pull origin master\n```\n\n将本地版本发布到远程端：\n\n```shell\n$ git push remote <remote> <branch>\n```\n\n删除远程端分支：\n\n```shell\n$ git push <remote> :<branch> (since Git v1.5.0)\n或\n$ git push <remote> --delete <branch> (since Git v1.7.0)\n```\n\n发布标签:\n\n```shell\n$ git push --tags\n```\n\n### 合并与重置\n\n将分支合并到当前HEAD中：\n\n```shell\n$ git merge <branch>\n```\n\n将当前HEAD版本重置到分支中:\n请勿重置已发布的提交!\n\n```shell\n$ git rebase <branch>\n```\n\n退出重置:\n\n```shell\n$ git rebase --abort\n```\n\n解决冲突后继续重置：\n\n```shell\n$ git rebase --continue\n```\n\n使用配置好的merge tool 解决冲突：\n\n```shell\n$ git mergetool\n```\n\n在编辑器中手动解决冲突后，标记文件为已解决冲突\n\n```shell\n$ git add <resolved-file>\n$ git rm <resolved-file>\n```\n\n### 撤销\n\n放弃工作目录下的所有修改：\n\n```shell\n$ git reset --hard HEAD\n```\n\n移除缓存区的所有文件（i.e. 撤销上次git add）:\n\n```shell\n$ git reset HEAD\n```\n\n放弃某个文件的所有本地修改：\n\n```shell\n$ git checkout HEAD <file>\n```\n\n重置一个提交（通过创建一个截然不同的新提交）\n\n```shell\n$ git revert <commit>\n```\n\n将HEAD重置到指定的版本，并抛弃该版本之后的所有修改：\n\n```shell\n$ git reset --hard <commit>\n```\n\n将HEAD重置到上一次提交的版本，并将之后的修改标记为未添加到缓存区的修改：\n\n```shell\n$ git reset <commit>\n```\n\n将HEAD重置到上一次提交的版本，并保留未提交的本地修改：\n\n```shell\n$ git reset --keep <commit>\n```\n\n## Git进阶指南\n\n### 问：如何修改 origin 仓库信息？\n\n#### 1、添加 origin 仓库信息\n\n```shell\n$ git remote add origin <git仓库地址>\n```\n\n#### 2、查看 origin 仓库信息\n\n```shell\n# 以下三种方式均可\n$ git config get --remote.origin.url\n$ git remote -v\n$ git remote show origin\n```\n\n#### 3、删除 origin 仓库信息\n\n```shell\n$ git remote rm origin\n```\n\n### 问：如何配置 git ssh keys ？\n\n在本地生成 ssh 私钥 / 公钥 文件\n\n将「公钥」添加到 git 服务（github、gitlab、coding.net 等）网站后台\n\n测试 git ssh 连接是否成功\n\n接下来以添加 github ssh keys 为例，请注意替换 github 文件名。\n\n注：如果对密钥机制不熟悉，建议不要指定 `-f` 参数，直接使用默认的 id_rsa 文件名。\n\n```shell\n# 运行以下命令，一直回车，文件名可随意指定\n$ ssh-keygen -t rsa -b 4096 -C \"kaiye@macbook\" -f ~/.ssh/github\n\n# 如果不是默认密钥 id_rsa ，则需要以下命令注册密钥文件，-K 参数将密钥存入 Mac Keychain\n$ ssh-add -K ~/.ssh/github\n\n# 将 pub 公钥的内容粘贴到线上网站的后台\n$ cat ~/.ssh/github.pub\n\n# 测试 git ssh 是否连接成功\n$ ssh -T git@github.com\n```\n\n### 问：如何撤销修改？\n\n修改包含四种情况，需单独区分。\n\n#### 1、新建的文件和目录，且从未提交至版本库\n\n此类文件的状态为 Untracked files ，撤销方法如下：\n\n```shell\n$ git clean -fd .\n```\n\n其中，. 表示当前目录及所有子目录中的文件，也可以直接指定对应的文件路径，以下其他情况类似。\n\n#### 2、提交过版本库，但未提交至暂存区的文件（未执行 git add）\n\n此类文件的状态为` Changes not staged for commit`，撤销方法：\n\n```shell\n$ git checkout .\n```\n\n#### 3、已提交至暂存区的文件\n\n此类文件的状态为 Changes to be committed，撤销方法：\n\n```shell\n$ git reset .\n```\n\n执行之后文件将会回到以上的 1 或者 2 状态，可继续按以上步骤执行撤销，若 git reset 同时加上 --hard 参数，将会把修改过的文件也还原成版本库中的版本。\n\n#### 4、已提交至版本库（执行了 git commit）\n\n每次提交都会生成一个 hash 版本号，通过以下命令可查阅版本号并将其回滚：\n\n```shell\n$ git log\n$ git reset <版本号>\n```\n\n如果需要「回滚至上一次提交」，可直接使用以下命令：\n\n```shell\n$ git reset head~1\n```\n\n执行之后，再按照 1 或者 2 状态进行处理即可，如果回滚之后的代码同时需要提交至 origin 仓库（即回滚 origin 线上仓库的代码），需要使用 -f 强制提交参数，且当前用户需要具备「强制提交的权限」。\n\n#### 5、如果回滚了之后又不想回滚了怎么办？\n\n如果是以上的情况 1 或者 2，只能歇屁了，因为修改没入过版本库，无法回滚。\n\n如果是情况 4，回滚之后通过 git log 将看不到回滚之前的版本号，但可通过 git reflog 命令（所有使用过的版本号）找到回滚之前的版本号，然后 git reset <版本号> 。\n\n### 问：遇到冲突了怎么解决？\n\n两个分支进行合并时（通常是 git pull 时），可能会遇到冲突，同时被修改的文件会进入 Unmerged 状态，需要解决冲突。\n\n#### 1、最快的办法\n\n大部分时候，「最快解决冲突」的办法是：使用当前 HEAD 的版本（ours），或使用合并进来的分支版本（theirs）。\n\n```shell\n# 使用当前分支 HEAD 版本，通常是冲突源文件的 <<<<<<< 标记部分，======= 的上方\n$ git checkout --ours <文件名>\n\n# 使用合并分支版本，通常是源冲突文件的 >>>>>>> 标记部分\n$ git checkout --theirs <文件名>\n\n# 标记为解决状态加入暂存区\n$ git add <文件名>\n```\n\n#### 2、最通用的办法\n\n用编辑器打开冲突的源文件进行修改，可能会发生遗留，且体验不好，通常需要借助 git mergetool 命令。\n\n在 Mac 系统下，运行 git mergetool <文件名> 可以开启配置的第三方工具进行 merge，默认的是 FileMerge 应用程序，还可以配置成 Meld 或 kdiff3，体验更佳。\n\n#### 3、最好的习惯\n\n有三个好的习惯，可以减少代码的冲突：\n\n在开始修改代码前先 git pull 一下；\n\n将业务代码进行划分，尽量不要多个人在同一时间段修改同一文件；\n\n通过Gitflow 工作流也可以提升 git流程效率，减少发生冲突的可能性。\n\n#### 4、最复杂的情况\n\n如果你的项目周期比较长，还应该养成「定期 rebase 的习惯」，git pull --rebase 可以让分支的代码和 origin 仓库的代码保持兼容，同时还不会破坏线上代码的可靠性。\n\n它的大概原理是，先将 origin 仓库的代码按 origin 的时间流在本地分支中提交，再将本地分支的修改记录追加到 origin 分支上。如果发生冲突，则可以即时的发现问题并解决，否则到项目上线时再解决冲突，可能会发生额外的风险。\n\nrebase 大概的操作步骤如下：\n\n```shell\n# 将当前分支的版本追加到从远程 pull 回来的节点之后\n$ git pull --rebase\n\n# 若发生冲突，则按以上其他方法进行解决，解决后继续\n$ git rebase --continue\n\n# 直到所有冲突得以解决，待项目最后上线前再执行\n$ git push origin\n\n# 若多次提交修改了同一文件，可能需要直接跳过后续提交，按提示操作即可\n$ git rebase --skip\n```\n\n### 问：如何在不提交修改的前提下，执行 pull / merge 等操作？\n\n有些修改没有完全完成之前，可能不需要提交到版本库，圡方法是将修改的文件 copy 到 git 仓库之外的目录临时存放，pull / merge 操作完成之后，再 copy 回来。\n\n这样的做法一个是效率不高，另外一个可能会遗漏潜在的冲突。此类需求最好是通过 git stash 命令来完成，它可以将当前工作状态（WIP，work in progress）临时存放在 stash 队列中，待操作完成后再从 stash 队列中重新应用这些修改。\n\n以下是 git stash 常用命令：\n\n```shell\n# 查看 stash 队列中已暂存了多少 WIP\n$ git stash list\n\n# 恢复上一次的 WIP 状态，并从队列中移除\n$ git stash pop\n\n# 添加当前 WIP，注意：未提交到版本库的文件会自动忽略，只要不运行 git clean -fd . 就不会丢失\n$ git stash\n\n# 恢复指定编号的 WIP，同时从队列中移除\n$ git stash pop stash@{num}\n\n# 恢复指定编号的 WIP，但不从队列中移除\n$ git stash apply stash@{num}\n```\n\n### 问：如何在 git log 中查看修改的文件列表？\n\n默认的 git log 会显示较全的信息，且不包含文件列表。使用 --name-status 可以看到修改的文件列表，使用 --oneline 可以将参数简化成一行。\n\n```shell\n$ git log --name-status --oneline\n```\n\n每次手动加上参数很麻烦，可以通过自定义快捷命令的方式来简化操作：\n\n```shell\n$ git config --global alias.ls 'log --name-status --oneline --graph'\n```\n\n运行以上配置后，可通过 git ls 命令来实现「自定义 git log」效果，通过该方法也可以创建 git st 、 git ci 等一系列命令，以便沿用 svn 命令行习惯。\n\n```shell\n$ git config --global alias.st 'status --porcelain'\n```\n\n更多 git log 参数，可通过 git help log 查看手册。\n\n如果是看上一次提交的版本日志，直接运行 git show 即可。\n\n此外，如果你的 Mac 安装了zsh（参考《全新Mac安装指南（编程篇），那么可以直接使用 gst、glog 等一系列快捷命令，详情见此列表：Plugin:git 。\n\n### 问：git submodule update 时出错怎么解决？\n\n例如，在执行 git submodule update 时有以下错误信息：\n\n>fatal: reference is not a tree: f869da471c5d8a185cd110bbe4842d6757b002f5\nUnable to checkout 'f869da471c5d8a185cd110bbe4842d6757b002f5' in submodule path 'source/i18n-php-server'\n\n在此例中，发生以上错误是因为 i18n-php-server 子仓库在某电脑 A 的「本地」commit 了新的版本 「f869da471c5d8a185cd110bbe4842d6757b002f5」，且该次 commit 未 push origin。但其父级仓库 i18n-www 中引用了该子仓库的版本号，且将引用记录 push origin，导致其他客户机无法 update 。\n\n解决方法，在电脑 A 上将 i18n-php-server 版本库 push origin 后，在其他客户机上执行 git submodule update 。或者用以上提到的 git reset 方法，将子仓库的引用版本号还原成 origin 上存在的最新版本号。\n\n### 其他问题\n\n设置本地分支与远程分支保持同步，在第一次 git push 的时候带上 `-u` 参数即可\n\n```shell\n$ git push origin master -u \n```\n\n支持中文目录与文件名的显示（git 默认将非 ASCII 编码的目录与文件名以八进制编码展示）\n\n```shell\n$ git config core.quotepath off\n```\n\n常用的打 tag 操作，更多请查看《Git 基础 - 打标签》\n\n```shell\n# 列出所有本地 tag\n$ git tag   \n\n# 本地新增一个 tag，推送至 origin 服务器\n$ git tag -a v1.0.0 -m 'tag description'\n$ git push origin v1.0.0\n\n# 删除本地与 origin tag\n$ git tag -d v1.0.0\n$ git push origin --delete v1.0.0\n```\n\n使用 git GUI 客户端（如，SoureTree、Github Desktop）能极大的提升分支管理效率。分支合并操作通常只有两种情况：从 origin merge 到本地，使用 git pull 即可；从另外一个本地分支 merge 到当前分支，使用 git merge <分支名>，以下是常用命令：\n\n```shell\n# 新建分支 branch1，并切换过去\n$ git checkout -b branch1\n\n# 查看所有本地与远程分支\n$ git branch -a\n\n# 修改完成后，切换回 master 分支，将 branch1 分支合并进来\n$ git checkout master\n$ git merge branch1\n\n# 删除已完成合并的分支 branch1\n$ git branch -d branch1\n```\n\n### 参考资料\n\n1. Pro Git 简体中文版\n2. Git权威指南\n3. 命令行man手册\n","tags":["git"],"categories":["git"]},{"title":"openvpn权限控制配置","url":"/2018-02-03/openvpn权限控制配置/","content":"\n\n## 需求定义\n针对不同用户的身份分配不同的路由\n\n## 配置文件说明\n- openvpn配置文件\n```\n# vi /etc/openvpn/server.conf\n#OpenVPN应该监听本机的哪些IP地址？\n#该命令是可选的，如果不设置，则默认监听本机的所有IP地址。\n;local a.b.c.d\n\n# OpenVPN应该监听哪个TCP/UDP端口？\n# 如果你想在同一台计算机上运行多个OpenVPN实例，你可以使用不同的端口号来区分它们。\n# 此外，你需要在防火墙上开放这些端口。\nport 1194\n\n#OpenVPN使用TCP还是UDP协议?\n;proto tcp\nproto udp\n\n# 指定OpenVPN创建的通信隧道类型。\n# \"dev tun\"将会创建一个路由IP隧道，\n# \"dev tap\"将会创建一个以太网隧道。\n#\n# 如果你是以太网桥接模式，并且提前创建了一个名为\"tap0\"的与以太网接口进行桥接的虚拟接口，则你可以使用\"dev tap0\"\n#\n# 如果你想控制VPN的访问策略，你必须为TUN/TAP接口创建防火墙规则。\n#\n# 在非Windows系统中，你可以给出明确的单位编号(unit number)，例如\"tun0\"。\n# 在Windows中，你也可以使用\"dev-node\"。\n# 在多数系统中，除非你部分禁用或者完全禁用了TUN/TAP接口的防火墙，否则VPN将不起作用。\n;dev tap\ndev tun\n\n# 如果你想配置多个隧道，你需要用到网络连接面板中TAP-Win32适配器的名称(例如\"MyTap\")。\n# 在XP SP2或更高版本的系统中，你可能需要有选择地禁用掉针对TAP适配器的防火墙\n# 通常情况下，非Windows系统则不需要该指令。\n;dev-node MyTap\n\n# 设置SSL/TLS根证书(ca)、证书(cert)和私钥(key)。\n# 每个客户端和服务器端都需要它们各自的证书和私钥文件。\n# 服务器端和所有的客户端都将使用相同的CA证书文件。\n#\n# 通过easy-rsa目录下的一系列脚本可以生成所需的证书和私钥。\n# 记住，服务器端和每个客户端的证书必须使用唯一的Common Name。\n#\n# 你也可以使用遵循X509标准的任何密钥管理系统来生成证书和私钥。\n# OpenVPN 也支持使用一个PKCS #12格式的密钥文件(详情查看站点手册页面的\"pkcs12\"指令)\nca ca.crt\ncert server.crt\nkey server.key  # 该文件应该保密\n\n# 指定迪菲·赫尔曼参数。\n# 你可以使用如下名称命令生成你的参数：\n#   openssl dhparam -out dh1024.pem 1024\n# 如果你使用的是2048位密钥，使用2048替换其中的1024。\ndh dh1024.pem\n\n# 设置服务器端模式，并提供一个VPN子网，以便于从中为客户端分配IP地址。\n# 在此处的示例中，服务器端自身将占用10.8.0.1，其他的将提供客户端使用。\n# 如果你使用的是以太网桥接模式，请注释掉该行。更多信息请查看官方手册页面。\nserver 10.8.0.0 255.255.255.0\n\n# 指定用于记录客户端和虚拟IP地址的关联关系的文件。\n# 当重启OpenVPN时，再次连接的客户端将分配到与上一次分配相同的虚拟IP地址\nifconfig-pool-persist ipp.txt\n\n# 该指令仅针对以太网桥接模式。\n# 首先，你必须使用操作系统的桥接能力将以太网网卡接口和TAP接口进行桥接。\n# 然后，你需要手动设置桥接接口的IP地址、子网掩码；\n# 在这里，我们假设为10.8.0.4和255.255.255.0。\n# 最后，我们必须指定子网的一个IP范围(例如从10.8.0.50开始，到10.8.0.100结束)，以便于分配给连接的客户端。\n# 如果你不是以太网桥接模式，直接注释掉这行指令即可。\n;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100\n\n# 该指令仅针对使用DHCP代理的以太网桥接模式，\n# 此时客户端将请求服务器端的DHCP服务器，从而获得分配给它的IP地址和DNS服务器地址。\n#\n# 在此之前，你也需要先将以太网网卡接口和TAP接口进行桥接。\n# 注意：该指令仅用于OpenVPN客户端，并且该客户端的TAP适配器需要绑定到一个DHCP客户端上。\n;server-bridge\n\n# 推送路由信息到客户端，以允许客户端能够连接到服务器背后的其他私有子网。\n# (简而言之，就是允许客户端访问VPN服务器自身所在的其他局域网)\n# 记住，这些私有子网也要将OpenVPN客户端的地址池(10.8.0.0/255.255.255.0)反馈回OpenVPN服务器。\n;push \"route 192.168.10.0 255.255.255.0\"\n;push \"route 192.168.20.0 255.255.255.0\"\n\n# 为指定的客户端分配指定的IP地址，或者客户端背后也有一个私有子网想要访问VPN，\n# 那么你可以针对该客户端的配置文件使用ccd子目录。\n# (简而言之，就是允许客户端所在的局域网成员也能够访问VPN)\n\n# 举个例子：假设有个Common Name为\"Thelonious\"的客户端背后也有一个小型子网想要连接到VPN，该子网为192.168.40.128/255.255.255.248。\n# 首先，你需要去掉下面两行指令的注释：\n;client-config-dir ccd\n;route 192.168.40.128 255.255.255.248\n# 然后创建一个文件ccd/Thelonious，该文件的内容为：\n#     iroute 192.168.40.128 255.255.255.248\n#这样客户端所在的局域网就可以访问VPN了。\n# 注意，这个指令只能在你是基于路由、而不是基于桥接的模式下才能生效。\n# 比如，你使用了\"dev tun\"和\"server\"指令。\n\n# 再举个例子：假设你想给Thelonious分配一个固定的IP地址10.9.0.1。\n# 首先，你需要去掉下面两行指令的注释：\n;client-config-dir ccd\n;route 10.9.0.0 255.255.255.252\n# 然后在文件ccd/Thelonious中添加如下指令：\n#   ifconfig-push 10.9.0.1 10.9.0.2\n\n# 如果你想要为不同群组的客户端启用不同的防火墙访问策略，你可以使用如下两种方法：\n# (1)运行多个OpenVPN守护进程，每个进程对应一个群组，并为每个进程(群组)启用适当的防火墙规则。\n# (2) (进阶)创建一个脚本来动态地修改响应于来自不同客户的防火墙规则。\n# 关于learn-address脚本的更多信息请参考官方手册页面。\n;learn-address ./script\n\n# 如果启用该指令，所有客户端的默认网关都将重定向到VPN，这将导致诸如web浏览器、DNS查询等所有客户端流量都经过VPN。\n# (为确保能正常工作，OpenVPN服务器所在计算机可能需要在TUN/TAP接口与以太网之间使用NAT或桥接技术进行连接)\n;push \"redirect-gateway def1 bypass-dhcp\"\n\n# 某些具体的Windows网络设置可以被推送到客户端，例如DNS或WINS服务器地址。\n# 下列地址来自opendns.com提供的Public DNS 服务器。\n;push \"dhcp-option DNS 208.67.222.222\"\n;push \"dhcp-option DNS 208.67.220.220\"\n\n# 去掉该指令的注释将允许不同的客户端之间相互\"可见\"(允许客户端之间互相访问)。\n# 默认情况下，客户端只能\"看见\"服务器。为了确保客户端只能看见服务器，你还可以在服务器端的TUN/TAP接口上设置适当的防火墙规则。\n;client-to-client\n\n# 如果多个客户端可能使用相同的证书/私钥文件或Common Name进行连接，那么你可以取消该指令的注释。\n# 建议该指令仅用于测试目的。对于生产使用环境而言，每个客户端都应该拥有自己的证书和私钥。\n# 如果你没有为每个客户端分别生成Common Name唯一的证书/私钥，你可以取消该行的注释(但不推荐这样做)。\n;duplicate-cn\n\n# keepalive指令将导致类似于ping命令的消息被来回发送，以便于服务器端和客户端知道对方何时被关闭。\n# 每10秒钟ping一次，如果120秒内都没有收到对方的回复，则表示远程连接已经关闭。\nkeepalive 10 120\n\n# 出于SSL/TLS之外更多的安全考虑，创建一个\"HMAC 防火墙\"可以帮助抵御DoS攻击和UDP端口淹没攻击。\n# 你可以使用以下命令来生成：\n#   openvpn --genkey --secret ta.key\n#\n# 服务器和每个客户端都需要拥有该密钥的一个拷贝。\n# 第二个参数在服务器端应该为'0'，在客户端应该为'1'。\n;tls-auth ta.key 0 # 该文件应该保密\n\n# 选择一个密码加密算法。\n# 该配置项也必须复制到每个客户端配置文件中。\n;cipher BF-CBC        # Blowfish (默认)\n;cipher AES-128-CBC   # AES\n;cipher DES-EDE3-CBC  # Triple-DES\n\n# 在VPN连接上启用压缩。\n# 如果你在此处启用了该指令，那么也应该在每个客户端配置文件中启用它。\ncomp-lzo\n\n# 允许并发连接的客户端的最大数量\n;max-clients 100\n\n# 在完成初始化工作之后，降低OpenVPN守护进程的权限是个不错的主意。\n# 该指令仅限于非Windows系统中使用。\n;user nobody\n;group nobody\n\n# 持久化选项可以尽量避免访问那些在重启之后由于用户权限降低而无法访问的某些资源。\npersist-key\npersist-tun\n\n# 输出一个简短的状态文件，用于显示当前的连接状态，该文件每分钟都会清空并重写一次。\nstatus openvpn-status.log\n\n# 默认情况下，日志消息将写入syslog(在Windows系统中，如果以服务方式运行，日志消息将写入OpenVPN安装目录的log文件夹中)。\n# 你可以使用log或者log-append来改变这种默认情况。\n# \"log\"方式在每次启动时都会清空之前的日志文件。\n# \"log-append\"这是在之前的日志内容后进行追加。\n# 你可以使用两种方式之一(但不要同时使用)。\n;log         openvpn.log\n;log-append  openvpn.log\n\n# 为日志文件设置适当的冗余级别(0~9)。冗余级别越高，输出的信息越详细。\n#\n# 0 表示静默运行，只记录致命错误。\n# 4 表示合理的常规用法。\n# 5 和 6 可以帮助调试连接错误。\n# 9 表示极度冗余，输出非常详细的日志信息。\nverb 3\n\n# 重复信息的沉默度。\n# 相同类别的信息只有前20条会输出到日志文件中。\n;mute 20\n\n```\n\n## 解决方案\n- 方案一\n```\n# 针对不同用户根据其权限获取路由模板然后再ccd下生成对应的路由文件\n# mkdir ccd\n# vi ccd/sys_user1 文件名为连接的用户名\n# 推送指定路由\npush \"route 10.252.10.191 255.255.255.255\"\npush \"route 10.252.10.192 255.255.255.255\"\n\n```\n- 方案二\n```\n# 对所有用户分发相同的路由但是指定对应的ip段,利用防火墙来隔离对应ip段的访问权限\n\n# mkdir ccd\n# vi ccd/sys_user2 如果有用户分组 则使用分组用户即可 同账号多用户\n# 推送指定路由\nifconfig-push 192.168.171.159/24 192.168.171.155/24\n\n# 防火墙设置\niptables -A FORWARD -i tun0 -s 192.168.171.159/24 -d 10.21.3.8 -j ACCEPT\n```","tags":["openvpn","config"],"categories":["运维"]},{"title":"分辨率/码率/帧率对视频流的影响","url":"/2018-02-03/webrtc_note_1/","content":"\n### 分辨率/码率/帧率对视频流的影响\n\n在这几天的调试中降低`码率`和`帧率`后,将会减少延时和花屏的情况,主要是提高了解码的速度,但是对画质的影响,主要在于什么地方?\n\n-   对帧率的修改,`60fps`降到`25fps`后,看到的显示效果没有变化,为什么?对用户体验的影响\n\n> 用户体验: 只关系视频的画面质量,如果需要存储还需要关心视频文件的大小\n>\n> -   流畅度: 主要受视频帧率(FPS)的影响\n> -   清晰度: 单位面积的画面所承载的信息，在对视频进行评价是就是当FPS和分辨率固定时，考察视频的清晰度。码率\n> -   分辨率: 视频画面的大小\n\n[](https://winddoing.github.io/post/60235.html#%E7%A0%81%E7%8E%87 \"码率\")码率\n-------------------------------------------------------------------------\n\n> 码率:比特率，英文为`Bit Rate`，是指每秒传送的比特(bit)数\n\n### [](https://winddoing.github.io/post/60235.html#%E7%A0%81%E7%8E%87%E5%BD%B1%E5%93%8D%E8%A7%86%E9%A2%91%E6%B8%85%E6%99%B0%E5%BA%A6 \"码率影响视频清晰度\")码率影响视频`清晰度`\n\n[![video_bit_rate](https://winddoing.github.io/images/2018/11/video_bit_rate.png)](https://winddoing.github.io/images/2018/11/video_bit_rate.png)\n\n> 当比特率越大，视频清晰度就越高。其影响存在边际效应递减现象，并且存在上限（视频原始数据的画质）。\n\n### [](https://winddoing.github.io/post/60235.html#%E7%A0%81%E7%8E%87%E5%86%B3%E5%AE%9A%E4%BA%86%E8%A7%86%E9%A2%91%E5%A4%A7%E5%B0%8F \"码率决定了视频大小\")码率决定了视频`大小`\n\n视频大小size和码率BitRate的关系：\n\nsize = VideoDuration∗BitRate8VideoDuration∗BitRate8\n\n[](https://winddoing.github.io/post/60235.html#%E5%B8%A7%E7%8E%87%E2%80%94FPS \"帧率---FPS\")帧率---FPS\n---------------------------------------------------------------------------------------------\n\n> FPS是图像领域中的定义，是指画面每秒传输帧数，通俗来讲就是指动画或视频的画面数。FPS是测量用于保存、显示动态视频的信息数量。每秒钟帧数愈多，所显示的动作就会越流畅。\n>\n> > 在游戏过程中一般人不觉得卡顿的FPS频率大约是30Hz，想要达到流畅等级则需要60Hz。\n\n1.  帧率对视频画质没有影响 帧率和视频大小正相关。视频大小和FPS/GOP_SIZE存在一定的线性相关性。\n\n    > GOP_SIZE是ffmpeg中的一个编码参数，表示的每GOP_SIZE个帧存在一个关键帧。\n\n2.  在显示\"分辨率\"不变的情况下，FPS越高，则对显卡的处理能力要求越高。\n\n[](https://winddoing.github.io/post/60235.html#%E5%88%86%E8%BE%A8%E7%8E%87 \"分辨率\")分辨率\n------------------------------------------------------------------------------------\n\n分辨率对视频大小和画质有着非常重要的影响。\n\n如果清晰度和流畅度不变，分辨与视频画质，大小正相关。但是前提条件是清晰度不变，单纯的增加分辨并不能带来更好的画质.\n\n> 结果显示，同等分辨率的前提下，显示屏幕越大，主观感受越差。\n\n[](https://winddoing.github.io/post/60235.html#%E5%8F%82%E8%80%83 \"参考\")参考\n-------------------------------------------------------------------------\n\n> bandwidth参数\n1. Bandwidth的定义：\n\nSDP中的bandwidth参数用于在OfferSDP中告知对方本设备的解码器可以接受的最大会话流或媒体流的bit率\n\n2. Bandwidth的格式：\n\n在SDP中的m行之前（关于会话的）或m行之后（关于对应媒体流的）都可以加bandwidth参数，具体格式为：b=:，其中不同的bwtype，对应不同的带宽限制的计算方法，\n\n\n-   [码率,帧率,分辨率对视频画质的影响](https://winddoing.github.io/post/60235.html)","tags":["视频流"],"categories":["WebRTC"]},{"title":"机器学习深度学习-文章收藏","url":"/2018-01-13/reference/DeepLearning/note-blog-link/","content":"\n> [沁原的硅谷创新课](<https://github.com/Fabsqrt/BitTigerLab>)\n>\n> [Github项目推荐 | 基于 deepfakes（视频换脸）的非官方项目deepfakes_faceswap](<https://ai.yanxishe.com/page/blogDetail/10036>)\n\n","tags":["DeepLearning"],"categories":["DeepLearning"]},{"title":"Dogs vs Cats (猫狗大战)","url":"/2018-01-03/reference/DeepLearning/DogsVsCats/","content":"\n> GitHub 项目地址：[Dogs vs Cats (猫狗大战)](<https://github.com/miaopei/cat_vs_dog>)\n\n## 项目说明\n\n本项目是优达学城的一个毕业项目。项目要求使用深度学习方法识别一张图片是猫还是狗\n\n- 输入：一张彩色图片\n- 输出：是猫还是狗\n\n<!-- more -->\n\n## 项目环境\n\n项目使用Anaconda搭建环境。可是使用environment目录下的yml进行环境安装。\n\n```shell\n$ conda env create -f environment.yml\n```\n\n## 数据来源\n\n数据集来自 kaggle 上的一个竞赛：[Dogs vs. Cats Redux: Kernels Edition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data)。\n\n下载kaggle猫狗数据集解压后分为 3 个文件 train.zip、 test.zip 和 sample_submission.csv。\n\ntrain 训练集包含了 25000 张猫狗的图片， 每张图片包含图片本身和图片名。命名规则根据“type.num.jpg”方式命名。\n\ntest 测试集包含了 12500 张猫狗的图片， 每张图片命名规则根据“num.jpg”，需要注意的是测试集编号从 1 开始， 而训练集的编号从 0 开始。\n\nsample_submission.csv 需要将最终测试集的测试结果写入.csv 文件中，上传至 kaggle 进行打分。\n\n## 基准模型\n\n项目使用ResNet50, Xception, Inception V3 这三个模型完成。本项目的最低要求是 kaggle Public Leaderboard 前10%。在kaggle上，总共有1314只队伍参加了比赛，所以需要最终的结果排在131位之前，131位的得分是0.06127，所以目标是模型预测结果要小于0.06127。\n\n## 评估指标\n\nkaggle 官方的评估标准是 LogLoss，下面的表达式就是二分类问题的 LogLoss 定义。\n\n<div align=\"center\"><a href=\"https://www.codecogs.com/eqnedit.php?latex=LogLoss&space;=&space;-\\frac{1}{n}\\sum_{i=1}^n&space;[y_ilog(\\hat{y}_i)&plus;(1-y_i)log(1-&space;\\hat{y}_i)]\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?LogLoss&space;=&space;-\\frac{1}{n}\\sum_{i=1}^n&space;[y_ilog(\\hat{y}_i)&plus;(1-y_i)log(1-&space;\\hat{y}_i)]\" title=\"LogLoss = -\\frac{1}{n}\\sum_{i=1}^n [y_ilog(\\hat{y}_i)+(1-y_i)log(1- \\hat{y}_i)]\" /></a></div>\n\n其中：\n\n- n 是测试集中图片数量\n\n- <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\hat{y}_i\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\hat{y}_i\" title=\"\\hat{y}_i\" /></a> 是图片预测为狗的概率\n\n- <a href=\"https://www.codecogs.com/eqnedit.php?latex=y_i\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?y_i\" title=\"y_i\" /></a> 如果图像是狗，则为1，如果是猫，则为0\n\n- <a href=\"https://www.codecogs.com/eqnedit.php?latex=log()\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?log()\" title=\"log()\" /></a> 是自然（基数 <a href=\"https://www.codecogs.com/eqnedit.php?latex=e\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?e\" title=\"e\" /></a>）对数\n\n对数损失越小，代表模型的性能越好。上述评估指标可用于评估该项目的解决方案以及基准模型。\n\n## 设计大纲\n\n```shell\n$ cd model_graphviz/\n$ make\n```\n\n<img src=\"/images/imageDeepLearning/model.png\">\n\n整个模型是在本地训练的，训练了三天才完成。建议使用云端 GPU 训练复现实验过程。\n\n**1. 数据预处理**\n\n- 从kaggle下载好图片\n- 将猫和狗的图片放在不同的文件夹以示分类，使用创建符号链接的方法\n- 对图片进行resize，保持输入图片信息大小一致\n\n**2. 模型搭建**\n\nKera的应用模块Application提供了带有预训练权重的Keras模型，这些模型可以用来进行预测、特征提取和微调整和。\n\n- Xception 默认输入图片大小是 `299*299*3`\n- InceptionV3 默认输入图片大小是 `299*299*3`\n- ResNet50 默认输入图片大小是 `224*224*3`\n\n在Keras中载入模型并进行全局平均池化，只需要在载入模型的时候，设置`include_top=False`, `pooling='avg'`. 每个模型都将图片处理成一个` 1*2048 `的行向量，将这三个行向量进行拼接，得到一个` 1*6144 `的行向量， 作为数据预处理的结果。\n\n**3. 模型训练&模型调参**\n\n载入预处理的数据之后，先进行一次概率为0.5的dropout，然后直接连接输出层，激活函数为Sigmoid，优化器为Adam，输出一个零维张量，表示某张图片中有狗的概率。\n\n**4. 模型评估**\n\n- 使用$Logloss$进行模型评估,上传Kaggle判断是否符合标准\n\n**5. 可视化**\n\n- 进行数据探索并且可视化原始数据\n- 可视化模型训练过程的准确率曲线，损失函数曲线等\n\n## 项目部署\n\n项目使用 Keras 和 Flask 搭建部署一个简单易用的深度学习图像网页应用，可以通过网页导入一张彩色猫或者狗的图片预测是猫或者狗的概率。\n\n项目目录结构：\n\n```python\n.\n├── README.md\n├── ResNet50_image_predict.ipynb\n├── app.py\n├── environmert.yml\n├── static\n│   ├── css\n│   │   └── main.css\n│   └── js\n│       └── main.js\n├── templates\n│   ├── base.html\n│   └── index.html\n├── models\n│   └── ResNet50_catdog_model.h5\n├── uploads\n│   ├── test01.jpg\n│   └── test02.jpg\n└── webapp_image_predict.ipynb\n```\n\n### 环境搭建\n\n```shell\n$ conda env create -f environmert.yml\n```\n\n### 运行\n\n```shell\n$ python app.py\n```\n\n这时候用浏览器打开 <http://localhost:5000/> 就可以进行网页导入图片预测图片是狗的概率了。\n\n### 快速复现webapp预测结果\n\n如果不想搭建环境复现实验结果，可以按照以下操作分分钟复现实验结果：\n\n```shell\n$ docker pull miaowmiaow/webapp:1.1.0\n$ docker run -p 5000:5000 miaowmiaow/webapp:1.1.0\n```\n\n到此就可以在浏览器中输入 [http://localhost:5000](http://localhost:5000) 就可以使用网页对导入的猫狗图片做预测了。\n\n下图为预测的效果图：\n\n<img src=\"/images/imageDeepLearning/webapp.gif\">","tags":["DeepLearning"],"categories":["DeepLearning"]},{"title":"WebSocket教程","url":"/2017-05-16/reference/WebSocket教程/","content":"\n转自[阮一峰网络编程](http://www.ruanyifeng.com/blog/2017/05/websocket.html)\n\n[WebSocket](http://websocket.org/) 是一种网络通信协议，很多高级功能都需要它。\n\n## 为什么需要 WebSocker\n\n初次接触 WebSocket 的人，都会问同样的问题：我们已经有了 HTTP 协议，为什么还需要另一个协议？它能带来什么好处？\n\n<!-- more -->\n\n答案很简单，因为 HTTP 协议有一个缺陷：通信只能由客户端发起。\n\n举例来说，我们想了解今天的天气，只能是客户端向服务器发出请求，服务器返回查询结果。HTTP 协议做不到服务器主动向客户端推送信息。\n\n![](http://i.imgur.com/5mUfWtm.jpg)\n\n这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。我们只能使用[\"轮询\"](https://www.pubnub.com/blog/2014-12-01-http-long-polling/)：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。最典型的场景就是聊天室。\n\n轮询的效率低，非常浪费资源（因为必须不停连接，或者 HTTP 连接始终打开）。因此，工程师们一直在思考，有没有更好的方法。WebSocket 就是这样发明的。\n\n## 简介\n\nWebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。\n\n它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于[服务器推送技术](https://en.wikipedia.org/wiki/Push_technology)的一种。\n\n![](http://i.imgur.com/Qutxs2j.png)\n\n其他特点包括：\n\n（1）建立在 TCP 协议之上，服务器端的实现比较容易。\n\n（2）与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。\n\n（3）数据格式比较轻量，性能开销小，通信高效。\n\n（4）可以发送文本，也可以发送二进制数据。\n\n（5）没有同源限制，客户端可以与任意服务器通信。\n\n（6）协议标识符是 `ws`（如果加密，则为 `wss` ），服务器网址就是 URL。\n\n```text\nws://example.com:80/some/path\n```\n\n![](http://i.imgur.com/UWC2xr3.jpg)\n\n## 客户端的简单示例\n\nWebSocket 的用法相当简单。\n\n下面是一个网页脚本的例子（点击[这里](http://jsbin.com/muqamiqimu/edit?js,console)看运行结果），基本上一眼就能明白。\n\n```javascript\nvar ws = new WebSocket(\"wss://echo.websocket.org\");\n\nws.onopen = function(evt) { \n  console.log(\"Connection open ...\"); \n  ws.send(\"Hello WebSockets!\");\n};\n\nws.onmessage = function(evt) {\n  console.log( \"Received Message: \" + evt.data);\n  ws.close();\n};\n\nws.onclose = function(evt) {\n  console.log(\"Connection closed.\");\n};   \n```\n\n## 客户端的 API\n\nWebSocket 客户端的 API 如下。\n\n### WebSocket 构造函数\n\nWebSocket 对象作为一个构造函数，用于新建 WebSocket 实例。\n\n```javascript\nvar ws = new WebSocket('ws://localhost:8080');\n```\n\n执行上面语句之后，客户端就会与服务器进行连接。\n\n实例对象的所有属性和方法清单，参见[这里](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket)。\n\n### webSocket.readyState\n\n`readyState` 属性返回实例对象的当前状态，共有四种。\n\n- CONNECTING：值为0，表示正在连接。\n- OPEN：值为1，表示连接成功，可以通信了。\n- CLOSING：值为2，表示连接正在关闭。\n- CLOSED：值为3，表示连接已经关闭，或者打开连接失败。\n\n下面是一个示例。\n\n```javascript\nswitch (ws.readyState) {\n  case WebSocket.CONNECTING:\n    // do something\n    break;\n  case WebSocket.OPEN:\n    // do something\n    break;\n  case WebSocket.CLOSING:\n    // do something\n    break;\n  case WebSocket.CLOSED:\n    // do something\n    break;\n  default:\n    // this never happens\n    break;\n}\n```\n\n### webSocket.onopen\n\n实例对象的 `onopen` 属性，用于指定连接成功后的回调函数。\n\n```javascript\nws.onopen = function () {\n  ws.send('Hello Server!');\n}\n```\n\n如果要指定多个回调函数，可以使用addEventListener`方法。\n\n```javascript\nws.addEventListener('open', function (event) {\n  ws.send('Hello Server!');\n});\n```\n\n### webSocket.onclose\n\n实例对象的`onclose`属性，用于指定连接关闭后的回调函数。\n\n```javascript\nws.onclose = function(event) {\n  var code = event.code;\n  var reason = event.reason;\n  var wasClean = event.wasClean;\n  // handle close event\n};\n\nws.addEventListener(\"close\", function(event) {\n  var code = event.code;\n  var reason = event.reason;\n  var wasClean = event.wasClean;\n  // handle close event\n});\n```\n\n### webSocket.onmessage\n\n实例对象的 `onmessage` 属性，用于指定收到服务器数据后的回调函数。\n\n```javascript\nws.onmessage = function(event) {\n  var data = event.data;\n  // 处理数据\n};\n\nws.addEventListener(\"message\", function(event) {\n  var data = event.data;\n  // 处理数据\n});\n```\n\n注意，服务器数据可能是文本，也可能是二进制数据（ `blob` 对象或 `Arraybuffer` 对象）。\n\n```javascript\nws.onmessage = function(event){\n  if(typeof event.data === String) {\n    console.log(\"Received data string\");\n  }\n\n  if(event.data instanceof ArrayBuffer){\n    var buffer = event.data;\n    console.log(\"Received arraybuffer\");\n  }\n}\n```\n\n除了动态判断收到的数据类型，也可以使用`binaryType`属性，显式指定收到的二进制数据类型。\n\n```javascript\n// 收到的是 blob 数据\nws.binaryType = \"blob\";\nws.onmessage = function(e) {\n  console.log(e.data.size);\n};\n\n// 收到的是 ArrayBuffer 数据\nws.binaryType = \"arraybuffer\";\nws.onmessage = function(e) {\n  console.log(e.data.byteLength);\n};\n```\n\n### webSocket.send( )\n\n实例对象的 `send( )` 方法用于向服务器发送数据。\n\n发送文本的例子。\n\n```javascript\nws.send('your message');\n```\n\n发送 Blob 对象的例子。\n\n```javascript\nvar file = document\n  .querySelector('input[type=\"file\"]')\n  .files[0];\nws.send(file);\n```\n\n发送 ArrayBuffer 对象的例子。\n\n```javascript\n// Sending canvas ImageData as ArrayBuffer\nvar img = canvas_context.getImageData(0, 0, 400, 320);\nvar binary = new Uint8Array(img.data.length);\nfor (var i = 0; i < img.data.length; i++) {\n  binary[i] = img.data[i];\n}\nws.send(binary.buffer);\n```\n\n### webSocket.bufferedAmount\n\n实例对象的 `bufferedAmount` 属性，表示还有多少字节的二进制数据没有发送出去。它可以用来判断发送是否结束。\n\n```javascript\nvar data = new ArrayBuffer(10000000);\nsocket.send(data);\n\nif (socket.bufferedAmount === 0) {\n  // 发送完毕\n} else {\n  // 发送还没结束\n}\n```\n\n### webSocket.onerror\n\n实例对象的`onerror`属性，用于指定报错时的回调函数。\n\n```javascript\nsocket.onerror = function(event) {\n  // handle error event\n};\n\nsocket.addEventListener(\"error\", function(event) {\n  // handle error event\n});\n```\n\n## 服务端的实现\n\nWebSocket 服务器的实现，可以查看维基百科的[列表](https://en.wikipedia.org/wiki/Comparison_of_WebSocket_implementations)。\n\n常用的 Node 实现有以下三种。\n\n- [µWebSockets](https://github.com/uWebSockets/uWebSockets)\n- [Socket.IO](http://socket.io/)\n- [WebSocket-Node](https://github.com/theturtle32/WebSocket-Node)\n\n具体的用法请查看它们的文档，这里不详细介绍了。\n\n## WebSocketd\n\n下面，我要推荐一款非常特别的 WebSocket 服务器：[Websocketd](http://websocketd.com/)。\n\n它的最大特点，就是后台脚本不限语言，标准输入（stdin）就是 WebSocket 的输入，标准输出（stdout）就是 WebSocket 的输出。\n\n![](http://i.imgur.com/a51CR69.png)\n\n举例来说，下面是一个 Bash 脚本 `counter.sh`。\n\n```bash\n#!/bin/bash\n\necho 1\nsleep 1\n\necho 2\nsleep 1\n\necho 3\n```\n\n命令行下运行这个脚本，会输出1、2、3，每个值之间间隔1秒。\n\n```bash\n$ bash ./counter.sh\n1\n2\n3\n```\n\n现在，启动`websocketd`，指定这个脚本作为服务。\n\n```bash\n$ websocketd --port=8080 bash ./counter.sh\n```\n\n上面的命令会启动一个 WebSocket 服务器，端口是 `8080` 。每当客户端连接这个服务器，就会执行 `counter.sh` 脚本，并将它的输出推送给客户端。\n\n```javascript\nvar ws = new WebSocket('ws://localhost:8080/');\n\nws.onmessage = function(event) {\n  console.log(event.data);\n};\n```\n\n上面是客户端的 JavaScript 代码，运行之后会在控制台依次输出1、2、3。\n\n有了它，就可以很方便地将命令行的输出，发给浏览器。\n\n```bash\n$ websocketd --port=8080 ls\n```\n\n上面的命令会执行`ls`命令，从而将当前目录的内容，发给浏览器。使用这种方式实时监控服务器，简直是轻而易举（[代码](https://github.com/joewalnes/web-vmstats)）。\n\n![](http://i.imgur.com/WMUStsh.jpg)\n\n更多的用法可以参考[官方示例](https://github.com/joewalnes/websocketd/tree/master/examples/bash)。\n\n- Bash 脚本[读取客户端输入](https://github.com/joewalnes/websocketd/blob/master/examples/bash/greeter.sh)的例子\n- 五行代码实现一个最简单的[聊天服务器](https://github.com/joewalnes/websocketd/blob/master/examples/bash/chat.sh)\n\n![](http://i.imgur.com/KfZKSmD.png)\n\nwebsocketd 的实质，就是命令行的 WebSocket 代理。只要命令行可以执行的程序，都可以通过它与浏览器进行 WebSocket 通信。下面是一个 Node 实现的回声服务 [`greeter.js`](https://github.com/joewalnes/websocketd/blob/master/examples/nodejs/greeter.js)。\n\n```javascript\nprocess.stdin.setEncoding('utf8');\n\nprocess.stdin.on('readable', function() {\n  var chunk = process.stdin.read();\n  if (chunk !== null) {\n    process.stdout.write('data: ' + chunk);\n  }\n});\n```\n\n启动这个脚本的命令如下。\n\n```bash\n$ websocketd --port=8080 node ./greeter.js\n```\n\n官方仓库还有其他[各种语言](https://github.com/joewalnes/websocketd/tree/master/examples)的例子。\n\n## 参考链接\n\n- [How to Use WebSockets](http://cjihrig.com/blog/how-to-use-websockets/)\n- [WebSockets - Send & Receive Messages](https://www.tutorialspoint.com/websockets/websockets_send_receive_messages.htm)\n- [Introducing WebSockets: Bringing Sockets to the Web](https://www.html5rocks.com/en/tutorials/websockets/basics/)\n\n","tags":["WebSocket"],"categories":["前端"]},{"title":"定制支持串口安装的ubuntu系统镜像","url":"/2017-05-15/reference/定制支持串口安装的ubuntu系统镜像/","content":"\n## 1、所需环境：\n\n**硬件环境：**\n\n* 笔记本\n* 串口调试线缆\n* 光盘\n* 显示器\n* FWA产品的任一机型（此次使用的是FWA-4210）\n* SATA或者USB光驱×1\n\n<!-- more -->\n\n**软件环境：**\n\n* 带有genisoimage(旧版是mkisofs)的linux发行版（此次使用的是Ubuntu 16.04 server版）\n* Ubuntu官网通用镜像ISO文件\n\n## 2、操作过程：\n\n### 2.1 开机进入系统，将光盘挂载到Ubuntu系统 \n\nCLI命令如下；\n\n```bash\n$ mount -o loop ubuntu-16.04.2-server-amd64.iso /mnt/temp\n```\n\n### 2.2 更改配置\n\n相关配置文件（menu.cfg、txt.cfg、isolinux.cfg此文件不是必须要修改，具体见下边解释）。将光盘文件，拷贝到临时目录（家目录或者自己新建目录均可，但建议拷贝到/var或/temp目录下），具体命令如下：\n\n```bash\n$ cp -rf /mnt/temp/ /var/mycdrom\n```\n\n因为 `/mnt` 目录的默认权限是 `333` ，所以在此使用 `-r` 和 `-f` 参数，`-r` 代表递归，即文件夹下所有文件都拷贝，`-f` 代表强制执行；\n\n更改 `menu.cfg` 文件，如下图，主要是注释掉标准安装的配置文件，以便可以定制安装。\n\n```bash\n$ cd /var/mycdrom/temp/isolinux\n\n$ vi menu.cfg\n```\n\n**注：**\n\n> vi有三种模式，普通模式、编辑模式、命令行模式；\n>\n> I o a进入编辑模式，\n>\n> 普通模式下数字+yy复制\n>\n> P黏贴\n>\n> 命令行模式：w写入，q离开，！强制执行\n\n注释 `menu.cfg` 内容如下红框所示：\n\n![](http://i.imgur.com/JM99sFZ.png)\n\n更改 `txt.cfg` 文件，主要用于定制串口安装（如下图）：\n\n![](http://i.imgur.com/pA7ruhP.png)\n\n更改 `isolinux.cfg` 文件，主要修改grub菜单等待时间（如下图），也可不修改；\n\n![](http://i.imgur.com/jE4zoMG.png)\n\n### 2.3 重新打包ISO文件\n\n命令如下：\n\n```bash\n$ genisoimage -o ubuntu-16.04.2-server-adm64-console_115200.iso -r -J -no-emul-boot -boot-load-size 4 -boot-info-table -b isolinux/isolinux.bin -c isolinux/boot.cat /var/mycdrom/temp\n```\n\n`genisoimage` 是linux各大发行版制作ISO镜像比较流行的工具，若要定制系统，最好在linux下更改相关配置，并使用此工具重新打包；若在Windows平台使用UltraISO等工具解压更改重新打包会出现不稳定的情况（无法找到镜像，无法找到安装源等）。\n\n* `-o` ：是output缩写，用来指定输出镜像名称\n* `-r` ： 即rational-rock，用来开放ISO文件所有权限（r、w、x） \n* `-J` ： 即Joliet，一种ISO9600扩展格式，用来增加兼容性，最好加上\n* `-no-emul-boot`  `-boot-load-size 4`  `-boot-info-table` ：指定兼容模式下虚拟扇区的数量，若不指定，有些BISO会出现一些问题\n* `-b` ：指定开机映像文件\n* `-c` ：具体开机配置文件\n* 最后加上输出目录\n\nReboot系统U盘启动，即可安装系统。\n\n## 3、文本安装系统注意事项\n\n### 3.1 进入安装模式\n\n关闭系统插入U盘，启动系统，看到如下提示按F12进入安装系统模式：\n\n```text\nPress  F12  for  boot  menu..\n```\n\n选择U盘所在的选项。\n\n### 3.2 分区\n\n若是硬盘已有linux发行版系统，那在如下界面，必须umount分区，才能将更改写入分区表\n\n\n\n\n\n### 3.3 自动更新\n\n如下界面，若有特许需求（需要安装一些特许软件apache、weblogic等）可以选择自动更新（需要联网），一般情况不选则自动更新\n","tags":["ubuntu"],"categories":["Ubuntu"]},{"title":"Shell脚本攻略笔记","url":"/2017-05-15/reference/Shell脚本攻略笔记/","content":"\n## 1. 基本命令\n\n### 1.1 shell 格式输出\n\n```bash\n$ echo 'Hello world !'\n-n\t# 忽略结尾的换行符\n-e\t# 激活转义字符\n-E\t# disable转义字符\n\n# echo会将一个换行符追加到输出文本的尾部。可以使用选项-n来忽略结尾的换行符。\n\n$ echo -e \"1\\t2\\t3\"\n```\n\n<!-- more -->\n\n\n打印彩色输出：\n\n```bash\n# 彩色文本\n# 重置=0，黑色=30，红色=31，绿色=32，黄色=33，蓝色=34，洋红=35，青色=36，白色=37\n$ echo -e \"\\e[1;31m This is red text \\e[0m\"\n\n# 彩色背景\n# 重置=0，黑色=40，红色=41，绿色=42，黄色=43，蓝色=44，洋红=45，青色=46，白色=47\n$ echo -e \"\\e[1;42m Green Background \\e[0m\"\n```\n\n```bash\n$ printf \"%-5s %-10s %-4s\\n\" No Name Mark\n```\n\n**原理：**\n\n` %-5s` 指明了一个格式为左对齐且宽度为5的字符串替换（ `- `表示左对齐）。如果不用 `-` 指定对齐方式，字符串就采用右对齐形式。\n\n`%s` 、 `%c` 、`%d` 和 `%f` 都是格式替换符（format substitution character），其所对应的参数可以置于带引号的格式字符串之后。 \n\n### 1.2 替换命令 tr\n\n```bash\n# tr 是 translate的简写\n$ tr '\\0' '\\n'\t\t# 将 \\0 替换成 \\n\n$ tr [选项]… 集合1 [集合2]\n选项说明：\n-c, -C, –complement 用集合1中的字符串替换，要求字符集为ASCII。\n-d, –delete 删除集合1中的字符而不是转换\n-s, –squeeze-repeats 删除所有重复出现字符序列，只保留第一个；即将重复出现字符串压缩为一个字符串。\n-t, –truncate-set1 先删除第一字符集较第二字符集多出的字符\n\n字符集合的范围：\n\\NNN 八进制值的字符 NNN (1 to 3 为八进制值的字符)\n\\\\ 反斜杠\n\\a Ctrl-G 铃声\n\\b Ctrl-H 退格符\n\\f Ctrl-L 走行换页\n\\n Ctrl-J 新行\n\\r Ctrl-M 回车\n\\t Ctrl-I tab键\n\\v Ctrl-X 水平制表符\nCHAR1-CHAR2 从CHAR1 到 CHAR2的所有字符按照ASCII字符的顺序\n[CHAR*] in SET2, copies of CHAR until length of SET1\n[CHAR*REPEAT] REPEAT copies of CHAR, REPEAT octal if starting with 0\n[:alnum:] 所有的字母和数字\n[:alpha:] 所有字母\n[:blank:] 水平制表符，空白等\n[:cntrl:] 所有控制字符\n[:digit:] 所有的数字\n[:graph:] 所有可打印字符，不包括空格\n[:lower:] 所有的小写字符\n[:print:] 所有可打印字符，包括空格\n[:punct:] 所有的标点字符\n[:space:] 所有的横向或纵向的空白\n[:upper:] 所有大写字母\n```\n\n### 1.3 打印变量\n\n```bash\n$ var=\"value\"\n$ echo $var\n或者\n$ echo ${var}\n```\n\n### 1.4 设置环境变量\n\n```bash\n# 在PATH中添加一条新路径\n$ export PATH=\"$PATH:/home/user/bin\"\n也可以使用：\n$ PATH=\"$PATH:/home/user/bin\"\n$ export PATH\n```\n\n### 1.5 Shell中三种引号的用法\n\n```bash\n# 单引号\n# 使用单引号时，变量不会被扩展（expand），将依照原样显示。\n$ var=\"123\"\n$ echo '$var' will print $var\n结果为：'$var' will print 123\n\n# 双引号\n# 输出引号中的内容，若存在命令、变量等，会先执行命令解析出结果再输出\n$ echo \"$var\" will print $var\n结果为：123 will print 123\n\n# 反引号\n# 命令替换\n$ var=`whoami`\n$ echo $var\n结果为：root\n\n# 备注：反引号和$()作用相同\n```\n\n### 1.6 获得字符串的长度\n\n```bash\n# 用法\n$ length=${#var}\n\n$ var=12345678901234567890\n$ echo ${#var}\n20\n```\n\n### 1.7 识别当前shell\n\n```bash\n$ echo $SHELL\n也可以使用：\n$ echo $0\n```\n\n### 1.8 使用shell进行数学运算\n\n在Bash shell环境中，可以利用 `let`、` (( ))` 和`[]` 执行基本的算术操作。而在进行高级操作时，`expr` 和 `bc` 这两个工具也会非常有用。\n\n使用 `let` 时，变量名之前不需要再添加 `$`\n\n```bash\n$ no1=4\n$ let no1++\n$ let no1+=6\t# 等同于let no=no+6\n```\n\n```bash\n# 操作符[]的使用方法和let命令类似\n$ result=$[ no1 + no2 ]\n# 在[]中也可以使用$前缀\n$ result=$[ $no1 + 5 ]\n```\n\n```bash\n# 使用(())时，变量名之前需要加上$\n$ result=$(( no1 + 50 ))\n```\n\n```bash\n# expr同样可以用于基本算术操作\n$ result=`expr 3 + 4`\n$ result=$(expr $no1 + 5)\n```\n\nbc是一个用于数学运算的高级工具，这个精密计算器包含了大量的选项 。此处不多介绍。\n\n### 1.9 shell中各种括号的作用()、(())、[\\]、[[]]、{}\n\n#### 1.9.1 小括号，圆括号（）\n\n1、单小括号 ( )\n\n* **命令组。**括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用。\n* **命令替换。**等同于`cmd`，shell扫描一遍命令行，发现了`$(cmd)结构` ，便将 `$(cmd)` 中的cmd执行一次，得到其标准输出，再将此输出放到原来命令。有些shell不支持，如tcsh。\n* **用于初始化数组。**如：array=(a b c d)。\n\n\n2、双小括号 (( ))\n\n* **整数扩展。**这种扩展计算是整数型的计算，不支持浮点型。((exp))结构扩展并计算一个算术表达式的值，如果表达式的结果为0，那么返回的退出状态码为1，或者 是\"假\"，而一个非零值的表达式所返回的退出状态码将为0，或者是\"true\"。若是逻辑判断，表达式exp为真则为1,假则为0。\n* **只要括号中的运算符、表达式符合C语言运算规则，都可用在 `$((exp))`中，甚至是三目运算符**。作不同进位(如二进制、八进制、十六进制)运算时，输出结果全都自动转化成了十进制。如：echo $((16#5f)) 结果为95 (16进位转十进制)。\n* **单纯用 (( )) 也可重定义变量值**，比如 a=5; ((a++)) 可将 $a 重定义为6。\n* **常用于算术运算比较，双括号中的变量可以不使用`$` 符号前缀**。括号内支持多个表达式用逗号分开。 只要括号中的表达式符合C语言运算规则,比如可以直接使用for((i=0;i<5;i++)), 如果不使用双括号, 则为for i in `seq 0 4`或者for i in {0..4}。再如可以直接使用 `if (($i<5))` , 如果不使用双括号, 则为 `if [ $i -lt 5 ]` 。\n\n#### 1.9.2 中括号，方括号[]\n\n1、单中括号 []\n\n* bash 的内部命令，[和test是等同的。如果我们不用绝对路径指明，通常我们用的都是bash自带的命令。if/test结构中的左中括号是调用test的命令标识，右中括号是关闭条件判断的。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码。if/test结构中并不是必须右中括号，但是新版的Bash中要求必须这样。\n* Test和[]中可用的比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq，-gt这种形式。无论是字符串比较还是整数比较都不支持大于号小于号。如果实在想用，对于字符串比较可以使用转义形式，如果比较\"ab\"和\"bc\"：[ ab \\< bc ]，结果为真，也就是返回状态为0。[ ]中的逻辑与和逻辑或使用-a 和-o 表示。\n* 字符范围。用作正则表达式的一部分，描述一个匹配的字符范围。作为test用途的中括号内不能使用正则。\n* 在一个array 结构的上下文中，中括号用来引用数组中每个元素的编号。\n\n2、双中括号 [[ ]]\n\n* [[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。\n* 支持字符串的模式匹配，使用=~操作符时甚至支持shell的正则表达式。字符串比较时可以把右边的作为一个模式，而不仅仅是一个字符串，比如[[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号。\n* 使用[[ ... ]]条件判断结构，而不是[ ... ]，能够防止脚本中的许多逻辑错误。比如，&&、||、<和> 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用 `if [[ $a != 1 && $a != 2 ]]` , 如果不适用双括号, 则为 `if [ $a -ne 1] && [ $a != 2 ] `或者 `if [ $a -ne 1 -a $a != 2 ]` 。\n* bash把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码。\n\n#### 1.9.3 大括号、花括号 {}\n\n1）常规用法\n\n* 大括号拓展。(通配(globbing))将对大括号中的文件名做扩展。在大括号中，不允许有空白，除非这个空白被引用或转义。第一种：对大括号中的以逗号分割的文件列表进行拓展。如 touch {a,b}.txt 结果为a.txt b.txt。第二种：对大括号中以点点（..）分割的顺序文件列表起拓展作用，如：touch {a..d}.txt 结果为a.txt b.txt c.txt d.txt\n* 代码块，又被称为内部组，这个结构事实上创建了一个匿名函数 。与小括号中的命令不同，大括号内的命令不会新开一个子shell运行，即脚本余下部分仍可使用括号内变量。括号内的命令间用分号隔开，最后一个也必须有分号。{}的第一个命令和左括号之间必须要有一个空格。\n\n2）几种特殊的替换结构\n\n```bash\n${var:-string},${var:+string},${var:=string},${var:?string}\n```\n\n* `${var:-string}` 和 `${var:=string}:` 若变量var为空，则用在命令行中用string来替换 `${var:-string}`，否则变量var不为空时，则用变量var的值来替换 `${var:-string}` ；对于 `${var:=string}` 的替换规则和 `${var:-string}` 是一样的，所不同之处是 `${var:=string}` 若var为空时，用string替换 `${var:=string}` 的同时，把string赋给变量 `var： ${var:=string}` 很常用的一种用法是，判断某个变量是否赋值，没有的话则给它赋上一个默认值。\n* `${var:+string}` 的替换规则和上面的相反，即只有当var不是空的时候才替换成string，若var为空时则不替换或者说是替换成变量 var的值，即空值。(因为变量var此时为空，所以这两种说法是等价的) 。\n* `${var:?string}` 替换规则为：若变量var不为空，则用变量var的值来替换 `${var:?string}` ；若变量var为空，则把string输出到标准错误中，并从脚本中退出。我们可利用此特性来检查是否设置了变量的值。\n\n补充扩展：在上面这五种替换结构中string不一定是常值的，可用另外一个变量的值或是一种命令的输出。\n\n3）四种模式匹配替换结构\n\n模式匹配记忆方法：\n\n```\n# 是去掉左边(在键盘上#在$之左边)\n% 是去掉右边(在键盘上%在$之右边)\n#和%中的单一符号是最小匹配，两个相同符号是最大匹配。\n```\n\n```bash\n${var%pattern},${var%%pattern},${var#pattern},${var##pattern}\n```\n\n* 第一种模式：`${variable%pattern}` ，这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最短的匹配模式\n\n\n* 第二种模式：`${variable%%pattern}`，这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式\n* 第三种模式：`${variable#pattern}` 这种模式时，shell在variable中查找，看它是否一给的模式pattern开始，如果是，就从命令行把variable中的内容去掉左边最短的匹配模式\n* 第四种模式：`${variable##pattern}` 这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式\n\n这四种模式中都不会改变variable的值，其中，只有在pattern中使用了匹配符号时，%和%%，#和##才有区别。结构中的pattern支持通配符，表示零个或多个任意字符，?表示仅与一个任意字符匹配，[...]表示匹配中括号里面的字符，[!...]表示不匹配中括号里面的字符。\n\n4）字符串提取和替换\n\n```bash\n${var:num},${var:num1:num2},${var/pattern/pattern},${var//pattern/pattern}\n```\n\n* 第一种模式：`${var:num}` ，这种模式时，shell在var中提取第num个字符到末尾的所有字符。若num为正数，从左边0处开始；若num为负数，从右边开始提取字串，但必须使用在冒号后面加空格或一个数字或整个num加上括号，如 `${var: -2}` 、`${var:1-3}` 或 `${var:(-2)}`。         \n* 第二种模式：`${var:num1:num2}`，num1是位置，num2是长度。表示从 `$var字符串的第$num1` 个位置开始提取长度为$num2的子串。不能为负数。\n* 第三种模式：`${var/pattern/pattern}`表示将var字符串的第一个匹配的pattern替换为另一个pattern。。         \n* 第四种模式：`${var//pattern/pattern}` 表示将var字符串中的所有能匹配的pattern替换为另一个pattern。\n\n#### 1.9.4 符号$后的括号\n\n* `${a}` 变量a的值, 在不引起歧义的情况下可以省略大括号。\n* `$(cmd)`  命令替换，和`cmd`效果相同，结果为shell命令cmd的输，过某些Shell版本不支持 `$()` 形式的命令替换, 如tcsh。\n* `$((expression))` 和`exprexpression`效果相同, 计算数学表达式exp的数值, 其中exp只要符合[C语言](http://lib.csdn.net/base/c)的运算规则即可, 甚至三目运算符和逻辑表达式都可以计算。\n\n#### 1.9.5 多条命令执行\n\n* 单小括号，`(cmd1;cmd2;cmd3)`  新开一个子shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后可以没有分号。\n* 单大括号，`{ cmd1;cmd2;cmd3;}`  在当前shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后必须有分号, 第一条命令和左括号之间必须用空格隔开。\n\n对 `{}` 和 `()` 而言, 括号中的重定向符只影响该条命令，而括号外的重定向符影响到括号中的所有命令。\n\n### 1.10 Shell特殊变量 `$0, $#, $*, $@, $?, ### 和命令行参数\n\n| 变量   | 含义                                    |\n| ---- | ------------------------------------- |\n| $0   | 当前脚本的文件名。                             |\n| $n   | 传递给脚本或函数的参数。n是一个数字，表示几个参数。            |\n| $#   | 传递给脚本或函数的参数个数。                        |\n| $*   | 传递给脚本或函数的所有参数。                        |\n| $@   | 传递给脚本或函数的所有采纳数。被双引号(\" \")包含是，与$* 稍有不同。 |\n| $?   | 上个命令的退出状态，或函数的返回值。                    |\n| $$   | 当前shell进程ID。对于shell脚本，就是这个脚本所在的进程ID。  |\n\n#### 1.10.1 命令行参数\n\n运行脚本时传递给脚本的参数称为命令行参数。命令行参数用 `$n` 表示，例如，`$1 ` 表示第一个参数，`$2` 表示第二个参数，依次类推。\n\n#### 1.10.2 `$*` 和 `$@` 的区别\n\n`$*` 和 `$@` 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以`\"$1\" \"$2\" … \"$n\"` 的形式输出所有参数。\n\n但是当它们被双引号(\" \")包含时，`\"$*\"` 会将所有的参数作为一个整体，以`\"$1 $2 … $n\"` 的形式输出所有参数；`\"$@\"` 会将各个参数分开，以 `\"$1\" \"$2\" … \"$n\" `的形式输出所有参数。\n\n#### 1.10.3 退出状态\n\n`$?` 可以获取上一个命令的退出状态。所谓退出状态，就是上一个命令执行后的返回结果。\n\n退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。\n\n不过，也有一些命令返回其他值，表示不同类型的错误。\n\n`$?` 也可以表示函数的返回值，此处不展开。\n\n### 1.11 Shell重定向\n\n1、重定向符号\n\n```text\n>               输出重定向到一个文件或设备 覆盖原来的文件\n>!              输出重定向到一个文件或设备 强制覆盖原来的文件\n>>              输出重定向到一个文件或设备 追加原来的文件\n<               输入重定向到一个程序 \n```\n\n2、标准输入刷出\n\n```text\n在 bash 命令执行的过程中，主要有三种输出入的状况，分别是：\n1. 标准输入；代码为 0 ；或称为 stdin ；使用的方式为 <\n2. 标准输出：代码为 1 ；或称为 stdout；使用的方式为 1>\n3. 错误输出：代码为 2 ；或称为 stderr；使用的方式为 2>\n```\n\n3、使用实例\n\n```bash\n# & 是一个描述符，如果1或2前不加&，会被当成一个普通文件。\n# 1>&2 意思是把标准输出重定向到标准错误.\n# 2>&1 意思是把标准错误输出重定向到标准输出。\n# &>filename 意思是把标准输出和标准错误输出都重定向到文件filename中\n\n$ cmd <> file\t\t# 以读写方式打开文件 file\n$ cmd >&n\t\t\t# 将 cmd 的输出发送到文件描述符 n\n$ cmd m>&n\t\t\t# 将本该输出到文件描述符 m 的内容, 发送到文件描述符 n\n$ cmd m<&n \t\t\t# 除了本该从文件描述符 m 处获取输入，改为从文件描述符 n 处获取\n$ cmd >&-\t\t\t# 关闭标准输出\n$ cmd <&-\t\t\t# 关闭标准输入\n$ cmd  >& file\t\t# 将标准输出和标准错误都发送到文件 file \n$ cmd  &> file\t\t# 作用同上, 更好的格式\n```\n\n要在终端中打印stdout，同时将它重定向到一个文件中，那么可以这样使用tee 。\n\n```bash\n# 用法：command | tee FILE1 FILE2\n$ cat a* | tee out.txt | cat -n\n# 默认情况下， tee命令会将文件覆盖，但它提供了一个-a选项，用于追加内容\n$ cat a* | tee -a out.txt | cat –n\n\n# 我们可以使用stdin作为命令参数。只需要将-作为命令的文件名参数即可\n# 用法：$ cmd1 | cmd2 | cmd -\n$ echo who is this | tee -\n```\n\n### 1.12 Shell数组和关联数组\n\n#### 1.12.1 简介\n\n数组是Shell脚本非常重要的组成部分，它借助索引将多个独立的独立的数据存储为一个集合。普通数组只能使用整数作为数组索引，关联数组不仅可以使用整数作为索引，也可以使用字符串作为索引。通常情况下，使用字符串做索引更容易被人们理解。Bash从4.0之后开始引入关联数组。\n\n#### 1.12.2 定义打印普通数组\n\n数组的方法有如下几种：\n\n```bash\n#在一行上列出所有元素\n$ array_var=(1 2 3 4 5 6)\n\n#以“索引-值”的形式一一列出\n$ array_var[0]=\"test1\"\n$ array_var[1]=\"test2\"\n$ array_var[2]=\"test3\"\n```\n\n注意：第一种方法要使用圆括号，否则后面会报错。\n\n数组元素的方法有如下几种：\n\n```bash\n$ echo ${array_var[0]}         #输出结果为 test1\n$ index=2\n$ echo ${array_var[$index]}    #输出结果为 test3\n$ echo ${array_var[*]}         #输出所有数组元素\n$ echo ${array_var[@]}         #输出所有数组元素\n$ echo ${#array_var[*]}        #输出值为 3\n```\n\n注意：在ubuntu 14.04中，shell脚本要以#!/bin/bash开头，且执行脚本的方式为 bash test.sh。\n\n#### 1.12.3 定义打印关联数组\n\n定义关联数组 \n在关联数组中，可以使用任何文本作为数组索引。定义关联数组时，首先需要使用声明语句将一个变量声明为关联数组，然后才可以在数组中添加元素，过程如下：\n\n```bash\n$ declare -A ass_array                           #声明一个关联数组\n$ ass_array=([\"index1\"]=index1 [\"index2\"]=index2)#内嵌“索引-值”列表法\n$ ass_array[\"index3\"]=index3\n$ ass_array[\"index4\"]=index4\n$ echo ${ass_array[\"index1\"]}                    #输出为index1\n$ echo ${ass_array[\"index4\"]}\n$ echo ${!ass_array[*]}                          #输出索引列表\n$ echo ${!ass_array[@]}                          #输出索引列表\n```\n\n注意：对于普通数组，使用上面的方法依然可以列出索引列表，在声明关联数组以及添加数组元素时，都不能在前面添加美元符$。\n\n### 1.13 使用别名\n\nalias命令的作用只是暂时的。一旦关闭当前终端，所有设置过的别名就失效了。为了使别名设置一直保持作用，可以将它放入~/.bashrc文件中。因为每当一个新的shell进程生成时，都会执行 ~/.bashrc中的命令。 \n\n```bash\n$ alias install='sudo apt-get install'\n```\n\n### 1.14 获取、设置日期和延时 \n\n时间方面 :\n\n```bash\n% : 印出\n% %n : 下一行\n%t : 跳格\n%H : 小时(00..23)\n%I : 小时(01..12)\n%k : 小时(0..23)\n%l : 小时(1..12)\n%M : 分钟(00..59)\n%p : 显示本地 AM 或 PM\n%r : 直接显示时间 (12 小时制，格式为 hh:mm:ss [AP]M)\n%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数 %S : 秒(00..61)\n%T : 直接显示时间 (24 小时制)\n%X : 相当于 %H:%M:%S\n%Z : 显示时区\n```\n\n日期方面 :\n\n```bash\n%a : 星期几 (Sun..Sat)\n%A : 星期几 (Sunday..Saturday)\n\n%b : 月份 (Jan..Dec)\n%B : 月份 (January..December)\n\n%y : 年份的最后两位数字 (00.99)\n%Y : 完整年份 (0000..9999)\n\n%c : 直接显示日期与时间\n%d : 日 (01..31)\n%D : 直接显示日期 (mm/dd/yy)\n%h : 同 %b\n%j : 一年中的第几天 (001..366)\n%m : 月份 (01..12)\n%U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形)\n%w : 一周中的第几天 (0..6)\n%W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形)\n%x : 直接显示日期 (mm/dd/yy)\n```\n\n若是不以加号作为开头，则表示要设定时间，而时间格式为 `MMDDhhmm[[CC]YY][.ss]`，其中：\n\n```bash\nMM \t为月份，\nDD \t为日，\nhh \t为小时，\nmm \t为分钟，\nCC \t为年份前两位数字，\nYY \t为年份后两位数字，\nss \t为秒数\n```\n\n参数 :\n\n-d datestr : 显示 datestr 中所设定的时间 (非系统时间)\n\n–help : 显示辅助讯息\n\n-s datestr : 将系统时间设为 datestr 中所设定的时间\n\n-u : 显示目前的格林威治时间\n\n–version : 显示版本编号\n\n例子：\n\n```bash\n$ date\t\t\t\t# 获取日期\n$ date +%s\t\t\t# 打印纪元时\n$ date \"+%d %B %Y\"\t# 用格式串结合 + 作为date命令的参数，可以按照你的选择打印出对应格式的日期\n20 May 2010\n$ date -s \"21 June 2009 11:01:22\" \t# 设置日期和时间\n```\n\n### 1.15 脚本调试\n\n#### 1.15.1使用选项–x，启用shell脚本的跟踪调试功能 \n\n```bash\n$ bash -x script.sh\n```\n\n#### 1.15.2 使用set -x和set +x对脚本进行部分调试 \n\n```typescript\n#!/bin/bash\n#文件名: debug.sh\nfor i in {1..6}\ndo\n\tset -x\n\techo $i\n\tset +x\ndone\necho \"Script executed\"\n```\n\n* set –x：在执行时显示参数和命令。 \n* set +x：禁止调试。 \n* set –v：当命令进行读取时显示输入。 \n* set +v：禁止打印输入。 \n\n#### 1.15.3 通过传递 _DEBUG环境变量调试\n\n```typescript\n#!/bin/bash\nfunction DEBUG()\n{\n\t[ \"$_DEBUG\" == \"on\" ] && $@ || :\n}\nfor i in {1..10}\ndo\n\tDEBUG echo $i\ndone\n```\n\n可以将调试功能置为\"on\"来运行上面的脚本：\n\n```bash\n$ _DEBUG=on ./script.sh\n```\n\n我们在每一个需要打印调试信息的语句前加上DEBUG。如果没有把 _DEBUG=on传递给脚本，那么调试信息就不会打印出来。在Bash中，命令 `:` 告诉shell不要进行任何操作。 \n\n#### 1.15.4 利用shebang来进行调试 \n\nshebang的妙用\n把shebang从 `#!/bin/bash` 改成 `#!/bin/bash -xv`，这样一来，不用任何其他选项就可以启用调试功能了。 \n\n### 1.16 函数参数\n\n```typescript\n$0 \t\t# 脚本名\n$1\t\t# 第一个参数\n$2\t\t# 第二个参数\n$n\t\t# 第n个参数\n\"$@\"\t# 被扩展成 \"$1\" \"$2\" \"$3\"等\n\"$*\"\t# 被扩展成 \"$1c$2c$3\"，其中c是IFS的第一个字符\n\"$@\" 要比\"$*\"用得多。由于 \"$*\"将所有的参数当做单个字符串，因此它很少被使用。\n```\n\n**导出函数：**\n\n函数也能像环境变量一样用export导出，如此一来，函数的作用域就可以扩展到子进程中，例如： \n\n```typescript\nexport -f fname \n```\n\n### 1.17 read命令\n\n```bash\n# 从输入中读取n个字符并存入变量\n$ read -n 2 var\n$ echo $var\n\n# 用无回显的方式读取密码\n$ read -s var\n\n# 显示提示信息\n$ read -p \"Enter input:\" var\n\n# 在特定时(秒)限内读取输入\n$ read -t timeout var\n```\n\n### 1.18 条件比较与测试 \n\n```bash\n# if条件\nif condition\nthen\n\tcommands\nfi\n\n# else if和else\nif condition\nthen\n\tcommands\nelse if condition then\n\tcommands\nelse\n\tcommands\nfi\n```\n\nif的条件判断部分可能会变得很长，但可以用逻辑运算符将它变得简洁一些： \n\n```bash\n[ condition ] && action\t\t # 如果condition为真，则执行action\n[ condition ] || action\t\t # 如果condition为假，则执行action\n```\n\n`&&` 是逻辑与运算符， `||` 是逻辑或运算符。编写Bash脚本时，这是一个很有用的技巧。现在来了解一下条件和比较操作。 \n\n算术比较：\n\n* `-gt` ：大于。 \n* `-lt` ：小于。 \n* `-ge` ：大于或等于。 \n* `-le` ：小于或等于。 \n\n可以按照下面的方法结合多个条件进行测试： \n\n```bash\n[ $var1 -ne 0 -a $var2 -gt 2 ] \t\t#使用逻辑与-a\n[ $var1 -ne 0 -o var2 -gt 2 ] \t\t#逻辑或 -o\n```\n\n文件系统相关测试：\n\n我们可以使用不同的条件标志测试不同的文件系统相关的属性。 \n\n* `[ -f $file_var ]` ：如果给定的变量包含正常的文件路径或文件名，则返回真。 \n* `[ -x $var ]` ：如果给定的变量包含的文件可执行，则返回真。 \n* `[ -d $var ]` ：如果给定的变量包含的是目录，则返回真。 \n* `[ -e $var ]` ：如果给定的变量包含的文件存在，则返回真。 \n* `[ -c $var ]` ：如果给定的变量包含的是一个字符设备文件的路径，则返回真。 \n* `[ -b $var ]` ：如果给定的变量包含的是一个块设备文件的路径，则返回真。 \n* `[ -w $var ]` ：如果给定的变量包含的文件可写，则返回真。 \n* `[ -r $var ]` ：如果给定的变量包含的文件可读，则返回真。 \n* `[ -L $var ]` ：如果给定的变量包含的是一个符号链接，则返回真。 \n\n字符串比较：\n\n<p style=\"color: red;\">使用字符串比较时，最好用双中括号，因为有时候采用单个中括号会产生错误，所以最好避开它们。 </p>\n\n可以用下面的方法检查两个字符串，看看它们是否相同。 \n\n* `[[ $str1 = $str2 ]]`：当str1等于str2时，返回真。也就是说， str1和str2包含\n  的文本是一模一样的。 \n* `[[ $str1 == $str2 ]]` ：这是检查字符串是否相等的另一种写法。 \n\n也可以检查两个字符串是否不同。 \n\n* `[[ $str1 != $str2 ]]` ：如果str1和str2不相同，则返回真。 \n\n我们还可以检查字符串的字母序情况，具体如下所示。 \n\n* `[[ $str1 > $str2 ]]` ：如果str1的字母序比str2大，则返回真。 \n* `[[ $str1 < $str2 ]]` ：如果str1的字母序比str2小，则返回真。 \n* `[[ -z $str1 ]]` ：如果str1包含的是空字符串，则返回真。 \n* `[[ -n $str1 ]]` ：如果str1包含的是非空字符串，则返回真。 \n\n使用逻辑运算符 && 和 || 能够很容易地将多个条件组合起来： \n\n```bash\nif [[ -n $str1 ]] && [[ -z $str2 ]] \nthen\n\tcommands\nfi\n```\n\ntest命令可以用来执行条件检测。用test可以避免使用过多的括号。之前讲过的[]中的测试条件同样可以用于test命令。 \n\n```bash\nif [ $var -eq 0 ]; then echo \"True\"; fi\n# 也可以写成：\nif test $var -eq 0 ; then echo \"True\"; fi\n```\n\n### 补充内容\n\n#### 1. 利用子shell生成一个独立的进程\n\n子shell本身就是独立的进程。可以使用 `( )`操作符来定义一个子shell ：\n\n```typescript\npwd;\n(cd /bin; ls);\npwd;\n```\n\n#### 2. 无限循环的实例\n\n```typescript\nrepeat() { while true; do $@ && return; done }\n```\n\n工作原理：\n\n函数repeat，它包含了一个无限while循环，该循环执行以参数形式（通过 `$@` 访问）传入函数的命令。如果命令执行成功，则返回，进而退出循环。 \n\n**一种更快的做法 ：**\n\n在大多数现代系统中， `true` 是作为 `/bin` 中的一个二进制文件来实现的。<p style=\"color: red;\">这就意味着每执行一次while循环， shell就不得不生成一个进程。</p>如果不想这样，可以使用shell内建的 `:`命令，它总是会返回为0的退出码： \n\n```typescript\nrepeat() { while :; do $@ && return; done } \n```\n\n尽管可读性不高，但是肯定比第一种方法快。 \n\n## 2. 命令之乐\n\n### 2.1 cat命令\n\n```bash\n# 摆脱多余的空白行\n$ cat -s file\n\n# 显示行号\n$ cat -n file\n# -n甚至会为空白行加上行号。如果你想跳过空白行，那么可以使用选项-b。\n```\n\n### 2.2 find命令\n\n```bash\n# 列出当前目录及子目录下所有的文件和文件夹\n$ find base_path\n\n$ find . -print\n# -print指明打印出匹配文件的文件名（路径）。当使用 -print时， '\\n'作为用于对输出的文件名进行分隔。就算你忽略-print， find命令仍会打印出文件名。\n# -print0指明使用'\\0'作为匹配的文件名之间的定界符。\n```\n\n1、find命令有一个选项 `-iname`（忽略字母大小写） \n\n```bash\n$ ls\nexample.txt EXAMPLE.txt file.txt\n$ find . -iname \"example*\" -print\n./example.txt\n./EXAMPLE.txt\n```\n\n2、如果想匹配多个条件中的一个，可以采用OR条件操作 :\n\n```bash\n$ ls\nnew.txt some.jpg text.pdf\n$ find . \\( -name \"*.txt\" -o -name \"*.pdf\" \\) -print\n./text.pdf\n./new.txt\n```\n\n3、选项-path的参数可以使用通配符来匹配文件路径。 `-name` 总是用给定的文件名进行匹配。`-path` 则将文件路径作为一个整体进行匹配。例如 :\n\n```bash\n$ find /home/users -path \"*/slynux/*\" -print\n/home/users/list/slynux.txt\n/home/users/slynux/eg.css\n```\n\n4、选项 `-regex` 的参数和 `-path` 的类似，只不过 `-regex` 是基于正则表达式来匹配文件路径的。 \n\n```bash\n$ ls\nnew.PY next.jpg test.py\n$ find . -regex \".*\\(\\.py\\|\\.sh\\)$\"\n./test.py\n# 类似地， -iregex可以让正则表达式忽略大小写。例如：\n$ find . -iregex \".*\\(\\.py\\|\\.sh\\)$\"\n./test.py\n./new.PY\n```\n\n5、find也可以用“!”否定参数的含义。例如： \n\n```bash\n$ ls\nlist.txt new.PY new.txt next.jpg test.py\n$ find . ! -name \"*.txt\" -print\n.\n./next.jpg\n./test.py\n./new.PY\n```\n\n6、基于目录深度的搜索\n\n```bash\n# 深度选项-maxdepth和 -mindepth来限制find命令遍历的目录深度\n# 下列命令将find命令向下的最大深度限制为1:\n$ find . -maxdepth 1 -name \"f*\" -print\n\n# 打印出深度距离当前目录至少两个子目录的所有文件:\n$ find . -mindepth 2 -name \"f*\" -print\n```\n\n**注：**-maxdepth和-mindepth应该作为find的第三个参数出现。如果作为第4个或之后的参数，就可能会影响到find的效率，因为它不得不进行一些不必要的检查。 \n\n根据文件类型搜索\n\n7、根据文件类型搜索\n\n`-type` 可以对文件搜索进行过滤 \n\n| 文件类型 | 类型参数 |\n| ---- | ---- |\n| 普通文件 | f    |\n| 符号链接 | l    |\n| 目录   | d    |\n| 字符设备 | c    |\n| 块设备  | b    |\n| 套接字  | s    |\n| FIFO | p    |\n\n8、根据文件时间进行搜索\n\n* 访问时间（-atime）：用户最近一次访问文件的时间。\n* 修改时间（-mtime）：文件内容最后一次被修改的时间。\n* 变化时间（-ctime）：文件元数据（例如权限或所有权）最后一次改变的时间。\n\n> -atime、 -mtime、 -ctime可作为find的时间选项。它们可以用整数值指定，单位是天。这些整数值通常还带有 - 或 + ： - 表示小于， + 表示大于。 \n\n```bash\n# 打印出在最近7天内被访问过的所有文件：\n$ find . -type f -atime -7 -print\n\n# 打印出恰好在7天前被访问过的所有文件：\n$ find . -type f -atime 7 -print\n\n# 打印出访问时间超过7天的所有文件：\n$ find . -type f -atime +7 -print\n```\n\n-atime、 -mtime以及-ctime都是基于时间的参数，其计量单位是“天”。还有其他一些基于时间的参数是以分钟作为计量单位的。这些参数包括： \n\n* -amin（访问时间）\n* -mmin（修改时间）\n* -cmin（变化时间）\n\n使用 `-newer` ，我们可以指定一个用于比较时间戳的参考文件，然后找出比参考文件更新的（更近的修改时间）所有文件 \n\n```bash\n# 找出比file.txt修改时间更近的所有文件：\n$ find . -type f -newer file.txt -print\n```\n\n9、基于文件大小的搜索\n\n```bash\n$ find . -type f -size +2k\n# 大于2KB的文件\n\n$ find . -type f -size -2k\n# 小于2KB的文件\n\n$ find . -type f -size 2k\n# 大小等于2KB的文件\n```\n\n* b —— 块（512字节）\n* c —— 字节\n* w —— 字（2字节）\n* k —— 1024字节\n* M —— 1024k字节\n* G —— 1024M字节\n\n10、删除匹配的文件\n\n`-delete` 可以用来删除find查找到的匹配文件。 \n\n```bash\n# 删除当前目录下所有的 .swp文件：\n$ find . -type f -name \"*.swp\" -delete\n```\n\n11、基于文件权限和所有权的匹配\n\n```bash\n$ find . -type f -perm 644 -print\n# 打印出权限为644的文件\n```\n\n-perm指明find应该只匹配具有特定权限值的文件。 \n\n12、利用find执行命令或动作 \n\nfind命令可以借助选项-exec与其他命名进行结合。 -exec算得上是find最强大的特性之一。 \n\n```bash\n$ find . -type f -user root -exec chown slynux {} \\;\n\n# {}是一个与 -exec选项搭配使用的特殊字符串。对于每一个匹配的文件，{}会被替换成相应的文件名。\n```\n\n`-exec` 结合多个命令 :\n\n我们无法在-exec参数中直接使用多个命令。它只能够接受单个命令，不过我们可以耍一个小花招。把多个命令写到一个shell脚本中（例如command.sh），然后在-exec中使用这个脚本：\n\n```typescript\n-exec ./commands.sh {} \\; \n```\n\n13、让find跳过特定的目录\n\n```bash\n$ find devel/source_path \\( -name \".git\" -prune \\) -o \\( -type f -print \\)\n\n# 以上命令打印出不包括在.git目录中的所有文件的名称（路径）。\n```\n\n`\\( -name \".git\" -prune \\)` 的作用是用于进行排除，它指明了 .git目录应该被排除在外，而` \\( -type f -print \\)` 指明了需要执行的动作。这些动作需要被放置在第二个语句块中（打印出所有文件的名称和路径）。 \n\n### 2.3 玩转xargs\n\n`xargs` 擅长将标准输入数据转换成命令行参数。\n\n`xargs` 命令把从 stdin接收到的数据重新格式化，再将其作为参数提供给其他命令。 \n\n####  2.3.1 将多行输入转换成单行输出\n\n只需要将换行符移除，再用\" \"（空格）进行代替，就可以实现多行输入的转换。 \n\n```bash\n$ cat example.txt # 样例文件\n1 2 3 4 5 6\n7 8 9 10\n11 12\n$ cat example.txt | xargs\n1 2 3 4 5 6 7 8 9 10 11 12\n```\n\n#### 2.3.2 将单行输入转换成多行输出\n\n指定每行最大的参数数量 `n`，我们可以将任何来自stdin的文本划分成多行，每行 `n` 个参数。 \n\n```bash\n$ cat example.txt | xargs -n 3\n1 2 3\n4 5 6\n7 8 9\n10 11 12\n```\n\n#### 2.3.3 定制定界符\n\n用 `-d` 选项为输入指定一个定制的定界符： \n\n```bash\n$ echo \"splitXsplitXsplitXsplit\" | xargs -d X\nsplit split split split\n\n$ echo \"splitXsplitXsplitXsplit\" | xargs -d X -n 2\nsplit split\nsplit split\n```\n\n在这里，我们明确指定X作为输入定界符，而在默认情况下， xargs采用内部字段分隔符（空格）作为输入定界符。 \n\n#### 2.3.4 读取stdin，将格式化参数传递给命令 \n\n`-I` 指定替换字符串，这个字符串在xargs扩展时会被替换掉。如果将 `-I` 与 `xargs` 结合使用，对于每一个参数，命令都会被执行一次。 \n\n```bash\n$ cat args.txt\narg1\narg2\narg3\n$ cat args.txt | xargs -I {} ./cecho.sh -p {} -l\n-p arg1 -l #\n-p arg2 -l #\n-p arg3 -l #\n```\n\n`-I {}` 指定了替换字符串。对于每一个命令参数，字符串 `{}` 都会被从stdin读取到的参数替换掉。 \n\n使用 `-I` 的时候，命令以循环的方式执行。 \n\nxargs和find算是一对死党。两者结合使用可以让任务变得更轻松。 不过人们通常却是以一种错误的组合方式使用它们。例如： \n\n```bash\n$ find . -type f -name \"*.txt\" -print | xargs rm -f\n```\n\n这样做很危险。 有时可能会删除不必要删除的文件。 \n\n只要我们把 `find` 的输出作为 `xargs` 的输入，就必须将 `-print0` 与 `find` 结合使用，以字符`null（'\\0'）`来分隔输出。 \n\n```bash\n$ find . -type f -name \"*.txt\" -print0 | xargs -0 rm -f\n# xargs -0将\\0作为输入定界符。\n\n$ find source_code_dir_path -type f -name \"*.c\" -print0 | xargs -0 wc -l\n# 统计源代码目录中所有C程序文件的行数\n```\n\n### 2.4 校验和与核实\n\n校验和（checksum）程序用来从文件中生成校验和密钥，然后利用这个校验和密钥核实文件的完整性。文件可以通过网络或任何存储介质分发到不同的地点。 \n\n最知名且使用最为广泛的校验和技术是md5sum和SHA-1。它们对文件内容使用相应的算法来生成校验和。 \n\n```bash\n$ md5sum filename\n68b329da9893e34099c7d8ad5cb9c940 filename\n\n$ md5sum filename > file_sum.md5\n\n$ md5sum file1 file2 file3 ..\n\n$ md5sum -c file_sum.md5\n# 这个命令会输出校验和是否匹配的消息\n\n# 如果需要用所有的.md5信息来检查所有的文件，可以使用：\n$ md5sum -c *.md5\n```\n\n计算SAH-1串的命令是sha1sum。其用法和md5sum的非常相似。只需要把先前讲过的那些命令中的md5sum替换成sha1sum就行了，记住将输入文件名从file_sum.md5改为file_sum.sha1。 \n\n对目录进行校验：\n\n```bash\n$ md5deep -rl directory_path > directory.md5\n# -r使用递归的方式\n# -l使用相对路径。默认情况下， md5deep会输出文件的绝对路径\n\n# 或者也可以结合find来递归计算校验和：\n$ find directory_path -type f -print0 | xargs -0 md5sum >> directory.md5\n\n# 用下面的命令进行核实：\n$ md5sum -c directory.md5\n```\n\n#### 2.4.1 加密工具与散列 \n\n`crypt`、 `gpg`、 `base64`、 `md5sum`、 `sha1sum` 以及 `openssl` 的用法。 \n\n1）crypt是一个简单的加密工具，它从stdin接受一个文件以及口令作为输入，然后将加密数据输出到Stdout（因此要对输入、输出文件使用重定向）。 \n\n```bash\n$ crypt <input_file >output_file\nEnter passphrase:\n# 它会要求输入一个口令。我们也可以通过命令行参数来提供口令。\n\n$ crypt PASSPHRASE <input_file >encrypted_file\n# 如果需要解密文件，可以使用：\n$ crypt PASSPHRASE -d <encrypted_file >output_file\n```\n\n2）gpg（GNU隐私保护）是一种应用广泛的工具，它使用加密技术来保护文件，以确保数据在送达目的地之前无法被读取。这里我们讨论如何加密、解密文件。 \n\n```bash\n# 用gpg加密文件：\n$ gpg -c filename\n# 该命令采用交互方式读取口令，并生成filename.gpg。使用以下命令解密gpg文件：\n$ gpg filename.gpg\n# 该命令读取口令，然后对文件进行解密。\n```\n\n3）Base64是一组相似的编码方案，它将ASCII字符转换成以64为基数的形式，以可读的ASCII字符串来描述二进制数据。 base64命令可以用来编码/解码Base64字符串。要将文件编码为Base64格式，可以使用： \n\n```bash\n$ base64 filename > outputfile\n# 或者\n$ cat file | base64 > outputfile\n# base64可以从stdin中进行读取。\n\n# 解码Base64数据：\n$ base64 -d file > outputfile\n# 或者\n$ cat base64_file | base64 -d > outputfile\n```\n\n4）md5sum与sha1sum都是单向散列算法，均无法逆推出原始数据。它们通常用于验证数据完整性或为特定数据生成唯一的密钥： \n\n```bash\n$ md5sum file\n8503063d5488c3080d4800ff50850dc9 file\n$ sha1sum file\n1ba02b66e2e557fede8f61b7df282cd0a27b816b file\n```\n\n这种类型的散列算法是存储密码的理想方案。密码使用其对应的散列值来存储。如果某个用户需要进行认证，读取该用户提供的密码并转换成散列值，然后将其与之前存储的散列值进行比对。如果相同，用户就通过认证，被允许访问；否则，就会被拒绝访问。 \n\n5）openssl\n\n用openssl生成shadow密码。 shadow密码通常都是salt密码。所谓SALT就是额外的一个字符串，用来起一个混淆的作用，使加密更加不易被破解。 salt由一些随机位组成，被用作密钥生成函数的输入之一，以生成密码的salt散列值。 \n\n```bash\n$ opensslpasswd -1 -salt SALT_STRING PASSWORD\n$1$SALT_STRING$323VkWkSLHuhbt1zkSsUG.\n# 将SALT_STRING替换为随机字符串，并将PASSWORD替换成你想要使用的密码。\n```\n\n### 2.5 排序、唯一与重复\n\n```bash\n# 对一组文件进行排序：\n$ sort file1.txt file2.txt > sorted.txt\n\n# 按照数字顺序进行排序：\n$ sort -n file.txt\n\n# 按照逆序进行排序：\n$ sort -r file.txt\n\n# 按照月份进行排序（依照一月，二月，三月……）：\n$ sort -M months.txt\n\n# 合并两个已排序过的文件：\n$ sort -m sorted1 sorted2\n\n# 找出已排序文件中不重复的行：\n$ sort file1.txt file2.txt | uniq\n```\n\n检查文件是否已经排序过：\n\n```typescript\n#!/bin/bash\n#功能描述：排序\nsort -C filename ;\nif [ $? -eq 0 ]; then\n\techo Sorted;\nelse\n\techo Unsorted;\nfi\n```\n\n`-k` 指定了排序应该按照哪一个键（key）来进行。键指的是列号，而列号就是执行排序时的依据。 `-r` 告诉sort命令按照逆序进行排序。例如： \n\n```bash\n# 依据第1列，以逆序形式排序\n$ sort -nrk 1 data.txt\n4 linux 1000\n3 bsd 1000\n2 winxp 4000\n1 mac 2000\n# -nr表明按照数字，采用逆序形式排序\n# 依据第2列进行排序\n$ sort -k 2 data.txt\n3 bsd 1000\n4 linux 1000\n1 mac 2000\n2 winxp 4000\n```\n\n有时文本中可能会包含一些像空格之类的不必要的多余字符。如果需要忽略这些字符，并以字典序进行排序，可以使用：\n\n```bash\n$ sort -bd unsorted.txt\n# 选项-b用于忽略文件中的前导空白行，选项-d用于指明以字典序进行排序。\n```\n\nsort选项：\n\n```bash\n-b：忽略每行前面开始出的空格字符；\n\n-c：检查文件是否已经按照顺序排序； \n\n-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符； \n\n-f：排序时，将小写字母视为大写字母； \n\n-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；\n\n-m：将几个排序号的文件进行合并； \n\n-M：将前面3个字母依照月份的缩写进行排序； \n\n-n：依照数值的大小排序； \n\n-o<输出文件>：将排序后的结果存入制定的文件； \n\n-r：以相反的顺序来排序； \n\n-t<分隔字符>：指定排序时所用的栏位分隔字符； \n\n+<起始栏位>-<结束栏位>：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。\n```\n\nuniq选项：\n\n```bash\n-c或——count：在每列旁边显示该行重复出现的次数； \n\n-d或--repeated：仅显示重复出现的行列； \n\n-f<栏位>或--skip-fields=<栏位>：忽略比较指定的栏位； \n\n-s<字符位置>或--skip-chars=<字符位置>：忽略比较指定的字符； \n\n-u或——unique：仅显示出一次的行列； \n\n-w<字符位置>或--check-chars=<字符位置>：指定要比较的字符。\n```\n\nwc选项：\n\n```bash\n-c或--bytes或——chars：只显示Bytes数； \t\t# 统计字符数\n\n-l或——lines：只显示列数； \t\t\t\t\t# 统计行数\n\n-w或——words：只显示字数。\t\t\t\t\t# 统计单词数\n\n# 当不使用任何选项执行wc时，它会分别打印出文件的行数、单词数和字符数：\n$ wc file\n1435 15763 112200\n\n# 使用-L选项打印出文件中最长一行的长度：\n$ wc file -L\n205\n```\n\n### 2.6 临时文件命名与随机数  \n\n```bash\n# 创建临时文件：\n$ filename=`mktemp`\n$ echo $filename\n/tmp/tmp.8xvhkjF5fH\n\n# 创建临时目录：\n$ dirname=`mktemp -d`\n$ echo $dirname\ntmp.NI8xzW7VRX\n\n# 如果仅仅是想生成文件名，又不希望创建实际的文件或目录，方法如下：\n$ tmpfile=`mktemp -u`\n$ echo $tmpfile\n/tmp/tmp.RsGmilRpcT\n\n# 根据模板创建临时文件名：\n$mktemp test.XXX\ntest.2tc\n```\n\n如果提供了定制模板， X会被随机的字符（字母或数字）替换。注意， mktemp正常工作的前提是保证模板中只少要有3个X。 \n\n### 2.7 split 分割文件和数据 \n\n```bash\n# 将文件分割成多个大小为10KB的文件\n$ split -b 10k data.file\n$ ls\ndata.file xaa xab xac xad xae xaf xag xah xai xaj\n```\n\n上面的命令将data.file分割成多个文件，每一个文件大小为10KB。这些文件以xab、 xac、 xad的形式命名。这表明它们都有一个字母后缀。如果想以数字为后缀，可以另外使用-d参数。此外，使用 -a length可以指定后缀长度： \n\n```bash\n$ split -b 10k data.file -d -a 4\n$ ls\ndata.file x0009 x0019 x0029 x0039 x0049 x0059 x0069 x0079\n```\n\n除了k（KB）后缀，我们还可以使用M（MB）、 G（GB）、 c（byte）、 w（word）等后缀。 \n\n```bash\n# 为分割后的文件指定文件名前缀 \n$ split -b 10k data.file -d -a 4 split_file\n$ ls\ndata.file\t   split_file0002 split_file0005 split_file0008 strtok.c\nsplit_file0000 split_file0003 split_file0006 split_file0009\nsplit_file0001 split_file0004 split_file0007\n\n# 如果不想按照数据块大小，而是需要根据行数来分割文件的话，可以使用 -l no_of_lines：\n$ split -l 10 data.file\n# 分割成多个文件，每个文件包含10行\n```\n\ncsplit。它能够依据指定的条件和字符串匹配选项对日志文件进行分割。 \n\n```bash\n$ cat server.log\nSERVER-1\n[connection] 192.168.0.1 success\n[connection] 192.168.0.2 failed\n[disconnect] 192.168.0.3 pending\n[connection] 192.168.0.4 success\nSERVER-2\n[connection] 192.168.0.1 failed\n[connection] 192.168.0.2 failed\n[disconnect] 192.168.0.3 success\n[connection] 192.168.0.4 failed\nSERVER-3\n[connection] 192.168.0.1 pending\n[connection] 192.168.0.2 pending\n[disconnect] 192.168.0.3 pending\n[connection] 192.168.0.4 failed\n$ csplit server.log /SERVER/ -n 2 -s {*} -f server -b \"%02d.log\" ; rm server00.log\n$ ls\nserver01.log server02.log server03.log server.log\n```\n\n有关这个命令的详细说明如下。 \n\n* /SERVER/ 用来匹配某一行，分割过程即从此处开始。 \n* /[REGEX]/ 表示文本样式。包括从当前行（第一行）直到（但不包括）包含“SERVER”的匹配行。 \n* {*} 表示根据匹配重复执行分割，直到文件末尾为止。可以用{整数}的形式来指定分割执行的次数。 \n* -s 使命令进入静默模式，不打印其他信息。 \n* -n 指定分割后的文件名后缀的数字个数，例如01、 02、 03等。 \n* -f 指定分割后的文件名前缀（在上面的例子中， server就是前缀）。 \n* -b 指定后缀格式。例如%02d.log，类似于C语言中printf的参数格式。在这里文件名=前缀+后缀=server + %02d.log。 \n\n因为分割后的第一个文件没有任何内容（匹配的单词就位于文件的第一行中），所以我们删除了server00.log。 \n\n#### 2.7.1 根据扩展名切分文件名$、## \n\n借助 `%` 操作符可以轻松将名称部分从 “名称.扩展名” 这种格式中提取出来。 \n\n```typescript\nfile_jpg=\"sample.jpg\"\nname=${file_jpg%.*}\necho File name is: $name\n输出结果：\nFile name is: sample\n```\n\n将文件名的扩展名部分提取出来，这可以借助 # 操作符实现。 \n\n```typescript\nextension=${file_jpg#*.}\necho Extension is: jpg\n输出结果：\nExtension is: jpg\n```\n\n`${VAR%.*}`  的含义如下所述： \n\n* 从 $VAR中删除位于 % 右侧的通配符（在前例中是.*）所匹配的字符串。通配符从右向左进行匹配。 \n* 给VAR赋值， VAR=sample.jpg。那么，通配符从右向左就会匹配到.jpg，因此，从 $VAR中删除匹配结果，就会得到输出sample。 \n\n%属于非贪婪（non-greedy）操作。它从右到左找出匹配通配符的最短结果。还有另一个操作符 %%，这个操作符与%相似，但行为模式却是贪婪的，这意味着它会匹配符合条件的最长的字符串。 \n\n操作符%%则用.*从右向左执行贪婪匹配（.fun.book.txt）。 \n\n`${VAR#*.}` 的含义如下所述：\n从$VAR中删除位于#右侧的通配符（即在前例中使用的*.）所匹配的字符串。通配\n符从左向右进行匹配。\n和 %% 类似， #也有一个相对应的贪婪操作符 ##。\n\n`##`从左向右进行贪婪匹配，并从指定变量中删除匹配结果。\n\n这里有个能够提取域名不同部分的实用案例。假定 `URL=\"www.google.com\"`：\n\n```bash\n$ echo ${URL%.*} # 移除.*所匹配的最右边的内容\nwww.google\n$ echo ${URL%%.*} # 将从右边开始一直匹配到最左边的*.移除（贪婪操作符）\nwww\n$ echo ${URL#*.} # 移除*.所匹配的最左边的内容\ngoogle.com\n$ echo ${URL##*.} # 将从左边开始一直匹配到最右边的*.移除（贪婪操作符）\ncom\n```\n\n### 2.8 批量重命名和移动  \n\n```bash\n# 将 *.JPG更名为 *.jpg：\n$ rename *.JPG *.jpg\n\n# 将文件名中的空格替换成字符“_”：\n$ rename 's/ /_/g' *\n\n# 转换文件名的大小写：\n$ rename 'y/A-Z/a-z/' *\n$ rename 'y/a-z/A-Z/' *\n\n# 将所有的 .mp3文件移入给定的目录：\n$ find path -type f -name \"*.mp3\" -exec mv {} target_dir \\;\n\n# 将所有文件名中的空格替换为字符“_”：\n$ find path -type f -exec rename 's/ /_/g' {} \\;\n```\n\n## 3 以文件之名\n\n### 3.1 生成任意大小的文件\n\n```bash\n$ dd if=/dev/zero of=junk.data bs=1M count=1\n```\n\n该命令会创建一个1MB大小的文件junk.data。来看一下命令参数： if代表输入文件（input file），of代表输出文件（output file）， bs代表以字节为单位的块大小（block size）， count代表需要被复制的块数。\n\n使用dd命令时一定得留意，该命令运行在设备底层。要是你不小心出了岔子，搞不好会把磁盘清空或是损坏数据。所以一定要反复检查dd命令所用的语法是否正确，尤其是参数of=。 \n\n| 单元大小        | 代码   |\n| ----------- | ---- |\n| 字节（1B）      | c    |\n| 字（2B）       | w    |\n| 块（512B）     | b    |\n| 千字节（1024B）  | k    |\n| 兆字节（1024KB） | M    |\n| 吉字节（1024MB） | G    |\n\n`ls -lS` 对当前目录下的所有文件按照文件大小进行排序，并列出文件的详细信息。  \n\n### 3.2 文件权限、所有权和粘滞位 \n\n用命令ls -l可以列出文件的权限： \n\n```\n-rw-r--r-- 1 slynux slynux 2497 2010-02-28 11:22 bot.py\ndrwxr-xr-x 2 slynux slynux 4096 2010-05-27 14:31 a.py\n-rw-r--r-- 1 slynux slynux 539 2010-02-10 09:11 cl.pl\n```\n\n* `-`—— 普通文件。 \n* d —— 目录。 \n* c —— 字符设备。 \n* b —— 块设备。 \n* l —— 符号链接。 \n* s —— 套接字。 \n* p —— 管道。 \n\n```bash\n# 更改所有权\n$ chown user.group filename\n\n# 设置粘滞位\n# 要设置粘滞位，利用chmod将 +t应用于目录：\n$ chmod a+t directory_name\n\n# 以递归的方式设置权限\n$ chmod 777 . -R\n\n# 以递归的方式设置所有权\n$ chown user.group . -R\n```\n\n### 3.3 创建不可修改的文件\n\nchattr能够将文件设置为不可修改。 \n\n```bash\n# 使用下列命令将一个文件设置为不可修改：\n$ chattr +i file\n\n# 如果需要使文件恢复可写状态，移除不可修改属性即可：\n$ chattr -i file\n```\n\n### 3.4 查找符号链接及其指向目标 \n\n```bash\n# 创建符号链接：\n$ ln -s target symbolic_link_name\n例如：\n$ ln -l -s /var/www/ ~/web\n#这个命令在已登录用户的home目录中创建了一个名为Web的符号链接。该链接指向/var/www。\n\n# 使用下面的命令来验证是否创建链接：\n$ ls -l web\nlrwxrwxrwx 1 slynux slynux 8 2010-06-25 21:34 web -> /var/www\n\n# 打印出当前目录下的符号链接：\n$ ls -l | grep \"^l\"\n\n# 使用find打印当前目录以及子目录下的符号链接：\n$ find . -type l -print\n\n# 使用readlink打印出符号链接所指向的目标路径：\n$ readlink web\n/var/www\n```\n\n### 3.5 列举文件类型统计信息\n\n```bash\n# 用下面的命令打印文件类型信息：\n$ file filename\n$ file /etc/passwd\n/etc/passwd: ASCII text\n\n# 打印不包括文件名在内的文件类型信息：\n$ file -b filename\nASCII text\n```\n\n### 3.6 使用环回文件 \n\n```bash\n# 下面的命令可以创建一个1GB大小的文件：\n$ dd if=/dev/zero of=loobackfile.img bs=1G count=1\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 37.3155 s, 28.8 MB/s\n# 你会发现创建好的文件大小超过了1GB。这是因为硬盘作为块设备，其分配存储空间时是按照块大小的整数倍来进行的。\n\n# 用mkfs命令将1GB的文件格式化成ext4文件系统：\n$ mkfs.ext4 loopbackfile.img\n\n# 使用下面的命令检查文件系统：\n$ file loobackfile.img\nloobackfile.img: Linux rev 1.0 ext4 filesystem data,\nUUID=c9d56c42-f8e6-4cbd-aeab-369d5056660a (extents) (large files) (huge files)\n\n# 现在就可以挂载环回文件了：\n$ mkdir /mnt/loopback\n$ mount -o loop loopbackfile.img /mnt/loopback\n# -o loop用来挂载环回文件系统。\n\n# 我们也可以手动来操作：\n$ losetup /dev/loop1 loopbackfile.img\n$ mount /dev/loop1 /mnt/loopback\n\n# 使用下面的方法进行卸载（umount）：\n$ umount mount_point\n```\n\n### 3.7 生成 ISO 文件及混合型 ISO  \n\n```bash\n#用下面的命令从/dev/cdrom创建一个ISO镜像：\n$ cat /dev/cdrom > image.iso\n\n#尽管可以奏效。但创建ISO镜像最好的方法还是使用dd工具：\n$ dd if=/dev/cdrom of=image.iso\n\n# mkisofs命令用于创建ISO文件系统。\n$ mkisofs -V \"Label\" -o image.iso source_dir/\n# 选项 -o指定了ISO文件的路径。 source_dir是作为ISO文件内容来源的目录路径，选项 -V指定了ISO文件的卷标。\n```\n\n### 3.8 diff命令\n\n```bash\n- 　\t\t\t\t# 指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。\n-a或--text 　\t\t# diff预设只会逐行比较文本文件。\n-b或--ignore-space-change 　# 不检查空格字符的不同。\n-B或--ignore-blank-lines 　 # 不检查空白行。\n-c 　\t\t\t# 显示全部内文，并标出不同之处。\n-C或--context \t# 与执行\"-c-\"指令相同。\n-d或--minimal \t# 使用不同的演算法，以较小的单位来做比较。\n-D或ifdef\t\t# 此参数的输出格式可用于前置处理器巨集。\n-e或--ed\t\t\t# 此参数的输出格式可用于ed的script文件。\n-f或-forward-ed\t# 输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。\n-H或--speed-large-files 　\t# 比较大文件时，可加快速度。\n-l或--ignore-matching-lines 　# 若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。\n-i或--ignore-case 　# 不检查大小写的不同。\n-l或--paginate\t   # 将结果交由pr程序来分页。\n-n或--rcs 　\t\t  # 将比较结果以RCS的格式来显示。\n-N或--new-file 　\t  # 在比较目录时，若文件A仅出现在某个目录中，预设会显示：Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。\n-p 　\t\t\t  # 若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。\n-P或--unidirectional-new-file 　# 与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。\n-q或--brief 　\t# 仅显示有无差异，不显示详细的信息。\n-r或--recursive 　# 比较子目录中的文件。\n-s或--report-identical-files 　# 若没有发现任何差异，仍然显示信息。\n-S或--starting-file 　# 在比较目录时，从指定的文件开始比较。\n-t或--expand-tabs 　\t# 在输出时，将tab字符展开。\n-T或--initial-tab 　\t# 在每行前面加上tab字符以便对齐。\n-u,-U或--unified= 　\t# 以合并的方式来显示文件内容的不同。\n-v或--version 　\t\t# 显示版本信息。\n-w或--ignore-all-space 　# 忽略全部的空格字符。\n-W或--width 　\t\t# 在使用-y参数时，指定栏宽。\n-x或--exclude 　\t\t# 不比较选项中所指定的文件或目录。\n-X或--exclude-from 　 # 您可以将文件或目录类型存成文本文件，然后在=中指定此文本文件。\n-y或--side-by-side 　 # 以并列的方式显示文件的异同之处。\n--help 　\t\t\t # 显示帮助。\n--left-column 　\t\t# 在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。\n--suppress-common-lines 　# 在使用-y参数时，仅显示不同之处。\n```\n\n生成目录的差异信息 ：\n\n```bash\n$ diff -Naur directory1 directory2\n```\n\n* -N：将所有缺失的文件视为空文件。 \n* -a：将所有文件视为文本文件。 \n* -u：生成一体化输出。 \n* -r：遍历目录下的所有文件。 \n\n```bash\n# 生成patch文件\n$ diff -u version1.txt version2.txt > version.patch\n\n# 用下列命令来进行修补：\n$ patch -p1 version1.txt < version.patch\npatching file version1.txt\n# version1.txt的内容现在和verson2.txt的内容一模一样。\n\n# 下面的命令可以撤销做出的修改：\n$ patch -p1 version1.txt < version.patch\npatching file version1.txt\nReversed (or previously applied) patch detected! Assume -R? [n] y\n# 修改被撤销\n```\n\n### 3.9 more、less、head与tail命令\n\n#### 3.9.1 more文件内容输出查看工具\n\n```bash\n$ more [参数选项] [文件] \n\n# 参数如下： \n+num   \t\t# 从第num行开始显示； \n-num   \t\t# 定义屏幕大小，为num行； \n+/pattern   # 从pattern 前两行开始显示； \n-c   \t\t# 从顶部清屏然后显示； \n-d   \t\t# 提示Press space to continue, 'q' to quit.（按空格键继续，按q键退出），禁用响铃功能； \n-l    \t\t# 忽略Ctrl+l （换页）字符； \n-p    \t\t# 通过清除窗口而不是滚屏来对文件进行换页。和-c参数有点相似； \n-s    \t\t# 把连续的多个空行显示为一行； \n-u    \t\t# 把文件内容中的下划线去掉退出more的动作指令是q \n```\n\n举例：\n\n```bash\n# 显示提示，并从终端或控制台顶部显示；\n$ more -dc /etc/profile \n\n# 从profile的第4行开始显示；\n$ more +4 /etc/profile     \n\n# 每屏显示4行；\n$ more -4 /etc/profile    \n\n# 从profile中的第一个MAIL单词的前两行开始显示；\n$ more +/MAIL /etc/profile   \n```\n\nmore 的动作指令：\n\n```typescript\nEnter       \t# 向下n行，需要定义，默认为1行； \nCtrl+f    \t\t# 向下滚动一屏； \n空格键\t\t\t # 向下滚动一屏； \nCtrl+b  \t\t# 返回上一屏； \n=         \t\t# 输出当前行的行号； \n:f      \t\t# 输出文件名和当前行的行号； \nv      \t\t\t# 调用vi编辑器； \n! 命令          # 调用Shell，并执行命令； \nq     \t\t\t# 退出more当我们查看某一文件时，想调用vi来编辑它，不要忘记了v动作指令，这是比较方便的； \n```\n\n其它命令通过管道和more结合的运用例子：\n\n```bash\n$ ls -l /etc |more \n```\n\n#### 3.9.2 less查看文件内容工具\n\n```bash\n-c \t\t# 从顶部（从上到下）刷新屏幕，并显示文件内容。而不是通过底部滚动完成刷新； \n-f \t\t# 强制打开文件，二进制文件显示时，不提示警告； \n-i \t\t# 搜索时忽略大小写；除非搜索串中包含大写字母； \n-I \t\t# 搜索时忽略大小写，除非搜索串中包含小写字母； \n-m \t\t# 显示读取文件的百分比； \n-M \t\t# 显法读取文件的百分比、行号及总行数； \n-N \t\t# 在每行前输出行号； \n-p \t\t# pattern 搜索pattern；比如在/etc/profile搜索单词MAIL，就用 less -p MAIL /etc/profile \n-s \t\t# 把连续多个空白行作为一个空白行显示； \n-Q \t\t# 在终端下不响铃； \n```\n\nless的动作命令：\n\n```bash\n回车键 \t# 向下移动一行； \ny \t\t  # 向上移动一行； \n空格键 \t# 向下滚动一屏； \nb \t\t  # 向上滚动一屏； \nd \t\t  # 向下滚动半屏； \nh \t\t  # less的帮助； \nu \t\t  # 向上洋动半屏； \nw \t\t  # 可以指定显示哪行开始显示，是从指定数字的下一行显示；比如指定的是6，那就从第7行显示； \ng \t\t  # 跳到第一行； \nG \t\t  # 跳到最后一行； \np \t\t  # n% 跳到n%，比如 10%，也就是说比整个文件内容的10%处开始显示； \n/pattern  # 搜索pattern ，比如 /MAIL表示在文件中搜索MAIL单词； \nv \t\t  # 调用vi编辑器； \nq \t\t  # 退出less \n!command  # 调用SHELL，可以运行命令；比如!ls 显示当前列当前目录下的所有文件； \n```\n\n#### 3.9.3 head\n\nhead 是显示一个文件的内容的前多少行：\n\n```bash\n$ head -n 10 /etc/profile \n```\n\n#### 3.9.4 tail\n\ntail 是显示一个文件的内容的最后多少行：\n\n```bash\n$ tail -n 5 /etc/profile \n```\n\n### 3.10 getopts 参数解析\n\n#### 3.10.1 getopts（shell内置命令）\n\n```bash\n$ type getopt\ngetopt 是 /usr/bin/getopt\n$ type getopts \ngetopts 是 shell 内建\n```\n\ngetopts不能直接处理长的选项（如：--prefix=/home等）\n\n关于getopts的使用方法，可以man bash  搜索getopts。\n\ngetopts有两个参数，第一个参数是一个字符串，包括字符和“：”，每一个字符都是一个有效的选项，如果字符后面带有“：”，表示这个字符有自己的参数。getopts从命令中获取这些参数，并且删去了“-”，并将其赋值在第二个参数中，如果带有自己参数，这个参数赋值在 `$OPTARG`中。提供getopts的shell内置了 `$OPTARG` 这个变变，getopts修改了这个变量。\n\n这里变量 `$OPTARG` 存储相应选项的参数，而 `$OPTIND` 总是存储原始 `$*` 中下一个要处理的元素位置。`while getopts \":a:bc\" opt`   #第一个冒号表示忽略错误；字符后面的冒号表示该选项必须有自己的参数\n\ngetopts后面的字符串就是可以使用的选项列表，每个字母代表一个选项，后面带:的意味着选项除了定义本身之外，还会带上一个参数作为选项的值，比如d:在实际的使用中就会对应-d 30，选项的值就是30；getopts字符串中没有跟随:的是开关型选项，不需要再指定值，相当于true/false，只要带了这个参数就是true。如果命令行中包含了没有在getopts列表中的选项，会有警告信息，如果在整个getopts字符串前面也加上个:，就能消除警告信息了。\n\n两个特殊变量：\n\n```bash\n$OPTIND     # 特殊变量，option index，会逐个递增, 初始值为1\n$OPTARG     # 特殊变量，option argument，不同情况下有不同的值\n```\n\n例子：\n\n```typescript\necho $*\nwhile getopts \":a:bc\" opt\ndo\n        case $opt in\n                a ) echo $OPTARG\n                    echo $OPTIND;;\n                b ) echo \"b $OPTIND\";;\n                c ) echo \"c $OPTIND\";;\n                ? ) echo \"error\"\n                    exit 1;;\n        esac\ndone\necho $OPTIND\nshift $(($OPTIND - 1))\n#通过shift $(($OPTIND - 1))的处理，$*中就只保留了除去选项内容的参数，可以在其后进行正常的shell编程处理了。\necho $0\necho $*\n```\n\n```bash\n$ ./getopts.sh -a 11 -b -c\n-a 11 -b -c\n11\n3\nb 4\nc 5\n5\n./getopts.sh\n```\n\n#### 3.10.2 getopt（一个外部工具）\n\n具体用用法可以 man getopt\n\n`-o` 表示短选项，两个冒号表示该选项有一个可选参数，可选参数必须紧贴选项，如 `-carg` 而不能是 `-c arg`。\n\n`--long` 表示长选项\n\n例子：\n\n```typescript\n#!/bin/bash\n\n# A small example program for using the new getopt(1) program.\n# This program will only work with bash(1)\n# An similar program using the tcsh(1) script. language can be found\n# as parse.tcsh\n\n# Example input and output (from the bash prompt):\n# ./parse.bash -a par1 'another arg' --c-long 'wow!*\\?' -cmore -b \" very long \"\n# Option a\n# Option c, no argument\n# Option c, argument `more'\n# Option b, argument ` very long '\n# Remaining arguments:\n# --> `par1'\n# --> `another arg'\n# --> `wow!*\\?'\n\n# Note that we use `\"$@\"' to let each command-line parameter expand to a\n# separate word. The quotes around `$@' are essential!\n# We need TEMP as the `eval set --' would nuke the return value of getopt.\n\n#-o表示短选项，两个冒号表示该选项有一个可选参数，可选参数必须紧贴选项\n#如-carg 而不能是-c arg\n#--long表示长选项\n#\"$@\"在上面解释过\n# -n:出错时的信息\n# -- ：举一个例子比较好理解：\n#我们要创建一个名字为 \"-f\"的目录你会怎么办？\n# mkdir -f #不成功，因为-f会被mkdir当作选项来解析，这时就可以使用\n# mkdir -- -f 这样-f就不会被作为选项。\n\nTEMP=`getopt -o ab:c:: --long a-long,b-long:,c-long:: \\\n     -n 'example.bash' -- \"$@\"`\n\nif [ $? != 0 ] ; then echo \"Terminating...\" >&2 ; exit 1 ; fi\n\n# Note the quotes around `$TEMP': they are essential!\n#set 会重新排列参数的顺序，也就是改变$1,$2...$n的值，这些值在getopt中重新排列过了\neval set -- \"$TEMP\"\n\n#经过getopt的处理，下面处理具体选项。\n\nwhile true ; do\n        case \"$1\" in\n                -a|--a-long) echo \"Option a\" ; shift ;;\n                -b|--b-long) echo \"Option b, argument \\`$2'\" ; shift 2 ;;\n                -c|--c-long)\n                        # c has an optional argument. As we are in quoted mode,\n                        # an empty parameter will be generated if its optional\n                        # argument is not found.\n                        case \"$2\" in\n                                \"\") echo \"Option c, no argument\"; shift 2 ;;\n                                *)  echo \"Option c, argument \\`$2'\" ; shift 2 ;;\n                        esac ;;\n                --) shift ; break ;;\n                *) echo \"Internal error!\" ; exit 1 ;;\n        esac\ndone\necho \"Remaining arguments:\"\nfor arg do\n   echo '--> '\"\\`$arg'\" ;\ndone\n```\n\n```bash\n$ ./getopt.sh --b-long abc -a -c33 remain\nOption b, argument `abc'\nOption a\nOption c, argument `33'\nRemaining arguments:\n--> `remain'\n```\n\n### 3.11 只列出目录的各种方法\n\n```bash\n# 使用ls –d：\n$ ls -d */\n\n# 使用grep结合ls –F：\n$ ls -F | grep \"/$\"\n# 当使用-F时，所有的输出项都会添加上一个代表文件类型的字符，如@、 *、 |等。目录对应的是 / 字符。我们用grep只过滤那些以 /$ 作为行尾标记的输出项。\n\n# 使用grep结合ls –l：\n$ ls -l | grep \"^d\"\n\n# 使用find：\n$ find . -type d -maxdepth 1 -print\n```\n\n### 3.12 使用pushd和popd进行快速定位\n\n使用pushd和popd时，可以无视cd命令。 \n\n```bash\n# 压入并切换路径：\n$ pushd /var/www\n\n# 再压入下一个目录路径：\n$ pushd /usr/src\n\n# 用下面的命令查看栈内容：\n$ dirs\n/usr/src /var/www ~ /usr/share /etc\n0 \t\t\t1 \t  2 \t3 \t\t4\n\n# 当你想切换到列表中任意一个路径时，将每条路径从0到n进行编号，然后使用你希望切换到的路径编号，例如：\n$ pushd +3\n# 这条命令会将栈进行翻转并切换到目录 /use/share。\n\n# 要删除最后添加的路径并把当前目录更改为上一级目录，可以使用以下命令：\n$ popd\n# 用popd +num可以从列表中移除特定的路径。num是从左到右，从0到n开始计数的。\n```\n\n### 3.13 tree打印目录树\n\n```bash\n# 重点标记出匹配某种样式的文件：\n$ tree PATH -P \"*.sh\" # 用一个目录路径代替PATH\n|-- home\n| |-- packtpub\n| | `-- automate.sh\n\n# 重点标记出除符合某种样式之外的那些文件：\n$ tree path -I PATTERN\n\n# 使用 -h选项同时打印出文件和目录的大小：\n$ tree -h\n```\n\n## 4 让文件飞 \n\n### 4.1 正则表达式\n\n| 正则表达式  | 描述                     | 示例                                       |\n| ------ | ---------------------- | :--------------------------------------- |\n| ^      | 行起始标记                  | ^tux 匹配以tux起始的行                          |\n| $      | 行尾标记                   | tux$ 匹配以tux结尾的行                          |\n| .      | 匹配任意一个字符               | Hack.匹配Hackl和Hacki，它只能匹配单个字符             |\n| [ ]    | 匹配包含在 [字符] 之中的任意一个字符   | coo[kl] 匹配cook或cool                      |\n| [ ^ ]  | 匹配除 ` [^字符]` 之外的任意一个字符 | `9[^01]`匹配92、 93，但是不匹配91或90              |\n| [ - ]  | 匹配 [ ] 中指定范围内的任意一个字符   | [1-5] 匹配从1～5的任意一个数字                      |\n| ?      | 匹配之前的项1次或0次            | colou?r 匹配color或colour，但是不能匹配colouur     |\n| +      | 匹配之前的项1次或多次            | Rollno-9+ 匹配Rollno-99、Rollno-9，但是不能匹配Rollno- |\n| *      | 匹配之前的项0次或多次            | co*l 匹配cl、 col、 coool等                   |\n| ( )    | 创建一个用于匹配的子串            | ma(tri)?x 匹配max或maxtrix                  |\n| {n}    | 匹配之前的项n次               | [0-9]{3} 匹 配 任 意 一 个 三 位 数 ， [0-9]{3} 可 以 扩 展 为`[0-9][0-9][0-9]` |\n| {n, }  | 之前的项至少需要匹配n次           | [0-9]{2,} 匹配任意一个两位或更多位的数字                |\n| {n, m} | 指定之前的项所必需匹配的最小次数和最大次数  | [0-9]{2,5} 匹配从两位数到五位数之间的任意一个数字           |\n| \\|     | 交替——匹配 \\| 两边的任意一项      | Oct  (1st \\| 2nd) 匹配Oct 1st或Oct 2nd      |\n| \\      | 转义符可以将上面介绍的特殊字符进行转义    | `a\\.b` 匹配a.b，但不能匹配ajb。通过在 . 之间加上前缀 \\ ，从而忽略了 . 的特殊意义 |\n\n| 正则表达式       | 描述            |\n| ----------- | ------------- |\n| [:alnum:]   | 所有的字母和数字      |\n| [:alpha:]   | 所有字母          |\n| [:blank:]   | 水平制表符，空白等     |\n| [:cntrl:]   | 所有控制字符        |\n| [:digit:]   | 所有的数字         |\n| `[:graph:]` | 所有可打印字符，不包括空格 |\n| [:lower:]   | 所有的小写字符       |\n| [:print:]   | 所有可打印字符，包括空格  |\n| [:punct:]   | 所有的标点字符       |\n| [:space:]   | 所有的横向或纵向的空白   |\n| [:upper:]   | 所有大写字母        |\n\n### 4.2 grep命令\n\n```bash\n-a\t\t\t\t# 不要忽略二进制的数据。\n-A<显示列数> \t # 除了显示符合范本样式的那一列之外，并显示该列之后的内容。\n-b\t\t\t\t# 在显示符合范本样式的那一列之前，标示出该列第一个字符的位编号。\n-B<显示列数>\t # 除了显示符合范本样式的那一列之外，并显示该列之前的内容。\n-c\t\t\t\t# 计算符合范本样式的列数。\n-C<显示列数>或-<显示列数>\t# 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。\n-d<进行动作>\t # 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。\n-e<范本样式>\t # 指定字符串做为查找文件内容的范本样式。\n-E\t\t\t\t# 将范本样式为延伸的普通表示法来使用。\n-f<范本文件>\t # 指定范本文件，其内容含有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每列一个范本样式。\n-F\t\t\t\t# 将范本样式视为固定字符串的列表。\n-G\t\t\t\t# 将范本样式视为普通的表示法来使用。\n-h\t\t\t\t# 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。\n-H\t\t\t\t# 在显示符合范本样式的那一列之前，表示该列所属的文件名称。\n-i\t\t\t\t# 忽略字符大小写的差别。\n-l\t\t\t\t# 列出文件内容符合指定的范本样式的文件名称。\n-L\t\t\t\t# 列出文件内容不符合指定的范本样式的文件名称。\n-n\t\t\t\t# 在显示符合范本样式的那一列之前，标示出该列的列数编号。\n-q\t\t\t\t# 不显示任何信息。\n-r\t\t\t\t# 此参数的效果和指定“-d recurse”参数相同。\n-s\t\t\t\t# 不显示错误信息。\n-v\t\t\t\t# 反转查找。\n-V\t\t\t\t# 显示版本信息。\n-w\t\t\t\t# 只显示全字符合的列。\n-x\t\t\t\t# 只显示全列符合的列。\n-o \t\t\t\t# 只输出文件中匹配到的部分。\n```\n\n```bash\n# 单个grep命令也可以对多个文件进行搜索：\n$ grep \"match_text\" file1 file2 file3 ...\n\n# grep -E选项——这意味着使用扩展（extended）正则表达式：\n$ grep -E \"[a-z]+\" filename\n# 或者\n$ egrep \"[a-z]+\" filename\n\n# 只输出文件中匹配到的文本部分，可以使用选项 -o：\n$ echo this is a line. | egrep -o \"[a-z]+\\.\"\nline.\n\n# 要打印除包含match_pattern行之外的所有行，选项-v可以将匹配结果进行反转（invert）。可使用：\n$ grep -v match_pattern file\n\n# 统计文件或文本中包含匹配字符串的行数：\n$ grep -c \"text\" filename\n10\n# 需要注意的是-c只是统计匹配行的数量，并不是匹配的次数。。例如：\n$ echo -e \"1 2 3 4\\nhello\\n5 6\" | egrep -c \"[0-9]\"\n2\n\n# 要文件中统计匹配项的数量，可以使用下面的技巧：\n$ echo -e \"1 2 3 4\\nhello\\n5 6\" | egrep -o \"[0-9]\" | wc -l\n6\n\n# 打印模式匹配所位于的字符或字节偏移：\n$ echo gnu is not unix | grep -b -o \"not\"\n7:not\n# 选项 -b总是和 -o配合使用。\n\n# 搜索多个文件并找出匹配文本位于哪一个文件中：\n$ grep -l linux sample1.txt sample2.txt\nsample1.txt\nsample2.txt\n# 和-l相反的选项是-L，它会返回一个不匹配的文件列表。\n\n# grep的选项-R和-r功能一样。\n\n# 忽略样式中的大小写\n$ echo hello world | grep -i \"HELLO\"\nhello\n\n# grep匹配多个样式\n$ echo this is a line of text | grep -e \"this\" -e \"line\" -o\nthis\nline\n\n# 在grep搜索中指定或排除文件\n$ grep \"main()\" . -r --include *.{c,cpp} \t\t# 目录中递归搜索所有的 .c和 .cpp文件\n# 如果需要排除目录，可以使用 --exclude-dir选项。\n# 如果需要从文件中读取所需排除的文件列表，使用--exclude-from FILE。\n\n# 使用0值字节作为后缀的grep与xargs，为了指明输入的文件名是以0值字节（\\0）作为终止符，需要在xargs中使用-0。\n# grep使用-Z选项输出以0值字节作为终结符的文件名（\\0）。\n$ grep \"test\" file* -lZ | xargs -0 rm\n# -Z通常和 -l结合使用。\n\n# grep的静默输出\n# grep的静默选项（-q）来实现。在静默模式中， grep命令不会输出任何内容。它仅是运行命令，然后根据命令执行成功与否返回退出状态。\n\n# 要打印匹配某个结果之后的3行，使用 -A选项：\n$ seq 10 | grep 5 -A 3\n5\n6\n7\n8\n\n# 要打印匹配某个结果之前的3行，使用 -B选项：\n$ seq 10 | grep 5 -B 3\n2\n3\n4\n5\n\n# 要打印匹配某个结果之前以及之后的3行，使用-C选项：\n$ seq 10 | grep 5 -C 3\n2\n3\n4\n5\n6\n7\n8\n\n# 如果有多个匹配，那么使用--作为各部分之间的定界符：\n$ echo -e \"a\\nb\\nc\\na\\nb\\nc\" | grep a -A 1\na\nb\n--\na\nb\n```\n\n### 4.3 cut 按列切分文件\n\n```bash\n# 显示第2列和第3列：\n$ cut -f 2,3 filename\n\n# \n```\n\n| 记法    | 范围                                |\n| ----- | --------------------------------- |\n| N -   | 从第N个字节，字符或字段到行尾                   |\n| N - M | 从第N个字节，字符或字段到第M个（包括第M个在内）字节、字符或字段 |\n| - M   | 第1个字节，字符或字段到第M个（包括第M个在内）字节、字符或字段  |\n\n结合下列选项将字段指定为某个范围内的字节或字符 ：\n\n* -b ：表示字节\n* -c ：表示字符\n* -f ：用于定义字段\n\n```bash\n$ cat range_fields.txt\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxyz\nabcdefghijklmnopqrstuvwxy\n\n# 打印第1个到第5个字符：\n$ cut -c1-5 range_fields.txt\nabcde\nabcde\nabcde\nabcde\n# 打印前2个字符：\n$ cut range_fields.txt -c -2\nab\nab\nab\nab\n```\n\n### 4.4 sed 进行文本替换 \n\n选项：\n\n```bash\n-e <script>\t\t\t# 以选项中指定的script来处理输入的文本文件\n-f <script>\t\t\t# 以选项中指定的script文件来处理输入的文本文件\n-h\t\t\t\t\t# 显示帮助\n-n\t\t\t\t\t# 仅显示script处理后的结果\n-V\t\t\t\t\t# 显示版本信息\n```\n\n命令：\n\n```bash\na\\ \t\t\t# 在当前行下面插入文本。\ni\\ \t\t\t# 在当前行上面插入文本。\nc\\ \t\t\t# 把选定的行改为新的文本。 \nd \t\t\t# 删除，删除选择的行。 \nD \t\t\t# 删除模板块的第一行。\ns \t\t\t# 替换指定字符 h 拷贝模板块的内容到内存中的缓冲区。 \nH \t\t\t# 追加模板块的内容到内存中的缓冲区。 \ng \t\t\t# 获得内存缓冲区的内容，并替代当前模板块中的文本。 \nG \t\t\t# 获得内存缓冲区的内容，并追加到当前模板块文本的后面。 \nl \t\t\t# 列表不能打印字符的清单。 \nn \t\t\t# 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。 \nN \t\t\t# 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。 \np \t\t\t# 打印模板块的行。 P(大写) 打印模板块的第一行。 \nq \t\t\t# 退出Sed。 \nb lable \t# 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。 \nr file \t\t# 从file中读行。 \nt label \t# if分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。 \nT label \t# 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。\nw file \t\t# 写并追加模板块到file末尾。 \nW file \t\t# 写并追加模板块的第一行到file末尾。 \n! \t\t\t# 表示后面的命令对所有没有被选定的行发生作用。 \n= \t\t\t# 打印当前行号码。 \n# 把注释扩展到下一个换行符以前。\n```\n\nsed 替换标记：\n\n```bash\ng \t\t# 表示行内全面替换。\np \t\t# 表示打印行。 \nw \t\t# 表示把行写入一个文件。 \nx \t\t# 表示互换模板块中的文本和缓冲区中的文本。 \ny \t\t# 表示把一个字符翻译为另外的字符（但是不用于正则表达式） \n\\1 \t\t# 子串匹配标记 \n& \t\t# 已匹配字符串标记\n```\n\nsed 元字符集：\n\n```bash\n^ \t\t# 匹配行开始，如：/^sed/匹配所有以sed开头的行。\n$ \t\t# 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 \n. \t\t# 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 \n* \t\t# 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 \n[] \t\t# 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 \n[^] \t# 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/ 匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。\n\\(..\\) \t# 匹配子串，保存匹配的字符，如s/(love)able/\\1rs，loveable被替换成lovers。 \n& \t\t# 保存搜索字符用来替换其他字符，如s/love/**&**/，love这成**love**。 \n\\< \t \t# 匹配单词的开始，如:/\\<love/匹配包含以开头的单词的行。\n\\>\t\t# 匹配单词的结束，如:/love\\>/匹配包含以love结尾的单词的行。\nx\\{m\\} \t\t# 重复字符x，m次，如：/0\\{5\\}/匹配包含5个0的行。 \nx\\{m,\\} \t# 重复字符x，至少m次，如：/0\\{5,\\}/匹配至少有5个0的行。 \nx\\{m,n\\} \t# 重复字符x，至少m次，不多于n次，如：/0\\{5,10\\}/匹配5~10个0的行。\n```\n\n```bash\n# sed可以替换给定文本中的字符串。\n$ sed 's/pattern/replace_string/' file\n\n# 如果需要在替换的同时保存更改，可以使用-i选项\n$ sed -i 's/text/replace/' file\n\n# 后缀/g意味着sed会替换每一处匹配。但是有时候我们只需要从第n处匹配开始替换。对此，可以使用/Ng选项。\n$ sed 's/pattern/replace_string/g' file\n$ echo thisthisthisthis | sed 's/this/THIS/2g'\nthisTHISTHISTHIS\n$ echo thisthisthisthis | sed 's/this/THIS/3g'\nthisthisTHISTHIS\n\n# 字符/在sed中被作为定界符使用。我们可以像下面一样使用任意的定界符：\n$ sed 's:text:replace:g'\n$ sed 's|text|replace|g'\n# 当定界符出现在样式内部时，我们必须用前缀\\对它进行转义：\n$ sed 's|te\\|xt|replace|g'\n# \\|是一个出现在样式内部并经过转义的定界符。\n\n# 移除空白行\n$ sed '/^$/d' file\n\n# 已匹配字符串标记（&）在sed中，我们可以用 &标记匹配样式的字符串，这样就能够在替换字符串时使用已匹配的内容。\n$ echo this is an example | sed 's/\\w\\+/[&]/g'\n[this] [is] [an] [example]\n# 正则表达式 \\w\\+ 匹配每一个单词，然后我们用[&]替换它。 & 对应于之前所匹配到的单词。\n\n# 组合多个表达式\n$ sed 'expression' | sed 'expression'\n# 它等价于\n$ sed 'expression; expression'\n# 或者\n$ sed -e 'expression' -e expression'\n\n# 引用。sed表达式通常用单引号来引用。双引号会通过对表达式求值来对其进行扩展。\n$ text=hello\n$ echo hello world | sed \"s/$text/HELLO/\"\nHELLO world\n```\n\n### 4.5 awk 进行高级文本处理\n\n#### 4.5.1 awk 常用命令选项\n\n* `-F fs`\t\tfs指定输入分隔符，fs可以是字符串或正则表达式，如`-F:`\n  * `-v var=value`   赋值一个用户定义变量，将外部变量传递给awk \n* `-f scripfile`      从脚本文件中读取awk命令 \n* `-m[fr] val`          对val值设置内在限制，`-mf` 选项限制分配给val的最大块数目；`-mr` 选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。\n\n#### 4.5.2 awk 脚本基本结构\n\n```bash\n$ awk 'BEGIN{ print \"start\" } pattern{ commands } END{ print \"end\" }' file\n# 一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被单引号或双引号中，例如：\n$ awk 'BEGIN{ i=0 } { i++ } END{ print i }' filename \n$ awk \"BEGIN{ i=0 } { i++ } END{ print i }\" filename\n\n\n```\n\n#### 4.5.3 awk 的工作原理\n\n```bash\n$ awk 'BEGIN{ commands } pattern{ commands } END{ commands }'\n```\n\n* 第一步：执行 `BEGIN{ commands }` 语句块中的语句\n* 第二步：从文件或标准输入(stdin)读取一行，然后执行 `pattern{ commands }` 语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕\n* 第三步：当读至输入流末尾时，执行 `END{ commands }` 语句块\n\n#### 4.5.4 awk 内置变量（预定义变量）\n\n**说明：**  `[A][N][P][G]`表示第一个支持变量的工具，`[A]=awk`、`[N]=nawk`、`[P]=POSIXawk`、`[G]=gawk`\n\n```bash\n$n \t\t\t\t# 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 \n$0 \t\t\t\t# 这个变量包含执行过程中当前行的文本内容。 \n[N] ARGC \t\t# 命令行参数的数目。 \n[G] ARGIND \t\t# 命令行中当前文件的位置（从0开始算）。 \n[N] ARGV \t\t# 包含命令行参数的数组。 \n[G] CONVFMT \t# 数字转换格式（默认值为%.6g）。 \n[P] ENVIRON \t# 环境变量关联数组。 \n[N] ERRNO \t\t# 最后一个系统错误的描述。 \n[G] FIELDWIDTHS # 字段宽度列表（用空格键分隔）。 \n[A] FILENAME \t# 当前输入文件的名。 \n[P] FNR \t\t# 同NR，但相对于当前文件。 \n[A] FS \t\t\t# 字段分隔符（默认是任何空格）。 \n[G] IGNORECASE \t# 如果为真，则进行忽略大小写的匹配。 \n[A] NF \t\t\t# 表示字段数，在执行过程中对应于当前的字段数。 \n[A] NR \t\t\t# 表示记录数，在执行过程中对应于当前的行号。 \n[A] OFMT \t\t# 数字的输出格式（默认值是%.6g）。 \n[A] OFS \t\t# 输出字段分隔符（默认值是一个空格）。 \n[A] ORS \t\t# 输出记录分隔符（默认值是一个换行符）。 \n[A] RS \t\t\t# 记录分隔符（默认是一个换行符）。 \n[N] RSTART \t\t# 由match函数所匹配的字符串的第一个位置。 \n[N] RLENGTH \t# 由match函数所匹配的字符串的长度。 \n[N] SUBSEP \t\t# 数组下标分隔符（默认值是34）。\n```\n\n```bash\n$ echo -e \"line1 f2 f3nline2 f4 f5nline3 f6 f7\" | awk '{print \"Line No:\"NR\", No of fields:\"NF, \"$0=\"$0, \"$1=\"$1, \"$2=\"$2, \"$3=\"$3}' \nLine No:1, No of fields:3 $0=line1 f2 f3 $1=line1 $2=f2 $3=f3 \nLine No:2, No of fields:3 $0=line2 f4 f5 $1=line2 $2=f4 $3=f5 \nLine No:3, No of fields:3 $0=line3 f6 f7 $1=line3 $2=f6 $3=f7\n\n# 使用print $NF可以打印出一行中的最后一个字段，使用$(NF-1)则是打印倒数第二个字段，其他以此类推：\n$ echo -e \"line1 f2 f3n line2 f4 f5\" | awk '{print $NF}' \nf3\nf5\n$ echo -e \"line1 f2 f3n line2 f4 f5\" | awk '{print $(NF-1)}' \nf2 \nf4\n\n# 打印每一行的第二和第三个字段：\n$ awk '{ print $2,$3 }' filename\n\n# 统计文件中的行数：\n$ awk 'END{ print NR }' filename\n\n# 一个每一行中第一个字段值累加的例子：\n$ seq 5 | awk 'BEGIN{ sum=0; print \"总和：\" } { print $1\"+\"; sum+=$1 } END{ print \"等于\"; print sum }' \n总和： \n1+ \n2+ \n3+ \n4+ \n5+ \n等于 \n15\n```\n\n#### 4.5.5 将外部变量值传递给awk \n\n借助 `-v` 选项，可以将外部值（并非来自stdin）传递给awk：\n\n```bash\n$ VAR=10000 \n$ echo | awk -v VARIABLE=$VAR '{ print VARIABLE }'\n\n# 另一种传递外部变量方法：\n$ var1=\"aaa\" \n$ var2=\"bbb\" \n$ echo | awk '{ print v1,v2 }' v1=$var1 v2=$var2\n\n# 当输入来自于文件时使用：\n$ awk '{ print v1,v2 }' v1=$var1 v2=$var2 filename\n```\n\n#### 4.5.6 awk 运算与判断\n\n**算数运算符：**\n\n| 运算符   | 描述            |\n| ----- | ------------- |\n| + -   | 加、减           |\n| * / & | 乘，除与求余        |\n| + - ! | 一元加、减和逻辑非     |\n| ^ *** | 求幂            |\n| ++ -- | 增加或减少，作为前缀或后缀 |\n\n```bash\n$ awk 'BEGIN{a=\"b\";print a++,++a;}' \n0 2\n```\n\n<p style=\"color=red\">**注意：**所有用作算术运算符进行操作，操作数自动转为数值，所有非数值都变为0</p>\n\n**赋值运算符：**\n\n| 运算符                     | 描述   |\n| ----------------------- | ---- |\n| = += -= *= /= %= ^= **= | 赋值语句 |\n\n**逻辑运算符：**\n\n| 运算符  | 描述   |\n| ---- | ---- |\n| \\|\\| | 逻辑或  |\n| &&   | 逻辑与  |\n\n```bash\n$ awk 'BEGIN{a=1;b=2;print (a>5 && b<=2),(a>5 || b<=2);}'\n0 1\n```\n\n**正则运算符：**\n\n| 运算符   | 描述               |\n| ----- | ---------------- |\n| ~  ~! | 匹配正则表达式和不匹配正则表达式 |\n\n```bash\n$ awk 'BEGIN{a=\"100testa\";if(a ~ /^100*/){print \"ok\";}}' \nok\n```\n\n**关系运算符：**\n\n| 运算符                  | 描述    |\n| -------------------- | ----- |\n| <  <=  >  >=  !=  == | 关系运算符 |\n\n```bash\n$ awk 'BEGIN{a=11;if(a >= 9){print \"ok\";}}' \nok\n```\n\n<p style=\"color=red\">**注意：**>  < 可以作为字符串比较，也可以用作数值比较，关键看操作数如果是字符串就会转换为字符串比较。两个都为数字才转为数值比较。字符串比较：按照ASCII码顺序比较。</p>\n\n**其他运算符：**\n\n| 运算符  | 描述         |\n| ---- | ---------- |\n| $    | 字段引用       |\n| 空格   | 字符串连接符     |\n| ? :  | C条件表达式     |\n| in   | 数组中是否存在某键值 |\n\n```bash\n$ awk 'BEGIN{a=\"b\";print a==\"b\"?\"ok\":\"err\";}' \nok \n\n$ awk 'BEGIN{a=\"b\";arr[0]=\"b\";arr[1]=\"c\";print (a in arr);}' \n0 \n\n$ awk 'BEGIN{a=\"b\";arr[0]=\"b\";arr[\"b\"]=\"c\";print (a in arr);}' \n1\n```\n\n运算级优先级表：\n\n| 级别   | 运算符                                      | 说明              |\n| ---- | ---------------------------------------- | --------------- |\n| 1    | =, +=, -=, *=, /=, %=, &=, ^=, \\|=, <<=, >>= | 赋值、运算           |\n| 2    | \\|\\|                                     | 逻辑或             |\n| 3    | &&                                       | 逻辑与             |\n| 4    | \\|                                       | 按位或             |\n| 5    | ^                                        | 按位异或            |\n| 6    | &                                        | 按位与             |\n| 7    | ==, !=                                   | 等于、不等于          |\n| 8    | <=, >=, <, >                             | 小于等于、大于等于、小于、大于 |\n| 9    | <<, >>                                   | 按位左移，按位右移       |\n| 10   | +, -                                     | 加、减             |\n| 11   | *, /, %                                  | 乘、除、取模          |\n| 12   | !, ~                                     | 逻辑非、按位取反或补码     |\n| 13   | -, +                                     | 正、负             |\n\n级别越高越优先\n\n#### 4.5.7 awk 高级输入输出\n\n**读取下一条记录：**\n\nawk中 `next` 语句使用：在循环逐行匹配，如果遇到 `next`，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。net语句一般用于多行合并：\n\n```bash\n$ cat text.txt \na \nb \nc \nd \ne \n\n$ awk 'NR%2==1{next}{print NR,$0;}' text.txt \n2 b \n4 d \n```\n\n当记录行号除以2余1，就跳过当前行。下面的 `print NR,$0` 也不会执行。下一行开始，程序有开始判断 `NR%2` 值。这个时候记录行号是 `：2`  ，就会执行下面语句块：`'print NR,$0'` \n\n分析发现需要将包含有 “web” 行进行跳过，然后需要将内容与下面行合并为一行： \n\n```bash\n$ cat text.txt \nweb01[192.168.2.100] \nhttpd \t\tok \ntomcat \t\tok \nsendmail \tok \nweb02[192.168.2.101] \nhttpd \t\tok \npostfix \tok \nweb03[192.168.2.102] \nmysqld \t\tok \nhttpd \t\tok \n0 \n\n$ awk '/^web/{T=$0;next;}{print T\":t\"$0;}' test.txt \nweb01[192.168.2.100]: httpd \t\tok \nweb01[192.168.2.100]: tomcat \t\tok \nweb01[192.168.2.100]: sendmail \t\tok \nweb02[192.168.2.101]: httpd \t\tok \nweb02[192.168.2.101]: postfix \t\tok \nweb03[192.168.2.102]: mysqld \t\tok \nweb03[192.168.2.102]: httpd \t\tok\n```\n\n**简单地读取一条记录：**\n\n`awk getline` 用法：输出重定向需用到 `getline函数`。getline从标准输入、管道或者当前正在处理的文件之外的其他输入文件获得输入。它负责从输入获得下一行的内容，并给NF,NR和FNR等内建变量赋值。<p style=\"color=red\">如果得到一条记录，getline函数返回1，如果到达文件的末尾就返回0，如果出现错误，例如打开文件失败，就返回-1。 </p>\n\n> getline语法：getline var，变量var包含了特定行的内容。 \n\nawk getline从整体上来说，用法说明：\n\n* **当其左右<p style=\"color=red\">无</p>重定向符 `|` 或 `<` 时：**getline作用于当前文件，读入当前文件的第一行给其后跟的变量 `var` 或 `$0`（无变量），应该注意到，由于awk在处理getline之前已经读入了一行，所以getline得到的返回结果是隔行的。\n* **当其左右<p style=\"color=red\">有</p>重定向符 `|` 或 `<` 时：**getline则作用于定向输入文件，由于该文件是刚打开，并没有被awk读入一行，只是getline读入，那么getline返回的是该文件的第一行，而不是隔行。\n\n```bash\n# 执行linux的date命令，并通过管道输出给getline，然后再把输出赋值给自定义变量out，并打印它：\n$ awk 'BEGIN{ \"date\" | getline out; print out }' test\n\n# 执行shell的date命令，并通过管道输出给getline，然后getline从管道中读取并将输入赋值给out，split函数把变量out转化成数组mon，然后打印数组mon的第二个元素：\n$ awk 'BEGIN{ \"date\" | getline out; split(out,mon); print mon[2] }' test\n\n# 命令ls的输出传递给geline作为输入，循环使getline从ls的输出中读取一行，并把它打印到屏幕。这里没有输入文件，因为BEGIN块在打开输入文件前执行，所以可以忽略输入文件。\n$ awk 'BEGIN{ while( \"ls\" | getline) print }'\n```\n\n**关闭文件：**\n\nawk中允许在程序中关闭一个输入或输出文件，方法是使用awk的close语句。\n\n```typescript\nclose(\"filename\")\n```\n\nfilename可以是getline打开的文件，也可以是stdin，包含文件名的变量或者getline使用的确切命令。或一个输出文件，可以是stdout，包含文件名的变量或使用管道的确切命令。\n\n**输出到一个文件：**\n\n```bash\n$ echo | awk '{printf(\"hello word!n\") > \"datafile\"}'\n或 \n$ echo | awk '{printf(\"hello word!n\") >> \"datafile\"}'\n```\n\n#### 4.5.8 设置字段定界符\n\n<p style=\"color=red\">默认的字段定界符是空格</p>，可以使用 `-F \"定界符\"` 明确指定一个定界符：\n\n```bash\n$ awk -F: '{ print $NF }' /etc/passwd \n或 \n$ awk 'BEGIN{ FS=\":\" } { print $NF }' /etc/passwd\n```\n\n在 `BEGIN语句块` 中则可以用 `OFS=“定界符”` 设置输出字段的定界符。\n\n#### 4.5.9 流程控制语句\n\n**条件判断语句：**\n\n```bash\n$ awk 'BEGIN{ \n\ttest=100; \n\tif(test>90){ \n\t\tprint \"very good\"; \n\t} else if(test>60){ \n\t\tprint \"good\"; \n\t} else{ \n\t\tprint \"no pass\"; \n\t} \n}' \n\nvery good\n```\n\n每条命令语句后面可以用 `;` <p style=\"color=red\">分号</p>结尾。\n\n**循环语句：**\n\nwhile语句：\n\n```bash\n$ awk 'BEGIN{ \n\ttest=100; \n\ttotal=0; \n\twhile(i<=test){ \n\t\ttotal+=i; i++; \n\t} \n\tprint total; \n}' \n\n5050\n```\n\nfor循环：\n\n格式1：\n\n```bash\n$ awk 'BEGIN{ \n\tfor(k in ENVIRON){ \n\t\tprint k\"=\"ENVIRON[k]; \n\t} \n}' \nTERM=linux \nG_BROKEN_FILENAMES=1 \nSHLVL=1 \npwd=/root/text \n... \nlogname=root \nHOME=/root \nSSH_CLIENT=192.168.1.21 53087 22\n```\n\n**注：**ENVIRON是awk常量，是子典型数组。\n\n格式2：\n\n```bash\n$ awk 'BEGIN{ \n\ttotal=0; \n\tfor(i=0;i<=100;i++){ \n\t\ttotal+=i; \n\t} \n\tprint total; \n}' \n\n5050\n```\n\ndo循环：\n\n```bash\n$ awk 'BEGIN{ \n\ttotal=0; \n\ti=0; \n\tdo {\n\t\ttotal+=i;i++;\n\t} while(i<=100) \n\tprint total; \n}' \n\n5050\n```\n\n**其他语句：**\n\n* **break**  \t\t当 break 语句用于 while 或 for 语句时，导致退出程序循环\n* **continue**       当 continue 语句用于 while 或 for 语句时，使程序循环移动到下一个迭代\n* **next**               能能够导致读入下一个输入行，并返回到脚本的顶部。这可以避免对当前输入行执行其他的操作过程\n* **exit**                 语句使主输入循环退出并将控制转移到END,如果END存在的话。如果没有定义END规则，或在END中应用exit语句，则终止脚本的执行\n\n#### 4.5.10 数组应用\n\n```bash\n# 得到数组长度\n$ awk 'BEGIN{info=\"it is a test\";lens=split(info,tA,\" \");print length(tA),lens;}' \n4 4\n# length返回字符串以及数组长度，split进行分割字符串为数组，也会返回分割得到数组长度。\n\n# asort对数组进行排序，返回数组长度。\n$ awk 'BEGIN{info=\"it is a test\";split(info,tA,\" \");print asort(tA);}' \n4\n\n# 输出数组内容（无序，有序输出）：\n$ awk 'BEGIN{info=\"it is a test\";split(info,tA,\" \");for(k in tA){print k,tA[k];}}' \n4 test \n1 it \n2 is \n3 a \n\n# for…in 输出，因为数组是关联数组，默认是无序的。所以通过 for…in 得到是无序的数组。如果需要得到有序数组，需要通过下标获得。\n$ awk 'BEGIN{info=\"it is a test\";tlen=split(info,tA,\" \");for(k=1;k<=tlen;k++){print k,tA[k];}}' \n1 it \n2 is \n3 a \n4 test\n# 注意：数组下标是从1开始，与C数组不一样。\n\n# 判断键值存在以及删除键值：\n$ awk 'BEGIN{tB[\"a\"]=\"a1\";tB[\"b\"]=\"b1\";if( \"c\" in tB){print \"ok\";};for(k in tB){print k,tB[k];}}' \na a1 \nb b1\n# 删除键值： \n$ awk 'BEGIN{tB[\"a\"]=\"a1\";tB[\"b\"]=\"b1\";delete tB[\"a\"];for(k in tB){print k,tB[k];}}' \nb b1\n\n# 二维、多维数组使用\n$ awk 'BEGIN{ \n\tfor(i=1;i<=9;i++){ \n\t\tfor(j=1;j<=9;j++){ \n\t\t\ttarr[i,j]=i*j; \n\t\t\tprint i,\"*\",j,\"=\",tarr[i,j]; \n\t\t} \n\t} \n}' \n1 * 1 = 1 \n1 * 2 = 2 \n1 * 3 = 3 \n1 * 4 = 4 \n1 * 5 = 5 \n1 * 6 = 6 \n... \n9 * 6 = 54 \n9 * 7 = 63 \n9 * 8 = 72 \n9 * 9 = 81\n# 可以通过array[k,k2]引用获得数组内容。\n\n# 另一种方法：\n$ awk 'BEGIN{ \n\tfor(i=1;i<=9;i++){ \n\t\tfor(j=1;j<=9;j++){ \n\t\t\ttarr[i,j]=i*j; \n\t\t} \n\t} \n\tfor(m in tarr){ \n\t\tsplit(m,tarr2,SUBSEP); print tarr2[1],\"*\",tarr2[2],\"=\",tarr[m]; \n\t} \n}'\n```\n\n#### 4.5.11 内置函数\n\nawk内置函数，主要分以下3种类似：算数函数、字符串函数、其它一般函数、时间函数。\n\n**算数函数：**\n\n| 格式              | 描述                                       |\n| --------------- | ---------------------------------------- |\n| atan2( y, x )   | 返回 y/x 的反正切                              |\n| cos( x )        | 返回 x 的余弦；x 是弧度                           |\n| sin( x )        | 返回 x 的正弦；x 是弧度                           |\n| exp( x )        | 返回 x 幂函数                                 |\n| log( x )        | 返回 x 的自然对数                               |\n| sqrt( x )       | 返回 x 平方根                                 |\n| int( x )        | 返回 x 的截断至整数的值                            |\n| rand( )         | 返回任意数字 n，其中 0 <= n < 1                   |\n| srand( [expr] ) | 将 rand 函数的种子值设置为 Expr 参数的值，或如果省略 Expr 参数则使用某天的时间。返回先前的种子值。 |\n\n```bash\n$ awk 'BEGIN{OFMT=\"%.3f\";fs=sin(1);fe=exp(10);fl=log(10);fi=int(3.1415);print fs,fe,fl,fi;}' \n0.841 22026.466 2.303 3\n\n# 获得随机数：\n$ awk 'BEGIN{srand();fr=int(100*rand());print fr;}' \n78 \n$ awk 'BEGIN{srand();fr=int(100*rand());print fr;}' \n31 \n$ awk 'BEGIN{srand();fr=int(100*rand());print fr;}' \n41 \n```\n\n**字符串函数：**\n\n| 格式                                  | 描述                                       |\n| ----------------------------------- | ---------------------------------------- |\n| gsub( Ere, Repl, [ In ] )           | 除了正则表达式所有具体值被替代这点，它和 sub 函数完全一样地执行       |\n| sub( Ere, Repl, [ In ] )            | 用 Repl 参数指定的字符串替换 In 参数指定的字符串中的由 Ere 参数指定的扩展正则表达式的第一个具体值。sub 函数返回替换的数量。出现在 Repl 参数指定的字符串中的 &（和符号）由 In 参数指定的与 Ere 参数的指定的扩展正则表达式匹配的字符串替换。如果未指定 In 参数，缺省值是整个记录（$0 记录变量） |\n| index( String1, String2 )           | 在由 String1 参数指定的字符串（其中有出现 String2 指定的参数）中，返回位置，从 1 开始编号。如果 String2 参数不在 String1 参数中出现，则返回 0（零） |\n| length [(String)]                   | 返回 String 参数指定的字符串的长度（字符形式）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量） |\n| blength [(String)]                  | 返回 String 参数指定的字符串的长度（以字节为单位）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量） |\n| substr( String, M, [ N ] )          | 返回具有 N 参数指定的字符数量子串。子串从 String 参数指定的字符串取得，其字符以 M 参数指定的位置开始。M 参数指定为将 String 参数中的第一个字符作为编号 1。如果未指定 N 参数，则子串的长度将是 M 参数指定的位置到 String 参数的末尾 的长度 |\n| match( String, Ere )                | 在 String 参数指定的字符串（Ere 参数指定的扩展正则表达式出现在其中）中返回位置（字符形式），从 1 开始编号，或如果 Ere 参数不出现，则返回 0（零）。RSTART 特殊变量设置为返回值。RLENGTH 特殊变量设置为匹配的字符串的长度，或如果未找到任何匹配，则设置为 -1（负一） |\n| split( String, A, [Ere] )           | 将 String 参数指定的参数分割为数组元素 A[1], A[2], . . ., A[n]，并返回 n 变量的值。此分隔可以通过 Ere 参数指定的扩展正则表达式进行，或用当前字段分隔符（FS 特殊变量）来进行（如果没有给出 Ere 参数）。除非上下文指明特定的元素还应具有一个数字值，否则 A 数组中的元素用字符串值来创建 |\n| tolower( String )                   | 返回 String 参数指定的字符串，字符串中每个大写字符将更改为小写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义 |\n| toupper( String )                   | 返回 String 参数指定的字符串，字符串中每个小写字符将更改为大写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义 |\n| sprintf(Format, Expr, Expr, . . . ) | 根据 Format 参数指定的 printf 子例程格式字符串来格式化 Expr 参数指定的表达式并返回最后生成的字符串 |\n\n**注：**Ere都可以是正则表达式。\n\n```bash\n# gsub,sub使用 \n$ awk 'BEGIN{info=\"this is a test2010test!\";gsub(/[0-9]+/,\"!\",info);print info}' \nthis is a test!test!\n\n# 查找字符串（index使用） \n$ awk 'BEGIN{info=\"this is a test2010test!\";print index(info,\"test\")?\"ok\":\"no found\";}' \nok\n\n# 正则表达式匹配查找(match使用） \n$ awk 'BEGIN{info=\"this is a test2010test!\";print match(info,/[0-9]+/)?\"ok\":\"no found\";}' \nok\n\n# 截取字符串(substr使用） \n$ awk 'BEGIN{info=\"this is a test2010test!\";print substr(info,4,10);}' \ns is a tes\n\n# 字符串分割（split使用） \n$ awk 'BEGIN{info=\"this is a test\";split(info,tA,\" \");print length(tA);for(k in tA){print k,tA[k];}}' \n4 \n4 test \n1 this \n2 is \n3 a\n```\n\n**格式化字符串输出（sprintf使用）** \n\n格式化字符串格式：\n\n| 格式   | 描述               |\n| ---- | ---------------- |\n| %d   | 十进制有符号整数         |\n| %u   | 十进制无符号整数         |\n| %f   | 浮点数              |\n| %s   | 字符串              |\n| %c   | 单个字符             |\n| %p   | 指针的值             |\n| %e   | 指数形式的浮点数         |\n| %x   | %X 无符号以十六进制表示的整数 |\n| %o   | 无符号以八进制表示的整数     |\n| %g   | 自动选择合适的表示法       |\n\n```bash\n$ awk 'BEGIN{n1=124.113;n2=-1.224;n3=1.2345; printf(\"%.2f,%.2u,%.2g,%X,%on\",n1,n2,n3,n1,n1);}' \n124.11,18446744073709551615,1.2,7C,174\n```\n\n**一般函数：**\n\n| 格式                                 | 描述                                       |\n| ---------------------------------- | ---------------------------------------- |\n| close( Expression )                | 用同一个带字符串值的 Expression 参数来关闭由 print 或 printf 语句打开的或调用 getline 函数打开的文件或管道。如果文件或管道成功关闭，则返回 0；其它情况下返回非零值。如果打算写一个文件，并稍后在同一个程序中读取文件，则 close 语句是必需的 |\n| system(command )                   | 执行 Command 参数指定的命令，并返回退出状态。等同于 system 子例程 |\n| Expression \\| getline [ Variable ] | 从来自 Expression 参数指定的命令的输出中通过管道传送的流中读取一个输入记录，并将该记录的值指定给 Variable 参数指定的变量。如果当前未打开将 Expression 参数的值作为其命令名称的流，则创建流。创建的流等同于调用 popen 子例程，此时 Command 参数取 Expression 参数的值且 Mode 参数设置为一个是 r 的值。只要流保留打开且 Expression 参数求得同一个字符串，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录 |\n| getline [ Variable ] < Expression  | 从 Expression 参数指定的文件读取输入的下一个记录，并将 Variable 参数指定的变量设置为该记录的值。只要流保留打开且 Expression 参数对同一个字符串求值，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录 |\n| getline [ Variable ]               | 将 Variable 参数指定的变量设置为从当前输入文件读取的下一个输入记录。如果未指定 Variable 参数，则 $0 记录变量设置为该记录的值，还将设置 NF、NR 和 FNR 特殊变量 |\n\n```bash\n# 打开外部文件（close用法） \n$ awk 'BEGIN{while(\"cat /etc/passwd\"|getline){print $0;};close(\"/etc/passwd\");}' \nroot:x:0:0:root:/root:/bin/bash \nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\n\n# 逐行读取外部文件(getline使用方法） \n$ awk 'BEGIN{while(getline < \"/etc/passwd\"){print $0;};close(\"/etc/passwd\");}' \nroot:x:0:0:root:/root:/bin/bash \nbin:x:1:1:bin:/bin:/sbin/nologin \ndaemon:x:2:2:daemon:/sbin:/sbin/nologin \n$ awk 'BEGIN{print \"Enter your name:\";getline name;print name;}' \nEnter your name: \nchengmo \nchengmo\n\n# 调用外部应用程序(system使用方法） \n$ awk 'BEGIN{b=system(\"ls -al\");print b;}' \ntotal 42092 \ndrwxr-xr-x 14 chengmo chengmo 4096 09-30 17:47 . \ndrwxr-xr-x 95 root root 4096 10-08 14:01 .. \n# b返回值，是执行结果。\n```\n\n**时间函数：**\n\n| 格式                                 | 描述                                  |\n| ---------------------------------- | ----------------------------------- |\n| 函数名                                | 说明                                  |\n| mktime( YYYY MM dd HH MM ss[ DST]) | 生成时间格式                              |\n| strftime([format [, timestamp]])   | 格式化时间输出，将时间戳转为时间字符串 具体格式，见下表.       |\n| systime()                          | 得到时间戳,返回从1970年1月1日开始到当前时间(不计闰年)的整秒数 |\n\n```bash\n# 建指定时间(mktime使用） \n$ awk 'BEGIN{tstamp=mktime(\"2001 01 01 12 12 12\");print strftime(\"%c\",tstamp);}' \n2001年01月01日 星期一 12时12分12秒 \n\n$ awk 'BEGIN{tstamp1=mktime(\"2001 01 01 12 12 12\");tstamp2=mktime(\"2001 02 01 0 0 0\");print tstamp2-tstamp1;}' \n2634468 \n\n# 求2个时间段中间时间差，介绍了strftime使用方法 \n$ awk 'BEGIN{tstamp1=mktime(\"2001 01 01 12 12 12\");tstamp2=systime();print tstamp2-tstamp1;}' \n308201392\n```\n\nstrftime日期和时间格式说明符\n\n| 格式   | 描述                            |\n| ---- | ----------------------------- |\n| %a   | 星期几的缩写（Sun）                   |\n| %A   | 星期几的完整写法（Sunday）              |\n| %b   | 月名的缩写（Oct）                    |\n| %B   | 月名的完整写法（October）              |\n| %c   | 本地日期和时间                       |\n| %d   | 十进制日期                         |\n| %D   | 日期 08/20/99                   |\n| %e   | 日期，如果只有一位会补上一个空格              |\n| %H   | 用十进制表示24小时格式的时间               |\n| %I   | 用十进制表示12小时格式的时间               |\n| %j   | 从1月1日期一年中的第几天                 |\n| %m   | 十进制表示的月份                      |\n| %M   | 十进制表示的分钟                      |\n| %p   | 12小时表示法（AM/PM）                |\n| %S   | 十进制表示的秒                       |\n| %U   | 十进制表示的一年中的第几个星期（星期天作为一个星期的开始） |\n| %w   | 十进制表示的星期几（星期天是0）              |\n| %W   | 十进制表示的一年中的第几个星期（星期一作为一个星期的开始） |\n| %x   | 重新设置本地日期（08/20/99）            |\n| %X   | 重新设置本地时间（12 : 00 : 00）        |\n| %y   | 两位数字表示的年（99）                  |\n| %Y   | 当前月份                          |\n| %Z   | 时区（PDT）                       |\n| %%   | 百分号（%）                        |\n\n###  4.6 find 对目录中的所有文件进行文本替换\n\n```bash\n# 将所有.cpp文件中的Copyright替换成Copyleft：\n$ find . -name *.cpp -print0 | xargs -I{} -0 sed -i 's/Copyright/Copyleft/g' {}\n\n# 选项-exec实现同样的效果：\n$ find . -name *.cpp -exec sed -i 's/Copyright/Copyleft/g' \\{\\} \\;\n```\n\n ## 5 一团乱麻\n\n### 5.1 wget命令\n\n```bash\n-a<日志文件>：\t\t\t# 在指定的日志文件中记录资料的执行过程； \n-A<后缀名>：\t\t\t # 指定要下载文件的后缀名，多个后缀名之间使用逗号进行分隔； \n-b：\t\t\t\t\t   # 进行后台的方式运行wget； \n-B<连接地址>：\t\t\t# 设置参考的连接地址的基地地址； \n-c：\t\t\t\t\t   # 继续执行上次终端的任务； \n-C<标志>：\t\t\t\t # 设置服务器数据块功能标志on为激活，off为关闭，默认值为on； \n-d：\t\t\t\t\t   # 调试模式运行指令； \n-D<域名列表>：\t\t    # 设置顺着的域名列表，域名之间用“，”分隔； \n-e<指令>：\t\t\t\t # 作为文件“.wgetrc”中的一部分执行指定的指令； \n-h：\t\t\t\t\t   # 显示指令帮助信息； \n-i<文件>：\t\t\t\t # 从指定文件获取要下载的URL地址； \n-l<目录列表>：\t\t\t# 设置顺着的目录列表，多个目录用“，”分隔； \n-L：\t\t\t\t\t   # 仅顺着关联的连接； \n-r：\t\t\t\t\t   # 递归下载方式； \n-nc：\t\t\t\t   # 文件存在时，下载文件不覆盖原有文件； \n-nv：\t\t\t\t   # 下载时只显示更新和出错信息，不显示指令的详细执行过程； \n-q：\t\t\t\t\t   # 不显示指令执行过程； \n-nh：\t\t\t\t   # 不查询主机名称； \n-v：\t\t\t\t\t   # 显示详细执行过程； \n-V：\t\t\t\t\t   # 显示版本信息； \n--passive-ftp：\t\t   # 使用被动模式PASV连接FTP服务器； \n--follow-ftp：\t\t   # 从HTML文件中下载FTP连接文件。\n```\n\n```bash\n# 使用wget下载单个文件 \n$ wget http://www.linuxde.net/testfile.zip\n\n# 下载并以不同的文件名保存 \n$ wget -O wordpress.zip http://www.linuxde.net/download.aspx?id=1080\n\n# wget限速下载 \n$ wget --limit-rate=300k http://www.linuxde.net/testfile.zip\n\n# 使用wget断点续传 \n$ wget -c http://www.linuxde.net/testfile.zip\n\n# 使用wget后台下载 \n$ wget -b http://www.linuxde.net/testfile.zip \nContinuing in background, pid 1840. \nOutput will be written to `wget-log'.\n# 对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载，你可以使用以下命令来察看下载进度： \n$ tail -f wget-log\n\n# 伪装代理名称下载 \n$ wget --user-agent=\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16\" http://www.linuxde.net/testfile.zip\n# 有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过--user-agent参数伪装。\n```\n\n**测试下载链接：**\n\n当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加--spider参数进行检查。 \n\n```bash\n$ wget --spider URL\n```\n\n如果下载链接正确，将会显示:\n\n```text\nSpider mode enabled. Check if remote file exists. \nHTTP request sent, awaiting response... 200 OK \nLength: unspecified [text/html] \nRemote file exists and could contain further links, \nbut recursion is disabled -- not retrieving.\n```\n\n这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误:\n\n```bash\n$ wget --spider url \nSpider mode enabled. Check if remote file exists. \nHTTP request sent, awaiting response... 404 Not Found \nRemote file does not exist -- broken link!!!\n```\n\n你可以在以下几种情况下使用--spider参数：\n\n* 定时下载之前进行检查\n* 间隔检测网站是否可用\n* 检查网站页面的死链接\n\n```bash\n# 增加重试次数 \n$ wget --tries=40 URL\n\n# 下载多个文件 \n$ wget -i filelist.txt \n# 首先，保存一份下载链接文件： \n$ cat > filelist.txt \nurl1 \nurl2 \nurl3 \nurl4 \n# 接着使用这个文件和参数-i下载。\n\n# 过滤指定格式下载 \n$ wget --reject=gif ur \n# 下载一个网站，但你不希望下载图片，可以使用这条命令。\n\n# 把下载信息存入日志文件 \n$ wget -o download.log URL \n# 不希望下载信息直接显示在终端而是在一个日志文件，可以使用。 \n\n# 限制总下载文件大小 \n$ wget -Q5m -i filelist.txt \n# 当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。\n```\n\n**镜像网站：**\n\n```bash\n$ wget --mirror -p --convert-links -P ./LOCAL URL \n```\n\n下载整个网站到本地。\n\n* --mirror 开户镜像下载\n* -p 下载所有为了html页面显示正常的文件\n* --convert-links 下载后，转换成本地的链接\n* -P ./LOCAL URL 保存所有文件和目录到本地指定目录\n\n**下载指定格式文件：**\n\n```bash\n$ wget -r -A.pdf url\n```\n\n可以在以下情况使用该功能：\n\n* 下载一个网站的所有图片\n* 下载一个网站的所有视频\n* 下载一个网站的所有PDF文件\n\n**FTP下载：**\n\n```bash\n$ wget ftp-url \n$ wget --ftp-user=USERNAME --ftp-password=PASSWORD url\n```\n\n可以使用wget来完成ftp链接的下载。 \n\n使用wget匿名ftp下载：\n\n```bash\n$ wget ftp-url\n```\n\n使用wget用户名和密码认证的ftp下载： \n\n```bash\n$ wget --ftp-user=USERNAME --ftp-password=PASSWORD url\n```\n\n### 5.2 curl 命令\n\n常见参数：\n\n```bash\n-A/--user-agent <string>              # 设置用户代理发送给服务器\n-b/--cookie <name=string/file>    \t  # cookie字符串或文件读取位置\n-c/--cookie-jar <file>                # 操作结束后把cookie写入到这个文件中\n-C/--continue-at <offset>             # 断点续转\n-D/--dump-header <file>               # 把header信息写入到该文件中\n-e/--referer                          # 来源网址\n-f/--fail                             # 连接失败时不显示http错误\n-o/--output                           # 把输出写到该文件中\n-O/--remote-name                      # 把输出写到该文件中，保留远程文件的文件名\n-r/--range <range>                    # 检索来自HTTP/1.1或FTP服务器字节范围\n-s/--silent                           # 静音模式。不输出任何东西\n-T/--upload-file <file>               # 上传文件\n-u/--user <user[:password]>           # 设置服务器的用户和密码\n-w/--write-out [format]               # 什么输出完成后\n-x/--proxy <host[:port]>              # 在给定的端口上使用HTTP代理\n-#/--progress-bar                     # 进度条显示当前的传送状态\n```\n\n```bash\n# 不显示进度信息使用--silent选项。\n$ curl URL --silent\n\n# 使用选项 -O 将下载的数据写入到文件，必须使用文件的绝对地址：\n$ curl http://man.linuxde.net/text.iso --silent -O\n\n# 选项-o将下载数据写入到指定名称的文件中，并使用--progress显示进度条：\n$ curl http://man.linuxde.net/test.iso -o filename.iso --progress\n######################################### 100.0%\n\n# 断点续传\n$ curl URL/File -C 偏移量 \n# 偏移量是以字节为单位的整数，如果让curl自动推断出正确的续传位置使用-C -： \n$ curl -C -URL\n\n# 使用--referer选项指定参照页字符串： \n$ curl --referer http://www.google.com http://man.linuxde.net \n\n# 用curl设置cookies 使用--cookie \"COKKIES\"选项来指定cookie，多个cookie使用分号分隔： \n$ curl http://man.linuxde.net --cookie \"user=root;pass=123456\" \n# 将cookie另存为一个文件，使用--cookie-jar选项： \n$ curl URL --cookie-jar cookie_file \n\n# 用curl设置用户代理字符串 有些网站访问会提示只能使用IE浏览器来访问，这是因为这些网站设置了检查用户代理，可以使用curl把用户代理设置为IE，这样就可以访问了。使用--user-agent或者-A选项：\n$ curl URL --user-agent \"Mozilla/5.0\" curl URL -A \"Mozilla/5.0\" \n# 其他HTTP头部信息也可以使用curl来发送，使用-H\"头部信息\" 传递多个头部信息，例如： \n$ curl -H \"Host:man.linuxde.net\" -H \"accept-language:zh-cn\" URL \n\n# curl的带宽控制和下载配额 使用--limit-rate限制curl的下载速度： \n$ curl URL --limit-rate 50k \n# 命令中用k（千字节）和m（兆字节）指定下载速度限制。 \n\n# 使用--max-filesize指定可下载的最大文件大小： \n$ curl URL --max-filesize bytes \n# 如果文件大小超出限制，命令则返回一个非0退出码，如果命令正常则返回0。 \n\n# 用curl进行认证 使用curl选项 -u 可以完成HTTP或者FTP的认证，可以指定密码，也可以不指定密码在后续操作中输入密码： \n$ curl -u user:pwd http://man.linuxde.net \n$ curl -u user http://man.linuxde.net \n\n# 只打印响应头部信息 通过-I或者-head可以只打印出HTTP头部信息： \n$ curl -I http://man.linuxde.net \nHTTP/1.1 200 OK \nServer: nginx/1.2.5 \ndate: Mon, 10 Dec 2012 09:24:34 GMT \nContent-Type: text/html; charset=UTF-8 \nConnection: keep-alive \nVary: Accept-Encoding \nX-Pingback: http://man.linuxde.net/xmlrpc.php\n```\n\n其他参数：\n\n\n\n```bash\n-a/--append                    # 上传文件时，附加到目标文件\n--anyauth                      # 可以使用“任何”身份验证方法\n--basic                        # 使用HTTP基本验证\n-B/--use-ascii                 # 使用ASCII文本传输\n-d/--data <data>               # HTTP POST方式传送数据\n--data-ascii <data>            # 以ascii的方式post数据\n--data-binary <data>           # 以二进制的方式post数据\n--negotiate                    # 使用HTTP身份验证\n--digest                       # 使用数字身份验证\n--disable-eprt                 # 禁止使用EPRT或LPRT\n--disable-epsv                 # 禁止使用EPSV\n--egd-file <file>              # 为随机数据(SSL)设置EGD socket路径\n--tcp-nodelay                  # 使用TCP_NODELAY选项\n-E/--cert <cert[:passwd]>      # 客户端证书文件和密码 (SSL)\n--cert-type <type>             # 证书文件类型 (DER/PEM/ENG) (SSL)\n--key <key>                    # 私钥文件名 (SSL)\n--key-type <type>              # 私钥文件类型 (DER/PEM/ENG) (SSL)\n--pass  <pass>                 # 私钥密码 (SSL)\n--engine <eng>                 # 加密引擎使用 (SSL). \"--engine list\" for list\n--cacert <file>                # CA证书 (SSL)\n--capath <directory>           # CA目   (made using c_rehash) to verify peer against (SSL)\n--ciphers <list>               # SSL密码\n--compressed                   # 要求返回是压缩的形势 (using deflate or gzip)\n--connect-timeout <seconds>    # 设置最大请求时间\n--create-dirs                  # 建立本地目录的目录层次结构\n--crlf                         # 上传是把LF转变成CRLF\n--ftp-create-dirs              # 如果远程目录不存在，创建远程目录\n--ftp-method [multicwd/nocwd/singlecwd]    # 控制CWD的使用\n--ftp-pasv                     # 使用 PASV/EPSV 代替端口\n--ftp-skip-pasv-ip             # 使用PASV的时候,忽略该IP地址\n--ftp-ssl                      # 尝试用 SSL/TLS 来进行ftp数据传输\n--ftp-ssl-reqd                 # 要求用 SSL/TLS 来进行ftp数据传输\n-F/--form <name=content>       # 模拟http表单提交数据\n-form-string <name=string>     # 模拟http表单提交数据\n-g/--globoff                   # 禁用网址序列和范围使用{}和[]\n-G/--get                       # 以get的方式来发送数据\n-h/--help                      # 帮助\n-H/--header <line>             # 自定义头信息传递给服务器\n--ignore-content-length        # 忽略的HTTP头信息的长度\n-i/--include                   # 输出时包括protocol头信息\n-I/--head                      # 只显示文档信息\n-j/--junk-session-cookies      # 读取文件时忽略session cookie\n--interface <interface>        # 使用指定网络接口/地址\n--krb4 <level>                 # 使用指定安全级别的krb4\n-k/--insecure                  # 允许不使用证书到SSL站点\n-K/--config                    # 指定的配置文件读取\n-l/--list-only                 # 列出ftp目录下的文件名称\n--limit-rate <rate>            # 设置传输速度\n--local-port<NUM>              # 强制使用本地端口号\n-m/--max-time <seconds>        # 设置最大传输时间\n--max-redirs <num>             # 设置最大读取的目录数\n--max-filesize <bytes>         # 设置最大下载的文件总量\n-M/--manual                    # 显示全手动\n-n/--netrc                     # 从netrc文件中读取用户名和密码\n--netrc-optional               # 使用 .netrc 或者 URL来覆盖-n\n--ntlm                         # 使用 HTTP NTLM 身份验证\n-N/--no-buffer                 # 禁用缓冲输出\n-p/--proxytunnel               # 使用HTTP代理\n--proxy-anyauth                # 选择任一代理身份验证方法\n--proxy-basic                  # 在代理上使用基本身份验证\n--proxy-digest                 # 在代理上使用数字身份验证\n--proxy-ntlm                   # 在代理上使用ntlm身份验证\n-P/--ftp-port <address>        # 使用端口地址，而不是使用PASV\n-Q/--quote <cmd>               # 文件传输前，发送命令到服务器\n--range-file                   # 读取（SSL）的随机文件\n-R/--remote-time               # 在本地生成文件时，保留远程文件时间\n--retry <num>                  # 传输出现问题时，重试的次数\n--retry-delay <seconds>        # 传输出现问题时，设置重试间隔时间\n--retry-max-time <seconds>     # 传输出现问题时，设置最大重试时间\n-S/--show-error                # 显示错误\n--socks4 <host[:port]>         # 用socks4代理给定主机和端口\n--socks5 <host[:port]>         # 用socks5代理给定主机和端口\n-t/--telnet-option <OPT=val>   # Telnet选项设置\n--trace <file>                 # 对指定文件进行debug\n--trace-ascii <file>           # Like --跟踪但没有hex输出\n--trace-time                   # 跟踪/详细输出时，添加时间戳\n--url <URL>                    # Spet URL to work with\n-U/--proxy-user <user[:password]>  \t# 设置代理用户名和密码\n-V/--version                   # 显示版本信息\n-X/--request <command>         # 指定什么命令\n-y/--speed-time                # 放弃限速所要的时间。默认为30\n-Y/--speed-limit               # 停止传输速度的限制，速度时间'秒\n-z/--time-cond                 # 传送时间设置\n-0/--http1.0                   # 使用HTTP 1.0\n-1/--tlsv1                     # 使用TLSv1（SSL）\n-2/--sslv2                     # 使用SSLv2的（SSL）\n-3/--sslv3                     # 使用的SSLv3（SSL）\n--3p-quote                     # like -Q for the source URL for 3rd party transfer\n--3p-url                       # 使用url，进行第三方传送\n--3p-user                      # 使用用户名和密码，进行第三方传送\n-4/--ipv4                      # 使用IP4\n-6/--ipv6                      # 使用IP6\n```\n\n### 5.3 curl wget两种方法模拟http的get post请求\n\n**get请求：**\n\n```bash\n# 使用curl命令：\n$ curl \"http://www.baidu.com\"  \t\t# 如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地\n$ curl -i \"http://www.baidu.com\"  \t# 显示全部信息\n$ curl -l \"http://www.baidu.com\" \t# 只显示头部信息\n$ curl -v \"http://www.baidu.com\" \t# 显示get请求全过程解析\n```\n\n```bash\n# 使用wget命令：\n$ wget \"http://www.baidu.com\"\n```\n\n**post请求：**\n\n```bash\n# 使用curl命令(通过-d参数，把访问参数放在里面)：\n$ curl -d \"param1=value1&param2=value2\" \"http://www.baidu.com\"\n```\n\n```bash\n# 使用wget命令：（--post-data参数来实现）\n$ wget --post-data 'user=foo&password=bar'  http://server.com/auth.PHP\n```\n\n## 6 B计划\n\n### 6.1 用tar归档\n\ntar支持的参数包括： `A`、 `c`、 `d`、 `r`、 `u`、 `x`、 `f` 和 `v` \n\n```bash\n# 用tar对文件进行归档：\n$ tar -cf output.tar file1 file2 file3 folder1 ..\n\n# 使用选项-t列出归档文件中所包含的文件：\n$ tar -tf archive.tar\nfile1\nfile2\n\n# 如果需要在归档或列出归档文件列表时获知更多的细节信息，可以使用-v或-vv参数\n$ $ tar -tvf archive.tar\n-rw-rw-r-- shaan/shaan 0 2013-04-08 21:34 file1\n-rw-rw-r-- shaan/shaan 0 2013-04-08 21:34 file2\n# 文件名必须紧跟在-f之后，而且-f应该是选项中的最后一个。\n\n# 向归档文件中添加文件,追加选项-r\n$ tar -rvf original.tar new_file\n\n# 用下面的方法列出归档文件中的内容：\n$ tar -tf archive.tar\nhello.txt\n\n# 从归档文件中提取文件或文件夹, -x 表示提取\n$ tar -xf archive.tar\n\n# 用选项-C来指定需要将文件提取到哪个目录：\n$ tar -xf archive.tar -C /path/to/extraction_directory\n\n# 可以通过将文件名指定为命令行参数来提取特定的文件：\n$ tar -xvf file.tar file1 file4\n# 上面的命令只提取file1和file4，忽略其他文件。\n\n# 在tar中使用stdin和stdout\n$ tar cvf - files/ | ssh user@example.com \"tar xv -C Documents/\"\n# 在上面的例子中，对files目录中的内容进行了归档并输出到stdout（由'-'指明）。\n\n# 拼接两个归档文件, -A 选项轻松地合并多个tar文件\n$ tar -Af file1.tar file2.tar\n# 查看内容，验证操作是否成功：\n$ tar -tvf file1.tar\n\n# 通过检查时间戳来更新归档文件中的内容\n# 可以用更新选项-u指明：只有比归档文件中的同名文件更新时才会被添加。\n$ tar -tf archive.tar\nfilea\nfileb\nfilec\n\n# 仅当filea自上次被加入archive.tar后出现了变动才对其进行追加，可以使用：\n$ tar -uf archive.tar filea\n# 如果两个filea的时间戳相同，则什么都不会发生。\n# 可用touch命令修改文件的时间戳，然后再用tar命令：\n$ tar -uvvf archive.tar filea\n-rw-r--r-- slynux/slynux 0 2010-08-14 17:53 filea\n\n# 比较归档文件与文件系统中的内容, 选项 -d 可以打印出两者之间的差别：\n$ tar -df archive.tar\nafile: Mod time differs\nafile: Size differs\n\n# 从归档文件中删除文件, --delete选项从给定的归档文件中删除文件\n$ tar -tf archive.tar\nfilea\nfileb\nfilec\n# 删除filea：\n$ tar --delete --file archive.tar filea\n$ tar -tf archive.tar\nfileb\nfilec\n```\n\n**压缩tar归档文件：**\n\n归档文件通常被压缩成下列格式之一： \n\n* file.tar.gz \n* file.tar.bz2 \n* file.tar.lzma \n\n不同的tar选项可以用来指定不同的压缩格式： \n\n* -j 指定bunzip2格式； \n* -z 指定gzip格式； \n* --lzma 指定lzma格式。 \n\n```bash\n# 为了让tar支持根据扩展名自动进行压缩，使用 -a或 --auto-compress选项：\n$ tar acvf archive.tar.gz filea fileb filec\n\n# 从归档中排除部分文件,  --exclude [PATTERN]排除匹配通配符样式的文件\n$ tar -cf arch.tar * --exclude \"*.txt\"\n# 样式应该使用双引号来引用，避免shell对其进行扩展。\n\n# 也可以将需要排除的文件列表放入文件中，同时配合选项 -X：\n$ cat list\nfilea\nfileb\n$ tar -cf arch.tar * -X list\n\n# 排除版本控制目录， 可以使用tar的 --exclude-vcs选项。例如：\n$ tar --exclude-vcs -czvvf source_code.tar.gz eye_of_gnome_svn\n\n# 打印总字节数，用–totals就可以在归档完成之后打印出总归档字节数：\n$ tar -cf arc.tar * --exclude \"*.txt\" --totals\nTotal bytes written: 20480 (20KiB, 12MiB/s)\n```\n\n### 6.2 用cpio归档\n\n```bash\n# 创建测试文件：\n$ touch file1 file2 file3\n\n# 将测试文件按照下面的方法进行归档：\n$ echo file1 file2 file3 | cpio -ov > archive.cpio\n\n# 列出cpio归档文件中的内容：\n$ cpio -it < archive.cpio\n\n# 从cpio归档文件中提取文件：\n$ cpio -id < archive.cpio\n```\n\n对于归档命令： \n\n* -o 指定了输出； \n* -v 用来打印归档文件列表。 \n\n在列出给定cpio归档文件所有内容的命令中： \n\n* -i 用于指定输入； \n* -t 表示列出归档文件中的内容。 \n\n当使用命令进行提取时， -d用来表示提取。 cpio在覆盖文件时不会发出提示。 \n\n### 6.3 使用gzip压缩数据\n\n```bash\n# 要使用gzip压缩文件，可以使用下面的命令：\n$ gzip filename\n$ ls\nfilename.gz\n\n# 将gzip文件解压缩的方法如下：\n$ gunzip filename.gz\n$ ls\nfile\n\n# 列出压缩文件的属性信息：\n$ gzip -l test.txt.gz\ncompressed uncompressed ratio uncompressed_name\n35 \t\t\t\t6 \t\t-33.3% \ttest.txt\n\n# gzip命令可以从stdin中读入文件，也可以将压缩文件写出到stdout，选项 -c用来将输出指定到stdout。\n$ cat file | gzip -c > file.gz\n\n# 我们可以指定gzip的压缩级别。用 --fast或 --best选项分别提供最低或最高的压缩比。\n```\n\n```bash\n# 压缩归档文件\n# 方法 1\n$ tar -czvvf archive.tar.gz [FILES]\n或者\n$ tar -cavvf archive.tar.gz [FILES]\n# 选项 -a表明从文件扩展名自动推断压缩格式。\n\n# 方法 2\n# 首先，创建一个tar归档文件：\n$ tar -cvvf archive.tar [FILES]\n# 压缩tar归档文件：\n$ gzip archive.tar\n\n# zcat——无需解压缩，直接读取gzip格式文件\n$ ls\ntest.gz\n$ zcat test.gz\nA test file\n# 文件test包含了一行文本\"A test file\"\n$ ls\ntest.gz\n\n# 压缩率\n# 我们可以指定压缩率，它共有9级，其中：\n# 1级的压缩率最低，但是压缩速度最快；\n# 9级的压缩率最高，但是压缩速度最慢。\n$ gzip -5 test.img\n# 这应该能在压缩速度和压缩比之间获得一个不错的平衡。\n\n# 使用bzip2，唯一的不同在于bzip2的压缩效率比gzip更高，但花费的时间比gzip更长\n$ bzip2 filename\n# 解压缩bzip2格式的文件：\n$ bunzip2 filename.bz2\n# 生成tar.bz2文件并从中提取内容的方法同之前介绍的tar.gz类似：\n$ tar -xjvf archive.tar.bz2\n# 其中-j表明该归档文件是bzip2格式。\n\n# 使用lzma\n# lzma是另一种压缩工具，它的压缩率甚至比gzip和bzip2更好。\n$ lzma filename\n# 解压缩lzma文件：\n$ unlzma filename.lzma\n# 可以使用tar命令的--lzma选项对生成的tar归档文件进行压缩或提取：\n$ tar -cvvf --lzma archive.tar.lzma [FILES]\n或者\n$ tar -cavvf archive.tar.lzma [FILES]\n# 如果要将经过lzma压缩过的tar归档文件中的内容提取到指定的目录中，可以使用：\n$ tar -xvvf --lzma archive.tar.lzma -C extract_directory\n# 其中， -x用于提取内容， --lzma指定使用lzma对归档文件进行解压缩。\n# 我们也可以用：\n$ tar -xavvf archive.tar.lzma -C extract_directory\n```\n\n### 6.4 用 zip 归档和压缩\n\n```bash\n# 对归档文件采用ZIP格式进行压缩：\n$ zip file.zip file\n\n# 对目录和文件进行递归操作, -r 用于指定递归操作：\n$ zip -r archive.zip folder1 folder2\n\n#  要从ZIP文件中提取内容，可以使用：\n$ unzip file.zip\n# 在完成提取操作之后， unzip并不会删除file.zip\n\n# 如果需要更新压缩文件中的内容，使用选项 -u：\n$ zip file.zip -u newfile\n\n# 从压缩文件中删除内容，则使用-d：\n$ zip -d arc.zip file.txt\n\n# 列出压缩文件中的内容：\n$ unzip -l archive.zip\n```\n\n### 6.5 更快的归档工具 pbzip2\n\n```bash\n# 压缩单个文件：\n$ pbzip2 myfile.tar\n# pbzip2会自动检测系统中处理器核心的数量，然后将myfile.tar压缩成myfile.tar.bz2。\n\n# 要将多个文件或目录进行归档及压缩，可以使用tar配合pbzip2来实现：\n$ tar cf myfile.tar.bz2 --use-compress-prog=pbzip2 dir_to_compress/\n或者\n$ tar -c directory_to_compress/ | pbzip2 -c > myfile.tar.bz2\n\n# 从pbzip2格式的文件中进行提取。\n# 如果是tar.bz2文件，我们可以一次性完成解压缩和提取工作：\n$ pbzip2 -dc myfile.tar.bz2 | tar x\n# 如果是经过pbzip2压缩过的归档文件，可以使用：\n$ pbzip2 -d myfile.tar.bz2\n\n# 手动指定处理器数量, 使用pbzip2的-p选项来手动指定处理器核心的数量\n$ pbzip2 -p4 myfile.tar\n# 上面的命令告诉pbzip2使用4个处理器核心。\n\n# 指定压缩比\n# 像其他压缩工具一样，我们可以使用从1到9的选项来分别指定最快和最优的压缩比。\n```\n\n### 6.6 创建压缩文件系统 \n\nsquashfs是一种具有超高压缩率的只读型文件系统，这种文件系统能够将2GB~3GB的数据压缩成一个700MB的文件。  \n\n```bash\n# 添加源目录和文件，创建一个squashfs文件：\n$ sudo mksquashfs /etc test.squashfs\nParallel mksquashfs: Using 2 processors\nCreating 4.0 filesystem on test.squashfs, block size 131072.\n[=======================================] 1867/1867 100%\n\n# 利用环回形式挂载squashfs文件：\n$ mkdir /mnt/squash\n$ mount -o loop compressedfs.squashfs /mnt/squash\n# 你可以访问/mnt/squashfs访问其中的内容。\n\n# 在创建squashfs文件时排除部分文件, 选项-e，将需要排除的文件列表以命令行参数的方式来指定。例如：\n$ sudo mksquashfs /etc test.squashfs -e /etc/passwd /etc/shadow\n# 也可以将需要排除的文件名列表写入文件，然后用 -ef指定该文件：\n$ cat excludelist\n/etc/passwd\n/etc/shadow\n$ sudo mksquashfs /etc test.squashfs -ef excludelist\n```\n\n### 6.7 使用 rsync 备份系统快照 \n\nrsync可以对位于不同位置的文件和目录进行同步，它利用差异计算以及压缩技术来最小化数据传输量。 \n\nrsync也支持压缩、加密等多种特性。 \n\n```bash\n# 将源目录复制到目的端：\n$ rsync -av /home/slynux/data slynux@192.168.0.6:/home/backups/data\n# 其中：\n -a表示要进行归档；\n -v表示在stdout上打印出细节信息或进度。\n\n# 将数据备份到远程服务器或主机：\n$ rsync -av source_dir username@host:PATH\n\n# 用下面的方法将远程主机上的数据恢复到本地主机：\n$ rsync -av username@host:PATH destination\n\n# 通过网络进行传输时，压缩数据能够明显改善传输效率。我们可以用rsync的选项 -z 指定在网络传输时压缩数据。例如：\n$ rsync -avz source destination\n\n# 将一个目录中的内容同步到另一个目录：\n$ rsync -av /home/test/ /home/backups\n# 这条命令将源目录（/home/test）中的内容（不包括目录本身）复制到现有的backups目录中\n\n# 在使用rsync进行归档的过程中排除部分文件\n$ rsync -avz /home/code/some_code /mnt/disk/backup/code --exclude \"*.txt\"\n# 或者我们可以通过一个列表文件指定需要排除的文件。\n# 这可以利用--exclude-from FILEPATH。\n\n# 在更新rsync备份时，删除不存在的文件, rsync并不会在目的端删除那些在源端已不存在的文件\n$ rsync -avz SOURCE DESTINATION --delete\n\n# 定期进行备份\n$ crontab -ev\n# 添加上这么一行：\n0 */10 * * * rsync -avz /home/code user@IP_ADDRESS:/home/backups\n# 上面的crontab条目将rsync调度为每10个小时运行一次。\n```\n\n### 6.8 用 fsarchiver 创建全盘镜像 \n\n```bash\n# 创建文件系统/分区备份。\n# 使用fsarchiver的savefs选项：\n$ fsarchiver savefs backup.fsa /dev/sda1\n\n# 同时备份多个分区。\n$ fsarchiver savefs backup.fsa /dev/sda1 /dev/sda2\n\n# 从备份归档中恢复分区。\n$ fsarchiver restfs backup.fsa id=0,dest=/dev/sda1\n# id=0 表 明 我 们 希 望 从 备 份 归 档 中 提 取 第 一 个 分 区 的 内 容 ， 将 其 恢 复 到 由 dest=/dev/sda1所指定的分区中。\n\n# 从备份归档中恢复多个分区。\n# 像之前一样，使用restfs选项：\n$ fsarchiver restfs backup.fsa id=0,dest=/dev/sda1 id=1,dest=/dev/sdb1\n```\n\n## 7 无网不利\n\n### 7.1 设置网络\n\n```bash\n# 手动设置网络接口的IP地址：\n$ ifconfig wlan0 192.168.0.80\n\n# 使用以下命令设置比IP地址的子网掩码：\n$ ifconfig wlan0 192.168.0.80 netmask 255.255.252.0\n\n# 自动配置网络接口\n$ dhclient eth0\n\n# 打印网络接口列表\n$ ifconfig | cut -c-10 | tr -d ' ' | tr -s '\\n'\nlo\nwlan0\n\n# 显示IP地址\n$ ifconfig wlan0 | egrep -o \"inet addr:[^ ]*\" | grep -o \"[0-9.]*\"\n192.168.0.82\n\n# 硬件地址（MAC地址）欺骗\n$ ifconfig eth0 hw ether 00:1c:bf:87:25:d5\n\n# 名字服务器与DNS（域名服务）\n$ cat /etc/resolv.conf\nnameserver 8.8.8.8\n# 我们可以像下面这样手动添加名字服务器：\n$ echo nameserver IP_ADDRESS >> /etc/resolv.conf\n\n# DNS查找\n$ host google.com\ngoogle.com has address 64.233.181.105\ngoogle.com has address 64.233.181.99\ngoogle.com has address 64.233.181.147\ngoogle.com has address 64.233.181.106\ngoogle.com has address 64.233.181.103\ngoogle.com has address 64.233.181.104\n\n$ nslookup google.com\nServer: 8.8.8.8\nAddress: 8.8.8.8#53\nNon-authoritative answer:\nName: google.com\nAddress: 64.233.181.105\nName: google.com\nAddress: 64.233.181.99\nName: google.com\nAddress: 64.233.181.147\nName: google.com\nAddress: 64.233.181.106\nName: google.com\nAddress: 64.233.181.103\nName: google.com\nAddress: 64.233.181.104\nServer: 8.8.8.8\n# 上面最后一行对应着用于DNS解析的默认名字服务器。\n\n# 如果不使用DNS服务器，也可以为IP地址解析添加符号名，这只需要向文件 /etc/hosts中加入条目即可。\n# 用下面的方法进行添加：\n$ echo IP_ADDRESS symbolic_name >> /etc/hosts\n# 例如：\n$ echo 192.168.0.9 backupserver >> /etc/hosts\n# 添加了条目之后，任何时候解析backupserver，都会返回192.168.0.9。\n\n# 显示路由表信息\n$ route\nKernel IP routing table\nDestination \tGateway \tGenmask \t\tFlags \tMetric \tRef \tUseIface\n192.168.0.0 \t* \t\t\t255.255.252.0 \tU \t\t2 \t\t0 \t\t0wlan0\nlink-local \t\t* \t\t\t255.255.0.0 \tU \t\t1000 \t0 \t\t0wlan0\ndefault \t\tp4.local \t0.0.0.0 \t\tUG \t\t0 \t\t0 \t\t0wlan0\n# 也可以使用：\n$ route -n\nKernel IP routing table\nDestination \tGateway \tGenmask \t\tFlags \tMetric \tRef \tUse \tIface\n192.168.0.0 \t0.0.0.0 \t255.255.252.0 \tU \t\t2 \t\t0 \t\t0 \t\twlan0\n169.254.0.0 \t0.0.0.0 \t255.255.0.0 \tU \t\t1000 \t0 \t\t0 \t\twlan0\n0.0.0.0 \t\t192.168.0.4 0.0.0.0 \t\tUG \t\t0 \t\t0 \t\t0 \t\twlan0\n# -n指定以数字形式显示地址。如果使用-n， route会以数字形式的IP地址显示每一个条目；否则，如果IP地址具有对应的DNS条目，就会显示符号形式的主机名。\n\n# 设置默认网关：\n$ route add default gw 192.168.0.1 wlan0\n```\n\n### 7.2 traceroute 命令\n\ntraceroute，它可以显示分组途径的所有网关的地址。 traceroute信息可以帮助我们搞明白分组到达目的地需要经过多少跳（hop）。中途的网关或路由器的数量给出了一个测量网络上两个节点之间距离的度量\n（metric）。 traceroute的输出如下： \n\n```bash\n$ traceroute google.com\ntraceroute to google.com (74.125.77.104), 30 hops max, 60 byte packets\n1 gw-c6509.lxb.as5577.net (195.26.4.1) 0.313 ms 0.371 ms 0.457 ms\n2 40g.lxb-fra.as5577.net (83.243.12.2) 4.684 ms 4.754 ms 4.823 ms\n3 de-cix10.net.google.com (80.81.192.108) 5.312 ms 5.348 ms 5.327 ms\n4 209.85.255.170 (209.85.255.170) 5.816 ms 5.791 ms 209.85.255.172\n(209.85.255.172) 5.678 ms\n5 209.85.250.140 (209.85.250.140) 10.126 ms 9.867 ms 10.754 ms\n6 64.233.175.246 (64.233.175.246) 12.940 ms 72.14.233.114\n(72.14.233.114) 13.736 ms 13.803 ms\n7 72.14.239.199 (72.14.239.199) 14.618 ms 209.85.255.166\n(209.85.255.166) 12.755 ms 209.85.255.143 (209.85.255.143) 13.803 ms\n8 209.85.255.98 (209.85.255.98) 22.625 ms 209.85.255.110\n(209.85.255.110) 14.122 ms\n*\n9 ew-in-f104.1e100.net (74.125.77.104) 13.061 ms 13.256 ms 13.484 ms\n```\n\n### 7.3 列出网络上所有的活动主机 (fping) \n\nfping的选项如下： \n\n* 选项 -a指定打印出所有活动主机的IP地址； \n* 选项 -u指定打印出所有无法到达的主机； \n* 选项 -g指定从 \"IP地址/子网掩码\"记法或者\"IP地址范围\"记法中生成一组IP地址； \n\n```bash\n$ fping -a 192.160.1/24 -g\n# 或者\n$ fping -a 192.160.1 192.168.0.255 -g\n\n# 我们可以用已有的命令行工具来查询网络上的主机状态：\n$ fping -a 192.160.1/24 -g 2> /dev/null\n192.168.0.1\n192.168.0.90\n# 或者，使用：\n$ fping -a 192.168.0.1 192.168.0.255 -g\n\n# >/dev/null将由于主机无法到达所产生的错误信息打印到null设备。\n$ fping -a 192.168.0.1 192.168.0.5 192.168.0.6\n# 将IP地址作为参数传递\n$ fping -a < ip.list\n# 从文件中传递一组IP地址\n```\n\n### 7.4 ssh 命令\n\n```bash\n# SSH的压缩功能,选项-C启用这一功能：\n$ ssh -C user@hostname COMMANDS\n\n# 将数据重定向至远程shell命令的stdin\n$ echo 'text' | ssh user@remote_host 'echo'\ntext\n# 或者\n# 将文件中的数据进行重定向\n$ ssh user@remote_host 'echo' < file\n\n# 在远程主机中执行图形化命令\n# 对此，你需要像这样设置变量$DISPLAY：\n$ ssh user@host \"export DISPLAY=:0 ; command1; command2\"\"\"\n# 这将启用远程主机上的图形化输出。如果你想在本地主机上也显示图形化输出，使用SSH的X11转发选项（forwarding option）：\n$ ssh -X user@host \"command1; command2\n```\n\n### 7.5 通过网络传输文件 \n\n计算机联网的主要目的就是资源共享。在资源共享方面，使用最多的是文件共享。有多种方法可以用来在网络中传输文件。这则攻略就讨论了如何用常见的协议FTP、 SFTP、 RSYNC和SCP传输文件。 \n\n通过FTP传输文件可以使用lftp命令，通过SSH连接传输文件可以使用sftp， RSYNC使用SSH与rsync命令， scp通过SSH进行传输。 \n\n**文件传输协议（File Transfer Protocol， FTP） ：**\n\n```bash\n# 要连接FTP服务器传输文件，可以使用：\n$ lftp username@ftphost\n# 它会提示你输入密码，然后显示一个像下面那样的登录提示符：\nlftp username@ftphost:~>\n```\n\n你可以在提示符后输入命令，如下所示。 \n\n* 用cd directory改变目录。 \n* 用lcd改变本地主机的目录。 \n* 用mkdir创建目录。 \n* 列出远程机器当前目录下的文件使用Is。 \n* 用get filename下载文件：\n  `lftp username@ftphost:~> get filename `\n* 用put filename从当前目录上传文件：\n  `lftp username@ftphost:~> put filename `\n* 用quit退出lftp会话。 \n\n**FTP自动传输 ：**\n\nftp是另一个可用于FTP文件传输的命令。相比较而言， lftp的用法更灵活。 lftp和ftp为用户启动一个交互式会话（通过显示消息来提示用户输入）。 \n\n**SFTP（Secure FTP，安全FTP） ：**\n\n```bash\n$ cd /home/slynux\n$ put testfile.jpg\n$ get serverfile.jpg\n# 运行sftp：\n$ sftp user@domainname\n```\n\n**rsync命令 ：**\n\nrsync广泛用于网络文件复制及系统备份。 \n\n**SCP（Secure Copy Program，安全复制程序） ：**\n\n```bash\n$ scp filename user@remotehost:/home/path\n\n$ scp user@remotehost:/home/path/filename filename\n```\n\n用SCP进行递归复制 :\n\n```bash\n$ scp -r /home/slynux user@remotehost:/home/backups\n# 将目录/home/slynux递归复制到远程主机中\n# scp的 -p 选项能够在复制文件的同时保留文件的权限和模式。\n```\n\n### 7.6 连接网线网络\n\n我们需要用ifconfig分配IP地址和子网掩码才能连接上有线网络。对于无线网络来说，还需要其他工具（如iwconfig和iwlist）来配置更多的参数。 \n\niwlist工具扫描并列出可用的无线网络。用下面的命令进行扫描： \n\n```bash\n$ iwlist scan\nwlan0 \t\tScan completed :\n\t\t\tCell 01 - Address: 00:12:17:7B:1C:65\n\t\t\t\t\tChannel:11\n\t\t\t\t\tFrequency:2.462 GHz (Channel 11)\n\t\t\t\t\tQuality=33/70 Signal level=-77 dBm\n                    Encryption key:on\n\t\t\t\t\tESSID:\"model-2\"\n```\n\n### 7.7 在本地挂载点上挂载远程驱动器 \n\nsshfs允许你将远程文件系统挂载到本地挂载点上。 \n\n```bash\n# 将位于远程主机上的文件系统挂载到本地挂载点上：\n$ sshfs -o allow_other user@remotehost:/home/path /mnt/mountpoint\nPassword:\n\n# 完成任务后，可用下面的方法卸载：\n$ umount /mnt/mountpoint\n```\n\n### 7.8 网络流量与端口分析 \n\n列出系统中的开放端口以及运行在端口上的服务的详细信息，可以使用以下命令： \n\n```bash\n$ lsof -i\n\n# 要列出本地主机当前的开放端口，可以使用：\n$ lsof -i | grep \":[0-9]\\+->\" -o | grep \"[0-9]\\+\" -o | sort | uniq\n```\n\n用netstat查看开放端口与服务 ：\n\n```bash\n# netstat -tnp列出开放端口与服务：\n$ netstat -tnp\n```\n\n### 7.9 创建套接字\n\n最简单的方法就是使用netcat命令（或nc）。我们需要两个套接字：一个用来侦听，一个用来连接。 \n\n```bash\n# 设置侦听套接字：\n$ nc -l 1234\n# 这会在本地主机的端口1234上创建一个侦听套接字。\n\n# 连接到该套接字：\n$ nc HOST 1234\n\n# 要想发送消息，只需要在执行第2步操作的主机终端中输入信息并按回车键就行了。消息会出现在执行第1步操作的主机终端中。\n```\n\n在网络上进行快速文件复制 ：\n\n```bash\n# 在接收端执行下列命令：\n$ nc -l 1234 > destination_filename\n\n# 在发送端执行下列命令：\n$ nc HOST 1234 < source_filename\n```\n\n### 7.10 iptables防火墙设置\n\n\n\n```bash\n# 阻塞发送到特定IP地址的流量：\n$ iptables -A OUTPUT -d 8.8.8.8 -j DROP\n\n# 阻塞发送到特定端口的流量：\n$ iptables -A OUTPUT -p tcp -dport 21 -j DROP\n\n#  iptables中的第一个选项-A表明向链（chain）中添加一条新的规则，该规则由后续参数给出。OUTPUT链，它可以对所有出站（outgoing）的流量进行控制。-d指定了所要匹配的分组目的地址。-j来使iptables丢弃（DROP）符合条件的分组。-p指定该规则是适用于TCP， -dport指定了对应的端口。\n\n# 清除对iptables链所做出的所有改动。\n$ iptables --flush\n```\n\n## 8 当个好管家\n\n### 8.1 监视磁盘使用情况 \n\n`df` 是disk free的缩写， `du` 是disk usage的缩写。 \n\n```bash\n# 找出某个文件（或多个文件）占用的磁盘空间：\n$ du file.txt\n\n# 要获得某个目录中所有文件的磁盘使用情况，并在每一行中显示各个文件的磁盘占用详情，可以使用：\n$ du -a DIRECTORY\n\n# 以KB、 MB或块（block）为单位显示磁盘使用情况\n$ du -h FILENAME\n\n# 显示磁盘使用总计, -c 可以输出作为命令参数的所有文件和目录的磁盘使用情况\n$ du -c process_log.shpcpu.sh\n4 process_log.sh\n4 pcpu.sh\n8 total\n\n# -s（summarize，合计）则只输出合计数据。它可以配合 -h打印出人们易读的格式。\n$ du -sh slynux\n680K slynux\n\n# 打印以字节（默认输出）为单位的文件大小：\n$ du -b FILE(s)\n\n# 打印以KB为单位的文件大小：\n$ du -k FILE(s)\n\n# 打印以MB为单位的文件大小：\n$ du -m FILE(s)\n\n# 打印以指定块为单位的文件大小：\n$ du -B BLOCK_SIZE FILE(s)\n\n# 从磁盘使用统计中排除部分文件\n$ du --exclude \"*.txt\" FILES(s)\n# 排除所有的.txt文件\n$ du --exclude-from EXCLUDE.txt DIRECTORY\n# EXCLUDE.txt包含了需要排除的文件列表\n\n# --max-depth指定du应该遍历的目录层次的最大深度。\n$ du --max-depth 2 DIRECTORY\n\n# 找出指定目录中最大的10个文件\n$ du -ak /home/slynux | sort -nrk 1 | head -n 4\n\n$ find . -type f -exec du -k {} \\; | sort -nrk 1 | head\n```\n\ndu提供磁盘使用情况信息，而df提供磁盘可用空间信息。 \n\n```bash\n$ df -h\nFilesystem \t\t\tSize \tUsed \tAvail \tUse% \tMounted on\n/dev/sda1 \t\t\t9.2G \t2.2G \t6.6G \t25% \t/\nnone \t\t\t\t497M \t240K \t497M \t1% \t\t/dev\nnone \t\t\t\t502M \t168K \t501M \t1% \t\t/dev/shm\nnone \t\t\t\t502M \t88K \t501M \t1% \t\t/var/run\nnone \t\t\t\t502M \t0 \t\t502M \t0% \t\t/var/lock\nnone \t\t\t\t502M \t0 \t\t502M \t0% \t\t/lib/init/rw\nnone \t\t\t\t9.2G \t2.2G \t6.6G \t25% \t/var/lib/ureadahead/debugfs\n```\n\n### 8.2 计算命令执行时间\n\n* real: %e \n* user: %U \n* sys: %S \n\n```bash\n$ time COMMAND\n\n# 可以用选项-o filename将相关的时间统计信息写入文件：\n$ /usr/bin/time -o output.txt COMMAND\n\n# 要将命令执行时间添加到文件而不影响其原有内容，使用选项-a以及-o：\n$ /usr/bin/time -a -o output.txt COMMAND\n\n# 创建格式化输出：\n$ /usr/bin/time -f \"Time: %U\" -a -o timing.log uname\nLinux\n\n# 用错误重定向操作符（2>）对时间信息重定向。\n$ /usr/bin/time -f \"Time: %U\" uname> command_output.txt 2>time.log\n$ cat time.log\nTime: 0.00\n$ cat command_output.txt\nLinux\n\n# 使用参数%Z显示系统页面大小：\n$ /usr/bin/time -f \"Page size: %Z bytes\" ls> /dev/null\nPage size: 4096 bytes\n```\n\n三种不同类型的时：\n\n* Real时间指的是挂钟时间（wall clock time），也就是命令从开始执行到结束的时间。这段时间包括其他进程所占用的时间片（time slice）以及进程被阻塞时所花费的时间（例如，为等待I/O操作完成所用的时间）。 \n* User时间是指进程花费在用户模式（内核之外）中的CPU时间。这是唯一真正用于执行进程所花费的时间。执行其他进程以及花费在阻塞状态中的时间并没有计算在内。 \n* Sys时间是指进程花费在内核中的CPU时间。它代表在内核中执行系统调用所使用的时间，这和库代码（library code）不同，后者仍旧运行在用户空间。与“user时间”类似，这也是真正由进程使用的CPU时间。 \n\ntime命令 一些可以使用的参数：\n\n| 参数   | 描述                                       |\n| ---- | ---------------------------------------- |\n| %C   | 进行计时的命令名称以及命令行参数                         |\n| %D   | 进程非共享数据区域的大小，以KB为单位                      |\n| %E   | 进程使用的real时间（挂钟时间），显示格式为[小时:]分钟:秒         |\n| %x   | 命令的退出状态                                  |\n| %k   | 进程接收到的信号数量                               |\n| %W   | 进程被交换出主存的次数                              |\n| %Z   | 系统的页面大小。这是一个系统常量，但在不同的系统中，这个常量值也不同       |\n| %P   | 进程所获得的CPU时间百分比。这个值等于user+system时间除以总运行时间。结果以百分比形式显示 |\n| %K   | 进程的平均总（data+stack+text）内存使用量，以KB为单位      |\n| %w   | 进程主动进行上下文切换的次数，例如等待I/O操作完成               |\n| %c   | 进程被迫进行上下文切换的次数（由于时间片到期）                  |\n\n### 8.3 收集与当前登录用户、启动日志及启动故障的相关信息 \n\n```bash\n# 获取当前登录用户的相关信息：\n$ who\nslynux \tpts/0 \t2010-09-29 05:24 (slynuxs-macbook-pro.local)\nslynux \ttty7 \t2010-09-29 07:08 (:0)\n\n# 获得有关登录用户更详细的信息：\n$ w\n  07:09:05 up 1:45, 2 users, load average: 0.12, 0.06, 0.02\nUSER \tTTY \tFROM \tLOGIN@ \tIDLE \tJCPU \tPCPU \tWHAT\nslynux \tpts/0 \tslynuxs 05:24 \t0.00s \t0.65s \t0.11s \tsshd: slynux\nslynux \ttty7 \t:0\t\t07:08 \t1:45m \t3.28s \t0.26s \tgnome-session\n# 第一行列出了当前时间，系统运行时间，当前登录的用户数量以及过去的1分钟、 5分钟、 15分钟内的系统平均负载。接下来的每一行显示了每一个登录用户的详细信息，其中包括登录名、 TTY、远程主机、登录时间、空闲时间、自该用户登录后所使用的总CPU时间、当前运行进程所使用的CPU时间以及进程所对应的命令行。\n\n# 列出当前登录主机的用户列表：\n$ users\nslynux slynux slynux hacker\n$ users | tr ' ' '\\n' | sort | uniq\nslynux\nhacker\n\n# 查看系统已经加电运行了多长时间：\n$ uptime\n21:44:33 up 3:17, 8 users, load average: 0.09, 0.14, 0.09\n$ uptime | grep -Po '\\d{2}\\:\\d{2}\\:\\d{2}'\n\n# 获取上一次启动以及用户登录会话的信息：\n$ last\nslynux tty7 :0 Tue Sep 28 18:27 still logged in\nreboot system boot 2.6.32-21-generic Tue Sep 28 18:10 - 21:46 (03:35)\nslynux pts/0 :0.0 Tue Sep 28 05:31 - crash (12:39)\n# last命令可以提供登录会话信息。它实际上是一个系统登录日志，包括了登录tty、登录时间、状态等信息。\n# last命令以日志文件/var/log/wtmp作为输入日志数据。它也可以用选项-f明确地指定日志文件。例如：\n$ last -f /var/log/wtmp\n\n# 获取单个用户登录会话的信息：\n$ last USER\n\n# 获取重启会话（reboot session）信息：\n$ last reboot\nreboot system boot 2.6.32-21-generi Tue Sep 28 18:10 - 21:48 (03:37)\nreboot system boot 2.6.32-21-generi Tue Sep 28 05:14 - 21:48 (16:33)\n\n# 获取失败的用户登录会话信息：\n$ lastb\ntest tty8 :0 Wed Dec 15 03:56 - 03:56 (00:00)\nslynux tty8 :0 Wed Dec 15 03:55 - 03:55 (00:00)\n```\n\n### 8.4 使用 watch 监视命令输出 \n\nwatch命令可以用来在终端中以固定的间隔监视命令输出。 \n\n```bash\n$ watch ls\n\n$ watch 'ls -l | grep \"^d\"'\n# 只列出目录\n# 命令默认每2秒更新一次输出。\n\n# -n SECOND指定更新输出的时间间隔。例如：\n$ watch -n 5 'ls -l'\n# 以5秒为间隔，监视ls -l的输出\n\n# 突出标示watch输出中的差异, -d 可以启用这一功能：\n$ watch -d 'COMMANDS'\n```\n\n### 8.5 用 logrotate 管理日志文件 \n\n用一种被称为轮替（rotation）的技术来限制日志文件的体积，一旦它超过了限定的大小，就对其内容进行抽取（strip），同时将 日志文件中的旧条目存储到日志目录中的归档文件内。旧的日志文件就会得以保存以便随后参阅。 \n\n`logrotate` 的配置目录位于/etc/logrotate.d。 \n\n```bash\n$ cat /etc/logrotate.d/program\n/var/log/program.log {\nmissingok\nnotifempty\nsize 30k\ncompress\nweekly\nrotate 5\ncreate 0600 root root\n}\n```\n\n配置文件中各个参数的含义：\n\n| 参数                    | 描述                                       |\n| --------------------- | ---------------------------------------- |\n| missingok             | 如果日志文件丢失，则忽略；然后返回（不对日志文件进行轮替）            |\n| notifempty            | 仅当源日志文件非空时才对其进行轮替                        |\n| size 30k              | 限制实施轮替的日志文件的大小。可以用1M表示1MB                |\n| compress              | 允许用gzip压缩较旧的日志                           |\n| weekly                | 指定进行轮替的时间间隔。可以是weekly、 yearly或daily      |\n| rotate 5              | 这是需要保留的旧日志文件的归档数量。在这里指定的是5，所以这些文件名将会是program.log.1.gz、 program.log.2.gz等直到program.log.5.gz |\n| create 0600 root root | 指定所要创建的归档文件的模式、用户以及用户组                   |\n\n### 8.6 用 syslog 记录日志 \n\n每一个标准应用进程都可以利用syslog记录日志信息。 \n\n使用命令logger通过syslogd记录日志。 \n\nLinux中一些重要的日志文件 ：\n\n| 日志文件                | 描述              |\n| ------------------- | --------------- |\n| /var/log/boot.log   | 系统启动信息          |\n| /var/log/httpd      | Apache Web服务器日志 |\n| /var/log/messages   | 发布内核启动信息        |\n| /var/log/auth.log   | 用户认证日志          |\n| /var/log/dmesg      | 系统启动信息          |\n| /var/log/mail.log   | 邮件服务器日志         |\n| /var/log/Xorg.0.log | X服务器日志          |\n\n```bash\n# 向系统日志文件/var/log/message中写入日志信息：\n$ logger This is a test log line\n$ tail -n 1 /var/log/messages\nSep 29 07:47:44 slynux-laptop slynux: This is a test log line\n \n# 如果要记录特定的标记（tag），可以使用：\n$ logger -t TAG This is a message\n$ tail -n 1 /var/log/messages\nSep 29 07:48:42 slynux-laptop TAG: This is a message\n# 但是当logger发送消息时，它用标记字符串来确定应该记录到哪一个日志文件中。 syslogd使用与日志相关联的TAG来决定应该将其记录到哪一个文件中。你可以从/etc/rsyslog.d/目录下的配置文件中看到标记字符串以及与其相关联的日志文件。\n\n# 要将另一个日志文件的最后一行记录到系统日志中，可以使用：\n$ logger -f /var/log/source.log\n```\n\n### 8.7 通过监视用户登录找出入侵者 \n\n入侵者定义为：屡次试图登入系统达两分钟以上，并且期间的登录过程全部失败。凡是这类用户都应该被检测出来并生成包含以下细节信息的报告： \n\n* 试图登录的账户 \n* 试图登录的次数 \n* 攻击者的IP地址 \n* IP地址所对应的主机 \n* 进行登录的时间段 \n\n为了处理SSH登录失败的情况，还得知道用户认证会话日志会被记录在日志文件/var/log/auth.log中。脚本需要扫描这个日志文件来检测出失败的登录信息，执行各种检查来获取所需要的数据。我们可以用host命令找出IP地址所对应的主机。 \n\n### 8.8 监视磁盘活动 \n\n```bash\n# 交互式监视, iotop的-o选项只显示出那些正在进行I/O活动的进程：\n$ iotop -o\n\n# 用于shell脚本的非交互式用法：\n$ iotop -b -n 2\n\n# 监视特定进程\n$ iotop -p PID\n```\n\n### 8.9 检查磁盘及文件系统错误 \n\n使用fsck的各种选项对文件系统错误进行检查和修复。 \n\n```bash\n# 要检查分区或文件系统的错误，只需要将路径作为fsck的参数：\n$ fsck /dev/sdb3\nfsck from util-linux 2.20.1\ne2fsck 1.42.5 (29-Jul-2012)\nHDD2 has been mounted 26 times without being checked, check forced.\nPass 1: Checking inodes, blocks, and sizes\nPass 2: Checking directory structure\nPass 3: Checking directory connectivity\nPass 4: Checking reference counts\nPass 5: Checking group summary information\nHDD2: 75540/16138240 files (0.7% non-contiguous), 48756390/64529088 blocks\n\n# 检查/etc/fstab中所配置的所有文件系统：\n$ fsck -A\n# 该命令会依次检查/etc/fstab中列出的文件系统。 fstab文件对磁盘及其挂载点之间的映射关系进行了配置，以便于更便捷地挂载文件系统\n\n# 指定fsck自动修复错误，无需询问是否进行修复：\n$ fsck -a /dev/sda2\n\n# 模拟fsck要执行的操作：\n$ fsck -AN\nfsck from util-linux 2.20.1\n[/sbin/fsck.ext4 (1) -- /] fsck.ext4 /dev/sda8\n[/sbin/fsck.ext4 (1) -- /home] fsck.ext4 /dev/sda7\n[/sbin/fsck.ext3 (1) -- /media/Data] fsck.ext3 /dev/sda6\n```\n\n## 9 管理重任\n\n### 9.1 收集进程信息  \n\n```bash\n# 为了包含更多的信息，可以使用-f（表示full）来显示多列，如下所示：\n$ ps -f\nUID PID PPID C STIME TTY TIME CMD\nslynux 1220 1219 0 18:18 pts/0 00:00:00 -bash\nslynux 1587 1220 0 18:59 pts/0 00:00:00 ps -f\n# 使用选项 -e（every）。选项-ax（all）也可以生成同样的输出。\n\n# 运行如下命令之一： ps –e， ps –ef， ps -ax或ps –axf。\n$ ps -e | head\nPID TTY TIME CMD\n1 ? 00:00:00 init\n2 ? 00:00:00 kthreadd\n3 ? 00:00:00 migration/0\n4 ? 00:00:00 ksoftirqd/0\n5 ? 00:00:00 watchdog/0\n6 ? 00:00:00 events/0\n7 ? 00:00:00 cpuset\n8 ? 00:00:00 khelper\n9 ? 00:00:00 netns\n\n# 用 -o 来指定想要显示的列，以便只打印出我们需要的内容。\n# -o 的参数以逗号操作符（,）作为定界符。值得注意的是，逗号操作符与它分隔的参数之间是没有空格的。\n# -e和过滤器结合使用没有任何实际效果，依旧会显示所有的进程。\n# 示例如下，其中comm表示COMMAND， pcpu表示CPU占用率：\n$ ps -eo comm,pcpu | head\nCOMMAND %CPU\ninit 0.0\nkthreadd 0.0\nmigration/0 0.0\nksoftirqd/0 0.0\nwatchdog/0 0.0\nevents/0 0.0\ncpuset 0.0\nkhelper 0.0\nnetns 0.0\n```\n\n选项-o可以使用不同的参数：\n\n| 参数    | 描述         |\n| ----- | ---------- |\n| pcpu  | CPU占用率     |\n| pid   | 进程ID       |\n| ppid  | 父进程ID      |\n| pmem  | 内存使用率      |\n| comm  | 可执行文件名     |\n| cmd   | 简单命令       |\n| user  | 启动进程的用户    |\n| nice  | 优先级        |\n| time  | 累计的CPU时间   |\n| etime | 进程启动后流逝的时间 |\n| tty   | 所关联的TTY设备  |\n| euid  | 有效用户ID     |\n| stat  | 进程状态       |\n\n```bash\n# top, 默认会输出一个占用CPU最多的进程列表。输出结果每隔几秒就会更新。\n$ top\n\n# 根据参数对ps输出进行排序\n$ ps -eo comm,pcpu --sort -pcpu | head\nCOMMAND \t\t\t%CPU\nXorg \t\t\t\t0.1\nhald-addon-stor \t0.0\nata/0 \t\t\t\t0.0\nscsi_eh_0 \t\t\t0.0\ngnome-settings- \t0.0\ninit \t\t\t\t0.0\nhald \t\t\t\t0.0\npulseaudio \t\t\t0.0\ngdm-simple-gree \t0.0\n$ ps -eo comm,pid,pcpu,pmem | grep bash\nbash \t\t1255 \t0.0 \t0.3\nbash \t\t1680 \t5.5 \t0.3\n\n# 找出给定命令名所对应的进程ID，在参数后加上=就可以移除列名。\n$ ps -C bash -o pid=\n1255\n1680\n$ pgrep bash\n1255\n1680\n\n# 如果不使用换行符作为定界符，而是要自行指定可以像下面这样：\n$ pgrep bash -d \":\"\n1255:1680\n\n# 指定进程的用户（拥有者）列表：\n$ pgrep -u root,slynux COMMAND\n\n# 根据真实用户或ID以及有效用户或ID过滤ps输出\n 用 -u EUSER1,EUSER2 …，指定有效用户列表；\n 用 -U RUSER1,RUSER2 …，指定真实用户列表\n$ ps -u root -U root -o user,pcpu\n\n# 用TTY过滤ps输出, 可以通过指定进程所属的TTY选择ps的输出。用选项 -t指定TTY列表：\n$ ps -t pts/0,pts/1\nPID TTY TIME CMD\n1238 pts/0 00:00:00 bash\n1835 pts/1 00:00:00 bash\n1864 pts/0 00:00:00 ps\n\n# 进程线程的相关信息\n# 通常与进程线程相关的信息在ps输出中是看不到的。我们可以用选项 –L 在ps输出中显示线程的相关信息。这会显示出两列： NLWP和NLP。 NLWP是进程的线程数量， NLP是ps输出中每个条目的线程ID。例如：\n$ ps -eLf\n\n# 指定输出宽度以及所要显示的列\n# 可以按照你自己的使用方式来进行应用。尝试以下选项:\n -f ps –ef\n u ps -e u\n ps ps -e w（w表示宽松输出）\n\n# 显示进程的环境变量\n# 了解某个进程依赖哪些环境变量，这类信息我们通常都用得着。进程的运行方式可能极其依赖某组环境变量。我们可以利用环境变量调试并修复与进程相关的问题。\n$ ps -eo pid,cmd e | tail -n 3\n1162 hald-addon-acpi: listening on acpid socket /var/run/acpid.socket\n1172 sshd: slynux [priv]\n1237 sshd: slynux@pts/0\n1238 -bash USER=slynux LOGNAME=slynux HOME=/home/slynux\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\nMAIL=/var/mail/slynux SHELL=/bin/bash SSH_CLIENT=10.211.55.2 49277 22\nSSH_CONNECTION=10.211.55.2 49277 10.211.55.4 22 SSH_TTY=/dev/pts/0 TERM=xterm-color\nLANG=en_IN XDG_SESSION_COOKIE=d1e96f5cc8a7a3bc3a0a73e44c95121a-1286499339.\n592429-1573657095\n```\n\n### 9.2 which、 whereis、 file、 whatis与平均负载 \n\n```bash\n# which, which命令用来找出某个命令的位置。\n$ which ls\n/bin/ls\n\n# whereis\n# whereis与which命令类似，但它不仅返回命令的路径，还能够打印出其对应的命令手册的位置以及命令源代码的路径（如果有的话）\n$ whereis ls\nls: /bin/ls /usr/share/man/man1/ls.1.gz\n\n# file\n$ file FILENAME\n# 该命令会打印出与该文件类型相关的细节信息。\n$ file /bin/ls\n/bin/ls: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked\n(uses shared libs), for GNU/Linux 2.6.15, stripped\n\n# whatis, whatis命令会输出作为参数的命令的简短描述信息。\n$ whatis ls\nls (1) \t\t\t- list directory contents\n\n# 平均负载\n$ uptime\n12:40:53 up 6:16, 2 users, load average: 0.00, 0.00, 0.00\n```\n\n### 9.3 杀死进程以及发送或响应信号 \n\n信号是Linux中的一种进程间通信机制。 当进程接收到一个信号时，它会通过执行对应的信号处理程序（signal handler）来进行响应。 \n\n```bash\n# 列出所有可用的信号：\n$ kill -l\n\n# 终止进程：\n$ kill PROCESS_ID_LIST\n# kill命令默认发出一个TERM信号。进程ID列表使用空格作为进程ID之间的定界符。\n\n# 要通过kill命令向进程发送指定的信号，可以使用：\n$ kill -s SIGNAL PID\n# 参数SIGNAL要么是信号名称，要么是信号编号。\n\n# 我们经常要强行杀死进程，可以使用：\n$ kill -s SIGKILL PROCESS_ID\n或者\n$ kill -9 PROCESS_ID\n```\n\n常用到的信号量：\n\n* SIGHUP 1——对控制进程或终端的终结进行挂起检测（hangup detection）\n* SIGINT 2——当按下Ctrl + C时发送该信号 \n* SIGKILL 9——用于强行杀死进程 \n* SIGTERM 15——默认用于终止进程 \n* SIGTSTP 20——当按下Ctrl + Z时发送该信号 \n\n```bash\n# killall命令通过命令名终止进程：\n$ killall process_name\n\n# 通过名称向进程发送信号：\n$ killall -s SIGNAL process_name\n\n# 通过名称强行杀死进程：\n$ killall -9 process_name\n\n# pkill命令和kill命令类似，不过默认情况下pkill接受的是进程名，而非进程ID。例如：\n$ pkill process_name\n$ pkill -s SIGNAL process_name\n# pkill不支持信号名称。\n\n# 捕捉并响应信号\n# trap命令在脚本中用来为信号分配信号处理程序。\n$ trap 'signal_handler_function_name' SIGNAL LIST\n```\n\n### 9.4 向用户终端发送消息 \n\n```bash\n# wall命令用来向当前所有登录用户的终端写入消息。\n$ cat message | wall\n或者\n$ wall< message\nBroadcast Message from slynux@slynux-laptop\n(/dev/pts/1) at 12:54 ...\nThis is a messag\n```\n\n### 9.5 采集系统信息 \n\n```bash\n# 打印当前系统的主机名：\n$ hostname\n或者\n$ uname -n\n\n# 打印Linux内核版本、硬件架构等详细信息：\n$ uname -a\n\n# 打印内核发行版本：\n$ uname -r\n\n# 打印主机类型：\n$ uname -m\n\n# 打印CPU相关信息：\n$ cat /proc/cpuinfo\n# 获取处理器名称：\n$ cat /proc/cpuinfo | sed -n 5p\n\n# 打印内存的详细信息：\n$ cat /proc/meminfo\n# 打印系统可用内存总量：\n$ cat /proc/meminfo | head -1\nMemTotal: 1026096 kB\n\n# 列出系统的分区信息：\n$ cat /proc/partitions\n或者\n$ fdisk -l #如果没有输出，切换到root用户执行该命令\n\n# 获取系统的详细信息：\n$ lshw #建议以root用户来执行\n```\n\n### 9.6 使用 proc 采集信息 \n\n以Bash为例，它的进程ID是4295（pgrep bash），那么就会有一个对应的目录/proc/4295。进程对应的目录中包含了大量有关进程的信息。 /proc/PID中一些重要的文件如下所示。 \n\n* environ：包含与进程相关的环境变量。使用cat /proc/4295/environ，可以显示所有传递给该进程的环境变量 \n\n* cwd： 是一个到进程工作目录（working directory）的符号链接 \n\n* exe：是一个到当前进程所对应的可执行文件的符号链接 \n\n  $ readlink /proc/4295/exe\n  /bin/bash\n\n* fd：包含了进程所使用的文件描述符 \n\n### 9.7 用 cron 进行调度 \n\n**crontab任务配置基本格式：**\n\n```\n*  \t\t\t*　 \t\t\t*　 \t\t\t*　 \t\t  *　　\t\t\t\t\tcommand\n分钟(0-59)　小时(0-23)　\t日期(1-31)　 月份(1-12)　星期(0-6,0代表星期天)　  命令\n```\n\ncron表中的每一个条目都由6部分组成，并按照下列顺序排列： \n\n- 分钟（0～59） \n- 小时（0～23） \n- 天（1～31） \n- 月份（1～12） \n- 工作日（0～6） \n- 命令（在指定时间执行的脚本或命令） \n\n星号（*）指定命令应该在每个时间段执行。 \n\n除了数字还有几个个特殊的符号就是 `\"*\"` 、`\"/\"` 和 `\"-\"` 、`\",\"` ，`*` 代表所有的取值范围内的数字，`\"/\"` 代表每的意思, `\"*/5\"` 表示每5个单位，`\"-\"` 代表从某个数字到某个数字, `\",\"` 分开几个离散的数字。以下举几个例子说明问题： \n\n```bash\n# 指定每小时的第5分钟执行一次ls命令\n5 * * * * ls \n\n# 指定每天的 5:30 执行ls命令\n30 5 * * * ls \n\n# 指定每月8号的7：30分执行ls命令\n30 7 8 * * ls \n\n# 指定每年的6月8日5：30执行ls命令\n30 5 8 6 * ls \n\n# 指定每星期日的6:30执行ls命令 [ 注：0表示星期天，1表示星期1，以此类推，也可以用英文来表示，sun表示星期天，mon表示星期一等。 ]\n30 6 * * 0 ls \n\n# 每月10号及20号的3：30执行ls命令 [注：“，”用来连接多个不连续的时段 ]\n30 3 10,20 * * ls \n\n# 每天8-11点的第25分钟执行ls命令 [注：“-”用来连接连续的时段 ]\n25 8-11 * * * ls \n\n# 每15分钟执行一次ls命令 [即每个小时的第0 15 30 45 60分钟执行ls命令 ]\n*/15 * * * * ls \n\n# 每个月中，每隔10天6:30执行一次ls命令[即每月的1、11、21、31日是的6：30执行一次ls命令。 ]\n30 6 */10 * * ls \n\n# 每天7：50以root 身份执行/etc/cron.daily目录中的所有可执行文件\n50 7 * * * root run-parts /etc/cron.daily   # [ 注：run-parts参数表示，执行后面目录中的所有可执行文件。 ]\n```\n\n**配置用户定时任务的语法：**\n\n```bash\n$ crontab [-u user]file\n\n$ crontab -u user[-i]\n```\n\n参数与说明：\n\n* crontab -u\t\t//设定某个用户的cron服务\n* crontab -l        //列出某个用户cron服务的详细内容\n* crontab -r              //删除没个用户的cron服务\n* crontab -e             //编辑某个用户的cron服务\n\n### 9.8 从终端截图 \n\n```bash\n# 取整个屏幕：\n$ import -window root screenshot.png\n\n# 手动选择部分区域进行抓取：\n$ import screenshot.png\n\n# 抓取特定窗口：\n$ import -window window_id screenshot.png\n```\n","tags":["shell"],"categories":["Shell"]},{"title":"Linux命令eval的用法","url":"/2017-04-13/reference/Linux命令eval的用法/","content":"\n## eval command-line\n\n其中command－line是在终端上键入的一条普通命令行。**然而当在它前面放上eval时，其结果是shell在执行命令行之前扫描它两次**。如：\n\n```bash\n$ pipe=\"|\"\n\n$ eval ls $pipe wc -l\n```\n\nshell第1次扫描命令行时，它替换出pipe的值｜，接着eval使它再次扫描命令行，这时shell把｜作为管道符号了。\n\n<!-- more -->\n\n**如果变量中包含任何需要shell直接在命令行中看到的字符（不是替换的结果），就可以使用eval。命令行结束符（；｜ &），I／o重定向符（< >）和引号就属于对shell具有特殊意义的符号，必须直接出现在命令行中。**\n\n## eval echo \\$# 取得最后一个参数\n\n如：\n\n```bash\n$ cat last\neval echo $$#\n\n$ ./last one two three four\nfour\n```\n\n第一遍扫描后，shell把反斜杠去掉了。当shell再次扫描该行时，它替换了$4的值，并执行echo命令\n\n## 以下示意如何用eval命令创建指向变量的“指针”：\n\n```bash\n$ x=100\n\n$ ptrx=x\n\n$ eval echo $$ptrx  #指向ptrx，用这里的方法可以理解b中的例子\n100 打印100\n\n$ eval $ptrx=50  #将50存到ptrx指向的变量中。\n\n$ echo $x\n50 打印50\n```","tags":["Linux Shell"],"categories":["Shell"]},{"title":"HTTP思维导图","url":"/2017-04-12/reference/HTTP/HTTP思维导图/","content":"\n文章来自[HTTP思维导图](http://yrq110.me/2017/03/04/20170304-http-mindmap/)。\n\nHTTP mindmap整理\n\nsource from 《HTTP权威指南》\n\n<!-- more -->\n\n### 概述-Summary\n\n<img src=\"/images/imageHttp/HTTP-1.svg\">\n\n### 报文-Message\n\n<img src=\"/images/imageHttp/HTTP-2.svg\">\n\n### 连接-Connection\n\n<img src=\"/images/imageHttp/HTTP-3.svg\">\n\n### 代理-Proxy\n\n<img src=\"/images/imageHttp/HTTP-4.svg\">\n\n### 缓存-Cache\n\n<img src=\"/images/imageHttp/HTTP-5.svg\">\n\n### 网关、隧道与中继-Gateway、Tunnel and Relay\n\n<img src=\"/images/imageHttp/HTTP-6.svg\">\n\n### 识别-Identification\n\n<img src=\"/images/imageHttp/HTTP-7.svg\">\n\n### 认证-Authentication\n\n<img src=\"/images/imageHttp/HTTP-8.svg\">\n\n### 安全-Security\n\n<img src=\"/images/imageHttp/HTTP-9.svg\">\n\n### 实体与编码-Entity and Encoding\n\n<img src=\"/images/imageHttp/HTTP-10.svg\">","tags":["http"],"categories":["http"]},{"title":"Webpack创建、运行vue.js项目及其目录结构详解","url":"/2017-04-10/reference/Webpack创建、运行vue-js项目及其目录结构详解/","content":"\n### 项目环境搭建：\n\n1.安装node\n\n进入[node官网]([https://nodejs.org/en/](https://nodejs.org/en/)进行下载。\n\n版本查看：\n\n```bash\n$ node -v\nv6.10.1\n```\n\n<p style=\"color:red;\">**注意：**node版本最好新一点，推介6.0以上。 </p>\n\n<!-- more -->\n\n2.全局安装vue-cli\n\n```bash\n$ npm install -g vue-cli\n```\n\n**注意：** 如果安装失败可能需要root权限重新安装。\n\n3.创建一个基于 `webpack` 模板的新项目\n\n```bash\n$ vue init webpack project-name\t \t#(默认安装2.0版本)\n$ vue init webpack#1.0 project-name #(安装1.0版本)\n```\n\n### 项目目录结构：\n\n![](http://i.imgur.com/P64Q8uK.png)\n\n\n\n![](http://i.imgur.com/beLRmUA.png)\n\n- main.js是入口文件，主要作用是初始化vue实例并使用需要的插件\n\n  ```js\n  // The Vue build version to load with the `import` command\n  // (runtime-only or standalone) has been set in webpack.base.conf with an alias.\n  import Vue from 'vue'\n  import App from './App'\n  import router from './router'\n\n  Vue.config.productionTip = false\n\n  /* eslint-disable no-new */\n  new Vue({\n    el: '#app',\n    router,\n    template: '<App/>',\n    components: { App }\n  })\n  ```\n\n- App.vue是我们的主组件，所有页面都是在App.vue下进行切换的。其实你也可以理解为所有的路由也是App.vue的子组件。所以我将router标示为App.vue的子组件。\n\n  ```vue\n  <template>\n    <div id=\"app\">\n      <img src=\"./assets/logo.png\">\n      <router-view></router-view>\n      <hello></hello>\n    </div>\n  </template>\n\n  <script>\n  export default {\n    name: 'app',\n    components: {\n      Hello\n    }\n  }\n  </script>\n\n  <style>\n  #app {\n    font-family: 'Avenir', Helvetica, Arial, sans-serif;\n    -webkit-font-smoothing: antialiased;\n    -moz-osx-font-smoothing: grayscale;\n    text-align: center;\n    color: #2c3e50;\n    margin-top: 60px;\n  }\n  </style>\n  ```\n\n- index.html文件入口\n\n- src放置组件和入口文件\n\n- node_modules为依赖的模块\n\n- config中配置了路径端口值等\n\n- build中配置了webpack的基本配置、开发环境配置、生产环境配置等\n\n### 运行项目：\n\n```bash\n$ cd project-name\n$ npm install\n$ npm run dev\n# 上述步骤都完成后在浏览器输入：localhost:8080\n```\n\n","tags":["webpack"],"categories":["Docker"]},{"title":"HTTP--cookie，session，token","url":"/2017-04-10/reference/HTTP/HTTP之Cookie和Session和Token/","content":"\n> [认识HTTP----Cookie和Session篇](<https://zhuanlan.zhihu.com/p/27669892>)\n>\n> [彻底理解cookie，session，token](<https://zhuanlan.zhihu.com/p/63061864>)\n>\n> [详解cookie、session和HTTP缓存](<https://juejin.im/post/5a7c6c415188257a780da590>)\n>\n> [COOKIE和SESSION机制详解](https://shuwoom.com/?p=3151)\n\nTODO\n\n","tags":["http"],"categories":["http"]},{"title":"HTTP协议详解","url":"/2017-04-06/reference/HTTP/HTTP协议详解/","content":"\nHTTP（HyperText Transfer Protocol 超文本传输协议)是互联网上应用最为广泛的一种网络协议，它是由万维网协会（World Wide Web Consortium）制定发布。\n\n文章主要**以一次HTTP请求的整个过程来讲解**(DNS解析不讲)：HTTP 起源、TCP/IP 协议、建立 TCP 连接、客户端请求、服务端响应、断开 TCP 连接，文章最后还捎带讲了与 HTTP 相关知识。\n\n<!-- more -->\n\n![HTTP请求大概流程图](/images/imageHttp/HTTP请求大概流程图.png)\n\n## 简介\n\n### 起源\n\n今天我们能够在网络中畅游，都得益于一位计算机科学家蒂姆·伯纳斯·李的构想。1991年8月6日，蒂姆·伯纳斯·李在位于欧洲粒子物理研究所（CERN）的NeXT计算机上，正式公开运行世界上第一个Web网站（http://info.cern.ch ），建立起基本的互联网基础概念和技术体系，由此开启了网络信息时代的序幕。\n\n伯纳斯·李的提案包含了网络的基本概念并逐步建立了所有必要的工具：\n\n- 提出**HTTP** (Hypertext Transfer Protocol) 超文本传输协议，允许用户通过单击超链接访问资源；\n\n- 提出使用**HTML超文本标记语言**(Hypertext Markup Language)作为创建网页的标准；\n- 创建了**统一资源定位器URL** (Uniform Resource Locator)作为网站地址系统，就是沿用至今的http://www URL格式；\n- 创建第一个**Web浏览器**，称为万维网浏览器，这也是一个Web编辑器；\n- 创建第一个**Web服务器**（http://info.cern.ch）以及描述项目本身的第一个Web页面。\n\n### 特点\n\n**HTTP 协议一共有五大特点：**\n\n- 支持客户/服务器模式。\n- 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。\n- 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由 Content-Type（Content-Type是HTTP包中用来表示内容类型的标识）加以标记。\n- 无连接：**无连接的含义是限制每次连接只处理一个请求**。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。\n- 无状态：**无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态**。即我们给服务器发送 HTTP 请求之后，服务器根据请求，会给我们发送数据过来，但是，发送完，不会记录任何信息（**Cookie 和Session 孕育而生**，后期再讲）。\n\n## TCP/IP协议\n\n我们经常听到一句话就是：**HTTP是一个基于TCP/IP协议簇来传递数据。**\n\n如何理解上面那句话？我们来看看**TCP/IP四层模型**就明白了。\n\n![TCP/IP基础--层次图](/images/imageHttp/TCPIP基础层次图.png)\n\n从上图我们可以清晰的看到HTTP使用的传输层协议为**TCP协议**，而网络层使用的是IP协议（当然还使用了很多其他协议），所以说HTTP是一个基于TCP/IP协议簇来传递数据。\n\n> 同样我们可以看到 ping 走的 ICMP 协议，这也就是为什么有时候我们开 vps 可以上网，但是 ping google 却ping 不通的原因，因为走的是不同的协议。\n\n那 TCP/IP 协议簇大致是如何工作的，我们再来看看下图：\n\n![TCPIP协议簇工作原理](/images/imageHttp/TCPIP协议簇工作原理.png)\n\n我们可以看到在数据发送端是一层一层封装数据，数据接收端一层一层拆封，最后应用层获得数据。\n\n## 建立TCP连接\n\n我们知道了TCP/IP协议簇大致的工作原理之后，我们来看看HTTP是如何建立连接的。\n\n### TCP包头信息\n\n前面咱们讲过HTTP是一个基于TCP/IP协议簇来传递数据，所以这HTTP建立连接也就是建立TCP连接，TCP如何建立连接，一起来看看TCP包信息结构吧。\n\n![TCP头信息](/images/imageHttp/TCP头信息.png)\n\n**TCP 报文包 = TCP 头信息 + TCP 数据体**，而在 TCP 头信息中包含了 6 种控制位（上图红色框中），这六种标志位就代表着 TCP 连接的状态：\n\n- **URG**：紧急数据（urgent data）—这是一条紧急信息\n\n- **ACK**：确认已收到\n\n- **PSH**：提示接收端应用程序应该立即从tcp接受缓冲区中读走数据\n\n- **RST**：表示要求对方重新建立连接\n\n- **SYN**：表示请求建立一个连接\n\n- **FIN**：表示通知对方本端要关闭连接了\n\n### 建立连接过程\n\n了解了TCP包头信息之后，我们就可以正式看看TCP建立连接的三次握手了。\n\n![TCP建立的三次握手](/images/imageHttp/TCP建立的三次握手.png)\n\n**三次握手讲解：**\n\n- 客户端发送位码为 `syn＝1` ,随机产生 `seq number=1234567` 的数据包到服务器，服务器由 `SYN=1` 知道客户端要求建立联机（客户端：我要连接你）\n- 服务器收到请求后要确认联机信息，向 A 发送 `ack number=(客户端的seq+1)` , `syn=1` , `ack=1` , 随机产生`seq=7654321` 的包（服务器：好的，你来连吧）\n- 客户端收到后检查 `ack number` 是否正确，即第一次发送的 `seq number+1` ,以及位码 `ack` 是否为 1，若正确，客户端会再发送 `ack number=(服务器的seq+1)` , `ack=1`，服务器收到后确认 `seq` 值与 `ack=1` 则连接建立成功。（客户端：好的，我来了）\n\n> 面试官：为什么 http 建立连接需要三次握手，不是两次或四次\n>\n> 答：三次是最少的安全次数，两次不安全，四次浪费资源\n\n## 客户端请求\n\n客户端与服务器连接上了之后，客户端就可以开始向服务器请求资源，就可以开始发送HTTP请求了。\n\n### HTTP请求报文结构\n\n我们之前说过 **TCP 报文包 = TCP 头信息 + TCP 数据体**，TCP 头信息我们已经讲了，现在来讲 TCP 数据体，也就是我们的 HTTP 请求报文。\n\n### HTTP请求实例\n\n来看看实际的HTTP请求例子：\n\n![HTTP请求报文](/images/imageHttp/HTTP请求报文.png)\n\n① 是请求方法，HTTP/1.1 定义的请求方法有 8 种：**GET、POST、PUT、DELETE、PATCH、HEAD、OPTIONS、TRACE**, 最常的两种 GET 和 POST，**如果是 RESTful 接口的话一般会用到 GET、POST、DELETE、PUT**\n\n② 为请求对应的URL地址，它和报文头的 Host 属性组成完整的请求 URL\n\n③ 是协议名称及版本号\n\n④ 是HTTP的报文头，报文头包含若干个属性，格式为 “**属性名 : 属性值**”，服务端据此获取客户端的信息\n\n⑤ 是报文体，它将一个页面表单中的组件值通过 `param1=value1&param2=value2` 的键值对形式编码成一个格式化串，它承载多个请求参数的数据。不但报文体可以传递请求参数，请求 URL 也可以通过类似于 “`/chapter15/user.html? param1=value1&param2=value2`” 的方式传递请求参数。\n\n请求头参数非常多，就不一一说明，只说明两个低级的反扒参数：\n\n- User-Agent：客户端使用的操作系统和浏览器的名称和版本，有些网站会限制请求浏览器\n- Referer：先前网页的地址，表示此请求来自哪里，有些网站会限制请求来源\n\n## 服务端响应\n\n服务器在收到客户端请求处理完需要响应并返回给客户端，而HTTP响应报文结构与请求结构体一致。\n\n### HTTP响应报文结构\n\n![HTTP响应报文结构](/images/imageHttp/HTTP响应报文结构.png)\n\n### HTTP响应实例\n\n![HTTP响应报文](/images/imageHttp/HTTP响应报文.png)\n\n### 响应状态码\n\n响应报文中我们重点关注下：服务器的响应状态码，面试也很容易问到，下面只列出分类，详细状态码自行上网查找了解。\n\n![HTTP状态码分类](/images/imageHttp/HTTP状态码分类.png)\n\n## 断开连接\n\n在服务器响应完毕后，一次会话就结束了，请问这时候连接会断开吗？\n\n### 长短连接\n\n是否断开我们需要区分 HTTP 版本：\n\n- 在 HTTP/1.0 版本的时候，客户端与服务器完成一个请求/响应之后，会将之前建立的 TCP 连接断开，下次请求的时候又要重新建立 TCP 连接，这也被称为短连接\n- 在 HTTP1.0 发布仅半年后（1997年1月） ，**HTTP/1.1** 版本发布并带来一个新的功能：在客户端与服务器完成一次请求/响应之后，允许不断开 TCP 连接，这意味着下次请求就直接使用这个TCP 连接而不再需要重新握手建立新连接，这也被称为长连接\n\n**注意**：长连接是指一次 TCP 连接允许多次 HTTP 会话，HTTP 永远都是一次请求/响应，会话结束，HTTP 本身不存在长连接之说。\n\n早在 1999 年 HTTP1.1 就推广普及，所以现在浏览器在请求时请求头中都会携带一个参数：`Connection:keep-alive`，这表示浏览器要求与服务器建立长连接，而服务器也可以设置是否愿意建立长连接。\n\n### 长连接优缺点\n\n对于服务器来说建立长连接有优点也有缺点：\n\n- **优点**：当网站中有大量静态资源（图片、css、js等）就可以开启长连接，这也几张图片就可以通过一次TCP连接发送。\n- **缺点**：当客户端请求一次时候不在请求，而服务器却开着长连接资源被占用着，这是严重浪费资源。\n\n所以是否开启长连接，长连接时间都需要根据网站自身来合理设置。\n\nps：大家不要小看这一个 TCP 连接，在一次客户端 HTTP 完整的请求中（DNS寻址、建立TCP连接、请求、等待、解析网页、断开TCP连接）建立 TCP 连接占用的时间比还是很大的。\n\n### 断开连接过程\n\n**在建立TCP连接时是三次握手，而断开TCP连接是四次挥手！**\n\n![TCP四次挥手](/images/imageHttp/TCP四次挥手.png)\n\n在前面讲 TCP/IP 协议时我们说过标志位：**FIN** 表示通知对方本端要关闭连接了，**那断开连接为何需要四次挥手呢？**\n\nTODO\n\n## 题外话\n\n### 面试必考题：http三次握手、四次挥手\n\n> 面试官：为何建立连接需要三次握手而关闭连接却需要四次挥手。\n\nTODO\n\n### http2.0\n\n![HTTP2.0](/images/imageHttp/HTTP2-0.png)\n\nHTTP/1.1 已经为我们服务了20年，而 HTTP/2.0 其实在 2015 就发布了，但是还没有推广开来，关于 HTTP/2.0 新特性也可以去网上查阅相关资料.\n\n### http&rpc\n\n因为 http 响应慢、请求头体积大等缺点，所以在微服务时代，大家都使用 rpc 来调用服务，rpc 相关概念感兴趣同学自行网上学习。\n\n### http&https\n\nhttp还有两个很大的缺点就是明文且不能保证完整性，所以目前会渐渐被HTTPS代替。\n\n\n\n","tags":["http"],"categories":["http"]},{"title":"lighttpd+fastcgi","url":"/2017-03-31/reference/HTTP/lighttpd-fastcgi/","content":"\n### 简介\n\n`lighttpd` 提供了一种外部程序调用的接口，即 `FastCGI` 接口。这是一种独立于平台和服务器的接口，它介于Web应用程序和Web服务器之间。\n\n这就意味着能够在 `Apache` 服务器上运行的 `FastCGI` 程序，也一定可以无缝的在 `lighttpd` 上使用。\n\n<!-- more -->\n\n### FastCGI介绍\n\n1）就像 `CGI` 一样，`FastCGI` 也是独立于编程语言的。\n2）就像 `CGI` 一样，`FastCGI` 程序运行在完全独立于核心 `Web Server` 之外的进程中，和 `API` 方式相比，提供了很大的安全性。（API会将程序代码与核心Web Server挂接在一起，这就意味着基于问题API的应用程序可能会使整个Web Server或另一个应用程序崩溃；一个恶意API还可以从核心Web Server或另一个应用程序中盗取安全密钥）\n\n3) 虽然 `FastCGI` 不能一夜之间复制CGI的所有功能，但是 `FastCGI` 一直宣扬开放，这也使得我们拥有很多免费的 `FastCGI` 应用程序库（C/C++、Java、Perl、TCL）和免费的Server模块（Apache、ISS、Lighttpd）。\n\n4) 就像 `CGI` 一样，`FastCGI` 并不依附于任何 `Web Server` 的内部架构，因此即使 `Server` 的技术实现变动，`FastCGI` 仍然非常稳定；而 `API` 设计是反映 `Web Server` 内部架构的，因此，一旦架构改变，API要随之变动。\n\n5) `FastCGI` 程序可以运行在任何机器上，完全可以和 `Web Server` 不在一台机器上。这种分布式计算的思想可以确保可扩展性、提高系统可用性和安全性。\n\n6) `CGI` 程序主要是对 `HTTP` 请求做计算处理，而 `FastCGI` 却还可以做得更多，例如模块化认证、授权检查、数据类型转换等等。在未来，`FastCGI` 还会有能力扮演更多角色。\n\n7) `FastCGI` 移除了 `CGI` 程序的许多弊端。例如，针对每一个新请求，`WebServer` 都必须重启 `CGI` 程序来处理新请求，这导致 `WebServer` 的性能会大受影响。而 `FastCGI` 通过保持进程处理运行状态并持续处理请求的方式解决了该问题，这就将进程创建和销毁的时间节省了出来。\n\n8) `CGI` 程序需要通过管道（pipe）方式与 `Web Server` 通信，而 `FastCGI` 则是通过 `Unix-Domain-Sockets` 或 `TCP/IP` 方式来实现与 `Web Server` 的通信。这确保了 `FastCGI` 可以运行在 `Web Server` 之外的服务器上。`FastCGI` 提供了 `FastCGI` 负载均衡器，它可以有效控制多个独立的 `FastCGI Server` 的负载，这种方式比 `load-balancer+apache+mod_php` 方式能够承担更多的流量。\n\n### FastCGI 模块\n\n若要 `lighttpd` 支持 `fastcgi`，则需要配置如下内容：\n\n在 `fastcgi.conf` 中配置\n\n```test\nserver.modules += ( \"mod_fastcgi\" )\n```\n\n及在 `module.conf` 中配置\n\n```test\ninclude \"conf.d/fastcgi.conf\"\n```\n\n### FastCGI 配置选项\n\n`lighttpd` 通过 `fastcgi` 模块的方式实现了对 `fastcgi` 的支持，并且在配置文件中提供了三个相关的选项：\n\n1） fastcgi.debug\n\n可以设置一个从0到65535的值，用于设定 `FastCGI` 模块的调试等级。当前仅有0和1可用。**1表示开启调试（会输出调试信息），0表示禁用**。例如：\n\n```test\nfastcgi.debug = 1\n```\n\n2） fastcgi.map-extentsions\n\n同一个 `fastcgi server` 能够映射多个扩展名，如 `.php3` 和 `.php4` 都对应 `.php`。例如：\n\n```test\nfastcgi.map-extensions = ( \".php3\" => \".php\" )\n```\n\nor for multiple\n\n```test\nfastcgi.map-extensions = ( \".php3\" => \".php\", \".php4\" => \".php\" )\n```\n\n3） fastcgi.server\n\n这个配置是告诉 `Web Server` 将 `FastCGI` 请求发送到哪里，其中每一个文件扩展名可以处理一个类型的请求。负载均衡器可以实现对同一扩展名的多个对象的负载均衡。\n\n`fastcgi.server` 的结构语法如下：\n\n```bash\n( <extension> =>\n  ( [ <name> => ]\n    ( # Be careful: lighty does *not* warn you if it doesn't know a specified option here (make sure you have no typos)\n      \"host\" => <string> ,\n      \"port\" => <integer> ,\n      \"socket\" => <string>,                 # either socket or host+port\n      \"bin-path\" => <string>,               # optional\n      \"bin-environment\" => <array>,         # optional\n      \"bin-copy-environment\" => <array>,    # optional\n      \"mode\" => <string>,                   # optional\n      \"docroot\" => <string> ,               # optional if \"mode\" is not \"authorizer\"\n      \"check-local\" => <string>,            # optional\n      \"max-procs\" => <integer>,             # optional - when omitted, default is 4\n      \"broken-scriptfilename\" => <boolean>, # optional\n      \"kill-signal\" => <integer>,           # optional, default is SIGTERM(15) (v1.4.14+)\n    ),\n    ( \"host\" => ...\n    )\n  )\n)\n```\n\n其中：\n> **extentsion** ：文件名后缀或以”/”开头的前缀（也可为文件名）\n> **name** ：这是一个可选项，表示handler的名称，在mod_status中用于统计功能，可以清晰的分辨出是哪一个handler处理了<extension>。\n> **host** ：FastCGI进程监听的IP地址。此处不支持hostname形式。\n> **port** ：FastCGI进程所监听的TCP端口号\n> **bin-path** ：本地FastCGI二进制程序的路径，当本地没有FastCGI正在运行时，会启动这个FastCGI程序。\n> **socket** ：unix-domain-socket所在路径。\n> **mode** ：可以选择FastCGI协议的模式，默认是“responder”，还可以选择authorizer。\n> **docroot** ：这是一个可选项，对于responder模式来讲，表示远程主机docroot；对于authorizer模式来说，它表示MANDATORY，并且指向授权请求的docroot。\n> **check_local** ：这是一个可选项，默认是enable。如果是enable，那么server会首先在本地（server.document-root）目录中检查被请求的文件是否存在，如果不存在，则给用户返回404（Not Found），而不会把这个请求传递给FastCGI。如果是disable，那么server不会检查本地文件，而是直接将请求转发给FastCGI。（disable的话，server从某种意义上说就变为了一个转发器）\n> **broken-scriptfilename** ：以类似PHP抽取PATH_INFO的方式，抽取URL中的SCRIPT_FILENAME。\n\n如果 `bin-path` 被设置了，那么：\n\n> **max-procs** ：设置多少个FastCGI进程被启动\n> **bin-environment** ：在FastCGI进程启动时设置一个环境变量\n> **bin-copy-environment** ：清除环境，并拷贝指定的变量到全新的环境中。\n> **kill-signal** ：默认的话，在停止FastCGI进程时，lighttpd会发送SIGTERM(-15)信号给子进程。此处可以设置发送的信号。\n\n**举例** ：\n\n使用前缀来对应主机：\n\n```bash\nfastcgi.server = (\n  \"/remote_scripts/\" =>\n  (( \"host\" => \"192.168.0.3\",\n     \"port\" => 9000,\n     \"check-local\" => \"disable\",\n     \"docroot\" => \"/\" # remote server may use\n                      # it's own docroot\n  ))\n)\n```\n\n如果有一个请求 \"http://my.example.org/remote_scripts/test.cgi\"，那么server会将其转发给192.168.0.3的9000端口，并且 `SCRIPT_NAME` 会被赋值为 `“/remote_scripts/test.cgi”`。如果所设置的 `handler` 的末尾不是 `“/”` ，那么会被认为是一个文件。\n\n**负载均衡** ：\n\n`FastCGI` 模块提供了一种在多台 `FastCGI` 服务器间负载均衡的方法。\n\n例如：\n\n```bash\nfastcgi.server = ( \".php\" =>\n  (\n    ( \"host\" => \"10.0.0.2\",\n      \"port\" => 1030\n    ),\n    ( \"host\" => \"10.0.0.3\",\n      \"port\" => 1030 )\n    )\n  )\n```\n\n为了更好的理解负载均衡实现的原理，建议你置 `fastcgi.debug` 为 `1` 。即使对于本机的多个 `FastCGI` ，你也会获得如下输出：\n\n```test\n  proc: 127.0.0.1 1031  1 1 1 31454\n  proc: 127.0.0.1 1028  1 1 1 31442\n  proc: 127.0.0.1 1030  1 1 1 31449\n  proc: 127.0.0.1 1029  1 1 2 31447\n  proc: 127.0.0.1 1026  1 1 2 31438\n  got proc: 34 31454\n  release proc: 40 31438\n  proc: 127.0.0.1 1026  1 1 1 31438\n  proc: 127.0.0.1 1028  1 1 1 31442\n  proc: 127.0.0.1 1030  1 1 1 31449\n  proc: 127.0.0.1 1031  1 1 2 31454\n  proc: 127.0.0.1 1029  1 1 2 31447\n```\n\n上述信息显示出了IP地址，端口号、当前链接数（也就是负载）（倒数第二列）、进程ID（倒数第一列）等等。整个输出信息总是以负载域来从小到大排序的。\n\n### 参考文献\n\n[](http://redmine.lighttpd.net/projects/1/wiki/Docs:ModFastCGI)\n\n[](http://www.fastcgi.com)\n\n[说说lighttpd的fastcgi](http://roclinux.cn/?p=2347)\n\n[Nginx + CGI/FastCGI + C/Cpp](http://www.cnblogs.com/skynet/p/4173450.html)\n\n[FastCGI+lighttpd开发之介绍和环境搭建](https://segmentfault.com/a/1190000004006596)\n\n\n\n### 附：QC V3 PP 版本 lighttpd.conf\n\n```bash\n$ cat /etc/qtilighttpd.conf \n# ------------------------------------------------------------------------------\n# Copyright (c) 2016 Qualcomm Technologies, Inc.\n# All Rights Reserved.\n# Confidential and Proprietary - Qualcomm Technologies, Inc.\n# ------------------------------------------------------------------------------\n\nserver.document-root = \"/opt/qcom/www\"\n\nserver.port    = 80\nserver.username    = \"apps\"\nserver.groupname  = \"apps\"\nserver.bind    = \"0.0.0.0\"\nserver.tag    = \"lighttpd\"\n$SERVER[\"socket\"] == \"[::]:80\" {  }\n\nserver.errorlog-use-syslog  = \"enable\"\naccesslog.use-syslog    = \"enable\"\n\nserver.modules    = (\n  \"mod_access\",\"mod_accesslog\", \"mod_cgi\", \"mod_fastcgi\"\n)\n\nfastcgi.debug = 1\nfastcgi.server = (\n    \"/fsmoam\" => (\n    \"fsmoam.fcgi.handler\" => (\n        \"socket\" => \"/tmp/fsmoam.fcgi.socket\",\n        \"check-local\" => \"disable\",\n        \"bin-path\" => \"/opt/qcom/bin/tests/fsmWebServer --default-log-level=DEBUG\",\n        \"max-procs\" => 1)\n     )\n)\n\n\n# mimetype mapping\nmimetype.assign    = (\n  \".pdf\"    =>  \"application/pdf\",\n  \".sig\"    =>  \"application/pgp-signature\",\n  \".spl\"    =>  \"application/futuresplash\",\n  \".class\"  =>  \"application/octet-stream\",\n  \".ps\"    =>  \"application/postscript\",\n  \".torrent\"  =>  \"application/x-bittorrent\",\n  \".dvi\"    =>  \"application/x-dvi\",\n  \".gz\"    =>  \"application/x-gzip\",\n  \".pac\"    =>  \"application/x-ns-proxy-autoconfig\",\n  \".swf\"    =>  \"application/x-shockwave-flash\",\n  \".tar.gz\"  =>  \"application/x-tgz\",\n  \".tgz\"    =>  \"application/x-tgz\",\n  \".tar\"    =>  \"application/x-tar\",\n  \".zip\"    =>  \"application/zip\",\n  \".mp3\"    =>  \"audio/mpeg\",\n  \".m3u\"    =>  \"audio/x-mpegurl\",\n  \".wma\"    =>  \"audio/x-ms-wma\",\n  \".wax\"    =>  \"audio/x-ms-wax\",\n  \".ogg\"    =>  \"audio/x-wav\",\n  \".wav\"    =>  \"audio/x-wav\",\n  \".gif\"    =>  \"image/gif\",\n  \".jpg\"    =>  \"image/jpeg\",\n  \".jpeg\"    =>  \"image/jpeg\",\n  \".png\"    =>  \"image/png\",\n  \".xbm\"    =>  \"image/x-xbitmap\",\n  \".xpm\"    =>  \"image/x-xpixmap\",\n  \".xwd\"    =>  \"image/x-xwindowdump\",\n  \".css\"    =>  \"text/css\",\n  \".html\"    =>  \"text/html\",\n  \".htm\"    =>  \"text/html\",\n  \".js\"    =>  \"text/javascript\",\n  \".asc\"    =>  \"text/plain\",\n  \".c\"    =>  \"text/plain\",\n  \".conf\"    =>  \"text/plain\",\n  \".text\"    =>  \"text/plain\",\n  \".txt\"    =>  \"text/plain\",\n  \".dtd\"    =>  \"text/xml\",\n  \".xml\"    =>  \"text/xml\",\n  \".mpeg\"    =>  \"video/mpeg\",\n  \".mpg\"    =>  \"video/mpeg\",\n  \".mov\"    =>  \"video/quicktime\",\n  \".qt\"    =>  \"video/quicktime\",\n  \".avi\"    =>  \"video/x-msvideo\",\n  \".asf\"    =>  \"video/x-ms-asf\",\n  \".asx\"    =>  \"video/x-ms-asf\",\n  \".wmv\"    =>  \"video/x-ms-wmv\",\n  \".bz2\"    =>  \"application/x-bzip\",\n  \".tbz\"    =>  \"application/x-bzip-compressed-tar\",\n  \".tar.bz2\"  =>  \"application/x-bzip-compressed-tar\"\n)\n\nindex-file.names = ( \"index.html\" )\n\ncgi.assign = ( \".sh\" => \"/bin/sh\" )\n```\n\n","tags":["lighttpd","FastCGI"]},{"title":"lighttpd 配置https","url":"/2017-03-31/reference/HTTP/lighttpd-配置https/","content":"\n### 确定安装的lighttpd支持ssl\n\n版本信息中含有（ssl）字样的信息说明支持ssl，可以在终端输入如下查看：\n\n```bash\n$ lighttpd -v\nlighttpd/1.4.35 (ssl) - a light and fast webserver\nBuild-Date: Apr 25 2017 10:25:18\n```\n\n<!-- more -->\n\n### 生成自签名证书\n\n完整的ssl证书分为四个部分：\n\n* CA根证书（root CA）\n* 中级证书（Intermediate Certificate）\n* 域名证书\n* 证书秘钥（仅由开发者提供）\n\n证书相当于公钥，pem相当于私钥。\n\nSelf-Signed Certificates：包含公钥和私钥的结合体，证书（公钥）会在连接请求的时候发给浏览器，以便浏览器解密和加密。\n\n创建Self-Signed Certificates：\n\n```bash\n$ openssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes\n```\n\n上边的命令生成一个server.pem文件。\n\n### lighttpd.conf 配置\n\n```bash\n$SERVER[\"socket\"] == \"[::]:443\" {  \n     ssl.engine      = \"enable\"\n     ssl.pemfile     = \"/mnt/flash/server.pem\"\n}\n```\n\n### 强制定向到HTTPS\n\n下面是 `lighttpd.conf` 文件中关于强制 HTTP 定向到 HTTPS 的部分配置：\n\n```bash\n$HTTP[\"scheme\"] == \"http\" {\n    # capture vhost name with regex conditiona -> %0 in redirect pattern\n    # must be the most inner block to the redirect rule\n    $HTTP[\"host\"] =~ \".*\" {\n        url.redirect = (\".*\" => \"https://%0$0\")\n    }\n}\n```\n\n此功能需要lighttpd `mod_redirect` 模块支持。使用此功能前确保模块已经安装。\n\n### lighttpd安全配置\n\n**禁用 SSL Compression (抵御 CRIME 攻击)**\n\n```bash\nssl.use-compression = \"disable\"\n```\n\n**禁用 SSLv2 及 SSLv3**\n\n```bash\nssl.use-sslv2 = \"disable\"\nssl.use-sslv3 = \"disable\"\n```\n\n**抵御 Poodle 和 SSL downgrade 攻击**\n\n需要支持 `TLS-FALLBACK-SCSV` 以自动开启此功能。下列 openSSL 版本包含对 `TLS-FALLBACK-SCSV` 的支持，lighttpd 会自动启用此特性。\n\n* OpenSSL **1.0.1** 在 `1.0.1j` 及之后的版本中支持\n* OpenSSL **1.0.0** 在 `1.0.0o` 及之后的版本中支持\n* OpenSSL **0.9.8** 在 `0.9.8zc` 及之后的版本中支持\n\n**加密及交换算法**\n\n一份推介的配置：\n\n```bash\nssl.cipher-list = \"EECDH+AESGCM:EDH+AESGCM:AES128+EECDH:AES128+EDH\"\n```\n\n如果您需要兼容一些老式系统和浏览器 (例如 Windows XP 和 IE6)，请使用下面的：\n\n```bash\nssl.cipher-list = \"EECDH+AESGCM:EDH+AESGCM:ECDHE-RSA-AES128-GCM-SHA256:AES256+EECDH:AES256+EDH:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4\"\n```\n\n**配置 Forward Secrecy 和 DHE 参数**\n\n生成强 DHE 参数：\n\n```bash\n$ cd /etc/ssl/certs\n$ openssl dhparam -out dhparam.pem 4096\n```\n\n**建议您使用性能强劲的平台生成此文件**，例如最新版的至强物理机。如果您只有一台小型 VPS，请使用 `openssl dhparam -out dhparam.pem 2048` 命令生成 2048bit 的参数文件。\n\n添加到 SSL 配置文件：\n\n```bash\nssl.dh-file = \"/etc/ssl/certs/dhparam.pem\"\nssl.ec-curve = \"secp384r1\"\n```\n\n**启用 HSTS**\n\n```bash\nserver.modules += ( \"mod_setenv\" )\n$HTTP[\"scheme\"] == \"https\" {\n    setenv.add-response-header  = ( \"Strict-Transport-Security\" => \"max-age=63072000; includeSubdomains; preload\")\n}\n```\n### 参考资料\n\n[Lighttpd](https://wiki.archlinux.org/index.php/Lighttpd)\n","tags":["http","lighttpd"]},{"title":"svn 常用操作命令","url":"/2017-03-29/reference/Git/svn-常用操作命令/","content":"\n## 检出\n\n```bash\n$ svn  checkout  http://路径(目录或文件的全路径)　[本地目录全路径] --username　用户名\n$ svn  checkout  svn://路径(目录或文件的全路径)　[本地目录全路径]  --username　用户名\n# 也可以使用缩写\n# 例子：\n$ svn co svn://localhost/测试工具 /home/testtools --username wzhnsc\n$ svn co http://localhost/test/testapp --username wzhnsc\n```\n\n<!-- more -->\n\n**注** ：如果不带--password 参数传输密码的话，会提示输入密码，建议不要用明文的--password 选项。 不指定本地目录全路径，则检出到当前目录下。\n\n## 导出（导出一个干净的不带.svn文件夹的目录树）\n\n```bash\n$ svn  export  [-r 版本号]  http://路径(目录或文件的全路径) [本地目录全路径]　--username　用户名\n$ svn  export  [-r 版本号]  svn://路径(目录或文件的全路径) [本地目录全路径]　--username　用户名\n$ svn  export  本地检出的(即带有.svn文件夹的)目录全路径  要导出的本地目录全路径\n# 例子：\n$ svn export svn://localhost/测试工具 /home/testtools --username wzhnsc\n$ svn export svn://localhost/test/testapp --username wzhnsc\n$ svn export /home/testapp /home/testtools\n```\n\n**注** ：第一种从版本库导出干净工作目录树的形式是指定URL，\n\n​\t 如果指定了修订版本号，会导出相应的版本，\n\n​\t 如果没有指定修订版本，则会导出最新的，导出到指定位置。\n\n​\t 如果省略 本地目录全路径，URL的最后一部分会作为本地目录的名字。\n\n​\t 第二种形式是指定 本地检出的目录全路径 到 要导出的本地目录全路径，所有的本地修改将会保留，\n\n​\t 但是不在版本控制下(即没提交的新文件，因为.svn文件夹里没有与之相关的信息记录)的文件不会拷贝。\n\n## 添加新文件\n\n```bash\n$ svn　add　文件名\n# 注：告诉SVN服务器要添加文件了，还要用svn commint -m真实的上传上去！\n# 例子：\n$ svn add test.php  # 添加test.php \n$ svn commit -m \"添加我的测试用test.php\" test.php\n$ svn add *.php  # 添加当前目录下所有的php文件\n$ svn commit -m \"添加我的测试用全部php文件\" *.php\n```\n\n## 提交\n\n```bash\n$ svn　commit　-m　\"提交备注信息文本\"　[-N]　[--no-unlock]　文件名\n$ svn　ci　-m　\"提交备注信息文本\"　[-N]　[--no-unlock]　文件名\n# 必须带上-m参数，参数可以为空，但是必须写上-m\n# 例子：\n$ svn commit -m \"提交当前目录下的全部在版本控制下的文件\" *   # 注意这个*表示全部文件\n$ svn commit -m \"提交我的测试用test.php\" test.php\n$ svn commit -m \"提交我的测试用test.php\" -N --no-unlock test.php   # 保持锁就用–no-unlock开关\n$ svn ci -m \"提交当前目录下的全部在版本控制下的文件\" *   # 注意这个*表示全部文件\n$ svn ci -m \"提交我的测试用test.php\" test.php\n$ svn ci -m \"提交我的测试用test.php\" -N --no-unlock test.php   # 保持锁就用–no-unlock开关\n```\n\n## 更新文件\n\n```bash\n$ svn　update\n$ svn　update　-r　修正版本　文件名\n$ svn　update　文件名\n# 例子：\n# 后面没有目录，默认将当前目录以及子目录下的所有文件都更新到最新版本\n$ svn update \n\n# 将版本库中的文件 test.cpp 还原到修正版本（revision）200\n$ svn update -r 200 test.cpp\n\n# 更新与版本库同步。提交的时候提示过期冲突，需要先 update 修改文件，然后清除svn resolved，最后再提交commit。\n$ svn update test.php 　　　　　 \n```\n\n## 删除文件\n\n```bash\n$ svn　delete　svn://路径(目录或文件的全路径) -m \"删除备注信息文本\"\n# 推荐如下操作：\n$ svn　delete　文件名 \n$ svn　ci　-m　\"删除备注信息文本\"\n# 例子：\n$ svn delete svn://localhost/testapp/test.php -m \"删除测试文件test.php\"\n# 推荐如下操作：\n$ svn delete test.php \n$ svn ci -m \"删除测试文件test.php\"\n```\n\n## 加锁 / 解锁\n\n```bash\n$ svn　lock　-m　\"加锁备注信息文本\"　[--force]　文件名 \n$ svn　unlock　文件名\n# 例子：\n$ svn lock -m \"锁信测试用test.php文件\" test.php \n$ svn unlock test.php\n```\n\n## 比较差异\n\n```bash\n$ svn　diff　文件名 \n$ svn　diff　-r　修正版本号m:修正版本号n　文件名\n# 例子：\n# 将修改的文件与基础版本比较\n$ svn diff test.php \n\n# 对修正版本号200 和 修正版本号201 比较差异\n$ svn diff -r 200:201 test.php \n```\n\n## 查看文件或者目录状态\n\n```bash\n$ svn st 目录路径/名\n# 目录下的文件和子目录的状态，正常状态不显示.\n# 【?：不在svn的控制中；  M：内容被修改；C：发生冲突；A：预定加入到版本库；K：被锁定】 \n$ svn status 目录路径/名　　　　　\n$ svn -v 目录路径/名\n\n# 显示文件和子目录状态\n# 【第一列保持相同，第二列显示工作版本号，第三和第四列显示最后一次修改的版本号和修改人】\n$ svn status -v 目录路径/名　　　　　　　　　  \n```\n\n**注** ：svn status、svn diff和 svn revert这三条命令在没有网络的情况下也可以执行的，原因是svn在本地的.svn中保留了本地版本的原始拷贝。\n\n## 查看日志\n\n```bash\n$ svn　log　文件名\n# 例子：\n# 显示这个文件的所有修改记录，及其版本号的变化\n$ svn log test.php \n```\n\n## 查看文件详细信息\n\n```bash\n$ svn　info　文件名\n# 例子：\n$ svn info test.php\n```\n\n## SVN 帮助\n\n```bash\n# 全部功能选项\n$ svn　help\n# 具体功能的说明\n$ svn　help　ci \n```\n\n## 查看版本库下的文件和目录列表\n\n```bash\n$ svn　list　svn://路径(目录或文件的全路径)\n$ svn　ls　svn://路径(目录或文件的全路径)\n# 例子：\n$ svn list svn://localhost/test\n\n# 显示svn://localhost/test目录下的所有属于版本库的文件和目录\n$ svn ls svn://localhost/test \n```\n\n## 创建纳入版本控制下的新目录\n\n```bash\n$ svn　mkdir　目录名\n$ svn　mkdir　-m　\"新增目录备注文本\"　http://目录全路径\n# 例子：\n$ svn mkdir newdir\n$ svn mkdir -m \"Making a new dir.\" svn://localhost/test/newdir \n```\n\n**注** ： 添加完子目录后，一定要回到根目录更新一下，不然在该目录下提交文件会提示“提交失败”\n\n```bash\n$ svn update\n```\n\n**注** ：如果手工在checkout出来的目录里创建了一个新文件夹newsubdir，\n\n​\t 再用svn mkdir newsubdir命令后，SVN会提示：\n\n​\t svn: 尝试用 “svn add”或 “svn add --non-recursive”代替？\n\n​\t svn: 无法创建目录“hello”: 文件已经存在\n\n  \t 此时，用如下命令解决：\n\n \t svn add --non-recursive newsubdir\n\n​\t 在进入这个newsubdir文件夹，用ls -a查看它下面的全部目录与文件，会发现多了：.svn目录\n\n​\t 再用 svn mkdir -m \"添hello功能模块文件\" [svn://localhost/test/newdir/newsubdir](svn://localhost/test/newdir/newsubdir) 命令，\n\n​\t SVN提示：\n\n​\t svn: File already exists: filesystem '/data/svnroot/test/db', transaction '4541-1',\n\n​\t path '/newdir/newsubdir '\n\n## 恢复本地修改\n\n```bash\n$ svn　revert　[--recursive]　文件名\n# 注意: 本子命令不会存取网络，并且会解除冲突的状况。但是它不会恢复被删除的目录。\n# 例子：\n# 丢弃对一个文件的修改\n$ svn revert foo.c\n\n# 恢复一整个目录的文件，. 为当前目录\n$ svn revert --recursive .\n```\n\n## 把工作拷贝更新到别的URL\n\n```bash\n$ svn　switch　http://目录全路径　本地目录全路径\n# 例子：\n# (原为123的分支)当前所在目录分支到localhost/test/456\n$ svn switch http://localhost/test/456 . \n```\n\n## 解决冲突\n\n```bash\n$ svn　resolved　[本地目录全路径]\n# 例子：\n$ svn update\nC foo.c\nUpdated to revision 31.\n# 如果你在更新时得到冲突，你的工作拷贝会产生三个新的文件：\n$ ls\nfoo.c\nfoo.c.mine\nfoo.c.r30\nfoo.c.r31\n# 当你解决了foo.c的冲突，并且准备提交，运行svn resolved让你的工作拷贝知道你已经完成了所有事情。\n# 你可以仅仅删除冲突的文件并且提交，但是svn resolved除了删除冲突文件，还修正了一些记录在工作拷贝管理区域的记录数据，所以我们推荐你使用这个命令。\n```\n\n## 不checkout而查看输出特定文件或URL的内容\n\n```bash\n$ svn　cat　http://文件全路径\n# 例子：\n$ svn cat http://localhost/test/readme.txt\n\n# 新建一个分支copy\n# 从branchA拷贝出一个新分支branchB\n$ svn copy branchA branchB  -m \"make B branch\" \n\n# 合并内容到分支merge\n# 把对branchA的修改合并到分支branchB\n$ svn merge branchA branchB \n```\n\n","tags":["svn"]},{"title":"hexo 使用指南","url":"/2017-03-28/reference/Hexo/hexo-使用指南/","content":"\n## 安装、初始化和配置\n\n### 准备工作\n\n* git\n* node.js\n* github\n\n<!-- more -->\n\n### 安装和初始化\n\n首先确定已经安装好了 `nodejs` 和 `npm` 以及 `git`\n\n```bash\n$ npm install hexo -g\n$ hexo init blog\n$ cd blog\n$ npm install\n$ hexo server\n```\n\n访问[http://localhost:4000](http://localhost:4000)，会看到生成好的博客。\n\n### 主目录结构\n\n```python\n|-- _config.yml\n|-- package.json\n|-- scaffolds\n|-- source\n   |-- _posts\n|-- themes\n|-- .gitignore\n|-- package.json\n```\n\n**_config.yml**\n\n全局配置文件，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。这个文件下面会做较为详细的介绍。\n\n**package.json**\n\nhexo框架的参数和所依赖插件，如下：  \n\n```json\n{\n  \"name\": \"hexo-site\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"hexo\": {\n    \"version\": \"3.2.0\"\n  },\n  \"dependencies\": {\n    \"hexo\": \"^3.2.0\",\n    \"hexo-generator-archive\": \"^0.1.4\",\n    \"hexo-generator-category\": \"^0.1.3\",\n    \"hexo-generator-index\": \"^0.2.0\",\n    \"hexo-generator-tag\": \"^0.2.0\",\n    \"hexo-renderer-ejs\": \"^0.2.0\",\n    \"hexo-renderer-stylus\": \"^0.3.1\",\n    \"hexo-renderer-marked\": \"^0.2.10\",\n    \"hexo-server\": \"^0.2.0\"\n  }\n}\n```\n\n**scaffold**\n\nscaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new 'title'）的时候，hexo是根据这个目录下的文件进行构建的。基本不用关心。\n\n**_config.yml文件**\n\n_config.yml 采用YAML语法格式，[具体语法自行学习](http://my.oschina.net/u/1861837/blog/526142?p=%7B%7BtotalPage%7D%7D) 。\n具体配置可以参考[官方文档](https://hexo.io/zh-cn/docs/configuration.html)，_config.yml 文件中的内容，并对主要参数做简单的介绍\n\n```yaml\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# Site\ntitle: Hexo   #网站标题\nsubtitle:     #网站副标题\ndescription:  #网站描述\nauthor: John Doe  #作者\nlanguage:    #语言\ntimezone:    #网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://yoursite.com   #你的站点Url\nroot: /                    #站点的根目录\npermalink: :year/:month/:day/:title/   #文章的 永久链接 格式   \npermalink_defaults:    #永久链接中各部分的默认值\n\n# Directory   \nsource_dir: source         #资源文件夹，这个文件夹用来存放内容\npublic_dir: public         #公共文件夹，这个文件夹用于存放生成的站点文件。\ntag_dir: tags              #标签文件夹     \narchive_dir: archives      #归档文件夹\ncategory_dir: categories   #分类文件夹\ncode_dir: downloads/code   #Include code 文件夹\ni18n_dir: :lang            #国际化（i18n）文件夹\nskip_render:               #跳过指定文件的渲染，您可使用 glob 表达式来匹配路径。    \n\n# Writing\nnew_post_name: :title.md   #新文章的文件名称\ndefault_layout: post       #预设布局\ntitlecase: false           #把标题转换为 title case\nexternal_link: true        #在新标签中打开链接\nfilename_case: 0           #把文件名称转换为 (1) 小写或 (2) 大写\nrender_drafts: false       #是否显示草稿\npost_asset_folder: false   #是否启动 Asset 文件夹\nrelative_link: false       #把链接改为与根目录的相对位址    \nfuture: true               #显示未来的文章\nhighlight:                 #内容中代码块的设置    \n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Category & Tag\ndefault_category: uncategorized\ncategory_map:          #分类别名\ntag_map:               #标签别名\n\n# Date / Time format\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD         #日期格式\ntime_format: HH:mm:ss           #时间格式    \n\n# Pagination\n## Set per_page to 0 to disable pagination\nper_page: 10    #分页数量\npagination_dir: page  \n\n# Extensions\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape   #主题名称\n\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\n#  部署部分的设置\ndeploy:     \n  type: git  #类型，常用的git\n  repo: https://github.com/nanshanyi/nanshanyi.github.io.git #github仓库的地址\n```\n\n### 注意\n\n**如果页面中出现中文，应以UTF-8无BOM编码格式，所以不要用win自带的记事本，而是用notepad++这种支持编码转换的编辑器。**\n\n由于google在天朝大陆被墙，进入 `themes\\landscape\\layout\\_partial` ，打开 `head.ejs` ，删掉第31行 `fonts.googleapis.com` 的链接。\n\n下载下来 `jQuery-2.0.3.min.js` ，放到 `themes\\landscape\\source\\js` 文件夹中。之后进入 `themes\\landscape\\layout\\_partial` ，打开 `after-footer.ejs` ，将第17行的路径替换为 `/js/jquery-2.0.3.min.js` 。\n\n至此大功告成。\n\n## 写文章&草稿\n\n### 文章\n\n命令行输入：\n\n```bash\n$ hexo new post \"new article\"\n```\n\n之后在 `soource/_posts` 目录下面多了一个 `new-article.md` 的文件。\n\n### 文章属性 \n\n| Setting    | Description | Default   |\n| ---------- | ----------- | --------- |\n| layout     | Layout      | post或page |\n| title      | 文章的标题       |           |\n| date       | 穿件日期        | 文件的创建日期   |\n| updated    | 修改日期        | 文件的修改日期   |\n| comments   | 是否开启评论      | true      |\n| tags       | 标签          |           |\n| categories | 分类          |           |\n| permalink  | url中的名字     | 文件名       |\n| toc        | 是否开启目录      | true      |\n| reward     | 是否开启打赏      | true      |\n\n### 分类和标签\n\n```text\ncategories:\n  - 日记\ntags:\n  - Hexo\n  - node.js\n```\n\n### 摘要\n\n`<!--more-->` 之上的内容为摘要。\n\n### 草稿\n\n草稿相当于很多博客都有的“私密文章”功能。\n\n```bash\n$ hexo new draft \"new draft\"\n```\n\n会在 `source/_drafts` 目录下生成一个 `new-draft.md` 文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到 `_drafts` 目录之中。\n\n如果你希望强行预览草稿，更改配置文件：\n\n```text\nrender_drafts: true\n```\n\n或者，如下方式启动server：\n\n```bash\n$ hexo server --drafts\n```\n\n下面这条命令可以把草稿变成文章，或者页面：\n\n```bash\n$ hexo publish [layout] <filename>\n\neg:\n$ hexo publish drafts hexo-使用指南\n```\n\n## Blog中出入图片和音乐\n\n文章推介：[Hexo 博客中插入音乐/视频](http://www.jianshu.com/p/53e0d2a617da)\n\n​\t\t   [使用七牛为Hexo存储图片](http://blog.shiqichan.com/use-qiniu-store-image-for-hexo/)\n\n  \t\t       [hexo主题中添加相册功能](http://www.cnblogs.com/xljzlw/p/5137622.html)\n\n​\t\t   [为 Hexo 主题添加多种图片样式(主题不错考虑移植)](http://wuchong.me/blog/2014/12/13/hexo-theme-creating-image-styles/?utm_source=tuicool&utm_medium=referral#)\n\n​\t\t   [Hexo折腾记——基本配置篇](https://yq.aliyun.com/articles/8607)\n\n​\t\t   [hexo博客进阶－相册和独立域名](http://www.cnblogs.com/jarson-7426/p/5515870.html)\n\n插入图片基本分为两种办法** ：\n\n（1） 放在本地文件\n\n首先在根目录下确认 `_config.yml` 中有 `post_asset_folder:true` 。\n在 hexo 目录，执行：\n\n```bash\n$ npm install https://github.com/CodeFalling/hexo-asset-image --save\n```\n\n之后再使用 `hexo new 'new' `创建新博客的时候，会在 `source/_posts` 里面创建 `.md` 文件的同时生成一个相同的名字的文件夹。把该文章中需要使用的图片放在该文件夹下即可。\n使用的时候\n\n```markdown\n![“图片描述”（可以不写）](/文件夹名/你的图片名字.JPG)\n例如：\n！[ ] (new/text.jpg)\n```\n\n（2）放在[七牛](https://portal.qiniu.com/signup?code=3lglas6pgi2qa)上，需要先注册，上传图片生成链接，直接在文章中使用链接即可。\n\n**插入音乐** ：\n\n可以使用网易云音乐，搜索想要的歌曲，点击歌曲名字进入播放器页面，点击生成外链播放器；复制代码，直接粘贴到博文中即可。这样会显示一个网易的播放器，可以把\n\n```html\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=298 height=52 src=\"http://music.163.com/outchain/player?type=2&id=32192436&auto=1&height=32\"></iframe>\n//其中的width=298 height=52 均改为0就看不到了，依然可以播放音乐\n```\n\n![](http://i.imgur.com/Y60twn8.png)\n\n![](http://i.imgur.com/i42cvBI.png)\n\n##  代码高亮highlight.js支持\n\n[highlightjs官网](https://highlightjs.org/)\n\n[highlightjs主题风格](https://highlightjs.org/static/demo/)\n\n\n\n## 其他\n\n[Hexo，Yilia主题添加站内搜索功能](http://www.yehbeats.com/2015/04/08/hexo-search/)\n\n[为Hexo博客添加目录](http://kuangqi.me/tricks/enable-table-of-contents-on-hexo/)\n\n[Hexo站点中添加文章目录以及归档](http://www.ituring.com.cn/article/199624)\n\n[使用LeanCloud平台为Hexo博客添加文章浏览量统计组件](http://crescentmoon.info/2014/12/11/popular-widget/)\n\n[使用hexo搭建静态博客](http://www.tuicool.com/articles/ABFn2qU)\n\n[Hexo Docs中文 ： （二）基本用法](http://www.ituring.com.cn/article/199035?utm_source=tuicool&utm_medium=referral)\n\n","tags":["hexo"],"categories":["hexo"]},{"title":"nodejs+webpack+vuejs 搭建开发环境学习套路","url":"/2017-03-27/reference/nodejs-webpack-vuejs-搭建开发环境学习套路/","content":"\n### 官方文档\n\n[官方手册](http://vuejs.org/v2/guide/)\n\n[中文官网](https://cn.vuejs.org/)\n\n[vuejs 2.0 中文文档](https://vuefe.cn/v2/guide/)\n\n[ECMAScript 6 入门](http://es6.ruanyifeng.com/)\n\n[node.js相关的中文文档及教程](http://cnodejs.org/topic/528c9a38d2b3893f2abb6eeb)\n\n[Node.js中文网API](http://nodejs.cn/api/)\n\n[Webpack 中文指南](http://webpackdoc.com/)\n\n[webpack2.2中文文档](http://www.css88.com/doc/webpack2/)\n\n以上是提供的一些官方资料，下面开始我们的套路吧：\n\n<!-- more -->\n\n### 环境构建\n\n1.新建一个目录`vuepro`\n2.初始化\n\n```bash\n$ cd vuepro\n\n# 初始化的时候可以一路回车，在最后输入\"yes\"后会生成package.json文件\n$ npm init\n```\n\n3.安装模块，先装这么多，有需要再安装\n\n```bash\n$ npm install vue webpack babel-loader babel-core babel-preset-env babel-cli babel-preset-es2015 html-webpack-plugin --save-dev\n```\n\n4.创建良好的目录层级\n\n```bash\n$ mkdir src\n$ cd src && mkdir -p html jssrc webapp \n```\n\n![](http://i.imgur.com/qkj7kJd.png)\t\n\n`html`放置模板文件，`jssrc`放置js文件，最终编译好的文件放置在`webapp`目录里，这个目录也就是我们网站的目录。\n\n5.在项目根目录下创建webpack配置文件：`webpack.config.js`\n\n```js\nvar HtmlWebpackPlugin = require('html-webpack-plugin');\nvar webpack=require(\"webpack\");\nmodule.exports =\n{\n    entry:\n    {\n        //入口文件\n        \"index\":__dirname+'/src/jssrc/index.js',\n    },\n    output: {\n        path: __dirname+'/src/webapp/js',  //输出文件夹\n        filename:'[name].js'   //最终打包生成的文件名(只是文件名，不带路径的哦)\n    },\n    /*resolve: {\n        alias: {\n            vue: 'vue/dist/vue.js'\n        }\n    },*/\n    externals: {\n\n    },\n    module:{\n        loaders:[\n            {test:/\\.js$/,loader:\"babel-loader\",query:{compact:true}},\n            //这里肯定要加入n个loader 譬如vue-loader、babel-loader、css-loader等等\n        ]\n    },\n    plugins:[\n        new HtmlWebpackPlugin({\n            filename: __dirname+'/src/webapp/index.html',   //目标文件\n            template: __dirname+'/src/html/index.html', //模板文件\n            inject:'body',\n            hash:true,  //代表js文件后面会跟一个随机字符串,解决缓存问题\n            chunks:[\"index\"]\n        })\n\n    ]\n}\n```\n\n6.同样在根目录下创建babel配置文件：`.babelrc`\n\n```text\n{\n    \"presets\" : [\"es2015\"]\n} \n```\n\n然后就可以在webpack里面配置loader，我们上面webpack配置中已经写了：\n\n```js\n loaders:[\n            {test:/\\.js$/,loader:\"babel-loader\",query:{compact:true}},\n   \t\t\t// 经过测试旧版用的是loader:\"babel\",在新版中用的是loader:\"babel-loader\"\n        ]\n```\n\n这句话意思就是：凡是 `.js` 文件都使用 `babel-loader` , 并且压缩。\n\n### 学习vue最简单的一个套路\n\n思考：数据如何渲染？\n\n套路如下：\n\n首先要有个数据块标记\n\nvue里面可以像模板引擎一样写上 `{\\{name\\}}`\n\n其中 `name` 就是变量名\n\n### 接下来进行实战练习\n\n![](http://i.imgur.com/UhW18FI.png)\t\n\nindex.htm l如下：\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>首页</title>\n</head>\n<body>\n    <div id=\"me\">\n        我的年龄是{age}\n    </div>\n</body>\n</html>\n```\n\nindex.js 如下：\n\n```js\nimport Vue from \"vue\"; //会去node_modules\\vue\\package.json\n\nnew Vue({\n    el:\"#me\",\n    data:{age:18}\n})\n```\n\n至此，我们需要用 `webpack` 打包，打包到 `webapp` 目录下。 \n\n需要修改2个地方： \n\n(1)因为我们的 `webpack` 不是全局安装的，所以不能直接执行 `webpack` 命令，我们这里借助 `npm` 来执行。所以需要修改项目根目录下的 `package.json` 文件，加入：\n\n```json\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\n    \"build\": \"webpack\"\n  },\n```\n\n表示：执行build，就会去node_modules.bin\\下去寻找webpack命令。`build` 这个名字是自定义的。\n\n(2)还需要修改 webpack 配置文件：`webpack.config.js`\n\n```js\nresolve: {\n        alias: {\n            vue: 'vue/dist/vue.js'\n        }\n    },\n```\n\n我们之前把这个注释掉了，现在打开。此处的意义是找到 `node_modules/vue/dist/vue.js`\n\n最后，我们就来打包，看看结果是怎样的？ \n\n终端里还是cd到项目根目录下，执行：\n\n```bash\n$ npm run build\n```\n\n![](http://i.imgur.com/wmjrYdu.png)\t\n\n`index.html`  就是打包之后的模板文件，`js/index.js` 就是打包之后的js文件，在 `index.html` 被引用了。\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>首页</title>\n</head>\n<body>\n    <div id=\"me\">\n        我的年龄是{age}\n    </div>\n<script type=\"text/javascript\" src=\"js/index.js?43c73980e35f1569ef72\"></script></body>\n</html>\n```\n\n预览一下index.html: \n\n![](http://i.imgur.com/6kHwB4L.png)\n\n这样就完成了 `vueJS` 的一个简单案列\n","tags":["nodejs"]},{"title":"Hexo + Github 博客多终端同步","url":"/2017-03-20/reference/Hexo/Hexo-Github-博客多终端同步/","content":"\n[原文链接](http://blog.csdn.net/Monkey_LZL/article/details/60870891)\n\n主体的思路是将博文内容相关文件放在Github项目中master中，将Hexo配置写博客用的相关文件放在Github项目的hexo分支上，这个是关键，多终端的同步只需要对分支hexo进行操作。下面是详细的步骤讲解：\n\n<!-- more -->\n\n## 1. 准备条件\n\n安装了Node.js,Git,Hexo环境 \n完成Github与本地Hexo的对接 \n这部分大家可以参考[史上最详细的Hexo博客搭建图文教程](https://xuanwo.org/2015/03/26/hexo-intor/)\n\n配置好这些，就可以捋起袖子大干一场了！\n\n## 2. 在其中一个中单操作，push本地文件夹Hexo中的必要文件到yourname.github.io的hexo分支上\n\n在利用Github+Hexo搭建自己的博客时，新建了一个Hexo的文件夹，并进行相关的配置，这部分主要是将这些配置的文件托管到Github项目的分支上，其中只托管部分用于多终端的同步的文件，如完成的效果图所示：\n\n``` bash\n# 初始化本地仓库\n$ git init\n\n# 将必要的文件依次添加，有些文件夹如npm install产生的node_modules由于路径过长不好处理，所以这里没有用`git add .`命令了，而是依次添加必要文件\n$ git add source\n$ git commit -m \"Blog Source Hexo\"\n\n# 新建hexo分支\n$ git branch hexo\n\n# 切换到hexo分支上\n$ git checkout hexo\n\n# 将本地与Github项目对接\n$ git remote add origin https://github.com/yourname/yourname.github.io.git\n\n# push到Github项目的hexo分支上\n$ git push origin hexo\n```\n\n这样你的github项目中就会多出一个Hexo分支，这个就是用于多终端同步关键的部分。\n\n## 3. 另一终端完成clone和push更新\n\n此时在另一终端更新博客，只需要将Github的hexo分支clone下来，进行初次的相关配置\n\n``` bash\n# 将Github中hexo分支clone到本地\n$ git clone -b hexo https://github.com/yourname/yourname.github.io.git\n\n# 切换到刚刚clone的文件夹内\n$ cd yourname.github.io\n\n# cheackout 远程代码到本地hexo分支\n$ git checkout -b hexo origin/hexo\n\n# 注意，这里一定要切换到刚刚clone的文件夹内执行，安装必要的所需组件，不用再init\n$ npm install\n\n# 新建一个.md文件，并编辑完成自己的博客内容\n$ hexo new post \"new blog name\"\n\n# 经测试每次只要更新sorcerer中的文件到Github中即可，因为只是新建了一篇新博客\n$ git add source\n$ git commit -m \"XX\"\n\n# 更新分支\n$ git push origin hexo\n\n# push更新完分支之后将自己写的博客对接到自己搭的博客网站上，同时同步了Github中的master\n$ hexo d -g\n```\n\n## 4. 不同终端间愉快地玩耍\n\n在不同的终端已经做完配置，就可以愉快的分享自己更新的博客 \n进入自己相应的文件夹\n\n``` bash\n# 先pull完成本地与远端的融合\n$ git pull origin hexo\n\n$ hexo new post \" new blog name\"\n\n$ git add source\n\n$ git commit -m \"XX\"\n\n$ git push origin hexo\n\n$ hexo d -g\n```\n","tags":["hexo"],"categories":["hexo"]},{"title":"使用Git Submodule管理子模块","url":"/2017-01-29/reference/Git/git-submodule/","content":"\n> [【Git】子模块：一个仓库包含另一个仓库](<https://www.jianshu.com/p/491609b1c426>)\n>\n> [如何在大型项目中使用Git子模块开发](<https://juejin.im/post/5c1c5d305188256a272aa0ec>)\n\ngit子模块\n\n到目前为止,将您的大项目分解为子项目.\n现在使用以下命令将每个子项目添加到主项目：\n\n```shell\n$ git submodule add <url>\n```\n\n项目添加到您的仓库后,您必须初始化并更新它.\n\n```shell\n$ git submodule init\n$ git submodule update\n```\n\n从Git 1.8.2开始,新选项 `–remote` 被添加\n\n```shell\n$ git submodule update --remote --merge\n```\n\n将从每个子模块的上游获取最新的更改,将它们合并,并检查子模块的最新版本.\n\n## 使用场景\n\n基于公司的项目会越来越多，常常需要提取一个公共的类库提供给多个项目使用，但是这个`library`怎么和`git`在一起方便管理呢？\n\n我们需要解决下面几个问题：\n\n- 如何在git项目中导入`library库`?\n- `library库`在其他的项目中被修改了可以更新到远程的代码库中?\n- 其他项目如何获取到`library库`最新的提交?\n- 如何在clone的时候能够自动导入`library库`?\n\n解决以上问题，可以考虑使用git的 `Submodule`来解决。\n\n## 什么是Submodule?\n\n`git Submodule` 是一个很好的多项目使用共同类库的工具，他允许类库项目做为`repository`,子项目做为一个单独的`git项目`存在父项目中，子项目可以有自己的独立的`commit`，`push`，`pull`。而父项目以`Submodule`的形式包含子项目，父项目可以指定子项目`header`，父项目中会的提交信息包含`Submodule`的信息，再`clone父项目`的时候可以把`Submodule`初始化。\n\n## 在项目中使用Submodule\n\n使用`git`命令可以直接添加`Submodule`：\n\n```shell\n$ git submodule add git@github.com:xxx.git pod-library\n```\n\n使用 `git status`命令可以看到\n\n```shell\n$ git status\n```\n\n```\nOn branch master\nChanges to be committed:\n\n    new file:   .gitmodules\n    new file:   pod-library\n```\n\n可以看到多了两个需要提交的文件：`.gitmodules`和 `pod-library` \n\n`.gitmodules` 内容包含`Submodule`的主要信息，指定`reposirory`,指定路径:\n\n```\n[submodule \"pod-library\"]\n    path = pod-library\n    url = git@github.com:xxx/pod-library.git\n```\n\n可以看到记录了子项目的目录和子项目的`git`地址信息。\n\n`pod-libray`内容只保护子项目的`commit id`，就能指定到对于的`git header`上,例如:\n\n```shell\nSubproject commit 4ac42d2f8b9ba0c2f0f2f2ec87ddbd529275fea5\n```\n\n`4ac42d2f8b9ba0c2f0f2f2ec87ddbd529275fea5`就是子项目的`commit id`，父项目的git并不会记录`Submodule`的文件变动，它是按照`commit git`指定`Submodule`的`git header`。\n\n另外,*这两个文件都需要提交到父项目的git中*。\n\n还可以这样使用命令添加`Submodule`\n\n```shell\n$ git add .gitmodules pod-ibrary\n$ git commit -m \"pod-library submodule\"\n$ git submodule init\n```\n\n## 修改Submodule\n\n**首先需要确认有对Submodule的commit权限**。\n\n进入`Submodule`目录里面:\n\n```shell\n$ cd pod-library/\n```\n\n修改其中的一个文件看下文件的可以用`git status`查看变动:\n\n```shell\n$ git status\nmodified: pod-library/UseAFHTTP.h\n```\n\n提交`Submodule`的更改内容：\n\n```shell\n$ git commit -a -m'test submodule'\n```\n\n然后`push` 到远程服务器:\n\n```shell\n$ git push\n```\n\n然后再回到父目录,提交`Submodule`在父项目中的变动：\n\n```shell\n$ cd ..\n$ git status\non branch master\nmodified: pod-library (new commits)\n```\n\n可以看到`pod-library`中已经变更为`Submodule`最新的`commit id`:\n\n```\nSubproject commit 330417cf3fc1d2c42092b20506b0d296d90d0b5f\n```\n\n需要把`Submodule`的变动信息推送到父项目的远程服务器\n\n```shell\n$ git commit -m'update submodule'\n$ git push\n```\n\n这样就把子模块的变更信息以及子模块的变更信息提交到远程服务器了，从远程服务器上更新下来的内容就是最新提交的内容了。\n\n## 更新Submodule\n\n更新`Submodule`有两种方式:\n\n在父项目的目录下直接运行\n\n```shell\n$ git submodule foreach git pull\n```\n\n在Submodule的目录下面更新\n\n```shell\n$ cd pod-library\n$ git pull\n```\n\n可以看到在`Submodule`的目录中,使用`git`和单独的一个项目是一样的,注意更新`Submodule`的时候如果有新的`commit id`产生，需要在父项目产生一个新的提交，pod-libray文件中的 `Subproject commit`会变为最新的`commit id`。\n\n## clone Submodule\n\n`clone Submodule`有两种方式 一种是采用递归的方式clone整个项目，一种是clone父项目，再更新子项目。\n\n1. 采用递归参数 `--recursive`\n\n```shell\n$ git clone git@github.com:xxx.git --recursive\n```\n\n输出结果：\n\n```\nloning into 'pod-project'...\nremote: Counting objects: 57, done.\nremote: Compressing objects: 100% (45/45), done.\nremote: Total 57 (delta 13), reused 49 (delta 8), pack-reused 0\nReceiving objects: 100% (57/57), 18.79 KiB | 0 bytes/s, done.\nResolving deltas: 100% (13/13), done.\nChecking connectivity... done.\nSubmodule 'pod-library' (git@github.com:xxx.git) registered for path 'pod-library'\nCloning into 'pod-library'...\nremote: Counting objects: 34, done.\nremote: Compressing objects: 100% (25/25), done.\nremote: Total 34 (delta 8), reused 30 (delta 7), pack-reused 0\nReceiving objects: 100% (34/34), 12.95 KiB | 0 bytes/s, done.\nResolving deltas: 100% (8/8), done.\nChecking connectivity... done.\nSubmodule path 'pod-library': checked out '330417cf3fc1d2c\n\n42092b20506b0d296d90d0b5f'\n```\n\n可以看到`init Submodule` 会自动被`clone`下来\n\n2. 第二种方法先clone父项目，再初始化`Submodule`\n\n```shell\n$ git clone git@github.com:xxx/pod-project.git\n$ cd pod-project\n$ git submodule init\n```\n\n输出:\n\n```\nSubmodule 'pod-library' (git@github.com:xxx/pod-library.git) \nregistered for path 'pod-library'\n```\n\n更新`Submodule`:\n\n```shell\n$ git submodule update\n```\n\n运行结果：\n\n```\nCloning into 'pod-library'...\nremote: Counting objects: 34, done.\nremote: Compressing objects: 100% (25/25), done.\nremote: Total 34 (delta 8), reused 30 (delta 7), pack-reused 0\nReceiving objects: 100% (34/34), 12.95 KiB | 0 bytes/s, done.\nResolving deltas: 100% (8/8), done.\nChecking connectivity... done.\nSubmodule path 'pod-library': checked out '330417cf3fc1d2c42092b20506b0d296d90d0b5f'\n```\n\n## 删除Submodule\n\n`git` 并不支持直接删除`Submodule`需要手动删除对应的文件:\n\n```shell\n$ cd pod-project\n$ git rm --cached pod-library\n$ rm -rf pod-library\n$ rm .gitmodules\n```\n\n更改git的配置文件`config`:\n\n```shell\n$ vim .git/config\n```\n\n可以看到`Submodule`的配置信息：\n\n```\n[submodule \"pod-library\"]\n  url = git@github.com:xxx/pod-library.git\n```\n\n删除submodule相关的内容,然后提交到远程服务器:\n\n```shell\n$ git commit -a -m 'remove pod-library submodule'\n```\n\n","tags":["git"],"categories":["git"]},{"title":"Git使用规范流程","url":"/2017-01-27/reference/Git/git-using-standard-process/","content":"\n团队开发中，遵循一个合理、清晰的Git使用流程，是非常重要的。\n\n否则，每个人都提交一堆杂乱无章的commit，项目很快就会变得难以协调和维护。\n\n下面是ThoughtBot 的Git使用规范流程。我从中学到了很多，推荐你也这样使用Git。\n\n<!-- more -->\n\n![](http://i.imgur.com/WjTakfD.png)\n\n## 第一步：新建分支\n\n首先，每次开发新功能，都应该新建一个单独的分支（这方面可以参考《Git分支管理策略》）。\n\n\t# 获取主干最新代码\n\t$ git checkout master\n\t$ git pull\n\t\n\t# 新建一个开发分支myfeature\n\t$ git checkout -b myfeature\n\n## 第二步：提交分支commit\n\n分支修改后，就可以提交commit了。\n\n\t$ git add --all\n\t$ git status\n\t$ git commit --verbose\n\n`git add` 命令的all参数，表示保存所有变化（包括新建、修改和删除）。从Git 2.0开始，all是 git add 的默认参数，所以也可以用 git add . 代替。\n\n`git status` 命令，用来查看发生变动的文件。\n\n`git commit` 命令的 `verbose` 参数，会列出 diff 的结果。\n\n## 第三步：撰写提交信息\n\n提交commit时，必须给出完整扼要的提交信息，下面是一个范本。\n\n\tPresent-tense summary under 50 characters\n\t\n\t* More information about commit (under 72 characters).\n\t* More information about commit (under 72 characters).\n\t\n\thttp://project.management-system.com/ticket/123\n\n第一行是不超过50个字的提要，然后空一行，罗列出改动原因、主要变动、以及需要注意的问题。最后，提供对应的网址（比如Bug ticket）。\n\n## 第四步：与主干同步\n\n分支的开发过程中，要经常与主干保持同步。\n\n\t$ git fetch origin\n\t$ git rebase origin/master\n\n## 第五步：合并commit\n\n分支开发完成后，很可能有一堆 `commit`，但是合并到主干的时候，往往希望只有一个（或最多两三个）`commit`，这样不仅清晰，也容易管理。\n\n那么，怎样才能将多个 `commit` 合并呢？这就要用到 `git rebase` 命令。\n\n\t$ git rebase -i origin/master\n\n`git rebase` 命令的 `i` 参数表示互动（interactive），这时git会打开一个互动界面，进行下一步操作。\n\n\tpick 07c5abd Introduce OpenPGP and teach basic usage\n\tpick de9b1eb Fix PostChecker::Post#urls\n\tpick 3e7ee36 Hey kids, stop all the highlighting\n\tpick fa20af3 git interactive rebase, squash, amend\n\t\n\t# Rebase 8db7e8b..fa20af3 onto 8db7e8b\n\t#\n\t# Commands:\n\t#  p, pick = use commit\n\t#  r, reword = use commit, but edit the commit message\n\t#  e, edit = use commit, but stop for amending\n\t#  s, squash = use commit, but meld into previous commit\n\t#  f, fixup = like \"squash\", but discard this commit's log message\n\t#  x, exec = run command (the rest of the line) using shell\n\t#\n\t# These lines can be re-ordered; they are executed from top to bottom.\n\t#\n\t# If you remove a line here THAT COMMIT WILL BE LOST.\n\t#\n\t# However, if you remove everything, the rebase will be aborted.\n\t#\n\t# Note that empty commits are commented out\n\n上面的互动界面，先列出当前分支最新的4个 `commit`（越下面越新）。每个 `commit` 前面有一个操作命令，默认是 `pick`，表示该行 `commit` 被选中，要进行 `rebase` 操作。\n\n4个commit的下面是一大堆注释，列出可以使用的命令。\n\n* pick：正常选中\n* reword：选中，并且修改提交信息；\n* edit：选中，rebase时会暂停，允许你修改这个commit（参考这里）\n* squash：选中，会将当前commit与上一个commit合并\n* fixup：与squash相同，但不会保存当前commit的提交信息\n* exec：执行其他shell命令\n\n上面这6个命令当中，`squash` 和 `fixup` 可以用来合并 `commit`。先把需要合并的 `commit` 前面的动词，改成 `squash`（或者s）。\n\n\tpick 07c5abd Introduce OpenPGP and teach basic usage\n\ts de9b1eb Fix PostChecker::Post#urls\n\ts 3e7ee36 Hey kids, stop all the highlighting\n\tpick fa20af3 git interactive rebase, squash, amend\n\n这样一改，执行后，当前分支只会剩下两个commit。第二行和第三行的commit，都会合并到第一行的commit。提交信息会同时包含，这三个commit的提交信息。\n\n\t# This is a combination of 3 commits.\n\t# The first commit's message is:\n\tIntroduce OpenPGP and teach basic usage\n\t\n\t# This is the 2nd commit message:\n\tFix PostChecker::Post#urls\n\t\n\t# This is the 3rd commit message:\n\tHey kids, stop all the highlighting\n\n如果将第三行的 `squash` 命令改成 `fixup` 命令。\n\n\tpick 07c5abd Introduce OpenPGP and teach basic usage\n\ts de9b1eb Fix PostChecker::Post#urls\n\tf 3e7ee36 Hey kids, stop all the highlighting\n\tpick fa20af3 git interactive rebase, squash, amend\n\n运行结果相同，还是会生成两个commit，第二行和第三行的commit，都合并到第一行的commit。但是，新的提交信息里面，第三行commit的提交信息，会被注释掉。\n\n\t# This is a combination of 3 commits.\n\t# The first commit's message is:\n\tIntroduce OpenPGP and teach basic usage\n\t\n\t# This is the 2nd commit message:\n\tFix PostChecker::Post#urls\n\t\n\t# This is the 3rd commit message:\n\t# Hey kids, stop all the highlighting\n\nPony Foo提出另外一种合并commit的简便方法，就是先撤销过去5个commit，然后再建一个新的。\n\n\t$ git reset HEAD~5\n\t$ git add .\n\t$ git commit -am \"Here's the bug fix that closes #28\"\n\t$ git push --force\n\n`squash` 和 `fixup` 命令，还可以当作命令行参数使用，自动合并commit。\n\n\t$ git commit --fixup  \n\t$ git rebase -i --autosquash \n\n这个用法请参考[http://fle.github.io/git-tip-keep-your-branch-clean-with-fixup-and-autosquash.html](http://fle.github.io/git-tip-keep-your-branch-clean-with-fixup-and-autosquash.html \"\")，这里就不解释了。\n\n## 第六步：推送到远程仓库\n\n合并commit后，就可以推送当前分支到远程仓库了。\n\n\t$ git push --force origin myfeature\n\n`git push` 命令要加上 `force` 参数，因为 `rebase` 以后，分支历史改变了，跟远程分支不一定兼容，有可能要强行推送。\n\n## 第七步：发出Pull Request\n\n提交到远程仓库以后，就可以发出 `Pull Request` 到 `master` 分支，然后请求别人进行代码 `review`，确认可以合并到 `master`。\n","tags":["git"],"categories":["git"]},{"title":"常用Git命令清单","url":"/2017-01-27/reference/Git/git-common-list/","content":"\n我每天使用 Git ，但是很多命令记不住。\n\n一般来说，日常使用只要记住下图6个命令，就可以了。但是熟练使用，恐怕要记住60～100个命令。\n\n<!-- more -->\n\n![](http://i.imgur.com/zYeQxr4.png)\n\n下面是我整理的常用 Git 命令清单。几个专用名词的译名如下。\n\n* Workspace：工作区\n* Index / Stage：暂存区\n* Repository：仓库区（或本地仓库）\n* Remote：远程仓库\n\n## 一、新建代码库\n\n\t# 在当前目录新建一个Git代码库\n\t$ git init\n\t\n\t# 新建一个目录，将其初始化为Git代码库\n\t$ git init [project-name]\n\t\n\t# 下载一个项目和它的整个代码历史\n\t$ git clone [url]\n\n## 二、配置\n\nGit的设置文件为 `.gitconfig`，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。\n\n\t# 显示当前的Git配置\n\t$ git config --list\n\t\n\t# 编辑Git配置文件\n\t$ git config -e [--global]\n\t\n\t# 设置提交代码时的用户信息\n\t$ git config [--global] user.name \"[name]\"\n\t$ git config [--global] user.email \"[email address]\"\n\n## 三、增加/删除文件\n\n\t# 添加指定文件到暂存区\n\t$ git add [file1] [file2] ...\n\t\n\t# 添加指定目录到暂存区，包括子目录\n\t$ git add [dir]\n\t\n\t# 添加当前目录的所有文件到暂存区\n\t$ git add .\n\t\n\t# 添加每个变化前，都会要求确认\n\t# 对于同一个文件的多处变化，可以实现分次提交\n\t$ git add -p\n\t\n\t# 删除工作区文件，并且将这次删除放入暂存区\n\t$ git rm [file1] [file2] ...\n\t\n\t# 停止追踪指定文件，但该文件会保留在工作区\n\t$ git rm --cached [file]\n\t\n\t# 改名文件，并且将这个改名放入暂存区\n\t$ git mv [file-original] [file-renamed]\n\n## 四、代码提交\n\n\t# 提交暂存区到仓库区\n\t$ git commit -m [message]\n\t\n\t# 提交暂存区的指定文件到仓库区\n\t$ git commit [file1] [file2] ... -m [message]\n\t\n\t# 提交工作区自上次commit之后的变化，直接到仓库区\n\t$ git commit -a\n\t\n\t# 提交时显示所有diff信息\n\t$ git commit -v\n\t\n\t# 使用一次新的commit，替代上一次提交\n\t# 如果代码没有任何新变化，则用来改写上一次commit的提交信息\n\t$ git commit --amend -m [message]\n\t\n\t# 重做上一次commit，并包括指定文件的新变化\n\t$ git commit --amend [file1] [file2] ...\n\n## 五、分支\n\n\t# 列出所有本地分支\n\t$ git branch\n\t\n\t# 列出所有远程分支\n\t$ git branch -r\n\t\n\t# 列出所有本地分支和远程分支\n\t$ git branch -a\n\t\n\t# 新建一个分支，但依然停留在当前分支\n\t$ git branch [branch-name]\n\t\n\t# 新建一个分支，并切换到该分支\n\t$ git checkout -b [branch]\n\t\n\t# 新建一个分支，指向指定commit\n\t$ git branch [branch] [commit]\n\t\n\t# 新建一个分支，与指定的远程分支建立追踪关系\n\t$ git branch --track [branch] [remote-branch]\n\t\n\t# 切换到指定分支，并更新工作区\n\t$ git checkout [branch-name]\n\t\n\t# 切换到上一个分支\n\t$ git checkout -\n\t\n\t# 建立追踪关系，在现有分支与指定的远程分支之间\n\t$ git branch --set-upstream [branch] [remote-branch]\n\t\n\t# 合并指定分支到当前分支\n\t$ git merge [branch]\n\t\n\t# 选择一个commit，合并进当前分支\n\t$ git cherry-pick [commit]\n\t\n\t# 删除分支\n\t$ git branch -d [branch-name]\n\t\n\t# 删除远程分支\n\t$ git push origin --delete [branch-name]\n\t$ git branch -dr [remote/branch]\n\n## 六、标签\n\n\t# 列出所有tag\n\t$ git tag\n\t\n\t# 新建一个tag在当前commit\n\t$ git tag [tag]\n\t\n\t# 新建一个tag在指定commit\n\t$ git tag [tag] [commit]\n\t\n\t# 删除本地tag\n\t$ git tag -d [tag]\n\t\n\t# 删除远程tag\n\t$ git push origin :refs/tags/[tagName]\n\t\n\t# 查看tag信息\n\t$ git show [tag]\n\t\n\t# 提交指定tag\n\t$ git push [remote] [tag]\n\t\n\t# 提交所有tag\n\t$ git push [remote] --tags\n\t\n\t# 新建一个分支，指向某个tag\n\t$ git checkout -b [branch] [tag]\n\n## 七、查看信息\n\n\t# 显示有变更的文件\n\t$ git status\n\t\n\t# 显示当前分支的版本历史\n\t$ git log\n\t\n\t# 显示commit历史，以及每次commit发生变更的文件\n\t$ git log --stat\n\t\n\t# 搜索提交历史，根据关键词\n\t$ git log -S [keyword]\n\t\n\t# 显示某个commit之后的所有变动，每个commit占据一行\n\t$ git log [tag] HEAD --pretty=format:%s\n\t\n\t# 显示某个commit之后的所有变动，其\"提交说明\"必须符合搜索条件\n\t$ git log [tag] HEAD --grep feature\n\t\n\t# 显示某个文件的版本历史，包括文件改名\n\t$ git log --follow [file]\n\t$ git whatchanged [file]\n\t\n\t# 显示指定文件相关的每一次diff\n\t$ git log -p [file]\n\t\n\t# 显示过去5次提交\n\t$ git log -5 --pretty --oneline\n\t\n\t# 显示所有提交过的用户，按提交次数排序\n\t$ git shortlog -sn\n\t\n\t# 显示指定文件是什么人在什么时间修改过\n\t$ git blame [file]\n\t\n\t# 显示暂存区和工作区的差异\n\t$ git diff\n\t\n\t# 显示暂存区和上一个commit的差异\n\t$ git diff --cached [file]\n\t\n\t# 显示工作区与当前分支最新commit之间的差异\n\t$ git diff HEAD\n\t\n\t# 显示两次提交之间的差异\n\t$ git diff [first-branch]...[second-branch]\n\t\n\t# 显示今天你写了多少行代码\n\t$ git diff --shortstat \"@{0 day ago}\"\n\t\n\t# 显示某次提交的元数据和内容变化\n\t$ git show [commit]\n\t\n\t# 显示某次提交发生变化的文件\n\t$ git show --name-only [commit]\n\t\n\t# 显示某次提交时，某个文件的内容\n\t$ git show [commit]:[filename]\n\t\n\t# 显示当前分支的最近几次提交\n\t$ git reflog\n\n## 八、远程同步\n\n\t# 下载远程仓库的所有变动\n\t$ git fetch [remote]\n\t\n\t# 显示所有远程仓库\n\t$ git remote -v\n\t\n\t# 显示某个远程仓库的信息\n\t$ git remote show [remote]\n\t\n\t# 增加一个新的远程仓库，并命名\n\t$ git remote add [shortname] [url]\n\t\n\t# 取回远程仓库的变化，并与本地分支合并\n\t$ git pull [remote] [branch]\n\t\n\t# 上传本地指定分支到远程仓库\n\t$ git push [remote] [branch]\n\t\n\t# 强行推送当前分支到远程仓库，即使有冲突\n\t$ git push [remote] --force\n\t\n\t# 推送所有分支到远程仓库\n\t$ git push [remote] --all\n\n## 九、撤销\n\n\t# 恢复暂存区的指定文件到工作区\n\t$ git checkout [file]\n\t\n\t# 恢复某个commit的指定文件到暂存区和工作区\n\t$ git checkout [commit] [file]\n\t\n\t# 恢复暂存区的所有文件到工作区\n\t$ git checkout .\n\t\n\t# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变\n\t$ git reset [file]\n\t\n\t# 重置暂存区与工作区，与上一次commit保持一致\n\t$ git reset --hard\n\t\n\t# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变\n\t$ git reset [commit]\n\t\n\t# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致\n\t$ git reset --hard [commit]\n\t\n\t# 重置当前HEAD为指定commit，但保持暂存区和工作区不变\n\t$ git reset --keep [commit]\n\t\n\t# 新建一个commit，用来撤销指定commit\n\t# 后者的所有变化都将被前者抵消，并且应用到当前分支\n\t$ git revert [commit]\n\t\n\t# 暂时将未提交的变化移除，稍后再移入\n\t$ git stash\n\t$ git stash pop\n\n## 十、其他\n\n\t# 生成一个可供发布的压缩包\n\t$ git archive\n","tags":["git"],"categories":["git"]},{"title":"Hexo的Next主题个性化设置","url":"/2016-12-23/reference/Hexo/Hexo的Next主题个性化设置/","content":"\n> [Hexo + GitHub (Coding) Pages 搭建博客](<https://github.com/HarleyWang93/blog/issues/1>)\n>\n> [Hexo的Next主题个性化设置（一）——基础设置](<http://blog.shenyuanluo.com/HexoConfig1.html>)\n\n","tags":["hexo"],"categories":["hexo"]},{"title":"Docker 学习笔记","url":"/2016-12-23/reference/Docker-学习笔记/","content":"\n# Docker基本命令\n\n### 常用Docker命令\n\n------\n\n<!-- more -->\n\n```bash\n# 开启Docker守护进程调试模式\n$ sudo docker daemon -D\n\n# 查看Docker信息\n$ sudo docker info \n\n# 停止或者启动Docker\n$ sudo service docker stop/start \n\n# 以命令行模式运行一个容器\n$ sudo docker run -i -t ubuntu /bin/bash \n\n# 给容器命名\n$ sudo docker run --name Micheal_container -i -t ubuntu /bin/bash\n\n# 启动或者停止运行的容器\n$ sudo docker start/stop Micheal_container \n\n# 附着到正在运行的容器\n$ sudo docker attach Micheal_container \n```\n\n**创建守护式容器**\n\n```bash\n$ sudo docker run --name daemon_dave -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n```\n\n> 上面的docker run 使用了`-d`参数，因此Docker会将容器放到后台运行。\n\n**Docker日志**\n\n```bash\n# 获取守护式容器的日志\n$ sudo docker logs daemon_dave\n\n# 跟踪守护式容器的日志\n$ sudo docker logs -f daemon_dave\n\n# 获取日志的最后10行\n$ sudo docker logs --tail 10 daemon_dave \n\n# 跟踪某个容器的最新日志\n$ sudo docker logs --tail 0 -f daemon_dave\n\n# -t 标志为每条日志项加上时间戳\n$ sudo docker logs -ft daemon_dave \n```\n\n**Docker日志驱动**\n\n```bash\n$ sudo docker run --log-driver=\"syslog\" --name daemon_dave -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n```\n\n> 使用syslog将会禁用docker logs命令，并且将所有容器的日志输出都重定向到Syslog。\n\n**查看容器内的进程**\n\n```bash\n$ sudo docker top daemon_dave\n```\n\n**Docker统计信息**\n\n```bash\n$ sudo docker stats daemon_dave daemon_kate daemon_clear daemon_sarah\n```\n\n> 以上命令可以看到一个守护容器的列表，以及他们的CPU、内存、网络I/O以及存储I/O的性能和指标。这对快速监控一台主机上的一组容器非常有用。\n\n**在容器内部运行进程**\n\n```bash\n$ sudo docker exec -d daemon_dave touch /etc/new_config_file\n```\n\n> `-d`表示需要运行一个后台进程\n\n```bash\n# 在容器内运行交互命令\n$ sudo docker exec -t -i daemon_dave /bin/bash \n```\n\n**自动重启容器**\n\n```bash\n$ sudo docker run --restart=always --name daemon_dave -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n```\n\n> `--restart`标志被设置为always。无论容器的退出代码是什么，Docker都会自动重启改容器。除了always，还可以将这个标志设为`on-failure`，这样，只有当容器的退出代码为非0值的时候，才会自动重启。另外，on-failure还接受一个可选的重启次数参数，`--restart=on-failure:5`,Docker会尝试自动重启改容器，最多重启5次。\n\n**深入容器**\n\n```bash\n$ sudo docker inspect daemon_dave\n```\n\n> docker inspect命令会对容器进行详细的检查，然后返回其配置信息，包括名称、命令、网络配置以及很多有用的数据。可以使用`-f`或者`--format`标志来选定查看结果。\n\n```bash\n$ sudo docker inspect --format='{.State.Running}' daemon_dave\n```\n\n> 查看多个容器\n\n```bash\n$ sudo docker inspect --format '{.Name} {.State.Running}' daemon_dave Micheal_container\n```\n\n**删除容器**\n​    \n\n```bash\n$ sudo docker rm daemon_dave\n\n# 删除所有容器\n$ sudo docker rm `sudo docker ps -a -q`\n```\n\n**列出所有镜像**\n\n```bash\n$ sudo docker images\n```\n\n**拉去镜像**\n\n```bash\n$ sudo docker pull ubuntu:16.04\n```\n\n**运行一个带标签的Docker镜像**\n\n```bash\n$ sudo docker run -i -t --name new_container ubuntu:16.04 /bin/bash\n```\n\n**查找镜像**\n\n```bash\n$ sudo docker search puppet\n```\n\n**构建镜像**\n\n- 使用`docker commit`命令\n- 使用`docker build`命令和`Dockerfile`文件\n\n**用Docker的commit命令创建镜像**\n\n```bash\n$ sudo docker run -i -t ubuntu /bin/bash\n\n# 接下来安装需要安装的工具，安装完成后exit退出容器, eg：\n$ apt-get -yqq update\n$ apt-get -y install apache2\n\n# 指定提交修改过的容器的ID（可以通过docker ps -l -q命令得到刚创建的容器的ID）\n$ sudo docker commit 4aab3cecb76 micheal/apache2  \n\n# 检查新创建的镜像\nsudo docker images micheal/apache2  \n\n# 提交另一个新定制容器\n# -m 选项用来指定新创建的镜像的提交信息，-a 用来列出该镜像的作者信息。\n$ sudo docker commit -m\"A new custom image\" -a\"Micheal\" 4aab3cecb76 micheal/apache2:webserver  \n```\n\n**用Dockerfile构建镜像**\n\nDockerfile文件示例：\n\n```shell\n# Vsersion: 0.0.1\nFROM ubuntu:16.04\nMAINTAINER Micheal \"miaopei@baicells.com\"\nRUN apt-get -yqq update && apt-get -y install nginx\nRUN echo 'Hi, I an in your container' > /usr/share/nginx/html/index.html\nEXPOSE 80\n```\n\n> Dockerfile中的指令会按照顺序从上到下执行，所以根据需要合理安排指令的顺序。\n>\n> 如果Dockerfile由于某些原因没有正常结束，那么用户得到了一个可以使用的镜像。这对调试非常有帮助：可以基于改镜像运行一个具备交互功能的容器，使用最后创建的镜像对为什么用户指令会失败进行调试。\n>\n> __每个Dockerfile的第一条指令必须是FROM__,FROM指令指定一个已经存在的镜像，后续指令都将基于该镜像进行，这个镜像被称为基础镜像。\n>\n> MAINTAINER指令告诉Docker镜像的作者是谁，以及作者的电子邮件。有助于标识镜像的所有者和联系方式。\n\n\n\n> 默认情况下，RUN指令会在shell里使用命令包装器`/bin/sh -c`来执行，如果是在一个不支持shell的平台上运行或者不希望在shell中运行（比如避免shell字符串篡改），也可以使用`exec`格式的RUN指令，如下所示：\n\n```shell\nRUN [ \"apt-get\", \" install\", \"-y\", \"nginx\" ]\n```\n\n> EXPOSE指令告诉Docker该容器内的应用程序将会使用该容器的指定端口。\n\n**基于Dockerfile构建新镜像**\n\n```bash\n$ sudo docker build -t=\"micheal/static_web\" .\n$ sudo docker build -t=\"micheal/static_web:v1\" .\n\n# 这里Docker假设在这个Git仓库的根目录下存在Dockerfile文件\n$ sudo docker build -t=\"micheal/static_web:v1\" git@github.com:micheal/docker_static_web  \n\n# 忽略Dockerfile的构建缓存\n$ sudo docker build --no-cache -t=\"micheal/static_web\" . \n```\n\n**查看镜像**\n\n```bash\n# 列出Docker镜像\n$ sudo docker images\n\n# 查看镜像每一层，以及创建这些层的Dockerfile指令\n$ sudo docker history micheal/static_web \n\n$ sudo docker run -d -p 80 --name statix_web micheal/static_web nginx -g \"daemon off;\"\n```\n\n> nginx -g \"daemon off;\",这将以前台的方式启动Nginx。\n>\n> `-p`标志用来控制Docker在运行时应该公开那些网络端口给外部（宿主机）。运行一个容器时，Docker可以通过两种方式来在宿主机上分配端口。\n>\n> - Docker可以在宿主机上随机选择一个位于32768 ~ 61000的一个比较大的端口号来映射到容器中的80端口上。\n> - 可以在Docker宿主机只指定一个具体的端口号来映射到容器中的80端口上。\n\n**查看Docker端口映射情况**\n\n```bash\n$ sudo docker ps -l\n\n# 返回宿主机中映射的端口\n$ sudo socker port static_web 80 \n\n# -p会将容器内的80端口绑定到宿主机的8080端口上\n$ sudo docker run -d -p 8080:80 --name statix_web micheal/static_web nginx -g \"daemon off;\" \n```\n\n**Dockerfile指令**\n\n1. CMD\n\n> CMD指令用于指定一个容器启动时要运行的命令。这有点儿类似于RUN指令，只是RUN指令是指定容器镜像被构建时要运行的命令，而CMD是指定容器被启动时要运行的命令。\n\n```bash\nCMD [\"/bin/bash/\", \"-l\"]\n```\n\n1. ENTRYPOINT\n\n> ENTRYPOINT和CMD指令非常类似，我们可在docker run命令行中覆盖CMD指令，而ENTRYPOINT指令提供的命令则不容易在启动容器的时候被覆盖。\n>\n> 可以组合使用ENTRYPOINT和CMD指令来完成一些巧妙的工作。\n\n```bash\nENTRYPOINT [\"/usr/sbin/nginx\"]\nCMD [\"-h\"]\n```\n\n1. WORKDIR\n\n> WORKDIR指令用来在从镜像创建一个新容器时，在容器内部设置一个工作目录，ENTRYPOINT和/或CMD指定的程序会在这个目录下执行。\n\n```bash\nWORKDIR /opt/webapp/db\nRUN bundle install\nWORKDIR /opt/webapp\nENTRYPOINT [\"rackup\"]\n```\n\n> 可以通过`-w`标志在运行时覆盖工作目录\n\n```bash\n$ sudo docker run -ti -w /var/log ubuntu pwd/var/log\n```\n\n1. ENV\n\n> ENV指令用来在镜像构建过程中设置环境变量。这些变量会持久保存到从我们镜像创建的任何容器中。\n\n```bash\nENV RVM_PATH /home/rvm\n```\n\n> 也可以使用docker run命令行的`-e`标志来传递环境变量。这些环境变量只会在运行时有效。\n\n```bash\n$ sudo docker run -ti -e \"WEB_PORT=8080\" ubuntu env\n```\n\n1. USER\n\n> USER指令用来指定该镜像会以什么样的用户身份来运行。我们可以指定用户名或者UID以及组或GID，甚至是两者的组合。\n\n```bash\nUSER user\nUSER user:group\nUSER uid\nUSER uid:gid\nUSER user:gid\nUSER uid:group\n```\n\n> 也可以在docker run命令行中通过`-u`标志覆盖该指令指定的值。\n\n1. VOLUME\n\n> VOLUME指令用来向基于镜像创建的容器添加卷。一个卷可以存在于一个或者多个容器内特定的目录，这个目录可以绕过联合文件系统，并提供如下共享数据或者对数据进行持久化的功能。\n>\n> - 卷可以在容器间共享和重用\n> - 一个容器可以不是必须和其他容器共享卷\n> - 对卷的修改是立即生效的\n> - 对卷的修改不会对更新镜像产生影响\n> - 卷会一直存在直到没有任何容器再使用它\n>\n> 卷功能让我们可以将数据（如源代码）、数据库或者其他内容添加到镜像中而不是将这些内容提交到镜像中，并且允许我们在多个容器间共享这些内容，我们可以利用此功能来测试容器和内部应用程序代码，管理日志，或者处理容器内部的数据库。\n\n```bash\nVOLUME [\"/opt/project\"]\n```\n\n> 这条指令将会基于此镜像的任何容器创建一个名为/opt/project的挂载点。\n>\n> 也可以通过指定数组的方式指定多个卷\n\n```bash\nVOLUME [\"/opt/project\", \"/data\"]\n```\n\n1. ADD\n\n> ADD指令用来将构建环境下的文件和目录复制到镜像中。不能对构建目录或者上下文之外的文件进行ADD操作。\n\n```bash\nADD software.lic /opt/application/software.lic\nADD latest.tar.gz /var/www/wordpress/   //这条指令会将归档文件解开到指定的目录下\n```\n\n1. COPY\n\n> COPY指令非常类似ADD，它们根本不同是COPY只关心构建上下文中复制本地文件，而不会去做文件提取（extraction）和解压（decompression）的工作。\n\n```bash\nCOPY conf.d/ /etc/apache2/\n```\n\n1. LABEL\n\n> LABEL指令用于为Docker镜像添加元数据。元数据以键值对的形式展现\n\n```bash\nLABEL version=\"1.0\"\nLABEL location=\"New York\" type=\"Data Center\" role=\"Web Server\"\n```\n\n> 可以使用docker inspect命令查看容器标签\n\n```bash\n$ sudo docker inspect micheal/apache2\n```\n\n1. STOPSIGNAL\n\n> STOPSIGNAL指令用来设置停止容器时发送什么系统调用信号给容器。\n\n1. ARG\n\n> ARG指令用来定义可以在docker build命令运行时传递给构建运行时的变量，我们只需要在构建时使用--build-arg标志即可。用户只能在构建时指定在Dockerfile文件汇总定义过的参数。\n\n```bash\nARG build\nARG webapp_user=user\n\n$ docker build --build-arg build=1234 -t micheal/webapp .\n```\n\n1. ONBUILD\n\n> ONBUILD指令能为镜像添加触发器（trigger）。当一个镜像被用做其他镜像的基础镜像时（比如用户的镜像需要从某未准备好的位置添加源代码，或者用户需要执行特定于构建镜像的环境的构建脚本），该镜像中的触发器将会被执行。\n>\n> 触发器会在构建过程中插入新指令，我们可以认为这些指令是紧跟在FROM之后指定的。触发器可以是任何构建指令。\n\n```bash\nONBUILD ADD . /app/src\nONBUILD RUN cd /app/src/ && make\n```\n\n> 上面的代码将会在创建的镜像中加入ONBUILD触发器，ONBUILD指令可以在镜像上运行docker inspect命令查看。\n\n**Docker Networking**\n\n> 容器之间的连接用网络创建，这被称为Docker Networking。Docker Networking允许用户创建自己的网络，容器可以通过这个网上互相通信。更重要的是，现在容器可以跨越不同的宿主机来通信，并且网络配置可以更灵活的定制。Docker Networking也和Docker Compose以及Swarm进行了集成。\n>\n> 要想使用Docker网络，需要先创建一个网络，然后在这个网络下启动容器。\n\n```bash\n$ sudo docker network create app\n```\n\n> 这里使用docker network命令创建了一个桥接网络，命名为app。可以使用docker network inspect命令查看新创建的这个网络。\n\n```bash\n$ sudo docker network inspect app\n```\n\n> 我们可以看到这个新网络是一个本地的桥接网络（这非常像docker0网络），而且现在没有容器再这个网络中运行。\n>\n> 可以使用`docker network ls`命令列出当前系统中所有的网络。\n\n```bash\n$ sudo docker network ls \n```\n\n> 也可以使用 `docker network rm`命令删除一个Docker网络。\n>\n> 在Docker网络中创建Redis容器\n\n```bash\n$ sudo docker run -d --net=app --name db micheal/redis\n```\n\n> `--net`标志指定了新容器将会在那个网络中运行。\n\n```bash\n$ sudo docker network inspect app\n```\n\n> 将已有容器连接到Docker网络\n\n```bash\n$ sudo docker network connect app db2\n```\n\n> 可以通过`docker network disconnect` 命令断开一个容器与指定网络的连接\n\n```bash\n$ sudo docker network disconnect app db2\n```\n\n**通过Docker链接连接容器**\n\n> 启动一个Redis容器\n\n```bash\n$ sudo docker run -d --name redis micheal/redis\n```\n\n> 注意：这里没有公开容器的任何端口。一会就能看到这么做的原因。\n\n\n\n> 链接Redis容器\n\n```bash\n$ sudo docker run -p 4567 --name webapp --link redis:db -t -i -v $PWD/webapp_redis:/opt/webapp micheal/sinatra /bin/bash\n```\n\n> 这个命令做了不少事情，我们逐一解释。首先，我们使用`-p`标志公开4567端口，这样就能从外面访问web应用程序。\n>\n> 我们还使用`--name`标志给容器命名为webapp，并且使用了`-v`标志把web应用程序目录作为卷挂载到了容器里。\n>\n> 然而，这次我们使用了一个新标志`--link`。`--link`标志创建了两个容器间的客户-服务链接。这个标志需要两个参数：一个是要链接的容器的名字，另一个是链接的别名。这个例子中我们创建了客户联系，webapp容器是客户，redis容器是“服务”，并且为这个服务增加了db作为别名。这个别名让我们可以一致地访问容器公开信息，而无须关注底层容器的名字。链接让服务容器有能力与客户容器通信，并且能分享一些连接细节，这些细节有助于在应用程序中配置并使用这个链接。\n\n\n\n> 连接也能得到一些安全上的好处。注意，启动 Redis 容器时，并没有使用`-p`标志公开Redis的端口。因为不需要这么做。通过把容器链接在一起，可以让客户直接访问任意服务容器的公开端口（即客户webapp容器可以连接到服务redis容器的6379端口）。更妙的是，只有使用`--link`标志链接到这个容器的容器才能连接到这个端口。容器的端口不需要对本地宿主机公开，现在我们已经拥有一个非常安全的模型。通过这个安全模型，就可以限制容器化应用程序被攻击面，减少应用暴露的网络。\n\n","tags":["Docker"],"categories":["Docker"]},{"title":"认知升级：提升理解层次的NLP思维框架","url":"/2010-01-06/认知升级/","content":"\n\n**一家零售店的困境**\n\n假设你是一个「某品牌运动鞋」的线下门店代理商，门店开在上海的闹市区有好几年了，你雇佣了几个伙计在经营着自己的小店面，你每周来店里一次了解经营情况，一直以来都比较稳定。\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-ba482f91bdc911db.jpg)\n\n可是最近，你发现生意越来越差，销售额一直在下滑，而且你还发现，某几款鞋子的「进货价」竟然比淘宝上的「零售价」还要高，很多客人来店里试了一圈鞋子，结果都跑去网上下单了。\n\n伙计们的士气也开始变的低落，客人进来了，都不太愿意主动去搭理，你刚要发火，一名员工却突然提出了辞职…\n\n你非常苦恼，这个地段的房租开始变得越来越贵；库存也因为滞销越积越多；甚至本来热闹的地段，现在逛街的人都开始变少了…\n\n店铺已经开始亏损，而你投入了大量的装修成本和库存，现在关门损失极大，你焦头烂额…\n\n请在这里停顿30秒，想象自己就是这个代理商，请问在这个时候，你会怎么办？\n\n… …\n\n** 有些人说可能会说：**\n\n“都是万恶的淘宝惹的祸，马云毁掉了实体经济！”\n\n“线下房租越来越高，卖一月的鞋还不够付房租，线下店谁做谁赔钱…”\n\n“现在的年轻人太不负责任了！生意有点波动，稍微有点压力，人就跑了…”\n\n** 还是你会这样思考？**\n\n“员工不积极，我就提高员工的销售提成呗，有钱能使鬼推磨，冰箱都能卖给爱斯基摩人，就不相信卖不出去几双鞋子，明天开始我亲自来盯店…”\n\n“我们上个月不是有几个企业定了一批鞋子吗？这个月我们多打点电话，联系更多的企业，做企业客户！”\n\n“我也可以开个淘宝店啊，把我的生意也搬到网上…”\n\n** 或者你选择另辟蹊径？**\n\n“时代变化太快，新时代一定有我不知道的新方法和技巧，我要去学习一下…像什么新零售啊，O2O，体验经济、短路经济、社群经济… 听说都是能解决目前这种困境的方法！”\n\n“其他同行他们是怎么解决的？有没有同行的资源，我去交流学习一下…”\n\n**为什么面对同一个困境，每个人的反应和解决方法会如此不同？**\n\n有些人抱怨环境，有些人变得勤奋，有些人却选择开始补习功课，寻找新的解决办法？\n\n**到底哪种方式才是正确的？**\n\n这里，我们就需要用到一个新概念「**NLP理解层次**」来解释这个现象：\n\n**#注释：**NLP（神经语言程序学）是由理查德·班德勒和约翰·格林德在1976年创办的一门学问，美国前总统克林顿、微软领袖比尔盖茨、大导演斯皮尔博格等许多世界名人都接受过 NLP培训,世界500强企业中的 60%采用NLP培训员工，理解层次是NLP中的一个核心概念。\n\n在这个世界上，每一件与我们有关系的事，我们都会赋予其一些意义。比如前面的例子，你可能会觉得造成这一切都是马云的错！\n\n由于每个人赋予的意义都会有所不同，因此我们的理解也会不一样，理解不一样，解决办法当然就会不同。\n\n**「NLP理解层次」说，对一件事情的理解，我们可以分成6个不同的层次，而这个层次是有高低之分的。**\n\n如果你用低维度的视角去看这个问题的时候，感觉它无法解决。但当你站在更高的一个维度去看它，也许就变成了一个很简单的问题，甚至连问题本身也消失了。\n\n就像马车的时代，大家都在寻找更快的马，但当汽车被发明出来后，这个问题就不存在了。\n\n**为了便于你理解，我们以每个人所处的不同「理解层次」，把人分成6种不同的类型。**理解层次越高的人，解决问题的能力也就越强，就越是我们社会需要的人才。\n\n接下来，我们就从这个线下门店的案例出发，看看这6类人，分别会如何思考，如何解决这个问题：你是第几流人才？\n\n**第5流人才**\n\n**别名：**怨妇\n\n**所处理解层次：**环境\n\n**典型思考模式：**都是外界环境的错！\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-072434becf9309e0.jpg)\n\n   **理解层次的最低层是「环境」**\n\n什么是环境？就是除你自己之外的一切都算是环境：你身边的人；你的领导、同事；你的公司；你的竞争对手；市场环境；天气；大众舆论……等等诸如此类。\n\n**处在这个理解层次的人，当问题发生的时候，他首先会把问题归结成「因为环境的不好」而产生的问题。**\n\n**比如：**\n\n工作不顺利，是因为领导是个蠢蛋…\n\n没有晋升机会，是因为公司的办公室政治严重，没有好的晋升机制…\n\n房子太贵买不起，都是因为那些黑心炒房团、政府调控无能、没有一个富爸爸…\n\n总之，发生了现在的这个困局，不是我的问题，是别人的问题，是公司的问题，是市场的问题，是政府的问题，是运气的问题，都是我命不好，生在了这样的一个时代，遇到了这样一群人….\n\n**而他寻找解决办法的路径，也会从改变环境的角度去思考。**\n\n**比如：**\n\n这家公司不好，导致我没有晋升机会，那我就换个公司呗…\n\n找了一个男朋友，他现在对我越来越差了，又是一个渣男，再换一个呗…\n\n不知道你身边有没有接触过这种人，只要一与他们接触，就会感受到这「满满的负能量」，感觉这人世间的不幸都被他们碰巧遇上了，命运多舛的不行，分分钟生活就无法继续了…\n\n我们通常称这种行为叫做「抱怨」，但你是不是也曾劝说过这些人不要抱怨？她们似乎好像也知道抱怨不好？但为什么他们还是在不断抱怨呢？\n\n那就是因为他们的理解层次处在了最低的「环境层」，他们对世界的理解被死死的困在了这个层次，并不是他们想抱怨，而是在他眼里，除了看到环境之外，再也无法看到其他的了。因此，他们能想到的最好办法，也就只能是换个更好的环境了。\n\n如果是第五流的人才遇到了案例中的困境，他似乎除了抱怨房租、淘宝、员工，他是真找不到还有什么原因能解释这个问题…\n\n**第4流人才**\n\n**别名：**行动派\n\n**所处理解层次：**行动\n\n**典型思考模式：**我还不够努力！\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-b7e6c86dd80cf62f.jpg)\n\n** 我们往上走一层，来到第二个层次「行为」**\n\n想要解决问题，那你就得开始行动啊！你不能改变环境，你能改变的只有你自己！你为什么还没有成功？就是因为你还不够努力！你不改变，环境如何改变？你不行动，环境如何改善？\n\n是不是听着很鸡汤？有点像成功学？\n\n处在这个理解层次的人，在外人看来是一位非常乐观，充满正能量的人，他们从不对环境妥协，他们相信上天不负有心人，只要我持续努力，事情一定会有转机！\n\n他们是人们眼中的「行动派」、「实干家」，是新时代的斜杠青年…\n\n**处在这个理解层次的人，当问题发生的时候，他首先会把问题归结成「因为我的努力还不够」而产生的问题。**\n\n**比如：**\n\n收入太低？因为我还不够努力…\n\n买不起房子？是因为我还不够努力….\n\n创业失败了？是因为我还不够努力….\n\n总之，发生了问题，先从自身找原因，看看是不是因为自己偷懒了？是不是努力程度还不够？是不是要加大工作量？\n\n如果你处在「行动」这个层次上，「环境」的问题就变得不是那么重要了，因为一切都是自己的原因，因为自己还不够努力！\n\n**要解决问题，你就会从「行为」这个层面去寻找解决办法，看看还有什么事情是可以去做，去改变的？**\n\n**比如：**\n\n都一年没涨工资了，今晚开始多加1个小时的班！\n\n女朋友为什么最近对我变得冷淡了？我要多发些消息，多打些电话去关心她！\n\n公司业绩变差了？一定是我睡觉睡的太多了，明天开始不睡觉！\n\n   **回到最初的线下门店的案例，如果是这第4流的人才遇到了这个困境，他会怎么办呢？**\n\n我付24小时的房租，只营业8小时！那怎么行？明天开始24小时营业，我全天待在店里亲自销售！员工两班倒，空闲时间拼命打电话找企业，我就不信了！\n\n员工偷懒？那我就加工资，加提成，每天请吃夜宵，只要你肯努力，有业绩，我就对你比亲儿子还好！\n\n但是，我们不禁要问，是不是努力了，所有问题就都能被解决了呢？\n\n   **越努力的人，获得的成就也就会越大？**\n\n200年前，人们的平均工作时间是16个小时；5000年前，人们也是每天日出而作，日落才息…\n\n他们也许比你更加勤奋，可产生的价值却不足现代社会的万分之一，这是为什么？\n\n**努力，的确是成功的一个必要条件，但远远不是充分条件。**\n\n为什么那么多人不喜欢鸡汤？反对成功学？就是因为它们只告诉了你要努力，却没有给你方法，它们只是帮助你脱离了最低的「环境」层，来到了第二低的「行为」层！以为给你打一针鸡血，你就开始奋斗了，就一定能成功了！\n\n问题的解决，时代的进步，并不是只靠「努力」就能完成的，一定有更重要的因素在背后推动，我们需要进入下一个理解层次…\n\n**第3流人才**\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-a86ad172876e832e.jpg)\n\n**别名：**战术家\n\n**所处理解层次：**能力\n\n**典型思考模式：**方法总比问题多！\n\n农业时代的人，比你更努力，但生产力不足现在的万分之一，这是为什么？\n\n因为现在的人更勤奋吗？\n\n当然不是，是因为他们没有经历过工业革命、信息革命，他们不会使用机器，也不会使用互联网来提高工作效率、协作效率。\n\n机器和互联网是什么？看似是工具，本质是扩展了你的能力。\n\n5000年前，你想要告诉一个人一件事情，你得策马奔腾三天三夜，而现在通过互联网不需要1秒钟，互联网扩展了你的沟通能力。\n\n**什么是能力？就是你能用更简单、更高效的方式解决同样的问题，有选择便是有能力。**\n\n   **理解层次处在「能力」层次的人，当问题发生的时候，首先会把问题归结成是「因为我的能力不足」而产生的。**\n\n所以，他们也会在「能力」这个层次里去寻找更好的「方法」来解决问题。\n\n**比如：**\n\n线下门店生意不好，是因为我的经营模式太陈旧，我需要学习新的方法…比如，可以通过社群经济的方式来降低我的获客成本，提高客户复购率….\n\n和男朋友关系处理不好，一定是我的沟通能力有问题，我要去学习能改善亲密关系的沟通技巧，比如《关键对话》、《幸福的婚姻》….\n\n以前我是做业务的，现在刚成为部门经理，团队业绩下滑，一定是我的管理能力有问题，我以前根本没有系统的学习过管理的方法，我得去报个MBA，从「古狄逊定理」开始学起…\n\n**这类人有非常强大的学习能力和应用能力，能把学习到的知识，转化为可操作的方法，进而改善效率，解决问题。**\n\n他们明白，任何问题都不是孤立存在的，一定有人曾经遇到过，并且已经有更好的解决办法了，只是我还不知道；我不应该在黑暗中独自前行，去重新发明轮子，也许我的顿悟，只是别人的基本功！我应该要站在巨人的肩膀上，学习更成熟的经验和方法，然后再来解决这个问题。\n\n如果你能走到这个层次，既有「行为层」的勤奋努力，又有「能力层」的方法套路，一般就能成为公司里的中高层了。普通的问题已经难不倒你了，你总能找到办法来解决它们。\n\n当然，这里说的每提高一个层次，并不是说就不要下一个层次了，比如有了方法就不需要努力了，而是在原来的基础上，上升了一个思考层次。不然就会变成空中楼阁，纸上谈兵。\n\n这一点很重要，切记！\n\n**「能力」这个理解层次，是我们「意识」能想到的最高层次了。**\n\n再往上走，就要进入到我们的「潜意识」区域，内容会变得比较模糊，之前你可能很少接触到这些层面，所以可能会比较难理解。\n\n看不懂的地方你可以多读几遍，多思考一下，毕竟看完不是目的，真的理解了，能改善自己，能获得更好的人生才是目的，才是这篇文章真正能带给你的价值。\n\n那什么问题，是你有「能力」也解决不了的呢？\n\n就是你选择错了问题！\n\n什么意思？\n\n**你在着手解决问题之前，你得先清楚，你要解决的问题是什么？**\n\n比如开始的案例，导致现状的原因看上去有很多，哪个才是最重要的问题？\n\n是团队管理的问题？营销方式的问题？还是商业模式的问题？\n\n是应该打折清库存减少损失，准备关门？还是战略转型，坚持到底？\n\n**每一个选择，都意味着人生的不同走向，一旦选择错了问题，你那优秀的「能力」和「行动力」只会让你越走越远。**\n\n那如何提高做选择题的能力呢？我们就需要进入到下一个层次…\n\n**第2流人才**\n\n**别名：**战略家\n\n**所处理解层次：**BVR（信念/价值观/规条）\n\n**典型思考模式：**什么才是更重要的？\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-34d03cff8a58bd1e.jpg)\n\n**如果说「能力层」是做解答题的能力，「BVR层」就是做选择题的能力**，什么可以做，什么不可以做，什么更重要，什么可以忽略不管？\n\n** 什么是BVR？**\n\n**B（Believe）：信念**，你相信什么是对的？\n\n你相信这个世界应该是怎么样的？从大了说可以是世界观，从小了说就是一个个概念。\n\n为什么我们的专栏第一季度全是围绕概念来讲的？就是在帮你构筑一个更完整的世界观，这是你的硬件，是你所有能力能够得以发挥的基石。\n\n**V（Value）：价值观**，你认为A和B哪个更重要？\n\n人生的不同是因为一次次选择的不同，那我们依靠什么来做选择的呢？就是我们的价值观。\n\n我们内心对每一个人、每一件事、每一个概念都会有一定的价值衡量。东西不同，价值就会有高低，每个人衡量的标准也不一样。最终我们会形成自己的价值排序，这就是你的价值观。\n\n因此，当出现A/B选择的时候，选择我们认为更有价值的一项。\n\n比如，你遭遇抢劫，别人问你「要钱还是要命？」通常你会选择要命，因为你的价值观是：命>钱\n\n但是当你现在有1个小时空闲时间，你打算用来看书？还是刷朋友圈？还是睡觉？每个人的选择就不一样了。因为每个人对这3者的价值衡量是不同的。\n\n为什么有些人有选择困难症？\n\n那是因为他内在的价值观是混乱的，缺少某些概念，或者对某些概念的理解不清楚，没有价值衡量的标准，因此他也就无法知道哪个更有价值，他就不知道该如何选择了。\n\n**R（Rule）：规条**，做人做事的原则。\n\n这就像是公司的规章制度，每个人也有自己的规则，这些规则是怎么来的？就是来自于信念和价值观。\n\n比如我的一条行为准则是「做时间的朋友」，这个规则是怎么来的？\n\n就是来自于我的一个信念，因为我相信「复利效应」，我只做有积累的事，能彼此增益的事，然后耐心等待复利的出现。\n\n比如很多人都有的一条行为准则「我答应你的一定会做到」，背后其实是源于两条价值观：\n\n「说道做到=诚信」而「诚信>一切」\n\n规条存在的意义，就是帮助你更高效的做出选择，不用每次都思考、讨论、互相权衡比较…\n\n**因此，「能力」层是让你把事情做对，而「BVR」层则是帮你选择做对的事情。**\n\n处在「BVR层」的人，当问题发生的时候，首先会先思考「哪个是最重要的问题？」、「除了我看到的这些问题，还有什么更重要的问题是我没有看到的？」\n\n   **回到开头的案例，第二流的人才可能会这样思考：**\n\n门店的出现业绩下滑，可能有以下多方面的因素对其造成影响：\n\n**1\\. 成本：**房租越来越贵；库存积压；已投入的装修成本；进货成本高于淘宝售价；\n\n**2\\. 团队：**员工士气低落，一个人提出辞职；\n\n**3\\. 市场：**门店人流越来越少，客户们现在习惯在网上购物，网上购物更便宜；\n\n**4\\. 营销：**目前营销的方式比较单一，就是门店等客户。\n\n**5\\. 渠道：**目前只有线下门店这单一渠道。\n\n没有看到的问题可能是什么？\n\n**1\\. 互联网时代的交易结构已经发生变化：**淘宝之所以能那么便宜，因为短路掉了中间环节，工厂直接到消费者，不需要再经过总代、省代、区代…价值传递效率大大提升，所以价格才能如此便宜。\n\n**2\\. 线下门店也有独特的优势：**我们的产品摸得到，能试穿，用户的体验感非常好，可信度高；\n\n那其中到底哪个是最关键的问题呢？经过一番思考，你画出了如下关系图：\n\n原来，一切的罪魁祸首，是因为互联网的连接效率变高，导致交易结构发生了变化，淘宝店家「短路」掉了中间总代、省代、区代等环节，直接面对消费者，所以价格才能那么低。由此导致了后面的一连串反应…\n\n好在我还有一个杀手锏，实体店的「体验感」你无法获得！\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-9046248968419768.jpg)\n\n**因此，你制定了2条核心战略：**\n\n**1\\. 短路经济：**既然淘宝店家能短路中间环节，我实体店为什么不可以？尽一切可能短路掉中间环节，把售价降下来！\n\n**2\\. 体验经济：**提高线下门店体验感，让用户来了就不想走，让用户在逛的时候，体验到乐趣与快感，而不仅仅是来这里购物！\n\n**只要这2个问题得以解决，其他问题都会迎刃而解。**（现实生活中已有成功案例，这里篇幅有限就不再赘述具体方法，大家可以上网自己查找）\n\n这就是理解层次在「BVR」的人给出的一种解决方案。\n\n而处在「能力」层的人很可能就会胡子眉毛一把抓，遇到问题解决问题，像摊大饼一样，面多了加水，水多了加面。看似有无穷的方法来应对，但问题却也变得越来越多，永远也解决不完。\n\n** BVR层的缺陷**\n\n细心的同学看到这里，可能会心生疑问，能做出这样的选择，是因为几个选项都有客观的价值标准，可有时候2个选择看似都是对，或者带有比较大的主观性，怎么办？\n\n比如你还是那个经销商，你是否会考虑这样一个问题：「我一定要当老板吗？还是回去打工？以我的能力至少也能百万年薪，还没有风险，何必那么辛苦？」\n\n你的答案会是什么呢？\n\n要解决这个主观选择的问题，就需要再往上走一个层次。\n\n**第1流人才**\n\n**别名：**觉醒者\n\n**所处理解层次：**身份\n\n**典型思考模式：**因为我是XXX，所以我会XXX\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-1d529a113ff27580.jpg)\n\n理解层次的第五层「身份」这是一个很高的层次，一般人很少能清楚的回答以下这个问题：\n\n**“你是谁？你想成为一个怎么样的人？”**\n\n为什么身份层次那么高，因为不同的身份层次，就意味着拥有着不同的BVR，它决定了你当下的每次选择，决定了你未来的人生方向。\n\n比如，你是想成为一名运动鞋设计师？还是想成为一名成功的商人，赚很多钱？或者是想开创一个新的运动品牌，成为新品牌的创始人？\n\n**你把自己定义成不同的身份，案例中那个问题的选择就会完全不同。**\n\n比如你想成为一名运动鞋设计师，你可能就会选择去打工，而把门店关掉；而如果你想成为一位新品牌的创始人，你可能就会选择去找代工厂，直接生产自己品牌的鞋子，短路掉所有中间环节，全渠道销售；\n\n**你之所以有时候会不知道该如何选择，除了对某些概念不清楚之外，最重要的就是你不知道自己想成为怎么样的一个人。**\n\n如果你不知道你想成为谁，你就不知道自己要什么，你不知道自己要什么，你就无法做出选择，你无法做出选择，你就什么也得不到。\n\n通常身份定义清楚了，答案也就出来了。\n\n   **说到这里，你可能会想到上节课我们讲过的「角色」话题，「角色」和「身份」有什么不同？**\n\n**角色是被动的，是别人给你的；身份是主动的，是你自己想成为的。**\n\n你可能有很多角色，但是你只有一个自己想成为的身份。每个角色或者身份，都对应着一套帮助他「能够更好的成为这个身份」的BVR体系。\n\n由于「角色」是被动获得的，所以你会觉得这套BVR是一种「束缚」；而「身份」是你主动想成为的，因此它的这套BVR会成为一种助力。\n\n「身份」这个层次，其实是对应着上节课里讲的「存在感知层」，你希望自己是一个怎么样的存在？\n\n上节课之所以想让你「去角色化」，就是想让你突破角色的束缚，获得一个更「主动」的人生，找到自己的「身份」层次。因为你身上的「角色」太多，会阻碍你看见自己真实的「身份」。\n\n**当你想清楚自己的「身份」定位后，就应该围绕它配套相应的BVR，再构建你的能力圈，并做出相应的计划与行动，你就会成为第一流的人才！**\n\n你能开创出一番自己的事业，设计出令人尖叫的产品，成为上市公司的领军人物。\n\n而在他们之上，还存在一类人，他们在人类历史的长河中都屈指可数，他们创造着奇迹，他们改变着世界，他们引领着时代，他们可以为了理想，放弃自己的生命…\n\n让我们再往上走一层，来观摩一下最顶级的人才是怎么样的…\n\n**顶级人才**\n\n**别名：**领袖/伟人\n\n**所处理解层次：**精神/使命\n\n**典型思考模式：**人活着就是为了改变世界！\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-89a9974fdd53680d.jpg)\n\n   **理解层次的最高层次「精神」**\n\n精神是什么意思？就是你与世界的关系。也就是我们经常听到的「人生使命」，你来到这个世界是为了什么？你能为别人，为社会，为整个人类带来什么？这个世界会因为你而有什么不同？\n\n**在这个层次，所有的思考，都围绕着2个字「利他」**，我如何选择能够让更多的人获益？如何才能够推动时代的进步？如果能达成这些，我愿意用我的所有来交换，包括我的生命…\n\n当然，这里还是要重申一下，**理解层次的逐级上升，不能脱离低层次而单独存在高层次，不然就是空中楼阁**，变得不切实际，这里的「精神」就会变成一种「情怀」了。\n\n「精神」层次一定要有「身份」层次的支撑，换句话说，如果你在身份层次，想不清楚自己要成为谁，可以试着来到「精神」层次，想想你能为这个世界做些什么？可以不用那么大，哪怕只是在某一方面，能帮助到为数不多的人，那一方面是什么？\n\n也许，这个就能成为你的人生使命，然后再去思考，什么样的身份能够更好的帮你完成这个使命？\n\n你就能想清楚身份层次的问题了。\n\n一旦踏入「精神」这个层次，我已经不知道能用什么语言来描述这类伟大的人物，唯有崇拜与敬仰，他们的名字就如同人类上空的繁星点点，照耀着人类的前行。请允许我借用其中的一位时代领袖「乔布斯」在1997年发布的一则苹果广告语来送给他们：\n\n向那些疯狂的家伙们致敬，他们特立独行，他们桀骜不驯，他们惹是生非，他们格格不入，他们用与众不同的眼光看待事物，他们不喜欢墨守成规，他们也不愿安于现状。\n\n你可以赞美他们，引用他们，反对他们，质疑他们，颂扬或是诋毁他们，但唯独不能漠视他们。\n\n因为他们改变了事物。他们发明，他们想象，他们治愈，他们探索，他们创造，他们启迪，他们推动人类向前发展。也许，他们必需要疯狂。\n\n你能盯着白纸，就看到美妙的画作么？你能静静坐着，就听见美妙的歌曲么？你能凝视火星，就想到神奇的太空轮么？\n\n我们为这些家伙制造良机。或许他们是别人眼里的疯子，但他们却是我们眼中的天才。因为只有那些疯狂到以为自己能够改变世界的人，才能真正地改变世界。\n\n希望未来的某一天，你也有机会成为改变世界的人，登上这片神圣的星空，引领着我们前进。\n\n   **回到最初的那个案例，如果是一个已经处在「精神」层次的人，遇到这样的情况会如何思考呢？**\n\n我也不知道，就把这个问题留给这个时代的伟人吧…如何才能成为顶级人才？\n\n以上对人才的分类只是为了让你更容易理解「理解层次」这个概念而做的极端化划分，现实情况中每个人其实6个层次都会涉及，只是会主要集中在某些层次中思考，而忽略其他层次，甚至根本不知道某些层次的存在。\n\n   **那我们应该如何从低层次，不断晋升到一流人才，甚至是顶级人才呢？**\n\n一级级往上打怪升级吗？\n\n不是！\n\n**当你处在低层次的时候，你的思维会被限制住，无法看到更多的可能性**，就像处在「环境」层的人，经常抱怨而不自知，完全看不到上方还有「行为层」可以帮助自己改变现状；更看不到「能力层」里还有其他办法可以解决眼下的问题……\n\n最可悲的人生，莫过于不知道自己不知道，还以为自己全都知道…\n\n那应该怎么办呢？\n\n答：**直接让自己成为一流人才或者顶级人才！**\n\n对，你需要对你的人生做顶层设计，从精神层开始往下规划：\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-69f537ec6e133d27.jpg)\n\n从自己理想的「精神层次 / 身份层次」发展出来的人生规划，**可能会与你的现实生活有很大的不同，将更具挑战性，但却能让你身心统一，因而能激发出你更强大的潜能！**\n\n![image](https://upload-images.jianshu.io/upload_images/18390058-fdf3cec91ce337b6.jpg)\n\n愿你从今天开始，**重新定义**自己的人生！\n\n这个过程可能会很困难，不会一蹴而就，也许需要你花费1天、1个月甚至1年的时间才能想清楚，然而你千万别放弃，努力去探寻，因为一旦想清楚，你的人生可能就会发生质的变化，我自己用了3年，希望你能比我更快！\n","tags":["认知升级","提升个人思维"],"categories":["认知升级"]}]